# Python 2
# Python 3
# Pathspec can either be run_id/step_name or run_id/step_name/task_id.
# get all tasks
# Pathspec can either be run_id/step_name or run_id/step_name/task_id.
# get all tasks
# TODO - move step and init under a separate 'internal' subcommand
# init is a separate command instead of an option in 'step'
# since we need to capture user-specified parameters with
# @add_custom_parameters. Adding custom parameters to 'step'
# is not desirable due to the possibility of name clashes between
# user-specified parameters and our internal options. Note that
# user-specified parameters are often defined as environment
# variables.
# There's a --with option both at the top-level and for the run
# subcommand. Why?
#
# "run --with shoes" looks so much better than "--with shoes run".
# This is a very common use case of --with.
#
# A downside is that we need to have the following decorators handling
# in two places in this module and we need to make sure that
# _init_decorators doesn't get called twice.
#obj.environment.init_environment(obj.logger)
# Package working directory only once per run.
# We explicitly avoid doing this in `start` since it is invoked for every
# step in the run.
# TODO(crk): Capture time taken to package and log to keystone.
# run/resume are special cases because they can add more decorators with --with,
# so they have to take care of themselves.
#TODO (savin): Enable lazy instantiation of package
# TODO set linter settings
# Ignore warning(s) and prevent spamming the end-user.
# TODO: This serves as a short term workaround for RuntimeWarning(s) thrown
# in py3.8 related to log buffering (bufsize=1).
# instantiate the Current singleton. This will be populated
# by task.MetaflowTask before a task is executed.
# Set
#
# - METAFLOW_DEBUG_SUBCOMMAND=1
#   to see command lines used to launch subcommands (especially 'step')
# - METAFLOW_DEBUG_SIDECAR=1
#   to see command lines used to launch sidecars
# - METAFLOW_DEBUG_S3CLIENT=1
#   to see command lines used by the S3 client. Note that this environment
#   variable also disables automatic cleaning of subdirectories, which can
#   fill up disk space quickly
# use debug.$type_exec(args) to log command line for subprocesses
# of type $type
# use the debug.$type flag to check if logging is enabled for $type
# No keyword arguments specified for the decorator, e.g. @foobar.
# The first argument is the class to be decorated.
# flow decorators add attributes in the class dictionary,
# _flow_decorators.
# Keyword arguments specified, e.g. @foobar(a=1, b=2).
# Return a decorator function that will get the actual
# function to be decorated as the first argument.
# No keyword arguments specified for the decorator, e.g. @foobar.
# The first argument is the function to be decorated.
# Only the first decorator applies
# Keyword arguments specified, e.g. @foobar(a=1, b=2).
# Return a decorator function that will get the actual
# function to be decorated as the first argument.
# Attach the decorator to all steps that don't have this decorator
# already. This means that statically defined decorators are always
# preferred over runtime decorators.
#
# Note that each step gets its own instance of the decorator class,
# so decorator can maintain step-specific state.
# python 3
# python 2
# Q: Why not use StepDecorators directly as decorators?
# A: Getting an object behave as a decorator that can work
#    both with and without arguments is surprisingly hard.
#    It is easier to make plain function decorators work in
#    the dual mode - see _base_step_decorator above.
# add flow-level decorators
# note that this dict goes into the code package
# so variables here should be relatively stable (no
# timestamps) so the hash won't change all the time
# type: (str) -> None
# worker processes that exit with this exit code are not retried
# worker processes that exit with this code should be retried (if retry counts left)
# Base Exception defines its own __reduce__ and __setstate__
# which don't work nicely with derived exceptions. We override
# the magic methods related to pickle to get desired behavior.
# For Python 3 compatibility
# NOTE this assume that InvalidNextException is only raised
# at the top level of next()
# Attributes that are not saved in the datastore when checkpointing.
# Name starting with '__', methods, functions and Parameters do not need
# to be listed.
# we import cli here to make sure custom parameters in
# args.py get fully evaluated before cli.py is imported.
# load the attribute from the datastore...
# ...and cache it in the object for faster access
# NOTE this is obviously an O(n) operation which also requires
# downloading the whole input data object in order to find the
# right split. One can override this method with a more efficient
# input data handler if this is a problem.
# this is where AttributeError happens:
# [ foreach x ]
#   [ foreach y ]
#     [ inner ]
#   [ join y ] <- call self.foreach_stack here,
#                 self.x is not available
# __getitem__ not supported, fall back to an iterator
# available_vars is the list of variables from inp that should be considered
# We have a conflict here
# We have unresolved conflicts so we do not set anything and error out
# If things are resolved, we go and fetch from the datastore and set here
# check: next() is called only once
# check: all destinations are methods of this object
# check: foreach and condition are mutually exclusive
# check: foreach is valid
# check: condition is valid
# check: non-keyword transitions are valid
# these attributes are populated by _parse
# these attributes are populated by _traverse_graph
# these attributes are populated by _postprocess
# end doesn't need a transition
# TYPE: end
# ensure that the tail an expression
# determine the type of self.next transition
# TYPE: foreach
# TYPE: split-or
# TYPE: split-and
# TYPE: linear
# any node who has a foreach as any of its split parents
# has is_inside_foreach=True *unless* all of those foreaches
# are joined by the node
# ignore joins without splits
# graph may contain loops - ignore them
# graph may contain unknown transitions - ignore them
# fix the order of in_funcs
# If we get an error here, since we know that the file exists already,
# it means that read failed which happens with Python 2.7 for large files
# check that incoming steps come from the same lineage
# (no cross joins)
# This is for python2 compatibility.
# Python3 has os.makedirs(exist_ok=True).
# metaflow URL
# metaflow chat
# metaflow help email
# print a short list of next steps.
# Get the local data store path
# Throw an exception
#') if paragraph]
# Skip hidden files (like .gitignore)
# Validate that the list is valid.
# Create destination `metaflow-tutorials` dir.
# Pull specified episodes.
# Check if episode has already been pulled before.
# TODO: Is the following redudant?
# Copy from (local) metaflow package dir to current.
# NOTE: This code needs to be in sync with metaflow/metaflow_config.py.
# Absence of default config is equivalent to running locally.
# TODO: Should we persist empty env_dict or notify user differently?
# Export its contents to a new file.
# resolve_path doesn't expand `~` in `path`.
# Write to file.
# Import configuration.
# Persist configuration.
# Prompt for user input.
# Decode the bytes to env_dict.
# TODO: Add the URL for contact us page in the error?
# Persist to a file.
# Datastore configuration.
# AWS Batch configuration (only if Amazon S3 is being used).
# Metadata service configuration.
# Disable multithreading security on MacOS
# Read configuration from $METAFLOW_HOME/config_<profile>.json.
# Initialize defaults required to setup environment variables.
###
# Default configuration
###
###
# Datastore configuration
###
# Path to the local directory to store artifacts for 'local' datastore.
# S3 bucket and prefix to store artifacts for 's3' datastore.
# S3 datatools root location
###
# Datastore local cache
###
# Path to the client cache
# Maximum size (in bytes) of the cache
###
# Metadata configuration
###
###
# AWS Batch configuration
###
# IAM role for AWS Batch container with S3 access
# Job queue for AWS Batch
# Default container image for AWS Batch
# Default container registry for AWS Batch
# Metadata service URL for AWS Batch
###
# Conda configuration
###
# Conda package root location on S3
###
# Debug configuration
###
###
# AWS Sandbox configuration
###
# Boolean flag for metaflow AWS sandbox access
# Metaflow AWS sandbox auth endpoint
# Metaflow AWS sandbox API auth key
# Internal Metadata URL
# AWS region
# Finalize configuration
# MAX_ATTEMPTS is the maximum number of attempts, including the first
# task, retries, and the final fallback task and its retries.
#
# Datastore needs to check all attempt files to find the latest one, so
# increasing this limit has real performance implications for all tasks.
# Decreasing this limit is very unsafe, as it can lead to wrong results
# being read from old tasks.
# the naughty, naughty driver.py imported by lib2to3 produces
# spam messages to the root logger. This is what is required
# to silence it:
# PINNED_CONDA_LIBS are the libraries that metaflow depends on for execution
# and are needed within a conda environment
# authenticate using STS
#!/usr/bin/env python
# This file is imported from https://github.com/aebrahim/python-git-version
# first see if git is in the path
# if this command succeeded, git is in the path
# catch the exception thrown if git was not found
# There are several locations git.exe may be hiding
# look in program files for msysgit
# look for the github version of git
# git was not found
# first, make sure we are actually in a Metaflow repo,
# not some other repo
# currently at a tag
# formatted as version-N-githash
# want to convert to version.postN-githash
# does not allow git hash afterwards
# not a git repository
# type: (str) -> None
# Python 2
# Python 3
# This module reimplements select functions from the standard
# Python multiprocessing module.
#
# Three reasons why:
#
# 1) Multiprocessing has open bugs, e.g. https://bugs.python.org/issue29759
# 2) Work around limits, like the 32MB object limit in Queue, without
#    introducing an external dependency like joblib.
# 3) Supports closures and lambdas in contrast to multiprocessing.
# make sure stdout and stderr are flushed before forking. Otherwise
# we may print multiple copies of the same output
# we must not let any exceptions escape this function
# which might trigger unintended side-effects
# we can't use sys.exit(0) here since it raises SystemExit
# that may have unintended side-effects (e.g. triggering
# finally blocks).
# python2
# python3
# handle files/folder with non ascii chars
# path = path[2:] # strip the ./ prefix
# if path and (path[0] == '.' or './' in path):
#    continue
# We want the following contents in the tarball
# Metaflow package itself
# the package folders for environment
# the user's working directory
# a modification time change should not change the hash of
# the package. Only content modifications will.
# Python2
# Python3
# ParameterContext allows deploy-time functions modify their
# behavior based on the context. We can add fields here without
# breaking backwards compatibility but don't remove any fields!
# currently we execute only one flow per process, so we can treat
# Parameters globally. If this was to change, it should/might be
# possible to move these globals in a FlowSpec (instance) specific
# closure.
# it is easy to introduce a deploy-time function that that accidentally
# returns a value whose type is not compatible with what is defined
# in Parameter. Let's catch those mistakes early here, instead of
# showing a cryptic stack trace later.
# note: this doesn't work with long in Python2 or types defined as
# click types, e.g. click.INT
# this is called by cli.main
# TODO: check that the type is one of the supported types
# make sure the user is not trying to pass a function in one of the
# fields that don't support function-values yet
# default can be defined as a function
# external_artfiact can be a function (returning a list), a list of
# strings, or a string (which gets converted to a list)
# note that separator doesn't work with DeployTimeFields unless you
# specify type=str
# this is needed to appease Pylint for JSONType'd parameters,
# which may do self.param['foobar']
# Impose length constraints on parameter names as some backend systems
# impose limits on environment variables (which are used to implement
# parameters)
# Account for the parameter values to unicode strings or integer
# values. And the name to be a unicode string.
# Ignore headers
# Ignore complaints about decorators missing in the metaflow module.
# Automatic generation of decorators confuses Pylint.
# Ignore complaints related to dynamic and JSON-types parameters
# Ditto for IncludeFile
# python2
# python3
#ms
# The following is a list of the (data) artifacts used by the runtime while
# executing a flow. These are prefetched during the resume operation by
# leveraging the MetaflowDatastoreSet.
# TODO option: output dot graph periodically about execution
# resume logic
# 0. If clone_run_id is specified, attempt to clone all the
# successful tasks from the flow with `clone_run_id`. And run the
# unsuccessful or not-run steps in the regular fashion.
# 1. With _find_origin_task, for every task in the current run, we
# find the equivalent task in `clone_run_id` using
# pathspec_index=run/step:[index] and verify if this task can be
# cloned.
# 2. If yes, we fire off a clone-only task which copies the
# metadata from the `clone_origin` to pathspec=run/step/task to
# mimmick that the resumed run looks like an actual run.
# 3. All steps that couldn't be cloned (either unsuccessful or not
# run) are run as regular tasks.
# Lastly, to improve the performance of the cloning process, we
# leverage the MetaflowDatastoreSet abstraction to prefetch the
# entire DAG of `clone_run_id` and relevant data artifacts
# (see PREFETCH_DATA_ARTIFACTS) so that the entire runtime can
# access the relevant data from cache (instead of going to the datastore
# after the first prefetch).
# fd -> subprocess mapping
# main scheduling loop
# 1. are any of the current workers finished?
# 2. push new tasks triggered by the finished tasks to the queue
# 3. if there are available worker slots, pop and start tasks
#    from the queue.
# TODO
# TODO
# on finish clean tasks
# assert that end was executed and it was successful
# If we are here, all children have received a signal and are shutting down.
# We want to give them an opportunity to do so and then kill
# While not all workers are dead and we have waited less than 5 seconds
# give killed workers a chance to flush their logs to datastore
# Store the parameters needed for task creation, so that pushing on items
# onto the run_queue is an inexpensive operation.
# if the next step is a join, we need to check that
# all input tasks for the join have finished before queuing it.
# CHECK: this condition should be enforced by the linter but
# let's assert that the assumption holds
# matching_split is the split-parent of the finished task
# next step is a foreach join
# required tasks are all split-siblings of the finished task
# next step is a split-and
# required tasks are all branches joined by the next step
# all tasks to be joined are ready. Schedule the next join step.
# CHECK: this condition should be enforced by the linter but
# let's assert that the assumption holds
# schedule all splits
# finished tasks include only successful tasks
# CHECK: ensure that runtime transitions match with
# statically inferred transitions
# Different transition types require different treatment
# Next step is a join
# Next step is a foreach child
# Next steps are normal linear steps
# worker did not finish successfully
# worker finished successfully
# Initialize the task (which can be expensive using remote datastores)
# before launching the worker so that cost is amortized over time, instead
# of doing it during _queue_push.
# any results with an attempt ID >= MAX_ATTEMPTS will be ignored
# by datastore, so running a task with such a retry_could would
# be pointless and dangerous
# task_id is preset only by persist_parameters()
# Open the output datastore only if the task is not being cloned.
# determine the number of retries of this task
# This is just for usability: We could rerun the whole flow
# if an unknown clone_run_id is provided but probably this is
# not what the user intended, so raise a warning
# all inputs must have the same foreach stack, so we can safely
# pick the first one
# Parent should be non-None since only clone the child if the parent
# was successfully cloned.
# foreach-join pops the topmost index
# foreach-split pushes a new index
# all other transitions keep the parent's foreach stack intact
# Store the mapping from current_pathspec -> origin_pathspec which
# will be useful for looking up origin_ds_set in find_origin_task.
# Clone in place without relying on run_queue.
# Store the origin pathspec in clone_origin so this can be run
# as a task by the runtime.
# Save a call to creating the results_ds since its same as origin.
# note: id is not available before the task has finished
# this is used to persist parameters before the start step
# Killed indicates that the task was forcibly killed
# with SIGKILL by the master process.
# A killed task is always considered cleaned
# A cleaned task is one that is shutting down and has been
# noticed by the runtime and queried for its state (whether or
# not is is properly shut down)
# disabling atlas sidecar for cloned tasks due to perf reasons
# decorators may modify the CLIArgs object in-place
# the env vars are needed by the test framework, nothing else
# NOTE bufsize=1 below enables line buffering which is required
# by read_logline() below that relies on readline() not blocking
# print('running', args)
# readline() below should never block thanks to polling and
# line buffering. If it does, things will deadlock
# this shouldn't block, since terminate() is called only
# after the poller has decided that the worker is dead
# consume all remaining loglines
# we set the file descriptor to be non-blocking, since
# the pipe may stay active due to subprocesses launched by
# the worker, e.g. sidecars, so we can't rely on EOF. We try to
# read just what's available in the pipe buffer
# ignore "resource temporarily unavailable" etc. errors
# caused due to non-blocking. Draining is done on a
# best-effort basis.
# Return early if the task is cloned since we don't want to
# perform any log collection.
# for python 2 compatibility
# type: (str) -> None
# unable to start subprocess, fallback to Null sidecar
# sidecar is disabled, ignore all messages
# drop message, do not retry on timeout
# Define message enums
# add module to python path if not already present
# todo handle other possible exceptions gracefully
# overwrite Parameters in the flow object
# make the parameter a read-only property
# note x=x binds the current value of x to the closure
# We prefer to use the parallelized version to initialize datastores
# (via MetaflowDatastoreSet) only with more than 4 datastores, because
# the baseline overhead of using the set is ~1.5s and each datastore
# init takes ~200-300ms when run sequentially.
# Prefetch 'foreach' related artifacts to improve time taken by
# _init_foreach.
# Note: Specify `pathspecs` while creating the datastore set to
# guarantee strong consistency and guard against missing input.
# initialize directly in the single input case.
# this guards against errors in input paths
# these variables are only set by the split step in the output
# data. They don't need to be accessible in the flow.
# There are three cases that can alter the foreach state:
# 1) start - initialize an empty foreach stack
# 2) join - pop the topmost frame from the stack
# 3) step following a split - push a new frame in the stack
# case 1) - reset the stack
# case 2) - this is a join step
# assert the lineage of incoming branches
# the topmost indices in the stack are all
# different naturally, so ignore them in the
# assertion
# assert that none of the inputs are splits - we don't
# allow empty foreaches (joins immediately following splits)
# Make sure that the join got all splits as its inputs.
# Datastore.resolve() leaves out all undone tasks, so if
# something strange happened upstream, the inputs list
# may not contain all inputs which should raise an exception
# foreach-join pops the topmost frame from the stack
# a non-foreach join doesn't change the stack
# case 3) - our parent was a split. Initialize a new foreach frame.
# push a new index after a split to the stack
# 1. initialize output datastore
# 2. initialize origin datastore
# any results with an attempt ID >= MAX_ATTEMPTS will be ignored
# by datastore, so running a task with such a retry_could would
# be pointless and dangerous
# 1. initialize output datastore
# 2. initialize input datastores
# 3. initialize foreach state
# 4. initialize the current singleton
# 5. run task
# init side cars
# Note: All internal flow attributes (ie: non-user artifacts)
# should either be set prior to running the user code or listed in
# FlowSpec._EPHEMERAL to allow for proper merging/importing of
# user artifacts in the user's step code.
# decorators can actually decorate the step function,
# or they can replace it altogether. This functionality
# is used e.g. by catch_decorator which switches to a
# fallback code if the user code has failed too many
# times.
# Join step:
# Ensure that we have the right number of inputs. The
# foreach case is checked above.
# Multiple input contexts are passed in as an argument
# to the step function.
# initialize parameters (if they exist)
# We take Parameter values from the first input,
# which is always safe since parameters are read-only
# Linear step:
# We are running with a single input context.
# The context is embedded in the flow.
# This should be captured by static checking but
# let's assert this again
# initialize parameters (if they exist)
# We take Parameter values from the first input,
# which is always safe since parameters are read-only
# terminate side cars
# this writes a success marker indicating that the
# "transaction" is done
# final decorator hook: The task results are now
# queryable through the client API / datastore
# python2
# unquote_bytes should be a function that takes a urlencoded byte
# string, encoded in UTF-8, url-decodes it and returns it as a
# unicode object. Confusingly, how to accomplish this differs
# between Python2 and Python3.
#
# Test with this input URL:
# b'crazypath/%01%C3%B'
# it should produce
# u'crazypath/\x01\xff'
# python3
# Provide a temporary directory since Python 2.7 does not have it inbuilt
# quote() works reliably only with (byte)strings in Python2,
# hence we need to .encode('utf-8') first. To see by yourself,
# try quote(u'\xff') in python2. Python3 converts the output
# always to Unicode, hence we need the outer to_bytes() too.
#
# We mark colon as a safe character to keep simple ASCII urls
# nice looking, e.g. "http://google.com"
# note: the order of the list matters
# Directories exists in other casewhich is fine
# Three output modes:
# 1. Just a comma-separated list
# 2. Prefix and a comma-separated list of suffixes
# 3. zlib-compressed, base64-encoded, prefix-encoded list
# interestingly, a typical zlib-encoded list of suffixes
# has plenty of redundancy. Decoding the data *twice* helps a
# lot
# Three input modes:
# 3. zlib-compressed, base64-encoded
# 2. Prefix and a comma-separated list of suffixes
# 1. Just a comma-separated list
# we need special handling for 'with' since it is a reserved
# keyword in Python, so we call it 'decospecs' in click args
# This function is imported from https://github.com/cookiecutter/whichcraft
# Check that a given file can be accessed with the correct mode.
# Additionally check that `file` is not a directory, as on Windows
# directories pass the os.access check.
# Forced testing
# If we're given a path with a directory part, look it up directly
# rather than referring to PATH directories. This includes checking
# relative to the current directory, e.g. ./script
# How to work with flows
# More questions?
# Flow spec
# current runtime singleton
# data layer
# Decorators
# this auto-generates decorator functions from Decorator objects
# in the top-level metaflow namespace
# Client
# Utilities
# this happens on remote environments since the job package
# does not have a version
# python2
# noqa E722
# python3
# Deduce from ms; if starts with http, use service or else use local
# see a comment about namespace initialization
# in Metaflow.__init__ below
# the default namespace is activated lazily at the first object
# invocation or get_namespace(). The other option of activating
# the namespace at the import time is problematic, since there
# may be other modules that alter environment variables etc.
# which may affect the namescape setting.
# We do not filter on namespace in the request because
# filtering on namespace on flows means finding at least one
# run in this namespace. This is_in_namespace() function
# does this properly in this case
# The JSON module in Python3 deals with Unicode. Tar gives bytes.
# TODO add
# @property
# def size(self)
# TODO add
# @property
# def type(self)
# exclude private data artifacts
# use the newest version of each key, hence sorting
# Raised if None is present in max
# All tasks have the same environment info so just use the first one
# exclude _parameters step
# maybe another client had already GC'ed the file away
# this is for python2 compatibility.
# Python3 has os.makedirs(exist_ok=True).
# maybe another client had already GC'ed the file away
# index objects lazily at the first request. This can be
# an expensive operation
# noqa E722
# noqa E722
#core client classes
# Python 2
# Python 3
# TODO sort by foreach index
# Very simple wrapper class to only keep one transform
# of an object. This is to force garbage collection
# on the transformed object if the transformation is
# successful
# Transformer is a function taking one argument (the current object) and returning another
# object which will replace the current object if transformer does not raise an
# exception
# noqa E722
# Datastore needs to implement the methods below
# new style paths = <attempt>.<name>
# old style paths.
# run_id may be None when datastore is used to save
# things not related to runs, e.g. the job package
# what is the latest attempt ID of this data store?
# In the case of S3, the has_metadata() below makes a
# HEAD request to a non-existent object, which results
# to this object becoming eventually consistent. This
# could result to a situation that has_metadata() misses
# the latest version although it is already existing.
# As long as nothing opens a datastore for reading before
# writing, this should not be a problem.
# We have to make MAX_ATTEMPTS HEAD requests, which is
# very unfortunate performance-wise (TODO: parallelize this).
# On Meson it is possible that some attempts are missing, so
# we have to check all possible attempt files to find the
# latest one. Compared to doing a LIST operation, these checks
# are guaranteed to be consistent as long as the task to be
# looked up has already finished.
# backwards-compatibility for pre-attempts.
# was the latest attempt completed successfully?
# load the data from the latest attempt
# Direct access mode used by the client. We effectively don't load any
# objects and can only access things using the load_* functions
# to ensure compatibility between python2 and python3, we use the
# highest protocol that works with both the versions
# this happens when you try to serialize an oversized
# object (2GB/4GB+)
# Pass-down from datastore origin all information related to vars to
# this datastore. In other words, this adds to the current datastore all
# the variables in vars (obviously, it does not download them or anything but
# records information about them). This is used to propagate parameters between
# datastores without actually loading the parameters
# Skip over properties of the class (Parameters)
# We will force protocol 4 for serialization for anything
# bigger than 1 GB
# initialize with old values...
# ...overwrite with new
# register artifacts with the metadata service
# Provides a fast-path to check if a given object is None.
# Conservatively check if the actual object is None, in case
# the artifact is stored using a different python version.
# Slow path since this has to get the object from S3.
# backwards compatibility: we might not have info for all objects
# Update (and not re-assign) the artifact_cache since each datastore
# created above has a reference to this object.
# Compute path for DATASTORE_SYSROOT_LOCAL
# Python2
# noqa E722
# We are no longer making upward progress
# Could not find any directory to use so create a new one
# Sort the file listing to iterate in increasing order of
# attempts.
# Read the corresponding metadata file.
# Only read the metadata if the latest attempt is also done.
# NOTE multiple tasks may try to save an object with the
# same sha concurrently, hence we need to use a proper tmp
# file
# NOTE compresslevel makes a huge difference. The default
# level of 9 can be impossibly slow.
# this is for python2 compatibility.
# Python3 has open(mode='x').
# python2
# python3
# the s3 client is shared across all S3DataStores
# so we don't open N connections to S3 unnecessarily
# Note: When `pathspecs` is specified, we avoid the eventually
# consistent `s3.list_paths` operation, and directly construct the
# task_urls list.
# Note for potential future optimization:
# Find the list of latest attempt for each task first, and
# follow up with a call to get done and metadata.
# files are in sorted order, so overwrite is ok.
# is_metadata_filename(fname) == True.
# NOTE compresslevel makes a huge difference. The default
# level of 9 can be impossibly slow.
# filename=None
# python2
# python3
# NOTE we deliberately regard NoSuchKey as an ignorable error.
# We assume that the file just hasn't appeared in S3 yet.
# decorator to retry functions that access S3
# MetaflowExceptions are not related to AWS, don't retry
# exponential backoff
# python2
# python3
# all fields of S3Object should return a unicode object
# 1. use a (current) run ID with optional customizations
# 2. use an explicit S3 prefix
# 3. use the client only with full URLs
# NOTE: All URLs are handled as Unicode objects (unicde in py2,
# string in py3) internally. We expect that all URLs passed to this
# class as either Unicode or UTF-8 encoded byte strings. All URLs
# returned are Unicode.
# missing entries per return_missing=True
# we need to recreate the StringIO object for retries since
# apparently upload_fileobj will/may close() it
# TODO specific error message for out of disk space
# add some jitter to make sure retries are not synchronized
# NOTE: re: _read_many_files and _put_many_files
# All file IO is through binary files - we write bytes, we read
# bytes. All inputs and outputs from these functions are Unicode.
# Conversion between bytes and unicode is done through url_quote
# and url_unquote.
# python2
# python3
# s3op can be launched as a stand-alone script. We must set
# PYTHONPATH for the parent Metaflow explicitly.
# we use Metaflow's parallel_imap_unordered instead of
# multiprocessing.Pool because https://bugs.python.org/issue31886
# We use error codes instead of Exceptions, which are trickier to
# handle reliably in a multi-process world
# I can't understand what's the right way to deal
# with boto errors. This function can be replaced
# with better error handling code.
# S3 worker pool
# TODO specific error message for out of disk space
# 1. push sources and destinations to the queue
# 2. push end-of-queue markers
# 3. start processes
# 4. wait for the processes to finish
# Utility functions
# S3Ops class is just a wrapper for get_size and list_prefix
# required by @aws_retry decorator, which needs the reset_client
# method. Otherwise they would be just stand-alone functions.
# note that an url may be both a prefix and an object
# - the trailing slash is significant in S3
# we get CommonPrefixes if Delimiter is a non-empty string
# We want to reuse an s3 client instance over multiple operations.
# This is accomplished by op_ functions below.
# this function generates a safe local file name corresponding to
# an S3 URL. URLs may be longer than maximum file length limit on Linux,
# so we mostly hash the URL but retain the leaf part as a convenience
# feature to ease eyeballing
# parallel op divides work equally amongst num_workers
# processes. This is a good strategy if the cost is
# uniform over the units of work, e.g. op_get_size, which
# is a single HEAD request to S3.
#
# This approach is less optimal with op_list_prefix where
# the cost of S3 listing per prefix can vary drastically.
# We could optimize this case by using a worker model with
# a queue, like for downloads but the difference here is
# that we need to return a value, which would require a
# bit more work - something to consider if this turns out
# to be a bottleneck.
# CLI
# Construct a list of URL (prefix) objects
# Construct a url->size mapping
# NOTE - we must retain the order of prefixes requested
# and the listing order returned by S3
# pretend zero size since we don't need it for anything.
# it can't be None though, to make sure the listing below
# works correctly (None denotes a missing file)
# exclude the non-existent files from loading
# Postprocess
# We currently just use the timestamp to create an ID. We can be reasonably certain
# that it is unique and this makes it possible to do without coordination or
# reliance on POSIX locks in the filesystem.
# Artifacts are actually part of the tasks in the filesystem
# Special handling of self, artifact, and metadata
# For the other types, we locate all the objects we need to find and return them
# this is for python2 compatibility.
# Python3 has os.makedirs(exist_ok=True).
# Error raised when directory exists
# In this case, the metadata information does not exist so we create it
# From https://stackoverflow.com/questions/22409430/portable-meta-class-between-python2-and-python3
# clean out class body
# Metadata is always only at the task level
# noqa E722
# Special handling of self, artifact, and metadata
# For the other types, we locate all the objects we need to find and return them
# first ensure that the flow exists
# noqa E722
# handling _foreach_var and _foreach_num_splits requires some
# deeper thinking, so let's not support that use case for now
# pretend that self.next() was called as usual
# store the exception
# there was no exception, set the exception var (if any) to None
# if the user code has failed max_user_code_retries times, @catch
# runs a piece of fallback code instead. This way we can continue
# running the flow downsteam, as we have a proper entry for this task.
# type: (Message) -> None
# type: (Timer) -> None
# type: (Message) -> None
# Prepare the package before any of the sub-commands are invoked.
# The total number of attempts must not exceed MAX_ATTEMPTS.
# attempts = normal task (1) + retries (N) + @catch fallback (1)
# Initialize secs in __init__ so other decorators could safely use this
# value without worrying about decorator order.
# Convert values in attributes to type:int since they can be type:str
# when passed using the CLI option --with.
# enable timeout only when executing user code
# 5 days.
# it is important that CLIs are not imported when
# __init__ is imported. CLIs may use e.g.
# parameters.add_custom_parameters which requires
# that the flow is imported first
# Add new CLI commands in this list
# Add new decorators in this list
# Add Conda environment
# Every entry in this list becomes a class-level flow decorator.
# Add an entry here if you need a new flow-level annotation. Be
# careful with the choice of name though - they become top-level
# imports from the metaflow package.
# Sidecars
# Add logger
# Add monitor
# Kill the job if it is still running by throwing an exception.
# python2
# noqa E722
# python3
# If we are here, we can download the object
# noqa F841
# Get retry information
# Set batch attributes
# Add the environment variables related to the input-paths argument
# don't retry killed tasks
# TODO: Check statusmessage to find if the job crashed instead of failing
# python2
# noqa E722
# python3
# we use the larger of @resources and @batch attributes
# after all attempts to run the user code have failed, we don't need
# Batch anymore. We can execute possible fallback code locally.
# We have a local metadata service so we need to persist it to the datastore.
# Note that the datastore is *always* s3 (see runtime_task_created function)
# The local metadata is stored in the local datastore
# which, for batch jobs, is always the DATASTORE_LOCAL_DIR
# At this point we upload what need to s3
# Create the conda environment
# Remove the conda environment
# Get Python interpreter for the conda environment
# List all conda environments associated with the flow
# Show conda installation configuration
# Show conda environment package configuration
# Not every parameter is exposed via conda cli hence this ignominy
# Print a message for now
# Apply conda decorator to all steps
# Guaranteed to have a conda decorator because of self.decospecs()
# Bootstrap conda and execution environment for step
# Add conda manifest file to job package at the top level.
# Disable (import-error) in pylint
# Get relevant python interpreter for step
#The tarball maybe missing when user invokes `conda clean`!
# force conda resolution for linux-64 architectures
# Create a symlink to installed version of metaflow to execute user code against
# For this example, we only need the movie title and the genres.
# Create a simple data frame as a dictionary of lists.
# Parse the CSV header.
# Populate our dataframe from the lines of the CSV file.
# Compute genre specific movies and a bonus movie in parallel.
# Find all the movies that are not in the provided genre.
# Choose one randomly.
# Find all the movies titles in the specified genre.
# Randomize the title names.
# Reassign relevant variables from our branches.
# Load the data set into a pandas dataaframe.
# The column 'genres' has a list of genres for each movie. Let's get
# all the unique genres.
# We want to compute some statistics for each genre. The 'foreach'
# keyword argument allows us to compute the statistics for each genre in
# parallel (i.e. a fan-out).
# The genre currently being processed is a class property called
# 'input'.
# Find all the movies that have this genre and build a dataframe with
# just those movies and just the columns of interest.
# Get some statistics on the gross box office for these titles.
# Join the results from other genres.
# Merge results from the genre specific computations.
# Print metadata provider
# Load the analysis from the MovieStatsFlow.
# Compute our two recommendation types in parallel.
# Concatenate all the genre specific data frames and choose a random
# movie.
# For the genre of interest, generate a potential playlist using only
# highest gross box office titles (i.e. those in the last quartile).
# Shuffle the playlist.
# Print the playlist.
# Use the specified version of python for this flow.
# Load the analysis from the MovieStatsFlow.
# Print metadata provider
# Load the analysis from the MovieStatsFlow.
# Get the dataframe from the start step before we sliced into into
# genre specific dataframes.
# Also grab the summary statistics.
# Compute our two recomendation types in parallel.
# Define a helper function to compute the similarity between two
# strings.
# Compute the distance and take the argmin to find the closest title.
# For the genre of interest, generate a potential playlist using only
# highest gross box office titles (i.e. those in the last quartile).
# Shuffle the content.
# Print the playlist.
###%s%s: %s ###" % (fstr, cstr, msg)
### %s ###" % msg
# write scripts
# expand environment variables
# nonce can be used to insert entropy in env vars.
# This is useful e.g. for separating S3 paths of
# runs, which may have clashing run_ids
# run flow
# check results
# copy coverage files
# HACK: The two separate files are needed to store the output in separate
# S3 buckets since jenkins test doesn't have access to `dataeng` bucket.
# Python 2
# Python 3
# if the step had multiple tasks, this will fail
# -*- coding: utf-8 -*-'
# -*- coding: utf-8 -*-'
# test 1) USER should be the default
# test 2) Run should be in the listing
# test 3) changing namespace should change namespace
# test 4) fetching results in the incorrect namespace should fail
# test 5) global namespace should work
# index must stay constant over multiple steps inside foreach
# Note this value is overridden in contexts.json
# parameters should be immutable
# -*- coding: utf-8 -*-
# TODO we could call self.tag() in some steps, once it is implemented
# CliChecker does not return a run object, that's ok
# test crazy unicode and spaces in tags
# these tags must be set with --tag option in contexts.json
# test different namespaces: one is a system-tag,
# another is a user tag
# the flow object should not have tags
# the run object should have the namespace tags
# filtering by a non-existent tag should return nothing
# a conjunction of a non-existent tag and an existent tag
# should return nothing
# all steps should be returned with tag filtering
# a conjunction of two existent tags should return the original list
# all tasks should be returned with tag filtering
# the run object should have the tags
# filtering by a non-existent tag should return nothing
# filtering by the tag should not exclude any tasks
# the task object should have the tags
# the data artifact should have the tags
# merge all incoming branches
# join step needs to reassign all artifacts.
# add data for the join step
# very basic sanity check for CLI
# we can't easily account for the number of foreach splits,
# so we only care about unique lineages (hence set())
# traverse all paths from the start step to the end,
# collect lineages on the way and finally compare them
# to the lineages produced by the actual run
# Set to different things
# Test to make sure non-merged values are reported
# Test to make sure nothing is set if failed merge_artifacts
# Test actual merge (ignores set values and excluded names, merges common and non modified)
# Ensure that everything we expect is passed down
# This is not a join so test exception for calling in non-join
# Check that all values made it through
# This test simply tests whether things set on a single branch will
# still get propagated down properly. Other merge_artifacts behaviors
# are tested in the main test (merge_artifacts.py). This test basically
# only matches with the small-foreach graph whereas the other test is
# more generic.
# Set different names to different things
# Ensure that everything we expect is passed down
# assert that lengths are correct
# assert that variables are correct given their indices
# Verify that the `current` singleton contains the correct origin
# run_id by double checking with the environment variables used
# for tests.
# foreach splits don't support @catch but @retry should work
# make sure we see the latest attempt version of the artifact
# the test uses a non-trivial derived exception on purpose
# which is non-trivial to pickle correctly
# die an ugly death
# 1 normal run + 2 retries = 3 attempts
# 1 normal run + 2 retries + 1 fallback = 4 attempts
# task.exception is None since the exception was handled
# The client API shouldn't choke on many tasks
