#!/usr/bin/env python
# for when the code runs on the webheads after deploy
# when the code runs on the admin node during deploy
# noqa
# noqa
#
# Kuma documentation build configuration file, created by
# sphinx-quickstart on Mon Aug  5 15:41:51 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.
# import sys, os
# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
# sys.path.insert(0, os.path.abspath('.'))
# -- General configuration -----------------------------------------------------
# If your documentation needs a minimal Sphinx version, state it here.
# needs_sphinx = '1.0'
# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
# Add any paths that contain templates here, relative to this directory.
# The suffix of source filenames.
# The encoding of source files.
# source_encoding = 'utf-8-sig'
# The master toctree document.
# General information about the project.
# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
# The full version, including alpha/beta/rc tags.
# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
# language = None
# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
# today = ''
#
# Else, today_fmt is used as the format for a strftime call.
# today_fmt = '%B %d, %Y'
# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# The reST default role (used for this markup: `text`) to use for all documents.
# default_role = None
# If true, '()' will be appended to :func: etc. cross-reference text.
# add_function_parentheses = True
# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
# add_module_names = True
# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
# show_authors = False
# The name of the Pygments (syntax highlighting) style to use.
# pygments_style = 'sphinx'
# A list of ignored prefixes for module index sorting.
# modindex_common_prefix = []
# If true, keep warnings as "system message" paragraphs in the built documents.
# keep_warnings = False
# -- Options for HTML output ---------------------------------------------------
# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
# Add any paths that contain custom themes here, relative to this directory.
# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
# A shorter title for the navigation bar.  Default is the same as html_title.
# html_short_title = None
# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
# html_logo = None
# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
# html_favicon = None
# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
# html_static_path = ['_static']
# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
# html_last_updated_fmt = '%b %d, %Y'
# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
# html_use_smartypants = True
# Custom sidebar templates, maps document names to template names.
# Additional templates that should be rendered to pages, maps page names to
# template names.
# html_additional_pages = {}
# If false, no module index is generated.
# html_domain_indices = True
# If false, no index is generated.
# html_use_index = True
# If true, the index is split into individual pages for each letter.
# html_split_index = False
# If true, links to the reST sources are added to the pages.
# html_show_sourcelink = True
# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
# html_show_sphinx = True
# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
# html_show_copyright = True
# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
# html_use_opensearch = ''
# This is the file name suffix for HTML files (e.g. ".xhtml").
# html_file_suffix = None
# Output file base name for HTML help builder.
# -- Options for LaTeX output --------------------------------------------------
# The paper size ('letterpaper' or 'a4paper').
# 'papersize': 'letterpaper',
# The font size ('10pt', '11pt' or '12pt').
# 'pointsize': '10pt',
# Additional stuff for the LaTeX preamble.
# 'preamble': '',
# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
# The name of an image file (relative to this directory) to place at the top of
# the title page.
# latex_logo = None
# For "manual" documents, if this is true, then top-level headings are parts,
# not chapters.
# latex_use_parts = False
# If true, show page references after internal links.
# latex_show_pagerefs = False
# If true, show URL addresses after external links.
# latex_show_urls = False
# Documents to append as an appendix to all manuals.
# latex_appendices = []
# If false, no module index is generated.
# latex_domain_indices = True
# -- Options for manual page output --------------------------------------------
# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
# If true, show URL addresses after external links.
# man_show_urls = False
# -- Options for Texinfo output ------------------------------------------------
# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
# Documents to append as an appendix to all manuals.
# texinfo_appendices = []
# If false, no module index is generated.
# texinfo_domain_indices = True
# How to display URL addresses: 'footnote', 'no', or 'inline'.
# texinfo_show_urls = 'footnote'
# If true, do not generate a @detailmenu in the "Top" node's menu.
# texinfo_no_detailmenu = False
# set the default Django settings module for the 'celery' program.
# Using a string here means the worker doesn't have to serialize
# the configuration object to child processes.
# - namespace='CELERY' means all celery-related configuration keys
#   should have a `CELERY_` prefix.
# Load task modules from all registered Django app configs.
# This can fail if Constance uses a cached database backend
# CONSTANCE_DATABASE_CACHE_BACKEND = False to disable
# The non-locale-based landing URL's
# The locale-based landing URL's
# Here the "shared_cache_control" decorator is an optimization. It
# informs the CDN to cache the redirect for a month, so once this URL
# has been requested by a client, all other client requests will be
# redirected by the CDN instead of this Django service.
# Django admin:
# We don't worry about decorating the views within django.contrib.admin
# with "never_cache", since most have already been decorated, and the
# remaining can be safely cached.
# Redirect if we try to use the "tidings" unsubscribe.
# The first argument to "decorator_include" can be an iterable
# of view decorators, which are applied in reverse order.
# Services and sundry.
# Serve sitemap files.
# We use our own views for setting language in cookies. But to just align with django, set it like this.
# Legacy MindTouch redirects. These go last so that they don't mess
# with local instances' ability to serve media.
# Fix the settings so that Whitenoise is happy
# Enable WhiteNoise
# noqa
# This will make sure the app is always imported when
# Django starts so that shared_task will use this app.
# Register signal handlers
# noqa
# The global cloudfront client object to be lazily defined
# Use this to turn the document IDs into pairs of (locale, slug).
# Build up this list for the benefit of triggering a
# CDN cache invalidation.
# In case the transform function decided to "opt-out" on a particular
# (locale, slug) it might return a falsy value.
# The 'CallerReference' just needs to be a unique string.
# By using a timestamp we get slightly more information
# than using a UUID or a random string. But it needs to
# be sufficiently "different" that's why we use 6
# significant figures to avoid the unlikely chance that
# this code gets executed concurrently within a small
# time window.
# overridden by --all or --locale
# Let's publish the documents in a group of chunks, where the
# tasks in the group can be run in parallel.
#{} of {}'.format(i + 1, num_tasks)
# Let's make them all errors.
# S3 excludes the "Deleted" key from its response
# if there are none.
# Otherwise, let's make the first one an error.
# S3 excludes the "Errors" key from its response if there are none.
#1', 'S3 Object #2', 'S3 Object #3']
#1')
#1')
#1')
#1')
#1'),
#2'),
#3'),
# By default, for all testing, the MDN_CLOUDFRONT_DISTRIBUTIONS
# should be set to an empty dict. Just sanity-check that.
# When configuring the MDN_CLOUDFRONT_DISTRIBUTIONS you need to specify
# the dotted path to the transform function. So this function needs to
# be possible to import by Django.
# When used, we need to reset it because it's a module global mutable
# specific to this test module.
# TODO: This API endpoint probably needs to handle redirect documents
# and documents that fall back to the en-US locale. See
# the document() function in wiki/views/document.py for a model to follow.
# Since we don't have the locale at the start of the path, our
# locale middleware can't set the translation language correctly
# and we need to do it explicitly. (We need to know the language
# so that we can provide translated language names for the
# translations menu.)
# Redirects within an S3 bucket must be prefixed with "/".
# This is a redirect to another document.
# This is a redirect to non-document page. For now, if it's the home
# page, return a relative path (so we stay on the read-only domain),
# otherwise return the full URL for the wiki site.
# Let's return a relative URL to the home page for this locale.
# Otherwise, let's return a full URL to the Wiki site.
# The original english slug for this document, for google analytics
# Add waffle data to the dict we're going to be returning.
# This is what the waffle.wafflejs() template tag does, but we're
# doing it via an API instead of hardcoding the settings into
# the HTML page. See also from waffle.views._generate_waffle_js.
#
# Note that if we upgrade django-waffle, version 15 introduces a
# pluggable flag model, and the approved way to get all flag
# objects will then become:
#    get_waffle_flag_model().get_all()
#
# Also ensure that we get exactly the same data by calling
# the document_api_data() function directly
# Create some fake waffle objects
# Create some fake waffle objects
# 'q' not present
# 'q' present but falsy
# 'q' present but locale invalid
# 'q' present but contains new line
# 'q' present but exceeds max allowed characters
# Now search in a non-en-US locale
# go through all revisions and trash them,
# they'll actually be deleted by the deletion of the attachment
# call the actual deletion of the attachment object
# call the actual deletion of the attachment revision object
# that also creates a trash item
# Register signal handlers
# noqa
# The `magic.Magic()` will, for unknown reasons, sometimes
# think an SVG image's mime type is `image/svg` which not
# a valid mime type actually.
# See https://www.iana.org/assignments/media-types/media-types.xhtml#image
# So correct that.
# These get filled from the current revision.
# This is somewhat like the bookkeeping we do for Documents, but
# is also slightly more permanent because storing this ID lets us
# map from old MindTouch file URLs (which are based on the ID) to
# new kuma file URLs.
# First let's see if there is already an intermediate object available
# for the current attachment, a.k.a. this was a previous uploaded file
# no previous uploads found, create a new document-attachment
# Does not allow wiki markup
# As with document revisions, bookkeeping for the MindTouch
# migration.
#
# TODO: Do we actually need full file revision history from
# MindTouch?
#%s)' %
# see if there is a previous revision
# if yes, make it the current revision of the attachment
# if a file entry is present, delete the file with the storage
# without saving the model instance
# Uploading via the Wiki is disabled
# Superusers and staff always allowed
# Explicit add permission overrides disallow
# Disallow generally applied via group, so per-user allow can
# override
# Check if the given dt is timezone aware and if not make it aware.
# Convert the datetime to UTC.
# Convert the datetime to UTC.
# Convert the UTC datetime to seconds since the epoch.
# Format the thing as a RFC1123 datetime.
# For now, the filesystem storage path will look like this:
#
# attachments/<year>/<month>/<day>/<attachment_id>/<md5>/<filename>
#
# The md5 hash here is of the full timestamp, down to the
# microsecond, of when the path is generated.
# Mime types used on MDN
# Attachments must be served from safe (untrusted) domains
# NOTE: All of this, just to support conditional requests (last-modified / if-modified-since)
# Very important while we're potentially serving attachments from disk.
# Far less important when we're just redirecting to S3.
# Consider removing?
# No access if no permissions to upload
# Only staff users are allowed to upload SVG files because SVG files
# can contain embedded inline scripts.
# adding the attachment to the document's files (M2M)
# in case we have lots of attachments we don't want Django's
# queryset iteration to break the deletion
# Generated by Django 1.11.23 on 2019-10-23 04:05
# Generated by Django 1.11.23 on 2019-10-16 14:36
# the attachment revision wasn't really deleted,
# only a trash item created
# deleting a revision without providing information
# still creates a trashedattachment item and leaves file in place
# adding a new revision sets the current revision automatically
# deleting it again resets the current revision to the previous
# deleting the only revision left raises an IntegrityError exception
# Get the negative and positive permissions
# Create a group with the negative permission.
# Create a group with the positive permission.
# User with no explicit permission is allowed
# User in group with negative permission is disallowed
# Superusers can do anything, despite group perms
# User with negative permission is disallowed
# User with positive permission overrides group
# Group with positive permission takes priority
# positive permission takes priority, period.
# All users, including superusers, are denied
# use view to create new attachment
# now stick it in/on a document
# view it and verify markup is escaped
# security bug 1272791
# HTTP 403 Forbidden
# Remember, self.client use logged in as user 'admin'
# means it worked
# means it didn't upload
#id_file"]')) == expected
# Force the HOST header to look like something other than "demos".
# Figure out the external scheme + host for our attachments bucket
# Verify we're redirecting to the intended bucket or custom frontend
# Shamelessly stolen from Django's own file-upload tests.
#{})</a>', url, key.user, key.id)
#{})</a>', url, self.content_type, obj.pk)
# 32 * 8 = 256 random bits
# Test with incorrect auth header
# Make a request to the view
# The user should not be authenticated and no error should be raised.
# method to cache-bust the user perms by re-fetching from DB
# https://docs.djangoproject.com/en/1.7/topics/auth/default/#permissions-and-authorization
# Give self.user (tester23) keys permissions
# Check out the creation page, look for the form.
# We don't have this key yet, right?
# Okay, create it.
# We have the key now, right?
# Okay, and it should belong to the logged-in user
# Take a look at the description and key shown on the result page.
# Ensure the secret on the page checks out.
#key-%s' % key.pk)
# Assemble some sample log lines
#%s' % i))
# Record the log lines for this key
# Reverse the lines for comparison.
# Iterate through 2 expected pages...
# Generated by Django 1.11.18 on 2019-03-26 00:57
# Generated by Django 1.11.23 on 2019-10-23 04:05
# Remove list delete action to enforce model soft delete in admin site
# Clean up expired sessions every 60 minutes
# LANGUAGES settings return a list of tuple with language code and their native name
# Make the language code lower and convert the tuple to dictionary
# E.g. 'elasticsearch:9200'
# TODO: Ideally, GOOGLE_ANALYTICS_ACCOUNT is only set in settings (from
# an environment variable) but for safe transition, we rely on
# constance if it hasn't been put into settings yet.
# Once we know with confidence, that GOOGLE_ANALYTICS_ACCOUNT is set
# and a valid value in the environment (for production!) then we
# can delete these lines of code.
# See https://bugzilla.mozilla.org/show_bug.cgi?id=1570076
# Because the 'settings.ES_URLS' might contain the username:password
# it's never appropriate to display in templates. So clean them up.
# But return it as a lambda so it only executes if really needed.
# We must call reverse at the view level, else the threadlocal
# locale prefixing doesn't take effect.
# Redirect back here afterwards?
#: A decorator to use for requiring a superuser
# Types of errors, and examples.
#
# TypeError: Not enough arguments for string
#   '%s %s %s' % ('foo', 'bar')
# KeyError: Bad variable name
#   '%(Foo)s' % {'foo': 10} or '{Foo}'.format(foo=10')
# ValueError: Incomplete Format, or bad format string.
#    '%(foo)a' or '%(foo)' or '{foo'
# IndexError: Not enough arguments for .format() style string.
#    '{0} {1}'.format(42)
# TODO: remove this and use strip kwarg once ticket #6362 is done
# @see http://code.djangoproject.com/ticket/6362
# Remove the default min and max length validators and add our own
# that format numbers in the error messages.
# Kuma: Convert Kuma to Django language code
# Kuma: Check for known override
# If 'fr-ca' is not supported, try special fallback or language-only 'fr'.
# Look for exact match
# Kuma: Convert to Kuma language code
# If fr-fr is not supported, try fr-ca.
# Kuma: Convert to Kuma language code
# Kuma: Always use the URL's language (force check_path=True)
# Kuma: Skip checking the session-stored language via LANGUAGE_SESSION_KEY
# Use the (valid) language cookie override
# Pick the closest langauge based on the Accept Language header
# Kuma: Assert accept_lang fits the language code pattern
# The regex check was added with a security fix:
# https://www.djangoproject.com/weblog/2007/oct/26/security-fix/
# In the Django version, non-matching accept_lang codes are skipped.
# However, it doesn't seem possible for parse_accept_lang_header to
# return codes that would fail this check.
# The assertion keeps the security aspect, and gives us an opportunity
# to add a test case to Kuma and Django.
# Kuma: Fallback to default settings.LANGUAGE_CODE.
# Django supports a case when LANGUAGE_CODE is not in LANGUAGES
# (see https://github.com/django/django/pull/824). but our LANGUAGE_CODE is
# always the first entry in LANGUAGES.
#switching-language-in-templates
#module-jinja2.ext
# Get the line number and desired language
# Parse the block body until {% endtranslation %}
# Return a node that will render body in the desired translation
#{self.version}'
# HACK: Yes, I really do want to allow tags in admin change lists
# HACK: This is expensive, too, but should help with list_filter in admin
# HACK: Yes, I really do want to allow tags in admin change lists
# Empty namespace is special - just look for absence of ':'
# Namespace requested, so generate filtered set
# No namespace requested, so collate into namespaces
# Invalid or no language requested, so don't redirect.
# Check if the requested language is already embedded in URL
# Includes querystring
# Redirect to same path with requested language and without ?lang
# Get the language code picked based on the path
# 404 URLs without locale prefixes should remain 404s
# Convert locale prefix to the preferred case (en-us -> en-US)
# Fix special cases (cn -> zh-CN, zh-Hans -> zh-CN)
# Convert regional to generic locale prefix (fr-FR -> fr)
# Case-insensitive so FR-Fr also goes to fr
# Convert generic to regional locale prefix (pt -> pt-PT)
# Case-insensitive so PT -> pt-PT and En -> en-US
# Replace the 404 with a redirect to the fixed locale
# No language fixup found, return the 404
# Activate the language, and add LANGUAGE_CODE to the request.
# In Django, this is self.process_request()
# Kuma: assume locale-prefix patterns, including default language
# Maybe the language code is missing in the URL? Try adding the
# language prefix and redirecting to that URL.
# Insert language after the script prefix and before the
# rest of the URL
# Kuma: Add caching headers to redirect
# Only the homepage should be redirected permanently.
# Kuma: Do not add 'Accept-Language' to Vary header
# if not (i18n_patterns_used and language_from_path):
#    patch_vary_headers(response, ('Accept-Language',))
# Kuma: Add a pragma, since never skipped
# No views set this header, so the middleware always sets it. The code
# could be replaced with an assertion, but that would deviate from
# Django's version, and make the code brittle, so using a pragma
# instead. And a long comment.
# pragma: no cover
# If not 403, return response unmodified
# mindtouch_to_kuma_redirect matches everything.
# Check if it would return a redirect or 404.
# Remove the trailing slash for a valid URL
# Add a trailing slash for a valid URL
# HTTP_X_FORWARDED_FOR can be a comma-separated list of IPs.
# The client's IP will be the first one.
# Kuma: Do not allow an implied default language
# Assumed to be True in:
# kuma.core.i18n.activate_language_from_request
# kuma.core.middleware.LocaleMiddleware
# Add derived language tags to the end of the list as a fallback.
# Couldn't find any acceptable locale.
# Use partition instead of split since it always returns 3 parts
# Treat locale as a single-item ranked list.
# Get the page from the request, make sure it's an int.
# Get a page of results, or the first page if there's a problem.
# http://stackoverflow.com/a/24339946/571420
# in case the file doesn't exist or couldn't be parsed
# HACK: Speculatively re-fetching the original object makes me feel
# wasteful and dirty. But, I can't think of another way to get
# to the original field's value. Should be cached, though.
# see also - http://code.djangoproject.com/ticket/11663#comment:10
# Special case - if there are no commas or double quotes in the
# input, we don't *do* a recall... I mean, we know we only need to
# split on spaces.
# Defer splitting of non-quoted sections until we know if there are
# any unquoted commas.
# Find the matching quote
# If we were parsing an open quote which was never closed treat
# the buffer as unquoted.
# Eager mode and chords don't get along. So we serialize
# the tasks as a workaround.
# HACK: Build a hash of the fields that should be unique, let MySQL
# chew on that for a unique index. Note that any changes to this algo
# will create all new unique hashes that don't match any existing ones.
# Replace, don't append.
# Replace, don't append.
# Turn a date into a datetime
# Expecting datetime value
# Babel sometimes stumbles over missing formatters in some locales
# e.g. bug #1247086
# we fall back formatting the value with the default language code
# Check if the date is today
# Unknown format
#definitions
# Set the default values.
# Override the default values and/or add new ones.
#requests.adapters.HTTPAdapter
#urllib3.util.retry.Retry
# noqa
# This "if" statement is exactly what PyQuery's constructor does.
# We'll run it ourselves once and if it matches, "ruin" it by
# injecting that extra space.
# see also: http://github.com/tav/scripts/raw/master/validate_jsonp.py
# Placed into the Public Domain by tav <tav@espians.com>
# ------------------------------------------------------------------------------
# javascript identifier unicode categories and "exceptional" chars
# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# regex to find array[index] patterns
# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# javascript reserved words -- including keywords and null/boolean literals
# ------------------------------------------------------------------------------
# potentially reserved in a future version of the ES5 standard
# 'let', 'yield'
# ------------------------------------------------------------------------------
# the core validation functions
# ------------------------------------------------------------------------------
# A script to generate a template which will be used to localize the supported locale name
# For more information see https://bugzil.la/859499#c11
#c11\n")
# -*- coding: utf-8 -*-
# Generated by Django 1.11.23 on 2019-12-06 08:05
# This is needed otherwise `apps.get_model('waffle', 'Flag')`
# will raise a Django app LookupError.
# -*- coding: utf-8 -*-
# Generated by Django 1.11.23 on 2019-12-12 06:38
# This is needed otherwise `apps.get_model('waffle', 'Flag')`
# will raise a Django app LookupError.
# If the arguments ever include quoted arguments with spaces then
# the simple split() call here is not going to be good enough.
# Yanking filters from Django.
# Note: this activation does not make Django attempt to use xx-YY
# Note: if this starts to fail for no apparent reason, it's probably
# because babel learned about pt-BR since this test was written.
# first call is returning a KeyError as if the format is broken
# second call returns the English fallback version as expected
# Choose user with non default timezone
# Convert tzvalue to user timezone
# Simple Accept-Language headers, one term
# No preference gets default en-US
# Default en is en-US
# Exact match for default
# Case-insensitive match for default
# Overly-specified locale gets default
# Overly-specified match is case-insensitive
# Real-world Accept-Language headers include quality value weights
# English without region gets en-US
# Any English gets en-US
# Request for en-US gets en-US
# Exact match of non-English language
# Highest locale-specific match wins
# First generic match wins
# Generic Portuguese matches pt-PT
# Portuguese-Brazil matches
# Respect partial match on prefix
# No matches gets default en-US
# Traditional Chinese matches zh-TW
# Any-language case gets default
# Unknown in Accept-Language gets default
# General to locale-specific in different general locale
# General to locale-specific
# It does a case-insensitive comparison
# Country-specific to language-only
# It does a case-insensitive comparison
# Ensure that en redirects to en-US, case insensitive
# Django-preferred to Mozilla standard locale
# Underscore and capitalization fix
# LocaleMiddleware handles this case, and it's a 301 instead
# of a 302 since it's the homepage.
# Paths that were once valid, but now should 404, rather than get a second
# chance with a locale prefix.
# Subset of tests.headless.map_301.LEGACY_URLS
# Correct number of <li>s on page 1.
# Correct number of <li>s in the middle.
# Ensure the current page has 'class="selected"'.
# Check default locale is in the first choice field
#language.autosubmit option")[0].text_content()
#34;evil&amp;ness-field&#34;&gt;' in html
#34;evil&amp;ness-non-field&#34;&gt;' in html
# Sanity check
# Test int
# Invalid string
# Empty string
# Wrong type
# Note! the `mock_requests` fixture is just there to make absolutely
# sure the whole test doesn't ever use requests.get().
# My not setting up expectations, and if it got used,
# these tests would raise a `NoMockAddress` exception.
# Note! Since this file uses `__future__.unicode_literals` the only
# way to produce a byte string is to use force_bytes.
# Byte strings in should continue to work.
# Non-ascii as Unicode
# 'b'
# It supports the possibility of '.' being present in the callback name, e.g.
# As well as the pattern of providing an array index lookup, e.g.
# Check language cookie is set
# Check that the max-age from the cookie is the same as our settings
# No language cookie should be saved as `foo` is not a supported locale
# Setup conditions for adding analytics with a flag check
# Create minimal request
# Generate the 500 page
# Clean the slate.
# Django fails to clear this cache.
# Django 1.4 RequestFactory requests can't be used to test views that
# call messages.add (https://code.djangoproject.com/ticket/17971)
# FIXME: HACK from http://stackoverflow.com/q/11938164/571420
# In views.py
# In test_something.py
# Required for non-translations, which is
# enforced in Document.clean().
# models.py
# Here the "shared_cache_control" decorator is an optimization. It
# informs the CDN to cache the redirect for a week, so once this URL
# has been requested by a client, all other client requests will be
# redirected by the CDN instead of this Django service.
# Rounded to nearby 7-day period for weekly cycles
# Is it a false positive?
# current period begins
# Iterate over the daily stats
# Gather daily raw stats
# Regenerate stats if there are change attempts with
# needs_review marked and the data is stale.
# Accumulate trends over periods
# Sum up the items in day_events with any items that may
# already be in the Counter at trends[period_id]
# Prepare output data
# Calculate positive and negative rates
# Accumulate the spam viewer counts for the previous and
# current periods of this length.
# Define the start and end dates/datetimes.
# Gather recent published spam
# Document is new; document is a translation
# We only care about the spam rev and the one immediately
# following, if there is one.
# How long was it active?
# Gather table data
# Update the data with the number of viewers from Google Analytics.
# We can validate right away because no field is required
# Build up a dict of the filter conditions, if any, then apply
# them all in one go.
# these are messy but work with timedelta's seconds format,
# and keep the form and url arguments human readable
# use the form date if present, otherwise, offset from now
# Get the 'Known Authors' group.
# If the filter is 'Known Authors', then query for the
# 'Known Authors' group, otherwise the filter is
# 'Unknown Authors', so exclude the 'Known Authors' group.
# prefetch_related needs to come after all filters have been applied to qs
# Serve the response HTML conditionally upon request type
# Combine data sources
# Number of revisions per user in dashboard_revisions
# Revisions are in order, most recent first
#show_ips_btn')
# Deleted document has a "deleted" tag
# Attempt to see spam dashboard as a logged-in user without permissions
# Give testuser wiki.add_revisionakismetsubmission permission
# Give testuser wiki.add_documentspamattempt permission
# Give testuser wiki.add_userban permission
# With all correct permissions testuser is able to see the dashboard
# The first response will say that the report is being processed
# Create some revisions by self.testuser
# Mark each revision as created yesterday
# Mark each of self.testuser's revisions as spam
# self.admin creates some revisions on a different document
# The first response will say that the report is being processed
# The first response will say that the report is being processed
# Period length
# Dates
# Revisions made by self.testuser: 3 made today, 3 made 3 days ago,
# 3 made 10 days ago, 3 made 35 days ago, 3 made 100 days ago
# Published spam by self.testuser
# Summary of spam submissions
# All of the spam_revs were published and then marked as spam
# Summary of self.testuser's ham submissions
# There were 2 correctly blocked spam attempts 3 days ago (within past week)
# There was 1 incorrectly blocked spam attempt 3 days ago
# The spam from 3 days ago was seen 3 times, from 10 days ago see 10 times,
# and from 35 days ago seen 35 times
# The mock Google Analytics return values for page views
# The first response will say that the report is being processed
# These are the columns in the spam dashboard spam trends table
# The periods are identified as 'Daily', 'Weekly', 'Monthly', 'Quarterly'
# The start dates for each period are correct
# The page views during the week, month, quarter
# The percentage change in spam viewers
# The spam viewers
# The daily average of spam viewers
# The published spam: 1 this week, 2 this month, 3 this quarter
# The blocked spam: there were 2 correctly blocked spam attempts 3 days ago
# The blocked ham: there was 1 incorrectly blocked spam attempt 3 days ago
# The true positive rate == blocked_spam / total spam
# The true negative rate == published ham / total ham
# Refresh Hacks Blog: every 10 minutes
# HTTP Headers
# If a feed has (severe) issues, it will be disabled
# This doesn't perform extremely well, but it's what we have to do
# to keep exactly `n` entries around, as LIMIT is invalid in a
# DELETE statement.
# Feed entry updated field
# Remove old entries if applicable.
# TODO: Should the feed be processed this round as well?
# We've got a live one...
# Reset disabled status.
# Did the entry change?
# Setup logging
# Setup fetch
# Fetch feeds
# Generated by Django 1.11.23 on 2019-10-23 04:05
# Check that entries are ordered oldest first
# None deleted
#1",
# URL of the Hacks blog RSS 2.0 feed
# A truncated result from parsing the Hacks blogs with feedparser
# Omited attributes: encoding, headers, namespaces
# Omited attributes: author_detail, authors, content, guidislink,
# links, summary_detail, tags, title_detail, comments, slash_comments,
# wfw_commentrss
# Omited attributes: generator, generator_detail, language, links,
# subtitle_detail, sy_updatefrequency, sy_updateperiod, title_detail,
# Won't be saved in enabled case
# Just one entry
# Is latest
# Confirm that we can use the database by making a fast query
# against the Document table. It's not important that the document
# with the requested primary key exists or not, just that the query
# completes without error.
# Check that database is reachable, populated
# Check that KumaScript is reachable
# Check that ElasticSearch is reachable, populated
# available but unpopulated (and maybe uncreated)
# Check if the testing accounts are available
# All users have the testing password
# mitigation option for a flood of violation reports
# Cannot decode CSP violation data, ignore
# Incomplete CSP report
# Normalize to docker development settings
# Need for both wiki and react homepage
# The default template name
#home-search-form input[type=hidden]')
# Subset of data returned for customer
# https://stripe.com/docs/api/customers/retrieve
# Redirects/rewrites/aliases migrated from SCL3 httpd config
# RewriteRule ^/media/(redesign/)?css/(.*)-min.css$
# /static/build/styles/$2.css [L,R=301]
# RewriteRule ^/media/(redesign/)?js/(.*)-min.js$ /static/build/js/$2.js
# [L,R=301]
# RewriteRule ^/media/(redesign/)?img(.*) /static/img$2 [L,R=301]
# RewriteRule ^/media/(redesign/)?css(.*) /static/styles$2 [L,R=301]
# RewriteRule ^/media/(redesign/)?js(.*) /static/js$2 [L,R=301]
# RewriteRule ^/media/(redesign/)?fonts(.*) /static/fonts$2 [L,R=301]
# RedirectMatch 302 /media/uploads/demos/(.*)$
# https://developer.mozilla.org/docs/Web/Demos_of_open_web_technologies/
# Django will then redirect based on Accept-Language
# RewriteRule ^(.*)//(.*)//(.*)$ $1_$2_$3 [R=301,L,NC]
# RewriteRule ^(.*)//(.*)$ $1_$2 [R=301,L,NC]
# The remaining redirects don't show explicit RewriteRule as comments,
# as they're all in the style of "static URL A now points at static URL B"
# Bug 1078186 - Redirect old static canvas examples to wiki pages
# canvas tutorial
#Rectangular_shape_example',
#Moving_the_pen',
#Lines',
#Arcs',
#Quadratic_Bezier_curves',
#Cubic_Bezier_curves',
#Drawing_images',
#Example.3A_Tiling_an_image',
#Example.3A_Framing_an_image',
#Art_gallery_example',
#An_example_using_rgba()',
#A_lineWidth_example',
#A_createLinearGradient_example',
#A_createRadialGradient_example',
#A_save_and_restore_canvas_state_example',
#Clipping_paths',
##################################
# MOZILLADEMOS
##################################
# canvas images
# canvas example in samples/domref
##################################
# MDN.GITHUB.IO
##################################
# canvas raycaster
# Bug 1215255 - Redirect static WebGL examples
# Bug 887428 - Misprinted URL in promo materials
# RewriteRule ^Firefox_OS/Security$ docs/Mozilla/Firefox_OS/Security
# [R=301,L,NC]
# Old landing pages. The regex, adapted from Bedrock, captures locale prefixes.
# RewriteRule ^(\w{2,3}(?:-\w{2})?/)?mobile/?$ /$1docs/Mozilla/Mobile
# [R=301,L]
# RewriteRule ^(\w{2,3}(?:-\w{2})?/)?addons/?$ /$1Add-ons [R=301,L]
# RewriteRule ^(\w{2,3}(?:-\w{2})?/)?mozilla/?$ /$1docs/Mozilla [R=301,L]
# RewriteRule ^(\w{2,3}(?:-\w{2})?/)?web/?$ /$1docs/Web [R=301,L]
# RewriteRule ^(\w{2,3}(?:-\w{2})?/)?learn/html5/?$
# /$1docs/Web/Guide/HTML/HTML5 [R=301,L]
# Some blanket section moves / renames
# RewriteRule ^En/JavaScript/Reference/Objects/Array$
# en-US/docs/JavaScript/Reference/Global_Objects/Array [R=301,L,NC]
# RewriteRule ^En/JavaScript/Reference/Objects$
# en-US/docs/JavaScript/Reference/Global_Objects/Object [R=301,L,NC]
# RewriteRule ^En/Core_JavaScript_1\.5_Reference/Objects/(.*)
# en-US/docs/JavaScript/Reference/Global_Objects/$1 [R=301,L,NC]
# RewriteRule ^En/Core_JavaScript_1\.5_Reference/(.*)
# en-US/docs/JavaScript/Reference/$1 [R=301,L,NC]
# RewriteRule ^([\w\-]*)/HTML5$ $1/docs/HTML/HTML5 [R=301,L,NC]
# RewriteRule web-tech/2008/09/12/css-transforms
# /docs/CSS/Using_CSS_transforms [R=301,L]
# RewriteRule ^([\w\-]*)/docs/?$ $1/docs/Web [R=301,L,NC]
# DevNews
# RewriteRule ^(\w{2,3}(?:-\w{2})?/)?devnews/index.php/feed.*
# https://blog.mozilla.org/feed/ [R=301,L]
# RewriteRule ^(\w{2,3}(?:-\w{2})?/)?devnews.*
# https://wiki.mozilla.org/Releases [R=301,L]
# Old "Learn" pages
# RewriteRule ^(\w{2,3}(?:-\w{2})?/)?learn/html /$1Learn/HTML [R=301,L]
# TODO: new path '/docs/Learn/HTML',
# RewriteRule ^(\w{2,3}(?:-\w{2})?/)?learn/css /$1Learn/CSS [R=301,L]
# TODO: new path '/docs/Learn/CSS',
# RewriteRule ^(\w{2,3}(?:-\w{2})?/)?learn/javascript /$1Learn/JavaScript
# [R=301,L]
# TODO: new path '/docs/Learn/JavaScript',
# RewriteRule ^(\w{2,3}(?:-\w{2})?/)?learn /$1Learn [R=301,L]
# TODO: new path '/docs/Learn',
# BananaBread demo (bug 1238041)
# RewriteRule ^(\w{2,3}(?:-\w{2})?/)?demos/detail/bananabread$
# https://github.com/kripken/BananaBread/ [R=301,L]
# RewriteRule ^(\w{2,3}(?:-\w{2})?/)?demos/detail/bananabread/launch$
# https://kripken.github.io/BananaBread/cube2/index.html [R=301,L]
# All other Demo Studio and Dev Derby paths (bug 1238037)
# RewriteRule ^(\w{2,3}(?:-\w{2})?/)?demos
# /$1docs/Web/Demos_of_open_web_technologies? [R=301,L]
# Legacy off-site redirects (bug 1362438)
# RewriteRule ^contests/ http://www.mozillalabs.com/ [R=302,L]
# RewriteRule ^es4 http://www.ecma-international.org/memento/TC39.htm [R=302,L]
# NOTE: The redirect for the case when there is no locale for a zone
# must be handled here, because if we let LocaleMiddleware handle the
# 404 response and redirect to the proper locale, the path would be
# considered invalid.
#distributing-your-addon'),
# Needed information about the supported Django models for fixtures
# The key is the app_label.model_name, such as wiki.revision for Revision:
#  Revision._meta.app_label == 'wiki'
#  Revision._meta.model_name == 'revision'
# The value is a dictionary of:
# - natural_key: Properties used to find existing database records
# - relations: Details of properties that are foreign keys
# - filters: Methods to run on values before saving to the database
# Parse and validate the model
# Parse and validate the natural key
# Parse and validate the remaining properties
# Check for required relations in the natural key
# Check for required relations in other properties
# Prepare the items in the key
# Prepare the other properties
# Get or create the new instance. Set the non-relation properties
# when creating an instance.
# For existing instances, set the properties
# Set the relations for new and existing instances
# Scrape progress report patterns
# Always run it once
# Run another round if there are new sources to scrape
# Stop if we're stuck on a blocked dependency
# Iterate over existing sources, starting with new dependencies
# If terminal condition, no processing to do
# Gather dependent sources
# Detect unfinished work and report on changed state (in debug)
# Add new sources
# It looks like nothing changed state this round, so we have
# a blocked dependency and won't finish.
# With ca/docs/Project:Quant_a, no document is found with the
# locales, slug or ID, but an IntegrityError is raised due to an
# ID collision when created. It will work as an update on the
# second pass.
# Don't make current rev
# Manually add tags, to avoid issues with adding two 'duplicate'
#  tags, like 'Firefox' and 'firefox'
# is_approved will update the document, avoid for old revisions
# Add review, localization tags
# Approve old revisions w/o making them current
# Load fixtures, which may include flags and settings
# Scrape data from MDN
# default
# Scraping states
# Freshness of scraped data
# Friendly name of the source's key parameter
# Types of source options. Used when merging options, to determine if
# the new or existing option value should win, possible resetting scraping.
# True > False
# 2 > 1 > 0
# 'all' > 2 > 0
# any new value > old value > ''
# The scrape options for this source, defaulting to no valid settings
# Format is name -> (option_type, default value)
# If possible, load and validate existing data for the source
# Save the data and load follow-on sources
# Return the additional prerequisite sources
# Load no more sources in a "done" state
# Standard options for Document-based sources
# Update existing Document records
# Scrape the topic tree to this depth
# Scrape this many past revisions
# Scrape the alternate translations
# Load data, gathering further source needs
# No parent to load
# Not a redirect, don't follow
# Load the destination page
# Redirects don't have UUIDs
# No English parent for English docs
# Metadata not loaded yet
# For translations - have we loaded the English document?
# Prepare data for a redirect document
# Prepare data for a full document
# Scrape the topic tree to this depth
# Gather this many revisions for each doc
# Scrape the alternate translations
# Scrape this many past revisions
# Document has a current revision, we're done
# Are all the requested revisions loaded?
# Wait for revisions to load
# Ask for one more revision for the document
# The common case is that the second-to-most-recent revision is the
# current revision (just before a page move)
# Pre-request the next revision, to help avoid dependency block
# Pre-request 2x revisions, to avoid dependency block detection
# and avoid Shlemiel the painter's algorithm.
# Scrape this many past revisions
# If translation, there may be an entry for the English source
# Is this a redirect?
# Scrape the topic tree to this depth
# Scrape this many past revisions
# Scrape the alternate translations
# Default to English homepage
# Strip trailing slashes
# Skip anchors and non-absolute links
# The URLAbsolutionFilter should convert to absolute links
# Skip API endpoints
# Skip other locales, non-translated pages, and the homepage
# Skip known non-wiki documents
# Revision this is based on
# Verbose mode
# Locale, like en-US
# literal '/docs/'
# Slug, like Mozilla/Firefox
# literal '$revision/'
# Revision ID
# Load document, using pre-loaded if available
# Load document metadata
# Load revision HTML
# Load revision creator
# Load revision this is based on
# Parse revision-info list
# Parse tags
# Revision content
#doc-source pre')[0]
# Update existing User records
# Scrape social links
# Set the email for the User
# Avoid cached config value
#
# Requester tests (mostly for coverage)
#
# Timeout doubles
# Timeout doubles again
# 1, 2, 4, 8...
#
# Tests for Scraper
#
# Number of gather rounds until done
# Generations of sources emitted
# Set state to STATE_ERROR
#Anchor_\u2014_With_Dash', 'Slug#Anchor_\u2014_With_Dash'),
# Basic metadata for a Document
# Omitted: json_modified, label, last_edit, etc.
# The data passed to Storage.save_document for this metadata
# Standard doc
# Empty for now
# No history
# Storage is skipped
# Standard doc
# Standard doc
# Standard doc
# Standard doc
# Better diff
# Standard doc
# Standard doc
# Partial data from
# https://wiki.developer.mozilla.org/en-US/docs/Web/Guide/HTML/HTML5$children
# Full child data
# Minimum data actually used
# Partial meta from
# /en-US/docs/Learn/Getting_started_with_the_web$json
# The exact list of resources will change as the homepage changes
# These appear on the homepage, but shouldn't be in the paths to scrape
#later">Later in this page.</a></li>
#later" not in path
# First pass, document is not available from storage (None twice)
# Second pass, document is available from storage
# Missing review and localization tags if it is not the current revision
# First call is for this revision, return None to scrape
# Second call is for the based_on revision, return it
# Register signal handlers
# noqa
# Configure Elasticsearch connections for connection pooling.
# Given a list of [<group_slug>, <tag_slug>, <shortcut>] we only want
# the tags.
# Note: Here we are replacing the query rather than calling
# `queryset.query` which would result in a boolean must query.
# (<query type>, <field>, <boost factor>)
# Skip if not given, or just wildcard
# Exact match of sanitized value
# Wildcard search of value as passed
# Incomplete filter has no tags, skip it
# User selected this filter - filter on the associated tags
# Add an AND filter as a subclause
# Extend list of tags for the OR clause
# Aggregate counts for active filters for sidebar
# Count documents across all tags
# Filter by tag only after counting documents across all tags
# Simultaneously create the index and the mappings, so live
# indexing doesn't get a chance to index anything between the two
# causing ES to infer a possibly bogus mapping (which causes ES to
# freak out if the inferred mapping is incompatible with the
# explicit mapping).
# Can ignore this since it indicates the index doesn't exist
# and therefore there's nothing to delete.
# Index all outdated documents to this index.
# Clear outdated.
# Promote this index.
# Allow only a single index to be promoted.
# Anything >=1,000 will result in a hard error in
# Elasticsearch which would happen before we even get a chance
# to validate that the range is too big. The error you would
# get from Elasticsearch 6.x is something like this:
#
#     Result window is too large, from + size must be less
#     than or equal to: [10000] but was [11000].
#
# See https://github.com/mdn/kuma/issues/6092
# Force the search to evaluate and then attach the count. We want to
# avoid an extra useless query even if there are no results, so we
# directly fetch the count from hits.
# Set the count to the results after post_filter
# Also store the aggregations, if any.
# Now that we have the count validate that the page number isn't higher
# than the possible number of pages and adjust accordingly.
#: list of filters to applies in order of listing, each implementing
#: the specific search feature
# Stash some data here for the serializer.
# Let's check if we can get the name from the gettext catalog
# return a sorted list of filters here
# Advanced search query paramenters.
# Check that \n not in query
# Pass the entire object through to `to_representation()`,
# instead of the standard attribute lookup.
# Note: "HTTP_REFERER" is spelled wrong in the spec and we use "referer"
# here to mirror that.
# Non-ASCII referers can be problematic.
# TODO: The 'ftfy' library can probably fix these, but may not be
# worth the effort.
# The referer url must be an MDN search--if not, then we return None.
# We verify the protocol, host and path.
# Check it if exists already. If so, delete.
# Disable automatic refreshing and replicas.
# Optimize.
# Update the settings.
# Update the `Index` object and mail admins.
# set all items with an empty value to an empty string
# make sure the parameter name and value aren't empty
# Since the search endpoint accepts user input (via query parameters) and its
# response is compressed, use rate limiting to mitigate the BREACH attack
# (see http://breachattack.com/). It still needs to allow a user to click
# the filter switches (bug 1426968).
# Alternate: forbid gzip by setting Content-Encoding: identity
# Determine if there were validation errors
# If q is returned in the data, there was a validation error for that field,
# so return 400 status.
# Generated by Django 1.11.23 on 2019-10-02 05:13
# Generated by Django 1.11.23 on 2019-10-23 04:05
# Testing version of named groups of tags for filtering
# This is serialized from the database in .views.SearchView.initial
# Test data from fixtures/search/filters.json
# Topic CSS
# Topic Add-ons & Extensions
# A FilterGroup with no tags (yet? to be deleted?)
# A FilterGroup with an 'AND' combination of two terms
# TODO: This isn't used in production, disallow AND
# TagGroupFilterBackend tests require these
# Update a document so that it has a `css_classname` and trigger a
# reindex via `render_done`.
# first create and populate an index
# then create a successor and render a document against the old index
# .populate() creates the index and populates it.
# Promotion reindexes outdated documents. Test that our change is
# reflected in the index.
# first create and populate the index
# then delete it and check if recreating works without blowing up
# APIRequestFactory doesn't actually return APIRequest objects
# but standard HttpRequest objects due to the way it initializes
# the request when APIViews are called
# In order to test this we just need an object that has
# 'locale' and 'META', but not the full attribute set of
# an HttpRequest. This is that object.
# FIXME: These tests aren't great because we can't verify exactly why we
# got a None so we can't distinguish between "right answer" and "right
# answer, but for the wrong reasons".
# Live indexing uses tasks which pass the index by pk, so we need to
# create and save one to the database here.
# TODO: Investigate this test failure. The ES debug output appears to
# be doing the correct thing but the ES delete call is returning a 404.
# bug 930300
# the filters are deduplicated
# queryset content
# metadata
# aggregations
# It's a bit odd that an invalid query string parameter
# causes a 404 Not Found but that's how the paginator for
# rest_framework works.
# Restore old setting.
# Any time we're doing a refresh, we're making sure that the
# index is ready to be queried.  Given that, it's almost
# always the case that we want to run all the generated tasks,
# then refresh.
# Ignore indices that do not exist.
# setting request.LANGUAGE_CODE correctly
# Django settings for kuma project.
# BASE_DIR used by django-extensions, such as ./manage.py notes
# ROOT used by some Kuma application code
# CONN_MAX_AGE: 'persistent' to keep open connection, or max seconds before
# releasing. Default is 0 for a new connection per request.
# These are the production settings for OPTIONS.
# As of django-recaptcha==2.0.4 it checks that you have set either
# settings.RECAPTCHA_PRIVATE_KEY or settings.RECAPTCHA_PUBLIC_KEY.
# If you haven't it assumes to use the default test keys (from Google).
# We don't set either of these keys so they think we haven't thought
# about using real values. However, we use django-constance for this
# and not django.conf.settings so the warning doesn't make sense to us.
# Cache Settings
# in seconds
# Email
# Ensure EMAIL_SUBJECT_PREFIX has one trailing space
# Addresses email comes from
# Local time zone for this installation. Choices can be found here:
# http://en.wikipedia.org/wiki/List_of_tz_zones_by_name
# although not all choices may be available on all operating systems.
# If running in a Windows environment this must be set to the same as your
# system time zone.
# Language code for this installation. All choices can be found here:
# http://www.i18nguy.com/unicode/language-identifiers.html
# Accepted locales.
# The order of some codes is important. For example, 'pt-PT' comes before
# 'pt-BR', so that 'pt-PT' will be selected when the generic 'pt' is requested.
# Candidate locales should be included here and in CANDIDATE_LOCALES
# English
# Arabic
# Bulgarian
# Bambara
# Bengali
# Catalan
# German
# Greek
# Spanish
# Persian
# Finnish
# French
# Hebrew
# Hindi (India)
# Hungarian
# Indonesian
# Italian
# Japanese
# Kabyle
# Korean
# Malay
# Burmese
# Dutch
# Polish
# Portuguese (Portugal)
# Portuguese (Brazil)
# Russian
# Swedish (Sweden)
# Thai
# Turkish
# Ukranian
# Vietnamese
# Chinese (China)
# Chinese (Taiwan, Province of China)
# When there are multiple options for a given language, this gives the
# preferred locale for that language (language => preferred locale).
# Locales being considered for MDN. This makes the UI strings available for
# localization in Pontoon, but pages can not be translated into this language.
# https://developer.mozilla.org/en-US/docs/MDN/Contribute/Localize/Starting_a_localization
# These should be here and in the ACCEPTED_LOCALES list
# Asserted here to avoid a unit test that is skipped when empty
# Override generic locale handling with explicit mappings.
# Keys are the requested locale (lowercase); values are the delivered locale.
# Create aliases for over-specific locales.
# Create aliases for locales which use region subtags to assume scripts.
# Map locale whose region subtag is separated by `_`(underscore)
# Language list sorted for forms (English, then alphabetical by locale code)
# List of MindTouch locales mapped to Kuma locales.
#
# Language in MindTouch pages are first determined from the locale in the page
# title, with a fallback to the language in the page record.
#
# So, first MindTouch locales were inventoried like so:
#
#     mysql --skip-column-names -uroot wikidb -B \
#           -e 'select page_title from pages  where page_namespace=0' \
#           > page-titles.txt
#
#     grep '/' page-titles.txt | cut -d'/' -f1 | sort -f | uniq -ci | sort -rn
#
# Then, the database languages were inventoried like so:
#
#     select page_language, count(page_id) as ct
#     from pages group by page_language order by ct desc;
#
# Also worth noting, these are locales configured in the prod Control Panel:
#
# en,ar,ca,cs,de,el,es,fa,fi,fr,he,hr,hu,it,ja,
# ka,ko,nl,pl,pt,ro,ru,th,tr,uk,vi,zh-cn,zh-tw
#
# The Kuma side was picked from elements of the MDN_LANGUAGES list in
# settings.py, and a few were added to match MindTouch locales.
#
# Most of these end up being direct mappings, but it's instructive to go
# through the mapping exercise.
# The number of seconds we are keeping the language preference cookie. (1 year)
# If you set this to False, Django will make some optimizations so as not
# to load the internationalization machinery.
# Absolute path to the directory that holds media.
# Example: "/home/media/media.lawrence.com/"
# URL that handles the media served from MEDIA_ROOT. Make sure to use a
# trailing slash if there is a path component (optional in other cases).
# Examples: "http://media.lawrence.com", "http://example.com/media/"
# Serve diagrams, presentations, and samples from 2005-2012
# Paths that don't require a locale prefix.
# Legacy files, circa 2008, served in AWS
# Legacy files, circa 2008, now return 404
# Legacy MediaWiki endpoint, return 404
# Served in AWS
# Make this unique, and don't share it with anybody.
#%tc(zja8j01!r#h_y)=hy!^k)9az74k+-ib&ij&+**s3-e^_z')
# must come before LocaleMiddleware
# LocaleMiddleware must be before any middleware that uses
# kuma.core.urlresolvers.reverse() to add locale prefixes to URLs:
# We don't want this in maintence mode, as it adds "Cookie"
# to the Vary header, which in turn, kills caching.
# For more config, see "Content Security Policy (CSP)" below
# Prints heavy query counts per request.
# Auth
# Handles User Bans
# Legacy
# Avatar needs a publicly available default image
# TODO: Figure out why changing the order of apps (for example, moving taggit
# higher in the list) breaks tests.
# django
# must be before kuma.wiki, or RemovedInDjango19Warning
# MDN
# util
# other
# Feed fetcher config
# in seconds
# Use jinja2/ for jinja templates
# Don't figure out which template loader to use based on
# file extension
# Tells the extract script what files to look for l10n in and what function
# handles the extraction.
# We can't say **.js because that would dive into any libraries.
# Combine JavaScript strings into React domain
# Cache non-versioned static files for one week
# Combines the mdn, wiki and wiki-compat-tables styles into
# one bundle for use by pages that are part of the new
# single page app.
# for maintenance mode page
# global maintenance-mode-styles
# embeded iframe for live samples
# Locales that are well supported by the Zilla family
# TODO: these are the last legacy files from the wiki site
# that we're still using on the React-based pages. Ideally
# we should just move these to the React code so webpack
# can deal with them.
# Custom Prism build
# TODO: the prism.js file should be imported dynamcally
# when we need it instead of being hardcoded in here.
# The react.js file is created by webpack and
# placed in the kuma/javascript/dist/ directory.
# Custom Prism build
# django-pipeline issue #614
# Pipeline compressor overrides
# For example, PIPELINE_YUGLIFY_BINARY will set YUGLIFY_BINARY
# https://django-pipeline.readthedocs.io/en/latest/compressors.html
# Session cookies
# This is a setting unique to Kuma which specifies the domain
# that will be used for all of the waffle cookies. It is used by
# kuma.core.middleware.WaffleWithCookieDomainMiddleware.
# bug 856061
# This should never be false for the production and stage deployments.
# Allow robots, but restrict some paths
# If the domain is a CDN, the CDN origin should be included.
# Allow robots, no path restrictions
# If the domain is a CDN, the CDN origin should be included.
# Allowed iframe URL patterns
# The format is a three-element tuple:
#  Protocol: Required, must match
#  Domain: Required, must match
#  Path: An optional path prefix or matching regex
# Default allowed iframe URL patterns, roughly ordered by expected frequency
# Live sample host
# https://developer.mozilla.org/en-US/docs/Web/CSS/filter
# Interactive Examples host
# On https://developer.mozilla.org/en-US/docs/Web/CSS/filter
# Samples, https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API/Tutorial/Getting_started_with_WebGL
# Videos, https://developer.mozilla.org/en-US/docs/Tools/Web_Console
# Samples, https://developer.mozilla.org/en-US/docs/Web/JavaScript/Closures
# Charts, https://developer.mozilla.org/en-US/docs/MDN/Kuma/Server_charts
# Test262 Report, https://test262.report/
# Add the overridden attachment / live sample host
# Add the overridden interactive examples service
# Add more iframe patterns from the environment
# Allow all iframe sources (for debugging)
# Email
# Content Security Policy (CSP)
# TODO fix things so that we don't need this
# TODO fix things so that we don't need this
# Using sentry to report. Optionally add revision and environment
# Celery (asynchronous tasks)
# Maximum tasks run before auto-restart of child process,
# to mitigate memory leaks. None / 0 means unlimited tasks
# Sadly, kuma depends on pickle being the default serializer.
# In Celery 4, the default is now JSON.
# It's probably too late to switch all tasks to work with either.
# Just remember, avoid passing types that are non-trivial and is
# different in pickle vs json. Keep things simple. Even if it means
# you have to do type conversions in the tasks' code.
# Do not change this without also deleting all wiki documents:
# must be an entry in the CACHES setting!
# Settings and defaults controllable by Constance in admin
# Staging demos
# Production demos
# Unit test demos
# Docker development demos
# Embedded samples server
# MDN/Kuma/Server_charts
# Embedded videos
# Embedded samples
# Embedded samples
# Embedded samples
# TODO: Delete this line once we know that the production environment
# definitely has 'GOOGLE_ANALYTICS_ACCOUNT' set.
# See https://bugzilla.mozilla.org/show_bug.cgi?id=1570076
# Google Analytics Tracking Account Number (0 to disable)
# Elasticsearch related settings.
# Specify the extra timeout in seconds for the indexing ES connection.
# Specify a max length for the q param to avoid unnecessary burden on
# elasticsearch for queries that are probably either mistakes or junk.
# Logging is merged with the default logging
# https://github.com/django/django/blob/stable/1.11.x/django/utils/log.py
# Drop mail_admins
# We need to explcitly set the trusted origins, because when CSRF_COOKIE_DOMAIN
# is explicitly set, as we do above, Django's CsrfViewMiddleware will reject
# the request unless the domain of the incoming referer header matches not just
# the CSRF_COOKIE_DOMAIN alone, but the CSRF_COOKIE_DOMAIN with the server port
# appended as well, and we don't want that behavior (a server port of 8000 is
# added both in secure local development as well as in K8s stage/production, so
# that will guarantee a mismatch with the referer).
# Set header X-XSS-Protection: 1; mode=block
# Set header X-Content-Type-Options: nosniff
# Set header Strict-Transport-Security header
# 63072000 in production (730 days)
# Honor the X-Forwarded-Proto header, to assume HTTPS instead of HTTP
# Auth and permissions related constants
# django-allauth configuration
# forces the use of the signup view
# used by the custom github provider
# Sync transport
# Loaded from environment for CSP reporting endpoint
# Tell django-taggit to use case-insensitive search for existing tags
# Ad Banner Settings
# Newsletter Signup Settings
# Whether or not to enable the BCD signalling feature.
# Affects loading of CSS (statically) and JS (in runtime).
# Enable or disable the multi auth(Google and Github) sign-in flow
# When disabled, Github will be the default and only Auth provider
# Content Experiments
# Must be kept up to date with PIPELINE_JS setting and the JS client-side
#  configuration. The 'id' should be a key in PIPELINE_JS, that loads
#  Traffic Cop and a client-side configuration like
#  kuma/static/js/experiment-wiki-content.js
# Only one experiment should be active for a given locale and slug.
#
# django-ratelimit
# Caching constants for the Cache-Control header.
# Stripe API KEY settings
# Settings used for communication with the React server side rendering server
# Setting for configuring the AWS S3 bucket name used for the document API.
# Serve and upload attachments via S3, instead of the local filesystem
# AWS S3 credentials and settings for uploading attachments
# For example, Cloudfront CDN domain
# Does the custom domain use TLS
# Silence warnings about defaults that change in django-storages 2.0
# When we potentially have multiple CDN distributions that do different
# things.
# Inside kuma, when a document is considered "changed", we trigger
# worker tasks that do things such as publishing/unpublishing to S3.
# Quite agnostic from *how* that works, this list of distributions,
# if they have an 'id', gets called for each (locale, slug) to
# turn that into CloudFront "paths".
# Note that the 'id' is optional because its ultimate value might
# or not might not be in the environment.
# TODO We should have a (Django) system check that checks that this
# transform callable works. For example, it *has* to start with a '/'.
# TODO We should have an entry here for the existing website.
# At the time of writing we conservatively set the TTL to 5 min.
# If this CloudFront invalidation really works, we can bump that 5 min
# to ~60min and put configuration here for it too.
# We use django-cacheback for a bunch of tasks. By default, when cacheback,
# has called the `.fetch` of a job class, it calls `cache.set(key, ...)`
# and then it immediately does `cache.get(key)` just to see that the `.set`
# worked.
# See https://bugzilla.mozilla.org/show_bug.cgi?id=1567587 for some more
# details about why we don't want or need this.
# Write down the override location for where DB migrations for third-party
# Django apps should go. This is relevant if an app we depend on requires
# new migrations that aren't in the released upstream package.
# One good example is: https://github.com/ubernostrum/django-soapbox/issues/5
# noqa
# Settings for Docker Development
# TODO: Use environment to override, not settings picker
# Default DEBUG to True, and recompute derived settings
# Elasticsearch related settings.
# Don't cache non-versioned static files in DEBUG mode
# noqa
# Email
# Cache
# noqa
# Enable recording of templates
# Disable the Constance database cache
# SHA1 because it is fast, and hard-coded in the test fixture JSON.
# Use the local memory cache in tests, so that test cacheback jobs
# will expire at the end of the test.
# Change the cache key prefix for tests, to avoid overwriting runtime.
# Use un-versioned file names, like main.css, instead of versioned
# filenames requiring hashing, like mdn.1cb62215bf0c.css
# Switch Pipeline to DEBUG=False / Production values
# The documents claim True means assets should be compressed, which seems like
# more work, but it is 4x slower when False, maybe because it detects the
# existence of the file and skips generating a new one.
# The documents suggest this does nothing when PIPELINE_ENABLED=True. But,
# testing shows that tests run faster when set to True.
# We need the real Sass compiler here instead of the pass-through used for
# local dev.
# Testing with django-pipeline 1.6.8, PipelineStorage
# Enabled=T, Collector=T -   482s
# Enabled=T, Collector=F -   535s
# Enabled=F, Collector=T - 18262s
# Enabled=F, Collector=F -  2043s
# Defer to django-pipeline's finders for testing
# This avoids reading the static folder for each test client request, for
# a 10x speedup on Docker on MacOS.
# This makes sure we our tests never actually use the real settings for
# this.
# Never rely on the .env
# Silence warnings about defaults that change in django-storages 2.0
# Use a dedicated minio bucket for tests
# Never enabled in tests.
# noqa
# See the following URL on why we set num_shards to 1 for tests:
# http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/relevance-is-broken.html
# Warning: This is the documented success response from the Akismet API
# Do not translate! Info: http://akismet.com/development/api/#submit-spam
# number of total retries
# retry once in case we can't connect to Akismet
# retry once in case we can't read the response from Akismet
# retry once in case we're redirect by Akismet
# definitely retry if Akismet is unwell
# blog is the only parameter required by all API endpoints
# quick bailout when the key isn't given
# We'll catch requests' exceptions here since we can't assume
# that the client will work if something went wrong during
# verification. We'll retry later by returning right away.
# In case we did get a response from Akismet we check if
# the response matches one of our expected results.
# Otherwise we assume it was not verified correctly.
#comment-check
# super method called
# We have stricter username requirements than django-allauth,
# because we don't want to allow '@' in usernames. So we check
# that before calling super() to make sure we catch those
# problems and show our error messages.
# let's ignore some messages
# promote the "account_connected" message to success
# when a next URL is set because of a multi step sign-in
# (e.g. sign-in with github, verified mail is found in other
# social accounts, agree to first log in with other to connect
# instead) and the next URL is not the edit profile page (which
# would indicate the start of the sign-in process from the edit
# profile page) we ignore the message "account connected" message
# as it would be misleading
# Bug 1229906#c2 - need from "create new account" page
# and add an extra tag to the account messages
# pragma: no cover
# commit will be True, unless extended by a derived class
# bug 1291892: Don't confuse next login with connecting accounts
# pragma: no cover
# Is there already a sociallogin_provider in the session?
# If the provider in the session is different from the provider in the
# request, the user is connecting a new provider to an existing account
# Does the request sociallogin match an existing user?
# go straight back to signup page with an error message
# BEFORE allauth over-writes the session sociallogin
# Is the user banned?
# sociallogin_provider is used in the UI to indicate what method was
# used to login to the website. The session variable
# 'socialaccount_sociallogin' has the same data, but will be dropped at
# the end of login.
# We have to call get_existing_user() again. The result of the earlier
# call (within the is_auto_signup_allowed() method), can't be cached as
# an attribute on the instance because a different instance of this
# class is used when calling this method from the one used when calling
# is_auto_signup_allowed().
# We can re-use an existing user instead of creating a new one.
# Let's guarantee this user has an unusable password, just in case
# we're recovering an old user that has never had this done before.
# This associates this new social account with the existing user.
# Since the "connect" call above does not add any email addresses
# from the social login that are missing from the user's current
# associated set, let's add them here.
# Now that we've successfully associated a GitHub/Google social
# account with this existing user, let's delete all of the user's
# associated Persona social accounts (if any). Users may have
# multiple associated Persona social accounts (each identified
# by a unique email address).
# pragma: no cover
# Users can have multiple associated EmailAddress objects, so
# let's use "distinct()" to remove any duplicate users.
# For now, we're only going to return a user if there's only one.
# Connect signal handlers
# noqa
# in case the username is not changed and the user has a legacy
# username we want to disarm the username regex
# bug 709938 - don't assume interests passed validation
# Gather matching active users
# Users using email as the primary contact email
# Users with a matching Persona account
# Users with that confirmed email
# Send one account recovery email to each matching user
# TODO figure out why this isn't a .delay() call.
# https://bugzilla.mozilla.org/show_bug.cgi?id=1544925
# Note the deliberate omission of the `choices=` here.
# That's because there's no good way to list all possible
# timezones as a 2-D tuple. The *name* of the timezone rarely
# changes but the human-friendly description of it easily does.
# a bunch of user URLs
# only send if the user has already verified
# at least one email address
# only send if the user has exactly one verified (the given)
# email address, in other words if it was just confirmed
# This may raise an exception if the Stripe API call fails.
# This will stop User deletion while an admin investigates.
# if the selected email address is "other" we cut things short
# and clean the value in the form's clean method instead
# otherwise we emmulate the functionality of the EmailField here
# stripping whitespaces, validating the content and then
# run allauth's own email value cleanup
# let's see if the email value was "other"
# and set the cleaned data to the cleaned other_email value
# also store the fact of using the other value in an attribute
# to be used in the view to check for it
# then run the usual email clean method again to apply
# the regular email validation and put the error into the
# email field specific value
# Email subject *must not* contain newlines
# Auth keys
# we have to import the signup form here due to allauth's odd form subclassing
# that requires providing a base form class (see ACCOUNT_SIGNUP_FORM_CLASS)
# TODO: Make this dynamic, editable from admin interface
# A list of common reasons for banning a user, loaded from constance
# Is this user already banned?
# Get revisions for the past 3 days for this user
# Is this user already banned?
# If the user is not banned, ban user; else, update 'by' and 'reason'
# The revisions to be submitted to Akismet and reverted,
# these must be sorted descending so that they are reverted accordingly
# 1. Submit revisions to Akismet as spam
# 2. If this is the most recent revision for a document:
#    Revert revision if it has a previous version OR
#    Delete revision if it is a new document
# Submit to Akismet or note that validation & sending to Akismet failed
# Since we only want to display 1 revision per document, only add to
# this list if this is one of the revisions for a distinct document
# If there is a current revision and the revision is not in the spam list,
# to be reverted, do not revert any revisions
# This document was previously deleted in this loop, continue
# This document has a more current revision, no need to revert
# Loop through all previous revisions to find the oldest spam
# revision on a specific document from this request.
# If this is a new revision on an existing document, revert it
# If the revert was unsuccessful, include this in the follow-up list
# If this is a new document/translation, delete it
# If the delete was unsuccessful, include this in the follow-up list
# Find just the latest revision for each document
# TODO: Phase V: If user made actions while reviewer was banning them
# Send an email to the spam watch mailing list.
# schedule a rendering of the new revision if it really was saved
# pragma: no branch
# Map of form field names to tag namespaces
# Form fields to receive tags filtered by namespace.
# Finally, set up the forms.
# Beta
# If there's no Beta Testers group, ignore that logic
# Update tags from form fields
# From the User abstract class
# All User attributes
# Protected references to users need to be manually deleted first.
# Some records are worth keeping prior to deleting the user
# but "re-assign" to the anonymous user.
# If the user has no revisions there's not choices on the form.
# When no username is provided, default to the local-part of the email address
# For GitHub/Google users, see if we can find matching user by username
# deleting the initial username because we found a matching user
# Discard email addresses that won't validate
# if we didn't get any extra email addresses from the provider
# but the default email is available, simply hide the form widget
# let the user choose from provider's extra email addresses, or enter
# a new one.
# build a mapping of the email addresses to their other values
# to be used later for resetting the social accounts email addresses
# build the choice list with the given email addresses
# if there is a main email address offer that as well (unless it's
# already there)
# we have to stash the selected email address here
# so that no email verification is sent again
# this is done by adding the email address to the session
# For GitHub/Google users, find matching legacy Persona social accounts
# only persona accounts have emails as UIDs
# but adding the provider criteria makes this explicit and future-proof
# TODO: changes this back to the walrus operator once pfylakes supports it
# related issue: https://github.com/PyCQA/pyflakes/pull/457
# Generated by Django 1.11.15 on 2018-08-13 15:27
# Generated by Django 1.11.15 on 2018-08-13 15:29
# Generated by Django 1.11.16 on 2018-10-24 07:30
# Generated by Django 1.11.16 on 2018-10-24 07:31
# Generated by Django 1.11.20 on 2019-03-08 07:44
# Generated by Django 1.11.20 on 2019-05-09 12:29
# Generated by Django 1.11.21 on 2019-06-10 08:22
# Generated by Django 1.11.23 on 2019-09-11 09:37
# Change bn-BD profile to bn
# Change bn-IN profile to bn
# Generated by Django 1.11.23 on 2019-09-12 16:34
# Generated by Django 1.11.23 on 2019-09-14 22:08
# Generated by Django 1.11.23 on 2019-09-14 22:09
# Generated by Django 1.11.23 on 2019-10-23 07:41
# Ignore all email addresses that have not been verified by GitHub.
#69; -> E."""
#%s;' % ord(i) for i in text])
# Returns a string representation of a user
# Returns a list of social authentication providers.
# get the login url and append params as url parameters
# Set up a pre-existing "Alternate" sign-in session
# Set up a in-process GitHub SocialLogin (unsaved)
# Verify the social_login receiver over-writes the provider
# stored in the session
# Set up a GitHub SocialLogin in the session
# Set up an un-matching alternate SocialLogin for request
# Set up a session-only GitHub SocialLogin
# These are created at the start of the signup process, and saved on
#  profile completion.
# Setup existing Persona SocialLogin for the same email
# Verify the social_login receiver over-writes the provider
# stored in the session
# Set up a session-only Google SocialLogin
# These are created at the start of the signup process, and saved on
#  profile completion.
# Setup existing Persona SocialLogin for the same email
# Verify the social_login receiver over-writes the provider
# stored in the session
# Set up a GitHub SocialLogin in the session
# Set up an un-matching GitHub SocialLogin for request
# Set up a GitHub SocialLogin in the session
# Ban the user
# Make explicit the assumption that the social-login provider which we're
# signing-up with is not yet associated with the existing user.
# Let's give the user a usable password so we can check later if that's
# been remedied if we find a match.
# Associate social accounts with the wiki_user other than the social-login
# provider we're signing-up with.
# Associate an email address with the wiki_user.
# Run through the same steps that django-allauth takes after the OAuth2
# dance has completed. Doing it this way avoids having to mock the entire
# OAuth2 dance.
# The response should be an instance of HttpResponseRedirect.
# If we found a matching user, we should have been logged-in without
# any further steps, and redirected to the home page. Otherwise we should
# have been redirected to the account-signup page to continue the process.
# The wiki_user's password should have been made unusable but only if we
# found a match.
# A new GitHub social account should have been associated with the existing
# wiki_user only if we found a match.
# The extra social-login email address not yet associated with the
# existing wiki_user should have been created only if we found a match.
# The associated Persona social account should have been deleted only if
# we found a match.
# let's try this with the username above
#109;&#101;&#64;&#100;&#111;&#109;&#97;&#105;&#110;&#46;&#99;'
#111;&#109;</span>' == public_email('me@domain.com'))
#110;&#111;&#116;&#46;&#97;&#110;&#46;&#101;&#109;&#97;&#105;'
#108;</span>' == public_email('not.an.email'))
# Assemble a set of test sites.
# Try a mix of assignment cases for the websites property
# Save and make sure a fresh fetch works as expected
# The same URL is returned on second call
# The same URL is returned on second call
# no email sent
# only one email, the welcome email, is sent, no confirmation needed
# emulate the phase in which the request for email confirmation is
# sent as the user's email address is not verified
# only one email, the confirmation email is sent
# Click on a similar confirm link (HMAC has timestamp, changes)
# a second email, the welcome email, is sent
# now add second unverified email address to the user
# and check if the usual confirmation email is sent out
# Confirm the second email address
# no increase in number of emails (no 2nd welcome email)
# re-enable registration
#id_username')[0]
#email_0')[0].attrib['value']
#email_1')[0].attrib['value']
# The "github_login" method mocks requests, so when the "render_react"
# call is made for the home page, it'll fail because there's no mock
# address defined for that SSR request. For the purpose of this test,
# we don't care about the content of the home page, so let's explicitly
# mock the "render" call.
# Test the signup form and our very custom email selector
# POST user choices to complete signup
# The rest of this test simply gets and checks the wiki home page.
# Check login user url is there
# There should be signout link in the form action
# decode slashes
# Check next url is provided as input field
# Ensure CSRF protection has not been added, since it creates problems
# when used with a CDN like CloudFront (see bugzilla #1456165).
# The common reasons to ban users (from constance) should be in template
# If there is an error in getting the common reasons from constance,
# then 'Spam' should still show up in the template as the default
# If the list of common reasons to ban users in constance is empty,
# then 'Spam' should still show up in the template as the default
# Create 3 revisions for testuser, titled 'Revision 1', 'Revision 2'...
# The title for each of the created revisions shows up in the template
# The original revision created by the admin user is not in the template
#ban-and-cleanup-form')
# There are some revisions made by self.testuser
# For self.testuser (not banned, and revisions need to be reverted) the
# button on the form should read "Ban User for Spam & Submit Spam"
# and there should be a link to ban a user for other reasons
#ban-and-cleanup-form button[type=submit]')
#ban-for-other-reasons')
# For self.testuser (not banned, no revisions needing to be reverted)
# the button on the form should read "Ban User for Spam". There should
# be no link to ban for other reasons
#ban-and-cleanup-form button[type=submit]')
#ban-for-other-reasons')
# For self.testuser2 (not banned, revisions already marked as spam)
# the button on the form should read "Ban User for Spam". There should
# be no link to ban for other reasons
# Create some revisions made by self.testuser2 and add a Spam submission for each
#ban-and-cleanup-form button[type=submit]')
#ban-for-other-reasons')
# There are some revisions made by self.testuser; none by self.testuser2
# Ban self.testuser
# For self.testuser (banned, but revisions need to be reverted) the
# button on the form should read "Submit Spam". There should
# be no link to ban for other reasons
#ban-and-cleanup-form button[type=submit]')
#ban-for-other-reasons')
# Ban self.testuser2
# For self.testuser2 (banned, has no revisions needing to be reverted)
# there should be no button on the form and no link to
# ban for other reasons
#ban-and-cleanup-form button[type=submit]')
#ban-for-other-reasons')
# For self.testuser2 (banned, revisions already marked as spam)
# there should be no button on the form and no link to
# ban for other reasons
# Create some revisions made by self.testuser2 and add a Spam submission for each
#ban-and-cleanup-form button[type=submit]')
#ban-for-other-reasons')
# The "Actions taken" section
#banned-user li').text()
#revisions-reverted li')
#revisions-deleted li')
#revisions-reported-as-spam li')
#revisions-reverted')
#revisions-deleted')
#revisions-followup')
# The "Needs follow up" section
#not-submitted-to-akismet li')
#not-deleted li')
#not-reverted li')
# TODO: Add in Phase V
# new_actions = page.find('#new-actions-by-user li')
# TODO: Add in Phase V
# assert len(new_actions) == 0
# The "No actions taken" section
#already-spam li')
#not-spam li')
# Mock the RevisionAkismetSubmissionSpamForm.is_valid() method
# Create 3 revisions for self.testuser, titled 'Revision 1', 'Revision 2'...
# Don't specify document so a new one is created for each revision
# The "Actions taken" section
#banned-user li').text()
#revisions-reported-as-spam li')
#revisions-reverted li')
#revisions-deleted li')
# The title for each of the created revisions shows up in the template
# The title for the original revision is not in the template
# The "Needs follow up" section
#not-submitted-to-akismet li')
#not-deleted li')
#not-reverted li')
# TODO: Add in Phase V
# new_actions = page.find('#new-actions-by-user li')
# TODO: Add in Phase V
# assert len(new_actions) == 0
# The "No actions taken" section
#already-spam li')
#not-spam li')
# Mock the RevisionAkismetSubmissionSpamForm.is_valid() method
# Create 3 revisions for self.testuser, titled 'Revision 1', 'Revision 2'...
# The "Actions taken" section
#banned-user li').text()
#revisions-reported-as-spam li')
#revisions-reverted li')
#revisions-deleted li')
# The "Needs follow up" section
#not-submitted-to-akismet li')
#not-deleted li')
#not-reverted li')
# TODO: Add in Phase V
# new_actions = page.find('#new-actions-by-user li')
# TODO: Add in Phase V
# assert len(new_actions) == 0
# The "No actions taken" section
#already-spam li')
#not-spam li')
# Mock the RevisionAkismetSubmissionSpamForm.is_valid() method
# Create 3 revisions for self.testuser, titled 'Revision 1', 'Revision 2'...
# The "Actions taken" section
#banned-user li').text()
#revisions-reported-as-spam li')
#revisions-reverted li')
#revisions-deleted li')
# The "Needs follow up" section
#not-submitted-to-akismet li')
#not-deleted li')
#not-reverted li')
# TODO: Add in Phase V
# new_actions = page.find('#new-actions-by-user li')
# TODO: Add in Phase V
# assert len(new_actions) == 0
# The "No actions taken" section
#already-spam li')
#not-spam li')
# Mock the RevisionAkismetSubmissionSpamForm.is_valid() method
# Create 3 revisions for self.testuser, titled 'Revision 1', 'Revision 2'...
# Create a revision on a new document
# The "Actions taken" section
#banned-user li').text()
#revisions-reported-as-spam li')
#revisions-reverted li')
#revisions-deleted li')
# The "Needs follow up" section
#not-submitted-to-akismet li')
#not-deleted li')
#not-reverted li')
# TODO: Add in Phase V
# new_actions = page.find('#new-actions-by-user li')
# Since no ids were posted nothing should have been submitted to Akismet
# TODO: Add in Phase V
# assert len(new_actions) == 0
# The "No actions taken" section
#already-spam li')
#not-spam li')
# The latest revision from each of the two documents should show up as 'not spam'
# Mock the RevisionAkismetSubmissionSpamForm.is_valid() method
# Create 3 revisions for self.testuser, titled 'Revision 1', 'Revision 2'...
# Create a revision on a new document
# The "Actions taken" section
#banned-user li').text()
#revisions-reported-as-spam li')
#revisions-reverted li')
#revisions-deleted li')
# The "Needs follow up" section
#not-submitted-to-akismet li')
#not-deleted li')
#not-reverted li')
# TODO: Add in Phase V
# new_actions = page.find('#new-actions-by-user li')
# There were no errors submitting to Akismet, so no follow up is needed
# TODO: Add in Phase V
# assert len(new_actions) == 0
# The "No actions taken" section
#already-spam li')
#not-spam li')
# One revision should show up for each of the documents
# Mock the RevisionAkismetSubmissionSpamForm.is_valid() method
# Create 3 revisions for self.testuser, titled 'Revision 1', 'Revision 2'...
# Create a new document and 3 revisions on it
# Create another new document and 3 revisions on it
# Create yet another new document and 3 revisions on it
# this new doc should be deleted when we send in all revisions.
# POST no revisions from self.document, the 1st from doc1,
# the 1st and 2nd revisions from doc2, and all revisions from doc 3
# doc 1 and doc 2 should have no action (newest revision is unchecked)
# doc 3 should be deleted
# The "Actions taken" section
#banned-user li').text()
#revisions-reported-as-spam li')
#revisions-reverted li')
#revisions-deleted li')
# The revisions shown are revs_doc_1[0], revs_doc_2[1], and revs_doc_3[2]
# Verify that the revision title matches what we're looking for
# The "Needs follow up" section
#not-submitted-to-akismet li')
#not-deleted li')
#not-reverted li')
# TODO: Add in Phase V
# new_actions = page.find('#new-actions-by-user li')
# TODO: Add in Phase V
# assert len(new_actions) == 0
# The "No actions taken" section
#already-spam li')
#not-spam li')
# Revisions from self.document, doc1, and doc2 should be considered 'not spam'
# Mock the RevisionAkismetSubmissionSpamForm.is_valid() method
# Create an original revision on a document by the self.testuser
# TODO: Phase V: arrange that this revision will go into the "New action by user"
# section.  Currently the document will wind up being deleted automatically.
# Create an original revision on another document by the self.testuser
# TODO: Phase V: The revision done after the reviewing has begun should
# have a delete link in the "New action by user" section under "Needs follow up"
# The revision on doc2 should have a delete link in the "Already identified as spam"
# section under "No actions taken"
#already-spam a[href="{url}"]'.format(
# There should be 1 delete link found in each section
# TODO: Phase V
# assert len(doc1_delete_link) == 1
# User makes a revision on another user's document
# TODO: Phase V: Create a revision by self.testuser after reviewing has
# begun so it shows up in the "New action by user" section
# TODO: PhaseV
# delete_link_new_action_section = page.find('#new-actions-by-user a[href="{url}"]'.format(
#     url=delete_url))
#already-spam a[href="{url}"]'.format(
# There should not be a delete link in any of these sections
# TODO: PhaseV
# assert len(delete_link_new_action_section) == 0
# User creates a document, but another user makes a revision on it
# User creates another document, but another user makes a revision on it
# TODO: PhaseV
# delete_url_new_action
#reverted a[href="{url}"]'.format(
# TODO: PhaseV
# delete_link_new_action_section = page.find('#new-actions-by-user a[href="{url}"]'.format(
#     url=delete_url_new_action))
#already-spam a[href="{url}"]'.format(
# There should not be a delete link in any of these sections
# TODO: PhaseV
# assert len(delete_link_new_action_section) == 0
# Create 3 revisions for self.testuser, titled 'Revision 1', 'Revision 2'...
# 'Actions taken' section
#revisions-deleted li')
#revisions-reverted li')
# No new documents by the spammer, so none deleted
# Document was not reverted, since there was a newer non-spam rev
# 'Needs follow-up' section
#new-actions-by-user li')
#skipped-revisions li')
#revisions-reported-as-spam li')
# No new revs were added by the user while we were working
# One document had revisions that were ignored, because there was a newer good rev
# Only one document is listed on the reported as spam list (distinct document)
# 'No action' section
#latest-revision-non-spam li')
#already-spam li')
#not-spam li')
# The only document was left unreverted due to having a good rev for its latest
# No documents had revs that were already marked as spam
# No documents had revs that were unchecked in the spam form
# Create two spam revisions, with one good revision in between.
# 'Actions taken' section
#revisions-deleted li')
#revisions-reverted li')
# No new documents by the spammer, so none deleted
# Only one set of reverted revisions
# 'Needs follow-up' section
#new-actions-by-user li')
#skipped-revisions li')
#revisions-reported-as-spam li')
# No new revs were added by the user while we were working
# One document had revisions that were ignored, because there was a newer good rev
# Only one document is listed on the reported as spam list (distinct document)
# 'No action' section
#latest-revision-non-spam li')
#already-spam li')
#not-spam li')
# No documents were left unreverted due to having a good rev for its latest
# No documents had revs that were already marked as spam
# No documents had revs that were unchecked in the spam form
# Create 3 revisions for self.testuser, titled 'Revision 1', 'Revision 2'...
# 'Actions taken' section
#revisions-deleted li')
#revisions-reverted li')
# No new documents by the spammer, so none deleted
# Only one set of reverted revisions
# 'Needs follow-up' section
#new-actions-by-user li')
#skipped-revisions li')
#revisions-reported-as-spam li')
# To be implemented in Phase V
# All of the spam revisions were covered by the reversion
# Only one document is listed on the reported as spam list (distinct document)
# 'No action' section
#latest-revision-non-spam li')
#already-spam li')
#not-spam li')
# No documents were left unreverted due to having a good rev for its latest
# No documents had revs that were already marked as spam
# No documents had revs that were unchecked in the spam form
# Create a new spam document with a single revision
# Just raise an IntegrityError to get delete_document to fail
# 'Actions taken' section
#revisions-deleted li')
#revisions-reverted li')
# The document failed to be deleted
# It wouldn't have been reverted anyway
# 'Needs follow-up' section
#new-actions-by-user li')
#skipped-revisions li')
#revisions-reported-as-spam li')
#not-deleted li')
#not-reverted li')
# To be implemented in Phase V
# Nothing happened
# Akismet reporting happens first
# The deletion failed, so it goes here
# 'No action' section
#latest-revision-non-spam li')
#already-spam li')
#not-spam li')
# No good revisions superceding bad ones
# No documents had revs that were already marked as spam
# No documents had revs that were unchecked in the spam form
# Create some spam revisions on a previously good document.
# Just raise an IntegrityError to get revert_document to fail
# 'Actions taken' section
#revisions-deleted li')
#revisions-reverted li')
# The document wouldn't have been deleted
# It failed to be reverted
# 'Needs follow-up' section
#new-actions-by-user li')
#skipped-revisions li')
#revisions-reported-as-spam li')
#not-deleted li')
#not-reverted li')
# To be implemented in Phase V
# Nothing happened
# Akismet reporting happens first
# The revert failed, so it goes here
# 'No action' section
#latest-revision-non-spam li')
#already-spam li')
#not-spam li')
# No good revisions superceding bad ones
# No documents had revs that were already marked as spam
# No documents had revs that were unchecked in the spam form
# The user is not banned, display appropriate links
#ban_link')
#cleanup_link')
# The user is banned, display appropriate links
#ban_link')
#cleanup_link')
# testuser doesn't have ban permission, can't ban.
# admin has ban permission, can ban.
# Attempting to ban a non-existent user should 404
# Attempting to ban without a reason should return the form
# POST without data kwargs
# POST with a blank reason
# User viewable if not banned
# Ban User
# User not viewable if banned
# Admin can view banned user
# For an unbanned user get the ban_user view
# For a banned user redirect to user detail page
# testuser doesn't have ban permission, can't ban.
# admin has ban permission, can ban.
# GET request
# Trying to delete a document that is None will fail without error.
# Calling on a real document deletes the document and creates the log object
# Create a spam revision on top of the original good rev.
# Reverting a non-existent rev raises a 404
# Reverting an existing rev succeeds
# If an IntegrityError is raised when we try to revert, it fails without error.
# Just get any old thing inside the call to raise an IntegrityError
# Assert that the ban exists, and 'by' and 'reason' fields are updated
# Create 3 revisions for self.testuser, titled 'Revision 1', 'Revision 2'...
# Don't specify document so a new one is created for each revision
# Enable Akismet and mock calls to it
# The request
# All of self.testuser's revisions have been submitted
# Akismet endpoints were called twice for each revision
# Enable Akismet and mock calls to it
# User has no revisions
# Akismet endpoints were not called
# Create 3 revisions for self.testuser, titled 'Revision 1', 'Revision 2'...
# Don't specify document so a new one is created for each revision
# Enable Akismet and mock calls to it
# User's revisions were not in request.POST (not selected in the template)
# No revisions submitted for self.testuser, since no revisions were selected
# Akismet endpoints were not called
# Create 3 revisions for self.testuser, titled 'Revision 1', 'Revision 2'...
# Enable Akismet and mock calls to it
# User being banned did not create the revisions being POSTed
# No revisions submitted for self.testuser2, since revisions in the POST
# were made by self.testuser
# Akismet endpoints were not called
# Create a new document and revisions as testuser
# Revisions will be reverted and then document will be deleted.
# Pass in all revisions, each should be reverted then the
# document will be deleted as well
# Test that the document was deleted successfully
# Create a new document and first revision as an admin
# and spam revisions as testuser.
# Document should be reverted with a new revision by admin.
# Before we send in the spam,
# last spam_revisions[] should be the current revision
# and testuser is the creator of this current revision
# Pass in all spam revisions, each should be reverted then the
# document should return to the original revision
# Make sure that the current revision is not the spam revision
# The most recent Revision object should be the document's current revision
# Admin is the creator of this current revision
# The new revision's content is the same as the original's
# Document A will have latest revision by the admin, but older spam revisions
# Document B will have latest revision by spammer
# Document A should not revert (although there are older spam revisions)
# Document B will revert
# Pass in all spam revisions:
# A revisions will not be reverted
# B revisions will be reverted
# Document A: No changes should have been made
# Total of 5 revisions, no new revisions were made
# Document B: Make sure that the current revision is not the spam revision
# The most recent Revision for this document
# should be the document's current revision
# Admin is the creator of this current revision
# 5 total revisions on B = 1 initial + 3 spam revisions + 1 new reverted revision
# Pass in spam revisions:
# No changes should have been made to the document
# Total of 5 revisions, no new revisions were made
# Create 4 revisions: one good, one spam, one good, then finally one spam
# Set the content of the last good revision, so we can compare afterwards
# Pass in spam revisions:
# The document should be reverted to the last good revision
# Make sure that the current revision is not either of the spam revisions
# And that it did actually revert
# Total of 5 revisions, a new revision was made
# Add an existing good document with a spam rev
# Add a new purely spam document
# Add a spammed document where a user submits a good rev on top
# Pass in spam revisions:
# Scrape out the existing significant form field values.
#{prefix}edit *[name="{prefix}{field}"]'
#user-head.vcard .nickname').text() == wiki_user.username
#user-head.vcard .fn').text() == wiki_user.fullname
#user-head.vcard .title').text() == wiki_user.title
#user-head.vcard .org').text() == wiki_user.organization
#user-head.vcard .loc').text() == wiki_user.location
#user-head.vcard .irc').text() ==
#user-head .edit .button').length == 0
#user-head .user-buttons #edit-user')
#user-edit input[name="user-fullname"]').val() ==
#user-edit input[name="user-title"]').val() ==
#user-edit input[name="user-organization"]').val() ==
#user-edit input[name="user-location"]').val() ==
#user-edit input[name="user-irc_nickname"]').val() ==
#user-head').length == 1
#user-head .fn').text() == new_attrs['user-fullname']
#user-head .user-info .title').text() ==
#user-head .user-info .org').text() ==
#id_user-beta').attr('checked') is None
#id_user-beta').attr('checked') == 'checked'
# Fill out the form with websites.
# Submit the form, verify redirect to user detail
#user-head').length == 1
# Verify the websites are saved in the user.
# Verify the saved websites appear in the editing form
#user-edit *[name="user-%s_url"]' % k).val() == v
# Github is not an editable field
#field_github_url div.field-account")
# Come up with some bad sites, either invalid URL or bad URL prefix
# Submit the form, verify errors for all of the bad sites
#user-edit').length == 1
#user-edit #users .%s .errorlist'
#user-head').length == 1
#user-head').length == 1
# Now, try some expertise tags not covered in interests
#id_user-expertise').length == 1
# if label is localized it's a lazy proxy object
# View page as a logged in user
# No redirect!
# No redirect!
# No redirect!
# No redirect!
# No redirect!
# first check if the public email address has been found
# then check if the private and verified-at-GitHub email address
# has been found
# then check that the invalid email is not present
# then check if the radio button's default value is the primary email
# address
# = use other_email
# Setup a user with a token and refresh token
# Login without a refresh token
# Refresh token is still in database
# Anonymous client gets redirected to sign in page.
# A username that doesn't exist.
# Attempting to delete someone else.
# sanity check fixtures
# otherwise it logs the user out
# These are plainly deleted
# Moved to anonymous user
# Sanity check the fixture
# The user_client should now become "invalid" since its session
# is going to point to no user.
# Let's doublecheck that
# Also, pretend that the user has a rich profile
# Sanity check the fixture
# Create some social logins
# Create a RevisionAkismetSubmission
# Create an authentication key
# Should still exist
# Should still be associated with the user
# But the account should be clean, apart from the username.
# The user_client should now become "invalid" since its session
# is going to point to no user.
# Let's doublecheck that
# There should be no Key left
# Make a recovery URL for a user that no longer exists.
# Some users to use in the tests
# Create an original revision on a document by the admin user
# If document is None, then we create a new document for each revision
# Unused profile items
# Added Feb 2017, bug 1339375
# Ensure GitHub is setup as an auth provider
# Start the login process
# Store state in the session, and redirect the user to GitHub
# Callback from GitHub, mock follow-on GitHub responses
# The callback view will make requests back to Github:
# The OAuth2 authentication token (or error)
# The authenticated user's profile data
# The user's emails, which could be an empty list
# Simulate the callback from Github
# Serve the revision hashes.
#{})</a>', rev_url, rev.id
#{})</a>', url, obj.parent.id
#{})</a>', url, obj.parent_topic.id
# Register signal handlers
# noqa
# Build sitemaps every day at 05:00
# Render stale documents: every 60 minutes
# Note: <iframe> is allowed, but src="" is filtered after bleach
# MathML
# Note: <iframe> is allowed, but src="" is pre-filtered before bleach
# MathML
# CSS
# TODO: Maybe not this one, it can load URLs
# CSS properties needed for live examples (pending proper solution):
# TODO: Put this under the control of Constance / Waffle?
# Flags used to signify revisions in need of review
# ?, whitespace, percentage, quote disallowed in slugs altogether
# how a redirect looks as rendered HTML
# Header to send that don't start with HTTP
# A few regex patterns for various parsing efforts in this file
# Regex to extract language from MindTouch code elements' function attribute
# map for mt syntax values that should turn into new brush values
# List of tags supported for section editing. A subset of everything that could
# be considered an HTML5 section
# Head tags to be included in the table of contents
# Allowed tags in the table of contents list
# Special paths within /docs/ URL-space that do not represent documents for the
# purposes of link annotation. Doesn't include everything from urls.py, but
# just the likely candidates for links.
# No document.html, then there's no point bothering to parse it.
# No point parsing it because we won't find anything!
# No point parsing it because we won't find anything!
# HACK: Ensure the extracted section has a container, in case it
# consists of a single element.
# If no section, fall back to plain old ID lookup
# HACK: syntaxhighlighter (ab)uses the className as a
# semicolon-separated options list...
# Bug 819999: &nbsp; gets decoded to \xa0, which trips up CSS
# Bug 1284781: &nbsp; is incorrectly parsed on embed sample
# remove empty paragraphs
# Create an SEO summary
# TODO:  Google only takes the first 180 characters, so maybe we find a
#        logical way to find the end of sentence before 180?
# Try constraining the search for summary to an explicit "Summary"
# section, if any.
# This line is ~20x times slower than doing the PyQuery analysis.
# Both `parse()` and `.serialize()` are slow and expensive.
# That's why we're careful to avoid it if we can.
# Need to add a BR to the page content otherwise pyQuery wont find
# a <p></p> element if it's the only element in the doc_html.
# Look for the SEO summary class first
# Checking for a parent length of 2
# because we don't want p's wrapped
# in DIVs ("<div class='warning'>") and pyQuery adds
# "<html><div>" wrapping to entire document
# Post-found cleanup
# remove markup chars
# remove spaces around some punctuation added by PyQuery
# NOTE: This started as an html5lib filter, but it started getting really
# complex. Seems like pyquery works well enough without corrupting
# character encoding.
# TODO: Need more external link prefixes, here?
# Pass #1: Gather all the link URLs and prepare annotations
# Squash site-absolute URLs to site-relative paths.
# Prepare annotations record for this path.
# Run through all the links and check for annotatable conditions.
# Is this an external URL?
# https://mathiasbynens.github.io/rel-noopener/
# TODO: Should this also check for old-school mindtouch URLs? Or
# should we encourage editors to convert to new-style URLs to take
# advantage of link annotation? (I'd say the latter)
# Is this a kuma doc URL?
# Check if this is a special docs path that's exempt from "new"
#' in href_path:
# If present, discard the hash anchor
#')
# Try to sort out the locale and slug through some of our
# redirection logic.
# Gather up this link for existence check
# If the slug used in the document has a trailing /
# remove that from here so that it stands a better chance
# to match existing Document slugs.
# Perform existence checks for all the links, using one DB query per
# locale for all the candidate slugs.
# Remove the slugs that pass existence check.
# Same slug by MySQL collation rules
# Some slugs are matched by collation rules, so use single checks
# Mark all the links whose slugs did not come back from the DB
# query as "new"
# Pass #2: Filter the content, annotating links
# Squash site-absolute URLs to site-relative paths.
# Update attributes on this link element.
#$%&+,/:;=?@[\\]^`{|}~\')('
# Strip leading, trailing and multiple whitespace,
# convert remaining whitespace to _.
# If we get into this code, 'token' will be the start tag of a
# header element. We're going to grab its text contents to
# generate a slugified ID for it, add that ID in, and then
# spit it back out. 'buffer' is the list of tokens we were in
# the process of handling when we hit this header.
# Loop through successive tokens in the stream of HTML
# until we find our end tag, building up in 'tmp' a list
# of those tokens to emit later, and in 'text' a list of
# the text content we see along the way.
# Note: This is naive, and doesn't track other
# start/end tags nested in the header. Odd things might
# happen in a case like <h1><h1></h1></h1>. But, that's
# invalid markup and the worst case should be a
# truncated ID because all the text wasn't accumulated.
# Slugify the text we found inside the header, generate an ID
# as a last resort.
# Create unique slug for heading tags with the same content
# Hand back buffer minus the bits we yanked out of it, and the
# new ID-ified header start tag and contents.
# First, collect all ID values already in the source HTML.
# Collect both 'name' and 'id' attributes since
# 'name' gets treated as a manual override to
# specify an ID.
# Then walk the tree again identifying elements in need of IDs
# and adding them.
# If this token isn't the start tag of a section or
# header, we don't add an ID and just short-circuit
# out to return the token as-is.
# Potential bug warning: there may not be any
# attributes, so doing a for loop over them to look
# for existing ID/name values is unsafe. Instead we
# dict-ify the attrs, and then check directly for the
# things we care about instead of iterating all
# attributes and waiting for one we care about to show
# up.
# First check for a 'name' attribute; if it's present,
# treat it as a manual override by the author and make
# that value be the ID.
# Sanitize the "name" attribute with self.slugify to
# prevent the injection of spaces (which are illegal
# for the "id" attribute) or any of the non-URL-safe
# characters listed above.
# Next look for <section> tags which don't have an ID
# set; since we don't generate an ID for them from
# their text contents, they just get a numeric one
# from gen_id().
# If we got here, we're looking at the start tag of a
# header which had no 'name' attribute set. We're
# going to pop out the text contents of the header,
# use them to generate a slugified ID for it, and
# return it with that ID added in.
#%s' % id}},
# Section start was deferred, so start it now.
# Have we encountered the section or heading element we're
# looking for?
# If we encounter a section element that matches the ID,
# then we'll want to scoop up all its children as an
# explicit section.
# Defer the start of the section, so the section parent
# itself isn't included.
# If we encounter a heading element that matches the ID, we
# start an implicit section.
# If started an implicit section, these rules apply to
# siblings...
# The implicit section should stop if we hit another
# sibling heading whose rank is equal or higher, since that
# starts a new implicit section
# If this is the first heading of the section and we want to
# omit it, note that we've found it
# If the parent of the section has ended, end the section.
# This applies to both implicit and explicit sections.
# If there's no replacement source, then this is a section
# extraction. So, emit tokens while we're in the section, as long
# as we're also not in the process of ignoring a heading
# If there is a replacement source, then this is a section
# replacement. Emit tokens of the source stream until we're in the
# section, then emit the replacement stream and ignore the rest of
# the source stream for the section. Note that an ignored heading
# is *not* replaced.
# If this looks like the end of a heading we were ignoring, clear
# the ignoring condition.
# FIXME: hgroup rank == highest rank of headers contained
# But, we'd need to track the hgroup and then any child headers
# encountered in the stream. Not doing that right now.
# For now, just assume an hgroup is equivalent to h1
# loop through all 'tokens'
# if this token is a start tag...
# increment counter that tracks nesting
# note we're in the matching section
# keep track of how nested we were when section started
# If the parent of the section has ended, end the section.
# reduce nesting counter
# emit tokens if we're not in the section being removed
# Strip out any attributes that start with "on"
# path must match a compiled regex
# path must start with this prefix
# We are using this switch temporarily to research bug 1104260.
# Disabling this code has no effect locally, but may have an effect on
# production.
# Support an option to bypass this decorator altogether, so one
# view can directly call another view.
# Parse the document path into locale and slug.
# Add check for "local" URL, remove trailing slash
# If there's no slug, then this is just a 404.
# HACK: There are and will be a lot of kumascript templates
# based on legacy DekiScript which will attempt to request
# old-style URLs. Skip 301 redirects for raw content.
# TODO: evaluate if this is still appropriate
# This catches old MindTouch locales, missing locale, and a few
# other cases to fire off a 301 Moved permanent redirect.
# Set the kwargs that decorated methods will expect.
# Don't use `previous` since it is cached. (see bug 1239141)
# USE_TZ is False, database has naive timestamps
# pragma: no cover
# USE_TZ is True, database has timezone-aware timestamps
# Check for a callback param, validate it before use
# Include some of the simple elements from the preprocessed item
# HACK: DocumentFeed is the superclass of RevisionFeed. In this
# case, current_revision is the revision itself.
# TODO: Refactor this out into separate DocumentFeed and
# RevisionFeed subclasses of Feed.
# Linkify the tags used in the feed item
# Temporarily storing the selected documents PKs in a list
# to speed up retrieval (max MAX_FEED_ITEMS size)
# Temporarily storing the selected documents PKs in a list
# to speed up retrieval (max MAX_FEED_ITEMS size)
# TODO: Need an HTML / dashboard version of this feed
# Temporarily storing the selected revision PKs in a list
# to speed up retrieval (max MAX_FEED_ITEMS size)
# TODO: put this in a jinja template if django syndication will let us
# Use captured Akismet submission
# when creating a new document with a parent, this will be set
# Default to the title, if missing.
# Prepend parent slug if given from view
# Convert to NFKC, required for URLs (bug 1357416)
# http://www.unicode.org/faq/normalization.html
# check both for disallowed characters and match for the allowed
# Guard against slugs that match reserved URL patterns.
# not strictly necessary since we didn't change
# any m2m data since we instantiated the doc
# when creating a new document with a parent, this will be set
# Ensure both title and slug are populated from parent document,
# if last revision didn't have them
# Since this form can change the URL of the page on which the editing
# happens, changes to the slug are ignored for an iframe submissions
# Get the cleaned slug
# Convert to NFKC, required for URLs (bug 1357416)
# http://www.unicode.org/faq/normalization.html
# first check if the given slug doesn't contain slashes and other
# characters not allowed in a revision slug component (without parent)
# edits can come in without a slug, so default to the current doc slug
# then if there is a parent document we prefix the slug with its slug
# There's another document with this value,
# and we're not a revision of it.
# This document-and-revision doesn't exist yet, so there
# shouldn't be any collisions at all.
# No existing document for this value, so we're good here.
# Note: The exact match query doesn't work correctly with
# MySQL with regards to case-sensitivity. If we move to
# Postgresql in the future this code may need to change.
# Write a log we can grep to help find pre-existing duplicate
# document tags for cleanup.
# The tag differs only by case. Do not add a new one,
# add the existing one.
# If we're editing a section, we need to replace the section content
# from the current revision.
# Make sure we start with content form the latest revision.
# Replace the section content with the form content.
# If there's no current_rev, just bail.
# This is a section edit. So, even though the revision has
# changed, it still might not be a collision if the section
# in particular hasn't changed.
# Oops. Looks like the section did actually get
# changed, so yeah this is a collision.
# No section edit, so this is a flat-out collision.
# If there's no document yet, just bail.
# For Akismet errors, save the submission and exception details
# For detected spam, save the details for review
# Wrapping this in a try/finally to make sure that even if
# creating a spam attempt object fails we call the parent
# method that raises a ValidationError
# New translation, compare to English document
# No content change, not spam
# have to check for first edit before we save
# Making sure we don't commit the saving right away since we
# want to do other things here.
# The logic to save a section is slightly different and may
# need to evolve over time; a section edit doesn't submit
# all the fields, and we need to account for that when we
# construct the new Revision.
# when enabled store the user's IP address
# send first edit emails
# schedule a document rendering
# schedule event notifications
# We only want the slug here; inputting a full URL would lead
# to disaster.
# Removes leading slash and {locale/docs/} if necessary
# IMPORTANT: This exact same regex is used on the client side, so
# update both if doing so
# Remove the trailing slash if one is present, because it
# will screw up the page move, which doesn't expect one.
# Don't synchronously fetch the contributor bar but schedule a fetch
# first get a list of user ID recently authoring revisions
# remove duplicates, preserving order
#   duplicates arise when a user makes multiple edits to a document
# then return the ordered results given the ID list, MySQL only syntax
# the empty result needs to be an empty list instead of None
# Spread the life time across a random
# number of days from 1 to 10 (in units of seconds).
# So that all the document cache do not get expired at same time
# If the user does a hard reload we see Cache-Control:no-cache in
# the request header. And we pass that header on to Kumascript so
# that it does not use its cache when re-rendering the page.
# Load just-in-time, since constance requires DB and cache
# TODO: Move to a standard Django setting w/ env override
# TODO(djf): This get() function is actually implemented on top of
# _post() and it performs an HTTP POST request. It should probably
# be renamed to render_document(), and the post() method above should
# be renamed to render_string(), maybe. For now, though, there are so
# many tests that mock kumascript.get() that I've left the name unchanged.
# Assemble some KumaScript env vars
# We defer bleach sanitation of kumascript content all the way
# through editing, source display, and raw output. But, we still
# want sanitation, so it finally gets picked up here.
# Extract all the log packets from headers.
# The FireLogger spec allows for multiple "packets". But,
# kumascript only ever sends the one, so flatten all messages.
# Ensure Normal Form C used on GitHub
# Return no documents
# Set to larger than number of macros
# Could raise TransportError
# Get active macros from KumaScript, returning early if none.
# Convert macro sources to fuller dict
# Record page usage for active macros for all locales
# For the first call, gracefully handle missing ES server, etc.
# Record page usage for active macros for English
# For second call, ES Server issue _is_ exceptional, raise error
# List translated pages without English source associated
# Only include fields needed for a list of links to docs
# Try getting the value using the DB field.
# DB field is blank, or we're forced to generate it fresh.
# whether or not this attachment was uploaded for the document
# whether or not this attachment is linked in the document's content
# NOTE: Documents are indexed by tags, but tags are edited in Revisions.
# Also, using a custom through table to isolate Document tags from those
# used in other models and apps. (Works better than namespaces, for
# completion and such.)
# DEPRECATED: Is this document a template or not?
# Droping or altering this column will require a table rebuild, so it
# should be done in a maintenance window.
# Is this a redirect or not?
# Is this document localizable or not?
# Latest approved revision. L10n dashboard depends on this being so (rather
# than being able to set it to earlier approved revisions).
# The Document I was translated from. NULL if this doc is in the default
# locale or it is nonlocalizable. TODO: validate against
# settings.WIKI_DEFAULT_LANGUAGE.
# The files attached to the document, represented by a custom intermediate
# model so we can store some metadata about the relation
# JSON representation of Document for API results, built on save
# Raw HTML of approved revision's wiki markup
# Cached result of kumascript and other offline processors (if any)
# Errors (if any) from the last rendering run
# Whether or not to automatically defer rendering of this page to a queued
# offline task. Generally used for complex pages that need time
# Timestamp when this document was last scheduled for a render
# Timestamp when a render for this document was last started
# Timestamp when this document was last rendered
# Maximum age (in seconds) before this document needs re-rendering
# Time after which this document needs re-rendering
# Whether this page is deleted.
# Last modified time for the document. Should be equal-to or greater than
# the current revision's created field
# TODO: There will be no need to "injectSectionIDs" when the code
#       that calls "clean_content" on Revision.save is deployed to
#       production, AND the current revisions of all docs have had
#       their content cleaned with "clean_content".
# TODO: There will be no need to "injectSectionIDs" when the code
#       that calls "clean_content" on Revision.save is deployed to
#       production, AND the current revisions of all docs have had
#       their content cleaned with "clean_content".
#L388
# Check if the slug belongs to any default language document
# TODO: Maybe @cache_with_field can build a registry over which this
# method can iterate?
# Check whether a scheduled rendering has waited for too long.  Assume
# failure, in this case, and allow another scheduling attempt.
# No start time, so False.
# Check whether an in-progress rendering has gone on for too long.
# Assume failure, in this case, and allow another rendering attempt.
# No rendering ever, so in progress.
# Finally, if the render start is more recent than last completed
# render, then we have one in progress.
# No rendered content yet, so schedule the first render.
# Unable to trigger a rendering right now, so we bail.
# If we have a cache_control directive, try scheduling a render.
# Parse JSON errors, if available.
# If the above resulted in an immediate render, we might have content.
# But, no such luck, so bail out.
# Avoid scheduling a rendering if already scheduled or in progress.
# Note when the rendering was scheduled. Kind of a hack, doing a quick
# update and setting the local property rather than doing a save()
# Attempt to queue a rendering. If celery.conf.ALWAYS_EAGER is
# True, this is also an immediate rendering.
# Attempt an immediate rendering.
# Disallow rendering while another is in progress.
# Note when the rendering was started. Kind of a hack, doing a quick
# update and setting the local property rather than doing a save()
# Perform rendering and update document
# A timeout of 0 should shortcircuit kumascript usage.
# Regenerate the cached content fields
# Finally, note the end time of rendering and update the document.
# If this rendering took longer than we'd like, mark it for deferred
# rendering in the future.
# TODO: Automatically clear the defer_rendering flag if the rendering
# time falls under the limit? Probably safer to require manual
# intervention to free docs from deferred jail.
# If there's a render_max_age, automatically update render_expires
# Otherwise, just clear the expiration time as a one-shot
# TODO: There will be no need to "injectSectionIDs" when the code
#       that calls "clean_content" on Revision.save is deployed to
#       production, AND the current revisions of all docs have had
#       their content cleaned with "clean_content".
# Have parsed data & don't care about freshness? Here's a quick out..
# Attempt to parse the current contents of self.json, taking care in
# case it's empty or broken JSON.
# Try to get ISO 8601 datestamps for the doc and the json
# If there's no parsed data or the data is stale & we care, it's time
# to rebuild the cached JSON data.
# If I am new or my title/slug changed...
# Can't save this translation if parent not localizable
# Can't make not localizable if it has translations
# This only applies to documents that already exist, hence self.pk
# remember the current revision's primary key for later
# get a list of review tag names for later
# reset primary key
# add a sensible comment
# set review tags
# populate model instance with fresh data from database
# make this new revision the current one for the document
# Accept optional field edits...
# To add comment, when Technical/Editorial review completed
# Accept HTML edits, optionally by section
# Finally, commit the revision changes and return the new rev.
# Check if the slug would collide with an existing doc
# If the existing doc is a redirect, delete it and clobber it.
# These are too important to leave to a (possibly omitted) is_valid
# call:
# If this is a translation without a topic parent, try to get one.
# Shortcut trick for getting an object with all the same
# values, but making Django think it's new.
# Page move is a 10-step process.
#
# Step 1: Sanity check. Has a page been created at this slug
# since the move was requested? If not, OK to go ahead and
# change our slug.
# Step 2: stash our current review tags, since we want to
# preserve them.
# Step 3: Create (but don't yet save) a Document and Revision
# to leave behind as a redirect from old location to new.
# Step 4: Update our breadcrumbs.
# If we found a Document at what will be our parent slug, set
# it as our parent_topic. If we didn't find one, then we no
# longer have a parent_topic (since our original parent_topic
# would already have moved if it were going to).
# Step 5: Save this Document.
# Step 6: Create (but don't yet save) a copy of our current
# revision, but with the new slug and title (if title is
# changing too).
# Step 7: Save the Revision that actually moves us.
# Step 8: Save the review tags.
# Step 9: Save the redirect.
# Finally, step 10: recurse through all of our children.
# Save the original slug and locale so we can use them in
# the error message if something goes wrong.
# A child move already caught this and created the
# correct exception + error message, so just propagate
# it up.
# One of the immediate children of this page failed to
# move.
# Bail, if this is not in fact a translation.
# Bail, if the translation parent has no topic parent
# Look for an existing translation of the topic parent
# No luck. As a longshot, let's try looking for the same slug.
# HACK: This same-slug/different-locale doc should probably
# be considered a translation. Let's correct that on the
# spot.
# Finally, let's create a translated stub for a topic parent
# Don't forget to clone a current revision
# HACK: Let's auto-add tags that flag this as a topic stub
# Finally, assign the new default parent topic
# We have at least some MindTouch files.
# We also have some kuma files. Use an OR query.
# We have only kuma files.
# If no files found, return an empty Attachment queryset.
# Delete all document-attachments-relations for attachments that
# weren't originally uploaded for the document to populate the list
# again below
# Reset the linked status for all attachments that are left
# Go through the attachments discovered in the HTML and
# create linked attachments
# If a document starts with REDIRECT_HTML and contains any <a> tags
# with hrefs, return the href of the first one. This trick saves us
# from having to parse the HTML every time.
# allow explicit domain and *not* '//'
# i.e allow "https://developer...." and "/en-US/docs/blah"
# Always return relative path instead of full url
# The parent doc should be at first
# This is a method, not a property, because it can do a lot of DB
# queries and so should look scarier. It's not just named
# 'children' because that's taken already by the reverse relation
# on parent_topic.
# In most cases, just return the language code, removing the country
# code if present (so, for example, 'en-US' becomes 'en').
# Check for the special case when we want the full locale (i.e.,
# including the country code). This is the case when this document's
# locale is not the preferred locale, and the preferred locale is
# among the other locales in which this document is available. For
# example, if this document is available in 'en-US', 'pt-PT', and
# 'pt-BR', the respective hreflang values would be 'en', 'pt', and
# 'pt-BR' (we're using the full locale for 'pt-BR'), but if it's only
# available in 'en-US' and 'pt-BR', we'll choose hreflang values of
# 'en' and 'pt' (we're not using the full locale for 'pt-BR', just the
# language code 'pt', so the one translation covers all countries for
# that language).
# The content is already clean.
# This is updated only if the document is not a translation,
# otherwise its original value is preserved.
# The current revision sometimes has an old slug that's
# different than its document's current slug, so let's ensure
# that they're the same so we don't trigger unique index errors
# when the document is saved within the make_current() call made
# within the rev.save() call below, or explicitly further below.
# Populate the model instance with fresh data from database.
# Make this new revision the current one for the document if it hasn't
# been done already when saving the revision above.
# We store the locale/slug because it's unique, and also because a
# ForeignKey would delete this log when the Document gets purged.
# Depth of table-of-contents in document display.
# Title and slug in document are primary, but they're kept here for
# revision history.
# wiki markup
# wiki markup
# wiki markup tidied up
# Keywords are used mostly to affect search rankings. Moderators may not
# have the language expertise to translate keywords, so we put them in the
# Revision so the translators can handle them:
# Tags are stored in a Revision as a plain CharField, because Revisions are
# not indexed by tags. This data is retained for history tracking.
# Tags are (ab)used as status flags and for searches, but the through model
# should constrain things from getting expensive.
# Maximum age (in seconds) before this document needs re-rendering
# The default locale's rev that was current when the Edit button was hit to
# create this revision. Used to determine whether localizations are out of
# date.
# TODO: limit_choices_to={'document__locale':
# settings.WIKI_DEFAULT_LANGUAGE} is a start but not sufficient.
# TODO(james): This could probably be simplified down to "if
# based_on is set, it must be a revision of the original document."
# based_on is set and points to the wrong doc.
# Else based_on is valid; leave it alone.
# All of the cleaning herein should be unnecessary unless the user
# messes with hidden form data.
# For clean()ing forms that don't have a document instance behind
# them yet
# Restoring translation source, so base on current_revision
# Guess a correct value.
# No more Mister Nice Guy
# TODO(erik): This error message ignores non-translations.
# When a revision is approved, update document metadata and re-cache
# the document's html content
# Since Revision stores tags as a string, we need to parse them first
# before setting on the Document.
# Re-create all document-attachment relations since they are based
# on the actual HTML content
#%s' % (self.document.locale,
# we may be lucky and have the tidied content already denormalized
# in the database, if so return it
# We still need this for the wiki.revision and wiki.translate endpoints
# (due to old revisions and "based_on" revisions whose content may have
# not been cleaned).
# don't delete the akismet submission but set the revision to null
# hi-fi -> hifi, hi-fi
# hi-fi -> hifi
# 90-210 -> 90210
# a custom analyzer that strips html and uses our own
# word delimiter filter and the elision filter
# (e.g. L'attribut -> attribut). The rest is the same as
# the snowball analyzer
# Get the list of document IDs to index.
# If there's no data we still create the index and finalize it.
# Only send for new instances, not fixtures or edits
# calling the task without delay here since we want to localize
# the processing of the chunk in one process
# not stale documents to render
# fetch a logger in case none is given
# If we're a translation, rebuild our source doc's JSON so its
# translation list includes our last edit date.
# Now that we know the move succeeded, re-render the whole tree.
# Get the parent document, if parent doc is None, it means its the parent document
# If the document is a translation we should include the parent document url to the list
# Add any non-document URL's, which will always include the home page.
# We *could* use the `Document.objects.filter_for_list()` manager
# but it has a list of `.only()` columns which isn't right,
# it has a list of hardcoded slug prefixes, and it forces an order by
# on 'slug' which is slow and not needed in this context.
# Be explicit about exactly only the columns we need.
# The logic for rendering a page will do various checks on each
# document to evaluate if it should be excluded from robots.
# Ie. in a jinja template it does...
#  `{% if reasons... %}noindex, nofollow{% endif %}`
# Some of those evaluations are complex and depend on the request.
# That's too complex here but we can at least do some low-hanging
# fruit filtering.
# We have to make the queryset ordered. Otherwise the GenericSitemap
# generator might throw this perfectly valid warning:
#
#    UnorderedObjectListWarning:
#     Pagination may yield inconsistent results with an unordered
#     object_list: <class 'kuma.wiki.models.Document'> QuerySet.
#
# Any order is fine. Use something definitely indexed. It's needed for
# paginator used by GenericSitemap.
# To avoid an extra query to see if the queryset is empty, let's just
# start iterator and create the sitemap on the first found page.
# Note, how we check if 'urls' became truthy before adding it.
# Make the sitemap files.
# result can be empty if no documents were found
# we retry the chord unlock 300 times, so 5 mins with an interval of 1s
# Retry in 2 minutes
# return the errors so we can look them up in the Celery task result
# These patterns inherit (?P<document_path>[^\$]+).
# Un/Subscribe to document edit notifications.
# Un/Subscribe to document tree edit notifications.
# internals
# Special pages
# Legacy KumaScript macro list, when they were stored in Kuma database
# Akismet Revision
# Feeds
# These patterns inherit (?P<document_path>[^\$]+).
# If there's a slash in the path, then the first segment could be a
# locale. And, that locale could even be a legacy MindTouch locale.
# The first segment looks like a MindTouch locale, remap it.
# The first segment looks like an MDN locale, redirect.
# No locale yet? Try the locale detected by the request or in path
# Still no locale? Probably no request. Go with the site default.
# Extract locale and path from URL:
# Never has errors AFAICT
# Only allow redirects on our site
# View imports Model, Model imports utils, utils import Views.
# In case something happens in pytidylib we'll try again with
# a proper encoding
# `dimension12` is the custom variable containing a page's rev #.
# PK of the developer.mozilla.org site on GA.
# overridden by --all or --locale
# Accept page paths from command line, but be liberal
# in what we accept, eg: /en-US/docs/CSS (full path);
# /en-US/CSS (no /docs); or even en-US/CSS (no leading slash)
# all relations are linked since they were found in
# the document's content
# first get the attachment to document list mapping
# get the attachment
# bail if there isn't a current attachment revision
# probably because faulty data
# the revision we'll use for some minor metadata when creating the
# attachment later
# get the list of documents that the attachment is contained in
# has the document that the attachment was originally uploaded to
# already been found?
# let's see if there is an English document, chances are that's
# what we want
# create the attachment and mark the original as found
# hm, no English document found, so let's just use the document
# with the lowest ID, create the attachment, and move on
# now go through the rest of the bunch but ignore the original
# document we already created an attachment for
# we failed, didn't find any document for this document
# yada yada yada
# Give some indication of progress, occasionally
# A page containing class="noinclude" is very likely to be used
# as included content on another page, so better prefetch. But,
# prefetching templates won't help us, since they don't get
# pre-processed by kumascript.
# Get an MD5 hash of the lowercased path
# Warm up the page_exists cache
# Now, prefetch all the documents flagged in need in the previous loop.
# overridden by --all
# Query all documents, excluding those whose `last_rendered_at` is
# within `min_render_age` or NULL.
# Accept page paths from command line, but be liberal
# in what we accept, eg: /en-US/docs/CSS (full path);
# /en-US/CSS (no /docs); or even en-US/CSS (no leading slash)
# Make it so.
# Gather up docs that claim to be translations,
# but have no topic parents.
# https://bugzilla.mozilla.org/show_bug.cgi?id=792417#c2
# Some translated pages really don't end up needing a
# breadcrumb repair, but we don't really know until we try and
# come up empty handed.
# We got a new parent topic, so save and report the result
# Positional arguments
# first get the deleted document logs for the last n days
# They use "spam"
# deleting spam revisions;
# the spam makes me cry.  -- willkg
# get the deleted document in question
# no document found with that locale and slug,
# probably purged at some point
# guess the document got undeleted at some point again,
# ignoring..
# no current revision found, which means something is fishy
# but we can't submit it as spam since we don't have a revision
# we're in dry-run, so let's continue okay?
# Generated by Django 1.11.15 on 2018-08-06 17:16
# Generated by Django 1.11.21 on 2019-06-11 09:38
# Generated by Django 1.11.21 on 2019-07-08 13:14
# Generated by Django 1.11.22 on 2019-08-05 03:06
# Generated by Django 1.11.23 on 2019-09-11 09:26
# Remove bn-IN
# First set all parent_topic to None so it does
# not raise error while deleting the parent topic
# Then delete all the `bn-IN` documents
# Generated by Django 1.11.23 on 2019-09-12 16:34
# Generated by Django 1.11.23 on 2019-10-23 07:41
# Generated by Django 1.11.23 on 2019-09-14 22:08
# Generated by Django 1.11.23 on 2019-09-14 22:09
#?(\d+)', re.IGNORECASE)
# If a page move, say so
#%s' % (from_revision.document.locale, from_revision.id)
#%s' % (to_revision.document.locale, to_revision.id)
# some diffs hit a max recursion error
# Simple formatting update: 784877
# we're doing something horrible here because this will show up
# in feed reader and other clients that don't load CSS files
#afa; text-decoration: none;"')
#faa; text-decoration: none;"')
#fe0; text-decoration: none;"')
# pass errors during construction
# pass errors during find/select
# We've exhausted all the types acceptable by the default JSON encoder.
# Django's improved JSON encoder handles a few other types, all of which
# are represented by strings. For these types, we apply JSON encoding
# immediately and then escape the result.
# If value contains custom subclasses of int, str, datetime, etc.
# arbitrary exceptions may be raised during escaping or serialization.
#' in path:
#', 1)
#' + fragment
# Try server side rendering
# POST the document data as JSON to the SSR server and we
# should get HTML text (encoded as plain text) in the body
# of the response
# Even though we've got fully rendered HTML now, we still need to
# send the document data along with it so that React can sync its
# state on the client side with what is in the HTML. When rendering
# a document page, the data includes long strings of HTML that
# we can get away without duplicating. So as an optimization when
# component_name is "document", we're going to make a copy of the
# data (because the original belongs to our caller) and delete those
# strings from the copy.
#
# WARNING: This optimization can save 20kb in data transfer
# for typical pages, but it requires us to be very careful on
# the frontend. If any components render conditionally based on
# the state of bodyHTML, tocHTML or quickLinkHTML, then they will
# render differently on the client than during SSR, and the hydrate
# will not just work cleanly, and those components will re-render
# with empty strings. This has already caused Bug 1558308, and
# I've commented it out because the benefit in file size doesn't
# seem worth the risk of client-side bugs.
#
# As an alternative, it ought to be possible to extract the HTML
# strings from the SSR'ed document and rebuild the document object
# on the client right before we call hydrate(). So if you uncomment
# the lines below, you should also edit kuma/javascript/src/index.jsx
# to extract the HTML from the document as well.
#
# if component_name == 'document':
#     data = data.copy()
#     data['documentData'] = data['documentData'].copy()
#     data['documentData'].update(bodyHTML='',
#                                 tocHTML='',
#                                 quickLinksHTML='')
#1",
#3",
#4",
#5",
#6",
# Headers are case-insensitive, so let's drive that point home.
# Enable admin's message_user
#id_revision')
# successfully created the submission record
# Base URL for annotateLinks tests
# Then, ensure all elements in need of an ID now all have unique IDs.
# Ensure 1, 2 doesn't turn into 3, 4
# h1 and h2, but not the next h1
# Same as test_extractSection_heading_in_section
# All headings inside the section
# All headings up to the next h1
# All headings up to the next h1
# h2 contents without the h2
#$%&+,/:;=?@[\\]^`{|}~\')(',
#HTML">HTML</a>
#HTML5_canvas_element">HTML5 <code>canvas</code> element</a></li>
#JavaScript">JavaScript</a>
#WebGL">WebGL</a>
#Audio">Audio</a>
#Audio-API">Audio API</a></li>
#CSS">CSS</a>
#CSS_transforms">CSS transforms</a>
#Gradients">Gradients</a>
#Scaling_backgrounds">Scaling backgrounds</a>
#HTML">HTML</a>
#JavaScript">JavaScript</a>
#CSS">CSS</a>
#HTML">HTML</a>
#HTML5_canvas_element">HTML5 <code>canvas</code> element</a></li>
#JavaScript">JavaScript</a>
#WebGL">WebGL</a>
#Audio">Audio</a>
#CSS">CSS</a>
#Gradients">Gradients</a>
#Print" rel="internal">Mastering<code>print</code></a>
#12345 again. <img src="http://davidwalsh.name" /> <a href="">javascript></a>'
#34;http://davidwalsh.name&#34; /&gt; &lt;a href=&#34;&#34;&gt;javascript&gt;&lt;/a&gt;'
#12345 again.'
#if1').attr('src') == embed_url
#if2').attr('src') == 'https://testserver'
#if3').attr('src') == ''
#if4').attr('src') == ''
#if5').attr('src') == ''
#test').attr('src') == url
#test').attr('src') == ''
#x09;cript:alert(1)',
#14;javascript:alert(1)',
#test')
#test')
#anchor"
#anchor"
# Sanity check our fixture
# Notice the 'en-US' after '/docs/'
# Notice the 'en-US' after '/docs/'
# Sample of tags from ALLOWED_TAGS
# Also saves
# Also saves
# bug 1173170
# bug 1269143
# bug 1269143
# Note! the `mock_requests` fixture is just there to make absolutely
# sure the whole test doesn't ever use requests.get().
# My not setting up expectations, and if it got used,
# these tests would raise a `NoMockAddress` exception.
# Doesn't matter if it's http or https
# If the content, afterwards, has real paragraphs, then the first
# line becomes the seo description
#%d
#%d
#%d
#%d
# No entries, translation is up to date
# Description is encoded HTML
# TODO: Investigate encoding issue w/ <h3> Content changes
#difflib_chg_to%(diff_id)s__top">t</a>
#difflib_chg_to%(diff_id)s__top">t</a></td>
# Format is difflib_chg_to[DIFF ID]__top
# Test that links use host domain
# Test that links are a mix of en-US and translated documents
# Created revision
# New revision
#afa; '
#afa; '
# Re-render document, RSS feed doesn't change
# Create a new edit, RSS feed changes
# Invalid callback names are rejected
# Create a revision with some tags
# Create another revision with some other tags
# Check document is latest tags feed
# Check document is not in the previous tags feed
# "Applications" in Greek (el)
# In NFC / NFKD, 'έ' is represented by two "decomposed" codepoints
#  03B5 (GREEK SMALL LETTER EPSILON)
#  0301 (COMBINING ACUTE ACCENT)
# In NFC / NFKC, 'έ' is represented by a "composed" codepoint
#  03AD (GREEK SMALL LETTER EPSILON WITH TONOS)
# "Firefox" in Bengali (bn)
# This slug is the same in NFC, NFD, NFKD, and NFKD. The second character
# has these codepoints:
# 09af BENGALI LETTER YA (য)
# 09bc BENGALI SIGN NUKTA
# 09be BENGALI VOWEL SIGN AA (non-breaking spacing mark)
# An alternate representation of the second character is:
# 09df BENGALI LETTER YYA (য়)
# 09be BENGALI VOWEL SIGN AA (non-breaking spacing mark)
# Note the lower-case "S".
# Keys for a new English page or new translation
# Keys for a page edit (English or translation)
# Default attributes of original revision
# Extra data from view, derived from POST
# Not included in edit POST
# The mock request content has to be a byte string
# Test that one message has been sent.
# Only verify key called
# Akismet sees a content change due to the whitespace
# Data passed by view, derived from POST
# Added in view from request.LANGUAGE_CODE
# In the view, the form data's locale is set from the request
# Test that one message has been sent.
# Default attributes of original English page
# Data passed by view, derived from POST
# Added in view from request.GET to_locale
# In view, includes cleaning
# Default attributes of original English page
# Default attributes of original French page
# Data passed by view, derived from POST
# Added in view from request.GET to_locale
# Form #1 - Document validation
# Form #2 - Revision validation and saving
# leading slash
# locale and docs
# leading docs
# leading docs without slash
# foreign locale with docs
# docs with later docs
# trailing slash
#fragment', 'https://testserver/woo?var=value#fragment'),
# No UnicodeEncodeError
#Option_1_I_like_words',
#Option_1_I_like_words'),
# Set this to true so we bypass the Celery task queue.
# Banned and inactive contributors should not be included.
# Delete the ban.
# The freshly un-banned user is now among the contributors because the
# cache has been invalidated.
# Another revision should invalidate the job's cache.
# The new contributor shows up and is first, followed
# by the freshly un-banned user, and then the rest.
# Ensure the env vars intended for kumascript match expected values.
# Normal form D, common on OSX
# Normal form C, used on GitHub, ElasticSearch
# Let's make the revision's slug and title different from the document
# to ensure that they're corrected in the end.
# See LEGACY_MINDTOUCH_NAMESPACES in ../constants.py
# Experiments aren't legacy yet
# Slugs without colons don't have namespaces
# Slugs with colons might not be legacy
# Translations are returned English first, then ordered, and omit self
# There should be a translation topic parent
# The locale of the topic parent should match the new translation
# But, the translation's topic parent must *not* be the translation
# parent's topic parent
# Still, since the topic parent is an autocreated stub, it shares its
# title with the original.
# Oh, and it should point to the original parent topic as its
# translation parent
# Build a path of docs in en-US
# Translate, but leave gaps for stubs
# Make sure trans_2 got the right parent
# Ensure the translated parents and stubs appear properly in the path
# Creating another approved revision replaces it again
# Creating another approved revision keeps initial content
# Revision of some other unrelated Document
# Revision of some other unrelated Document
# Make English rev:
# Make Deutsch translation:
# Set based_on to a de rev to simulate fixing broken translation source
# First, try getting the rendered version of a document. It should
# trigger a call to kumascript.
# Next, get a fresh copy of the document and try getting a rendering.
# It should *not* call out to kumascript, because the rendered content
# should be in the DB.
# No DocumentRenderingInProgress raised
# Scheduling for a non-deferred render should happen on the spot.
# Reset the significant fields and try a deferred render.
# Scheduling for a deferred render should result in a queued task.
# And, since our mock delay() doesn't actually queue a task, this
# document should appear to be scheduled for a pending render not yet
# in progress.
# Fresh
# Stale, exactly now
# Stale, a little while ago
# HACK: Exact time comparisons suck, because execution time.
# Test descendant counts
# All
# Test detection at one level removed.
# And at two levels removed.
# Simple multi-level tree:
#
#  - top
#    - child1
#    - child2
#      - grandchild
# Now we do a simple move: inserting a prefix that needs to be
# inherited by the whole tree.
# And for each document verify three things:
#
# 1. The new slug is correct.
# 2. A new revision was created when the page moved.
# 3. A redirect was created.
# We should find the conflict if it's at the slug the document
# will move to.
# Or if it will involve a child document.
# But a redirect should not trigger a conflict.
# move grandma under grandpa
# assert the parent_topics are correctly rooted at grandpa
# note we have to refetch these to see any DB changes.
# move page to new slug
# TODO: Fix this assertion?
# assert 'admin' == page_moved_doc.current_revision.creator.username)
# First move, to new slug.
# Appropriate redirects were left behind.
# Moved documents still have the same IDs.
# Second move, back to original slug.
# Once again we left redirects behind.
# The documents at the original URLs aren't redirects anymore.
# The redirects created in the first move no longer exist in the DB.
# cache the tags of the document and check its the tag that we created and it is sorted
# Create another revision with some other tags and check tags get invalidate and get updated
# To compare JSON strings (rather than object equality) we really need
# to ensure that the properties of a dict are in a deteriministic order.
# so we're going to patch json.dumps to call this function that sorts
# the dict keys alphabetically.
# This is the input to the mock Node server
# This will be the output sent by the mock Node server
# Run the template tag
# Make sure the output is as expected
# Check for duplicates.
# Check for duplicates.
# Simplify the test
# Make one document for every mindtouch legacy namespace.
# Add an "experiment" document
# Add a document with no HTML content
# Note!
# Add a document without a revision
# Exclude the inter-linking sitemaps
# Now check exactly which slugs we expect in entirety.
# Note that this automatically asserts that all the legacy docs
# created above don't get returned.
# created is auto-set to current time, update bypasses model logic
# Soft-delete it
#content div.document-head h1').text() ==
#wikiArticle').text() == r.document.html
#content div.document-head h1').text() == d1.title
#content div.document-head h1').text() == d2.title
#content div.document-head h1').text() == d1.title
# No parents, no breadcrumbs
#content div.document-head h1').text() == d2.title
#content div.document-head h1').text() == str(r.document.title)
#wikiArticle').text())
#content div.document-head h1').text() == str(d2.title)
# HACK: fr doc has different message if locale/ is updated
#wikiArticle').text()) or
#wikiArticle').text()))
#content div.document-head h1').text() == str(d2.title)
# Fallback message is shown.
#doc-pending-fallback')) == 1
#edit-button').attr('href')
# Removing this as it shows up in text(), and we don't want to depend
# on its localization.
#doc-pending-fallback').remove()
# Included content is English.
#wikiArticle').text()
#content div.document-head h1').text() ==
# Fallback message is shown.
#doc-pending-fallback')) == 1
#edit-button').attr('href')
# Removing this as it shows up in text(), and we don't want to depend
# on its localization.
#doc-pending-fallback').remove()
# Included content is English.
#wikiArticle').text()
# Ordinarily, a document with no approved revisions cannot have HTML,
# but we shove it in manually here as a shortcut:
#translations li').text())
# Make it non-localizable
#translations li').text())
# The requeseted document language name should be at first
# The parent document language should be at at second
# Then should be ar, bn, fr
#languages-menu-submenu ul#translations li a")
# The requeseted document language name should not be at button
# Parent document language name should be at first
# Then should be ar, bn, fr
#content div.document-head h1').text()
#wikiArticle').text() == r.document.html
#doc-experiment')
# src attribute of the content experiment <script> tag
# Can't use pyquery for <head> elements
# Googla Analytics custom dimension calls
#edit-button')
#edit-button')
#doc-source pre').text() == rev.content
#wiki-page-edit input[name="title"]'))
# TODO: push test_strings functionality up into a test helper
#id_title').attr('placeholder')
# You shouldn't be able to make a new doc in a non-default locale
# without marking it as non-localizable. Unskip this when the non-
# localizable bool is implemented.
#edit-document '
#wiki-page-edit textarea[name="content"]')) == 1
# Sign up for notifications:
# Edit a document
# Assert notifications fired and have the expected content:
# 1 email for the first time edit notification
# 1 email for the EditDocumentEvent to sam@example.com
# Regression check:
# messing with context processors can
# cause notification emails to error
# and stop being sent.
# There are no approved revisions, so it's based_on nothing:
# Create a translation
# Make sure is_localizable hidden field is rendered
# Change Revisions link goes to the French document history page
# From revision link goes to the English document
# To revision link goes to the French document
# initial translation should include slug input
# Invalid slug
# Content is required
# First create the first one with test above
# Approve the translation
# Create and approve a new en-US revision
# Verify the form renders with correct content
#id_content').text().strip() == rev_es.content
# Post the translation and verify
# subsequent translations should NOT include slug input
# No new revisions
# Title is updated
# New revision is created
# Title isn't updated
#id_content').text().strip() == existing_rev.content
# While Fred is editing the above, Martha approves a new rev:
# Then Fred saves his edit:
# Don't mutate arg.
#wikiArticle h1').text() == 'Test Content'
# Create a test document and translation.
# Preview content that links to it and verify link is in locale.
#doc-content a')
#select-locale ul.locales li')) ==
# All except for 1 (en-US)
# Select the list item and revision requested in the test
# The date text links to the expected revision page
# Check if there is a previous link
# The comment has a marker if it is the English source page
# The revert button is included if it makes sense for the revision
# Disable the discovery cache, so that we can fully control the http requests
# with HttpMockSequence below
# Disable the discovery cache, so that we can fully control the http requests
# with HttpMockSequence below
# Check that the last request's parameters contain a
# representation of the start date, not the starting datetime.
# Disable the discovery cache, so that we can fully control the http requests
# with HttpMockSequence below
# Check that the last request's parameters contain a
# representation of the end date, not the ending datetime.
# Disable the discovery cache, so that we can fully control the http requests
# with HttpMockSequence below
# Check that the last request's parameters contain a
# representation of the end date, not the ending datetime.
# Disable the discovery cache, so that we can fully control the http requests
# with HttpMockSequence below
# Disable the discovery cache, so that we can fully control the http requests
# with HttpMockSequence below
# This is the type of data we get back if the rev doesn't match anything.
# http 400
# Disable the discovery cache, so that we can fully control the http requests
# with HttpMockSequence below
# http 401
# Disable the discovery cache, so that we can fully control the http requests
# with HttpMockSequence below
# http 403
# Disable the discovery cache, so that we can fully control the http requests
# with HttpMockSequence below
# Mock the network traffic, just in case.
# Mock the network traffic, just in case.
#Head_2" rel="internal">Head 2</a></ol>'
# Basic structure creation testing
# Depth parameter testing
# Sorting test
# Test if we are serving an error json if document does not exist
# Test error json if document is a redirect
# just checking if the contributor link is rendered
#" onload=alert(3)>Hahaha</a>
#wikiArticle'))
#">Hahaha</a>' in ct
#wikiArticle').parent().attr('open')
#doc-source').parent().attr('open') is None
# TODO: upgrade mock to 0.8.0 so we can do this.
# self.mock_kumascript_get = (
#         mock.patch('kuma.wiki.kumascript.get'))
# self.mock_kumascript_get.return_value = self.doc.html
# TODO: upgrade mock to 0.8.0 so we can do this.
# self.mock_kumascript_get.stop()
# No UnicodeDecodeError, it's a unicode string
# Should not raise Http404
# Utility to make a quick doc
# Test nested document titles
# Additional tests for /Web/*  changes
# Create the doc
# Connect to newly created page
# Test pages - very basic
# No content, no seo
# No summary, no seo
# Warning paragraph ignored
# Warning paragraph ignored, first one chosen if multiple matches
# Don't take legacy crumbs
# Take the seoSummary class'd element
# Two summaries append
# No brackets
#id_content').text().strip()
# Create the parent page.
# Establish attribs of child page.
# Ensure redirect to create new page on attempt to visit non-existent
# child page.
# Ensure real 404 for visit to non-existent page with params common to
# kumascript and raw content API.
# Ensure root level documents work, not just children
# Move the document to new slug
# Try to create a child with the old slug
# The parent id of the query should be same because while moving,
# a new document is created with old slug and make redirect to the
# old document
# Not testing slug changes separately; the model tests cover those plus
# slug+title changes. If title changes work in the view, the rest
# should also.
# Not testing slug changes separately; the model tests cover those plus
# slug+title changes. If title changes work in the view, the rest
# should also.
# create parent doc & rev along with child doc & rev
# Create a new doc.
# Create another new doc.
# Now, post an update with duplicate slug
#id_slug"]').length > 0
# Create a new doc.
# Change title and slug
# Change title and slug back to originals, clobbering the redirect
# Test that slugs with the same "specific" slug but in different levels
# in the heiharachy are validated properly upon submission.
# Create base doc
# Create child, try to use same slug, should work
# grab new revision ID
# Editing newly created child "length/length" doesn't cause errors
# Creating a new translation of parent and child
# named "length" and "length/length" respectively
# doesn't cause errors
# Create an English original and a Spanish translation.
# Sanity to make sure the translate succeeded.
# Settings
# Create the one-level English Doc
# Translate to ES
# Go to edit the translation, ensure the the slug is correct
# Create an English child now
# Translate to ES
# Create english doc
# Create french doc
# Check edit doc page for choose parent box
# Set the parent
# Check the languages drop-down
#translations li')
# Create a revision with some tags
# Update the document with some other tags
# Check only last added tags are related with the documents
# Update the document with some other tags
# Check document is showing the new tags
# Create a new doc with one review tag
# Ensure there's now a doc with that expected tag in its newest
# revision
# Now, post an update with two tags
# Ensure the doc's newest revision has both tags.
# Now, ensure that review form appears for the review tags.
# Since the client is logged-in, the response should not be cached.
#id_request_technical').length == 1
#id_request_editorial').length == 1
# Ensure the page appears on the listing pages
# Also, ensure that the page appears in the proper feeds
# HACK: Too lazy to parse the XML. Lazy lazy.
# Post an edit that removes the technical review tag.
# Ensure only one of the tags' warning boxes appears, now.
#id_request_technical').length == 0
#id_request_editorial').length == 1
# Ensure the page appears on the listing pages
# Also, ensure that the page appears in the proper feeds
# HACK: Too lazy to parse the XML. Lazy lazy.
# Post a new document.
# This is the url to post new revisions for the rest of this test
# Edit #1 starts...
# Edit #2 starts...
# Update data for the POST we are about to attempt
# If this is a translation test, then create a translation and a
# revision on it. Then update data.
# Edit #2 submits successfully
# Edit #1 submits, but receives a mid-aired notification
# The url of the document's history
# The midair collision error, with the document url
# If this is not an ajax post, then the error comes back in escaped
# html. We unescape the resp.content, but not all of it, since that
# causes ascii errors.
# Add an some extra characters to the end, since the unescaped length
# is a little less than the escaped length
# Note: Akismet is enabled by the Flag overrides
# The return value of akismet.check_comment is set to True
# self.client.login(username='admin', password='testpass')
# Create a new document.
# Create a revision on the document
# This is the url to post new revisions for the rest of this test
# If this is a translation test, then create a translation and a revision on it
# rev_id = translation_rev.id
# Get the rev id
# Edit submits
# Post a new document.
# This is the url to post new revisions for the rest of this test
# Post a new translation on doc
# Edit #1
# Edit #1 submits successfully
#1',
# Edit #2
# Edit #2 submits successfully
#2',
# For Ajax requests the response is a JsonResponse
# test translation fails as well
# TODO: Do we need this test? This seems broken in that the
#       parent specified via the parent topic doesn't get it's
#       slug prepended to the new document's slug, as happens
#       when specifying the parent via the URL.
# Test that the 'discard' button on an edit goes to the original page
# Test that the 'discard button on a new translation goes
# to the en-US page'
# Test that the 'discard' button on an existing translation goes
# to the 'es' page
# Test that the compare URL points to the right revisions
# Subscribe another user and assert 2 emails sent this time
# Since the client is logged-in, the response should not be cached.
# Since the client is logged-in, the response should not be cached.
# Since the client is logged-in, the response should not be cached.
# Since the client is logged-in, the response should not be cached.
# Since the client is logged-in, the response should not be cached.
# Edit #1 starts...
# Edit #2 starts...
# Edit #2 submits successfully
# Edit #1 submits, but since it's a different section, there's no
# mid-air collision
# No conflict, but we should get a 205 Reset as an indication that the
# page needs a refresh.
# Finally, make sure that all the edits landed
# Since the client is logged-in, the response should not be cached.
# Also, ensure that the revision is slipped into the headers
# Edit #1 starts...
# Edit #2 starts...
# Edit #2 submits successfully
# Edit #1 submits, but since it's the same section, there's a collision
# We receive the midair collission message
# A note on these tests: we could try to use assertRedirects on
# these, but for the most part we're just constructing a URL
# similar enough to the wiki app's own built-in redirects that
# it'll pick up the request and do what we want with it. But it
# may end up issuing its own redirects, which are tricky to sort
# out from the ones the legacy MindTouch handling will emit, so
# instead we just test that A) we did issue a redirect and B) the
# URL we constructed is enough for the document views to go on.
# One for each namespace.
# Check the last redirect chain url is correct document url
# Disable TOC, makes content inspection easier.
#wikiArticle').text()
#doc-rendering-in-progress').length
#doc-render-raw-fallback').length
# Make the document look like there's a rendering in progress.
#wikiArticle').text()
# Even though a rendering looks like it's in progress, ensure the
# last-known render is displayed.
#doc-rendering-in-progress').length
# Only for logged-in users, ensure the render-in-progress warning is
# displayed.
#doc-rendering-in-progress').length
# Make the document look like there's no rendered content, but that a
# rendering is in progress.
# Now, ensure that raw content is shown in the view.
#wikiArticle').text()
#doc-render-raw-fallback').length
# Only for logged-in users, ensure that a warning is displayed about
# the fallback
#doc-render-raw-fallback').length
# Make sure nothing has happended (i.e. the docs haven't been purged).
# One RevisionAkismetSubmission record should exist for this revision.
# Check that the Akismet endpoints were called.
# Check that one RevisionAkismetSubmission instance exists.
# Create another Akismet revision via the endpoint.
# Check that the Akismet endpoints were called.
# Redirects to login page when without permission.
# No RevisionAkismetSubmission record should exist.
# Check that the Akismet endpoints were not called.
# No RevisionAkismetSubmission record should exist.
# Check that the Akismet endpoints were not called.
# Setting the KUMASCRIPT_TIMEOUT to a non-zero value forces kumascript
# rendering so we ensure that path is tested for these requests that use
# a restricted urlconf environment.
# Upload an attachment
# Add a relative reference to the sample content
# URL is in the sample
# Getting the URL redirects to the attachment
# dict of case-name --> tuple of slug and expected status code
#id_toc_depth')
# Check discard button.
# Starting with Django 1.11, condition headers will be
# considered only for GET requests. The one exception
# is a PUT request to the wiki.document_api endpoint,
# but that's not relevant here.
# Confirm that the PUT worked.
# The section_id should have no effect on the results, but we'll see.
# Confirm that the PUT worked.
#kserrors-list a[href="https://github.com/'
#kserrors-list a[href="https://github.com/'
#updating-macros"]')
# The response tags should be sorted
# Create a revision with no tags
# There should be no tag
# missing title
# Subscribe
# Unsubscribe
#wikiArticle')
#wikiArticle')
# For the purpose of this test, we don't care about the content of the
# document page, so let's explicitly mock the "server_side_render" call.
# The translation plus the English revision
# The translation alone
# There should be 4 documents in the 'en-US' locale from
# doc_hierarchy, plus the root_doc (which is pulled-in by
# the redirect_doc), but the redirect_doc should not be one of them.
#document-list ul.document-list li')) == 1
# Changing the tags to something other than what we're
# searching for should take the results to zero.
#document-list ul.document-list li')) == 0
# fr
# All translations have a parent.
# fr
# fr
# The root document is pulled-in by the redirect_doc fixture.
# All locales
# The root document is pulled-in by the redirect_doc fixture.
# update() to skip the tidy_revision_content post_save signal handler
# Ensure there is no redirect.
# Model makers. These make it clearer and more concise to create objects in
# test cases. They allow the significant attribute values to stand out rather
# than being hidden amongst the values needed merely to get the model to
# validate.
# Create translation parent...
# Then, translate it to de
# End model makers.
# Restrict rendering of live code samples to specified hosts
# TODO: Integrate this into a new exception-handling middleware
# a fake title based on the initial slug passed via a query parameter
# in case we want to create a sub page under a different document
# If a parent ID is provided via GET, confirm it exists
# in case we want to create a new page by cloning an existing document
# Render the confirmation page
# schedule a rendering of the new revision if it really was saved
# HACK: https://bugzil.la/972545 - Don't delete pages that have children
# TODO: https://bugzil.la/972541 - Deleting a page that has subpages
# A logged-in user can schedule a full re-render with Shift-Reload
# Shift-Reload sends Cache-Control: no-cache
# There was no rendered content available, and we were unable
# to render it on the spot. So, fall back to presenting raw
# content
# If the SEO root doc is the parent topic, save a query
# If ?summary is on, just serve up the summary as doc HTML
# Shortcut the parsing & filtering, if none of these relevant rendering
# params are set.
# TODO: One more view-time content parsing instance to refactor
# ?raw view is often used for editors - apply safety filtering.
# TODO: Should this stuff happen in render() itself?
# TODO: There will be no need to call "injectSectionIDs" or
#       "filterEditorSafety" when the code that calls "clean_content"
#       on Revision.save is deployed to production, AND the current
#       revisions of all docs have had their content cleaned with
#       "clean_content".
# If a section ID is specified, extract that section.
# TODO: Pre-extract every section on render? Might be over-optimization
# If this user can edit the document, inject section editing links.
# TODO: Rework so that this happens on the client side?
# If this is an include, filter out the class="noinclude" blocks.
# TODO: Any way to make this work in rendering? Possibly over-optimization,
# because this is often paired with ?section - so we'd need to store every
# section twice for with & without include sections
# If there's a translation to the requested locale, take it:
# Found a translation but its current_revision is None
# and OK to fall back to parent (parent is approved).
# There is no translation
# and OK to fall back to parent (parent is approved).
# Optimizing the queryset to fetch the required values only
# This is a translation but its current_revision is None
# and OK to fall back to parent (parent is approved).
# This page is under a content experiment
# Which variant was selected?
# Valid variant selected
# No (valid) variant selected
# Not a content experiment
# TODO: There will be no need for the following line of code when the
#       code that calls "clean_content" on Revision.save is deployed to
#       production, AND the current revisions of all docs have had their
#       content cleaned with "clean_content".
# From the Wiki domain, a logged-in user can demand fresh data with
# a shift-reload (which sends "Cache-Control: no-cache").
# Redirect is not to a Document, can't create subpage
# This is a "base level" redirect, i.e. no parent
# Is there a document at this slug, in this locale?
# Possible the document once existed, but is now deleted.
# If so, show that it was deleted.
# Show deletion log and restore / purge for soft-deleted docs
# We can throw a 404 immediately if the request type is HEAD.
# TODO: take a shortcut if the document was found?
# Check if we should fall back to default locale.
# If a Document is not found, we may 404 immediately based on
# request parameters.
# The user may be trying to create a child page; if a parent exists
# for this document, redirect them to the "Create" page
# Otherwise, they could be trying to create a main level doc.
# We found a Document. Now we need to figure out how we're going
# to display it.
# If we're a redirect, and redirecting hasn't been disabled, redirect.
# Obey explicit redirect pages:
# Don't redirect on redirect=no (like Wikipedia), so we can link from a
# redirected-to-page back to a "Redirected from..." link, so you can edit
# the redirect.
# TODO: Re-enable the link in this message after Django >1.5 upgrade
# Redirected from <a href="%(url)s?redirect=no">%(url)s</a>
# Read some request params to see what we're supposed to do.
# Are we in a content experiment?
# Get us some HTML to play with.
# Start parsing and applying filters.
# Get the SEO summary
# Get the additional title information, if necessary.
# Retrieve pre-parsed content hunks
# Record the English slug in Google Analytics,
# to associate translations
# Bundle it all up and, finally, return.
# We're doing this to prevent any unknown intermediate public HTTP caches
# from erroneously caching without considering cookies, since cookies do
# affect the content of the response. The primary CDN is configured to
# cache based on a whitelist of cookies.
# If any query parameter is used that is only supported by the wiki view,
# redirect to the wiki domain.
# Is there a document at this slug, in this locale?
# We can throw a 404 immediately if the request type is HEAD.
# TODO: take a shortcut if the document was found?
# Check if we should fall back to default locale.
# We found a Document. Now we need to figure out how we're going
# to display it.
# If we're a redirect, and redirecting hasn't been disabled, redirect.
# Obey explicit redirect pages:
# Don't redirect on redirect=no (like Wikipedia), so we can link from a
# redirected-to-page back to a "Redirected from..." link, so you can edit
# the redirect.
# TODO: Re-enable the link in this message after Django >1.5 upgrade
# Redirected from <a href="%(url)s?redirect=no">%(url)s</a>
# Get the SEO summary
# Get the additional title information, if necessary.
# Get the JSON data for this document
# Bundle it all up and, finally, return.
# TODO: anything we're actually using in the template ought
# to be bundled up into the json object above instead.
# Try parsing one of the supported content types from the request
# TODO: Refactor this into wiki.content ?
# First pass: Just assume the request body is an HTML fragment.
# Second pass: Try parsing the body as a fuller HTML document,
# and scrape out some of the interesting parts.
# Look for existing document to edit:
# Use ETags to detect mid-air edit collision
# see: http://www.w3.org/1999/04/Editing/
# Django's parse_etags returns a list of quoted rather than
# un-quoted ETags starting with version 1.11.
# TODO: There should be a model utility for creating a doc...
# Let's see if this slug path implies a parent...
# Apparently, this is a root page!
# There's a parent implied, so make sure we can find it.
# Create and save the new document; we'll revise it immediately.
# No section editing for new document!
# Process the content as if it were about to be saved, so that the
# html_diff is close as possible.
# Process the original content for a diff, extracting a section if we're
# editing one.
# When dealing with the raw content API, we need to signal the conflict
# differently so the client-side can escape out to a conflict
# resolution UI.
# Make this response iframe-friendly so we can hack around the
# save-and-edit iframe button
# TODO: Stop repeating this knowledge here and in Document.allows_editing_by.
# Required until CKEditor 4.7
# If this document has a parent, then the edit is handled by the
# translate view. Pass it on.
# Keep hold of the full post slug
# Update the slug, removing the parent path, and
# *only* using the last piece.
# This is only for the edit form.
# Need to make check *here* to see if this could have a translation parent
# You can't do anything on this page, so get lost.
# POST
# Attempt to set a parent
# Comparing against localized names for the Save button bothers me, so
# I embedded a hidden input:
# if must be here for section edits
# Get the possibly new slug for the imminent redirection:
# for rev_form.clean()
# Come up with the original revision to which these changes
# would be applied.
# Get the document's actual current revision.
# If this was an Ajax POST, then return a JsonResponse
# Was there a mid-air collision?
# Make the error message safe so the '<' and '>' don't
# get turned into '&lt;' and '&gt;', respectively
# Jump out to a function to escape indentation hell
# Was this an Ajax submission that was marked as spam?
# Return a JsonResponse
# If this is the raw view, and there was an original
# revision, but the original revision differed from the
# current revision at start of editing, we should tell
# the client to refresh the page.
# Is this an Ajax POST?
# This is the most recent revision id
# Construct the redirect URL, adding any needed parameters
# Only need to carry over ?edit_links with ?raw,
# because they're on by default in the normal UI
# If a section was edited, and we're using the raw
# content API, constrain to that section.
# Parameter for the document saved, so that we can delete the cached draft on load
# If a section was edited, jump to the section anchor
# if we're not getting raw content.
#%s' % (url, section_id)
# Legacy MindTouch redirects.
# These namespaces carry the old locale in their URL, which
# simplifies figuring out where to send them.
# For users, we look up the latest revision and get the locale
# from there.
# TODO: Tests do not include a matching revision
# If that doesn't work, bail out to en-US.
# Templates, etc. don't actually have a locale, so we give
# them the default.
# TODO: new_locale is unused, no alternate branch
# Convert from Django-based LocaleMiddleware path to zamboni/amo style
# MindTouch's default templates. There shouldn't be links to
# them anywhere in the wild, but just in case we 404 them.
# TODO: Tests don't exercise this branch
# If there's a trailing slash, snip it off.
# The namespaces (Talk:, User:, etc.) get their own
# special-case handling.
# TODO: Test invalid namespace
# Last attempt: we try the request locale as the document locale,
# and see if that matches something.
# Taggit offers a slug - but use name here, because the slugification
# stinks and is hard to customize.
# Load document with only fields for history display
# Process the requested page size
# Get ordered revision IDs
# Create pairs (this revision, previous revision)
# Paginate the revision pairs, or use all of them
# Include original English revision of the first translation
# Gather revisions on this history page, restricted to display fields
# Only handle actual autosuggest requests, not requests for a
# memory-busting list of all documents.
# Retrieve all documents that aren't redirects
# Remove old talk pages
# All locales are assumed, unless a specific locale is requested or banned
# Generates a list of acceptable docs
# TODO: Get doc ID from JSON.
# It should also be possible to compare from the parent document revision
# TODO: Find a better way to bail out of a collision.
# Ideal is to kick them to the diff view, but that expects
# fully-filled-out editing forms, and we don't have those
# here.
# We approved something, make the new revision.
# Required until CKEditor 4.7
# TODO: Refactor this view into two views? (new, edit)
# That might help reduce the headache-inducing branchiness.
# The parent document to translate from
# Get the mapping here and now so it can be used for input validation
# HACK: Seems weird, but sticking the translate-to locale in a query
# param is the best way to avoid the MindTouch-legacy locale
# redirection logic.
# The 'tolocale' query string parameters aren't free-text. They're
# explicitly listed on the "Select language" page (`...$locales`)
# If a locale was entered that wasn't a link it's a user bug.
# Set a "Discard Changes" page
# Don't translate to the default language.
# Find the "real" parent topic, which is its translation
# If there's an existing doc, populate form from it.
# If no existing doc, bring over the original title and slug.
# TODO: There will be no need to "filterEditorSafety" when the code
#       that calls "clean_content" on Revision.save is deployed to
#       production, AND the current revisions of all docs have had
#       their content cleaned with "clean_content".
# Grab the posted slug value in case it's invalid
# Sending a new copy of post so the slug change above
# doesn't cause problems during validation
# If we are submitting the whole form, we need to check that
# the Revision is valid before saving the Document.
# update the post data with the toc_depth of original
# Pass in the locale for the akistmet "blog_lang".
# for rev_form.clean()
# Attempt to set a parent
# If this is an Ajax POST, then return a JsonResponse
# Construct the redirect URL, adding any needed parameters
# Parameter for the document saved, so that we can delete the cached draft on load
# If this is an Ajax POST, then return a JsonResponse with error
# Make the error message safe so the '<' and '>' don't
# get turned into '&lt;' and '&gt;', respectively
# with this given: "some/kind/of/Path"
# 'Path'
# 'some/kind/of'
# 'some/kind/of/Path'
# ['some', 'kind', 'of']
# 4
# 'some'
# 'some'
# noqa
# noqa
#!/usr/bin/env python2.7
#
# Whenever a new table is created, add appropriate steps to anonymize.sql and
# then add the table here.  anonymize.sql may be run independantly, instead of
# this script, so make sure anonymize.sql performs sanitization as well.
#
# To remove a table from the anonymized database:
# Remove it from TABLES_TO_DUMP
# Add DROP TABLE IF EXISTS {table name}; to anonymize.sql
#
# To ensure an empty table in the anonymized database:
# Add to TABLES_TO_DUMP
# Add TRUNCATE {table name}; to anonymize.sql
#
# To anonymize records:
# Add to TABLES_TO_DUMP
# Add UPDATE {table name} ...; to anonymize.sql
#
# To keep production records:
# Add to TABLES_TO_DUMP
# Add a comment to anonymize.sql so future devs know you considered the table
#
# TODO: replace dump, create, import with mysqldbcopy
# https://dev.mysql.com/doc/mysql-utilities/1.3/en/mysqldbcopy.html
#!/usr/bin/env python
# API keys and IDs are set with environment variables rather than on the
# command line, to avoid displaying the key in logs
# URL patterns to compare two commits
# Create deployment parameters
# Send New Relic deployments
# Send SpeedCurve deployments
# Ignore this error
#add-a-deploy
# Split by commas or whitespace
# The pytest-base-url plugin adds --base-url, and sets the default from
# environment variable PYTEST_BASE_URL. If still unset, force to staging.
# Process the server status from _kuma_status.json
# Process the settings for this Kuma instance
# Setup dynamic fixtures
# Converted from SCL3 Apache files
#Rectangular_shape_example"),
#Moving_the_pen"),
#Lines"),
#Arcs"),
#Quadratic_Bezier_curves"),
#Cubic_Bezier_curves"),
#Drawing_images"),
#Example.3A_Tiling_an_image"),
#Example.3A_Framing_an_image"),
#Art_gallery_example"),
#An_example_using_rgba()"),
#A_lineWidth_example"),
#A_createLinearGradient_example"),
#A_createRadialGradient_example"),
#A_save_and_restore_canvas_state_example"),
#Clipping_paths"),
# Converted from SCL3 Apache files - demos moved to GitHub
# http://mdn.github.io
# canvas raycaster
# Bug 1215255 - Redirect static WebGL examples
# Converted from SCL3 Apache files - move to untrusted domain
# https://mdn.mozillademos.org/
# Converted from SCL3 Apache files - MindTouch / old hosted files
# bug 1362438
# bug 962148
# Test with a "docs" based path as well if it makes sense.
# The zone root without a trailing slash.
# The zone root with a trailing slash.
# A zone child page with query parameters.
# The zone root with $edit.
# A zone path with curly braces {}
# Redirects added after 2017 AWS move
#distributing-your-addon'),
# Verbose regex mode
# meta tag followed by whitespace
# name=robots
# capture the content
# end meta tag
# Pages with legacy MindTouch namespaces like 'User:' never get
# indexed, regardless of what the base url is
#wikiArticle')
# Test value tuple is:
# - Expected locale prefix
# - Accept-Language header value
# - django-language cookie settings (False to omit)
# - ?lang param value (False to omit)
#Color',
# while these test methods are similar, they're each testing a
# subset of redirects, and it was easier to work with them separately.
# Use pytest verbose asserts
# https://stackoverflow.com/questions/41522767/pytest-assert-introspection-in-helper-function
# seconds
# Untrusted attachments and samples domains that are indexed
# Main attachments domain
# Alternate attachments domain (testing)
# Attachments origin
# Kuma web domains that are indexed
# https://github.com/mozilla/bedrock/blob/master/tests/redirects/base.py
# urljoin messes with query strings too much
# https://github.com/mozilla/bedrock/blob/master/tests/redirects/base.py
# so that the value will appear in locals in test output
# all query values must be lists
# parse the QS from resp location header and compare to query arg
# since order doesn't matter.
# strip off query for further comparison
# https://github.com/mozilla/bedrock/blob/master/tests/redirects/base.py
