//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Spectral envelope estimation on the basis of the idea of CheapTrick.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// SmoothingWithRecovery() carries out the spectral smoothing and spectral
// recovery on the Cepstrum domain.
//-----------------------------------------------------------------------------
// We can control q1 as the parameter. 2015/9/22 by M. Morise
// const double q1 = -0.09;  // Please see the reference in CheapTrick.
//-----------------------------------------------------------------------------
// GetPowerSpectrum() calculates the power_spectrum with DC correction.
// DC stands for Direct Current. In this case, the component from 0 to F0 Hz
// is corrected.
//-----------------------------------------------------------------------------
// FFT
// Calculation of the power spectrum.
// DC correction
//-----------------------------------------------------------------------------
// SetParametersForGetWindowedWaveform()
//-----------------------------------------------------------------------------
// Designing of the window function
//-----------------------------------------------------------------------------
// GetWindowedWaveform() windows the waveform by F0-adaptive window
//-----------------------------------------------------------------------------
// F0-adaptive windowing
//-----------------------------------------------------------------------------
// CheapTrickGeneralBody() calculates a spectral envelope at a temporal
// position. This function is only used in CheapTrick().
// Caution:
//   forward_fft is allocated in advance to speed up the processing.
//-----------------------------------------------------------------------------
// F0-adaptive windowing
// Calculate power spectrum with DC correction
// Note: The calculated power spectrum is stored in an array for waveform.
// In this imprementation, power spectrum is transformed by FFT (NOT IFFT).
// However, the same result is obtained.
// This is tricky but important for simple implementation.
// Smoothing of the power (linear axis)
// forward_real_fft.waveform is the power spectrum.
// Smoothing (log axis) and spectral recovery on the cepstrum domain.
// namespace
// q1 is the parameter used for the spectral recovery.
// Since The parameter is optimized, you don't need to change the parameter.
// f0_floor and fs is used to determine fft_size;
// We strongly recommend not to change this value unless you have enough
// knowledge of the signal processing in CheapTrick.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// common.cpp includes functions used in at least two files.
// (1) Common functions
// (2) FFT, IFFT and minimum phase analysis.
//
// In FFT analysis and minimum phase analysis,
// Functions "Initialize*()" allocate the mamory.
// Functions "Destroy*()" free the accolated memory.
// FFT size is used for initialization, and structs are used to keep the memory.
// Functions "GetMinimumPhaseSpectrum()" calculate minimum phase spectrum.
// Forward and inverse FFT do not have the function "Get*()",
// because forward FFT and inverse FFT can run in one step.
//
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// SetParametersForLinearSmoothing() is used in LinearSmoothing()
//-----------------------------------------------------------------------------
// namespace
//-----------------------------------------------------------------------------
// Fundamental functions
//-----------------------------------------------------------------------------
// DCCorrection() corrects the input spectrum from 0 to f0 Hz because the
// general signal does not contain the DC (Direct Current) component.
// It is used in CheapTrick() and D4C().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// LinearSmoothing() carries out the spectral smoothing by rectangular window
// whose length is width Hz and is used in CheapTrick() and D4C().
//-----------------------------------------------------------------------------
// These parameters are set by the other function.
//-----------------------------------------------------------------------------
// NuttallWindow() calculates the coefficients of Nuttall window whose length
// is y_length and is used in Dio() and D4C().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FFT, IFFT and minimum phase analysis
// Mirroring
// This fft_plan carries out "forward" FFT.
// To carriy out the Inverse FFT, the sign of imaginary part
// is inverted after FFT.
// Since x is complex number, calculation of exp(x) is as following.
// Note: This FFT library does not keep the aliasing.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Band-aperiodicity estimation on the basis of the idea of D4C.
//-----------------------------------------------------------------------------
// for std::sort()
//-----------------------------------------------------------------------------
// SetParametersForGetWindowedWaveform()
//-----------------------------------------------------------------------------
// Designing of the window function
// Hanning window
// Blackman window
//-----------------------------------------------------------------------------
// GetWindowedWaveform() windows the waveform by F0-adaptive window
// In the variable window_type, 1: hanning, 2: blackman
//-----------------------------------------------------------------------------
// F0-adaptive windowing
//-----------------------------------------------------------------------------
// GetCentroid() calculates the energy centroid (see the book, time-frequency
// analysis written by L. Cohen).
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetStaticCentroid() calculates the temporally static energy centroid.
// Basic idea was proposed by H. Kawahara.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetSmoothedPowerSpectrum() calculates the smoothed power spectrum.
// The parameters used for smoothing are optimized in davance.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetStaticGroupDelay() calculates the temporally static group delay.
// This is the fundamental parameter in D4C.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetCoarseAperiodicity() calculates the aperiodicity in multiples of 3 kHz.
// The upper limit is given based on the sampling frequency.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// D4CGeneralBody() calculates a spectral envelope at a temporal
// position. This function is only used in D4C().
// Caution:
//   forward_fft is allocated in advance to speed up the processing.
//-----------------------------------------------------------------------------
// Revision of the result based on the F0
// namespace
// Since the window function is common in D4CGeneralBody(),
// it is designed here to speed up.
// Linear interpolation to convert the coarse aperiodicity into its
// spectral representation.
//  printf("Number of bands for aperiodicity: %d\n", number_of_aperiodicities);
// Since the window function is common in D4CGeneralBody(),
// it is designed here to speed up.
// osw
// osw: store coarse aper directly, don't store constant end values 
//        printf("     band number %d\n", j); 
//        printf("     band number %f\n", coarse_aperiodicity[j+1]); 
//         
//     // Linear interpolation to convert the coarse aperiodicity into its
//     // spectral representation.
//     interp1(coarse_frequency_axis, coarse_aperiodicity,
//         number_of_aperiodicities + 2, frequency_axis, fft_size / 2 + 1,
//         aperiodicity[i]);
//     for (int j = 0; j <= fft_size / 2; ++j)
//       aperiodicity[i][j] = pow(10.0, aperiodicity[i][j] / 20.0);
// This struct is dummy.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// F0 estimation based on DIO (Distributed Inline-filter Operation).
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// struct for RawEventByDio()
// "negative" means "zero-crossing point going from positive to negative"
// "positive" means "zero-crossing point going from negative to positive"
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// DesignLowCutFilter() calculates the coefficients the filter.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetDownsampledSignal() calculates the spectrum for estimation.
// This function carries out downsampling to speed up the estimation process
// and calculates the spectrum of the downsampled signal.
//-----------------------------------------------------------------------------
// Initialization
// Downsampling
// Removal of the DC component (y = y - mean value of y)
// Low cut filtering (from 0.1.4)
// Cutoff is 50.0 Hz
//-----------------------------------------------------------------------------
// GetBestF0Contour() calculates the best f0 contour based on stabilities of
// all candidates. The F0 whose stability is minimum is selected.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixStep1() is the 1st step of the postprocessing.
// This function eliminates the unnatural change of f0 based on allowed_range.
//-----------------------------------------------------------------------------
// Initialization
// Processing to prevent the jumping of f0
//-----------------------------------------------------------------------------
// FixStep2() is the 2nd step of the postprocessing.
// This function eliminates the suspected f0 in the anlaut and auslaut.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// CountNumberOfVoicedSections() counts the number of voiced sections.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// SelectOneF0() corrects the f0[current_index] based on
// f0[current_index + sign].
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixStep3() is the 3rd step of the postprocessing.
// This function corrects the f0 candidates from backward to forward.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// BackwardCorrection() is the 4th step of the postprocessing.
// This function corrects the f0 candidates from forward to backward.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixF0Contour() calculates the definitive f0 contour based on all f0
// candidates. There are four steps.
//-----------------------------------------------------------------------------
// memo:
// These are the tentative values. Optimization should be required.
//-----------------------------------------------------------------------------
// GetFilteredSignal() calculates the signal that is the convolution of the
// input signal and low-pass filter.
// This function is only used in RawEventByDio()
//-----------------------------------------------------------------------------
// Nuttall window is used as a low-pass filter.
// Cutoff frequency depends on the window length.
// Convolution
// Compensation of the delay.
//-----------------------------------------------------------------------------
// CheckEvent() returns 1, provided that the input value is over 1.
// This function is for RawEventByDio().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// ZeroCrossingEngine() calculates the zero crossing points from positive to
// negative. Thanks to Custom.Maid http://custom-made.seesaa.net/ (2012/8/19)
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetFourZeroCrossingIntervals() calculates four zero-crossing intervals.
// (1) Zero-crossing going from negative to positive.
// (2) Zero-crossing going from positive to negative.
// (3) Peak, and (4) dip. (3) and (4) are calculated from the zero-crossings of
// the differential of waveform.
//-----------------------------------------------------------------------------
// x_length / 4 (old version) is fixed at 2013/07/14
//-----------------------------------------------------------------------------
// GetF0CandidatesSub() calculates the f0 candidates and deviations.
// This is the sub-function of GetF0Candidates() and assumes the calculation.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetF0Candidates() calculates the F0 candidates based on the zero-crossings.
// Calculation of F0 candidates is carried out in GetF0CandidatesSub().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// DestroyZeroCrossings() frees the memory of array in the struct
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// RawEventByDio() calculates the zero-crossings.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetF0CandidateAndStabilityMap() calculates all f0 candidates and
// their stabilities.
//-----------------------------------------------------------------------------
// Calculation of the acoustics events (zero-crossing)
// A way to avoid zero division
//-----------------------------------------------------------------------------
// DioGeneralBody() estimates the F0 based on Distributed Inline-filter
// Operation.
//-----------------------------------------------------------------------------
// normalization
// Calculation of the spectrum used for the f0 estimation
// f0map represents all F0 candidates. We can modify them.
// Selection of the best value based on fundamental-ness.
// Postprocessing to find the best f0-contour.
// namespace
// You can change default parameters.
// You can use the value from 1 to 12.
// Default value 11 is for the fs of 44.1 kHz.
// The lower value you use, the better performance you can obtain.
// You can give a positive real number as the threshold.
// The most strict value is 0, and there is no upper limit.
// On the other hand, I think that the value from 0.02 to 0.2 is reasonable.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// This file represents the functions about FFT (Fast Fourier Transform)
// implemented by Mr. Ooura, and wrapper functions implemented by M. Morise.
// We can use these wrapper functions as well as the FFTW functions.
// Please see the FFTW web-page to show the usage of the wrapper functions.
// Ooura FFT:
//   (Japanese) http://www.kurims.kyoto-u.ac.jp/~ooura/index-j.html
//   (English) http://www.kurims.kyoto-u.ac.jp/~ooura/index.html
// FFTW:
//   (English) http://www.fftw.org/
// 2012/08/24 by M. Morise
//-----------------------------------------------------------------------------
// c2r
// c2c
// r2c
// c2c
// namespace
// ifft
//-----------------------------------------------------------------------
// The following functions are reffered by
// http://www.kurims.kyoto-u.ac.jp/~ooura/index.html
// -------- child routines --------
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Matlab functions implemented for WORLD
// Since these functions are implemented as the same function of Matlab,
// the source code does not follow the style guide (Names of variables
// and functions).
// Please see the reference of Matlab to show the usage of functions.
// Caution:
//   The functions wavread() and wavwrite() were removed to the /src.
//   they were moved to the test/audioio.cpp. (2016/01/28)
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FilterForDecimate() calculates the coefficients of low-pass filter and
// carries out the filtering. This function is only used for decimate().
//-----------------------------------------------------------------------------
// filter Coefficients
// fs : 44100 (default)
// fs : 48000
// fs : 32000
// fs : 24000 and 22050
// fs : 16000
// fs : 8000
// Filtering on time domain.
// namespace
// Bug was fixed at 2013/07/14 by M. Morise
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// F0 estimation based on instantaneous frequency.
// This method is carried out by using the output of Dio().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetIndexRaw() calculates the temporal positions for windowing.
// Since the result includes negative value and the value that exceeds the
// length of the input signal, it must be modified appropriately.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetMainWindow() generates the window function.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetDiffWindow() generates the differentiated window.
// Diff means differential.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetSpectra() calculates two spectra of the waveform windowed by windows
// (main window and diff window).
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixF0() fixed the F0 by instantaneous frequency.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetTentativeF0() calculates the F0 based on the instantaneous frequency.
// Calculated value is tentative because it is fixed as needed.
// Note: The sixth argument in FixF0() is not optimized.
//-----------------------------------------------------------------------------
// If the fixed value is too large, the result will be rejected.
//-----------------------------------------------------------------------------
// GetMeanF0() calculates the instantaneous frequency.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetRefinedF0() fixes the F0 estimated by Dio(). This function uses
// instantaneous frequency.
//-----------------------------------------------------------------------------
// A safeguard was added (2015/12/02).
// bug fix 2015/11/29
// If amount of correction is overlarge (20 %), initial F0 is employed.
// namespace
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Voice synthesis based on f0, spectrogram and aperiodicity.
// forward_real_fft, inverse_real_fft and minimum_phase are used to speed up.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetAperiodicResponse() calculates an aperiodic response.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetPeriodicResponse() calculates an aperiodic response.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetOneFrameSegment() calculates a periodic and aperiodic response at a time.
//-----------------------------------------------------------------------------
// Synthesis of the periodic response
// Synthesis of the aperiodic response
// namespace
//  printf("%d\n", number_of_pulses);
//    printf("%d %d\n", i, number_of_pulses);
//-----------------------------------------------------------------------------
// 
// Author: Zhizheng Wu (wuzhizheng@gmail.com)
// Date: 11-03-2016
//
// To extract F0, spectrum and band aperiodicities with WORLD vocoder
//
// This is modified based on Msanori Morise's test.cpp. Low-dimensional band aperiodicities are used as suggested by Oliver.
//
// analysis input_waveform F0_file spectrogram_file aperiodicity_file
//
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Note: win.sln uses an option in Additional Include Directories.
// To compile the program, the option "-I $(SolutionDir)..\src" was set.
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
// Modification of the option
// When you You must set the same value.
// If a different value is used, you may suffer a fatal error because of a
// illegal memory access.
// Valuable option.speed represents the ratio for downsampling.
// The signal is downsampled to fs / speed Hz.
// If you want to obtain the accurate result, speed should be set to 1.
// You should not set option.f0_floor to under world::kFloorF0.
// If you want to analyze such low F0 speech, please change world::kFloorF0.
// Processing speed may sacrify, provided that the FFT length changes.
// You can give a positive real number as the threshold.
// Most strict value is 0, but almost all results are counted as unvoiced.
// The value from 0.02 to 0.2 would be reasonable.
// Parameters setting and memory allocation.
// StoneMask is carried out to improve the estimation performance.
// This value may be better one for HMM speech synthesis.
// Default value is -0.09.
// Important notice (2016/02/02)
// You can control a parameter used for the lowest F0 in speech.
// You must not set the f0_floor to 0.
// It will cause a fatal error because fft_size indicates the infinity.
// You must not change the f0_floor after memory allocation.
// You should check the fft_size before excucing the analysis/synthesis.
// The default value (71.0) is strongly recommended.
// On the other hand, setting the lowest F0 of speech is a good choice
// to reduce the fft_size.
// Parameters setting and memory allocation.
// Parameters setting and memory allocation.
//    world_parameters->aperiodicity[i] =
//      new double[world_parameters->fft_size / 2 + 1];
// option is not implemented in this version. This is for future update.
// We can use "NULL" as the argument.
// F0 scaling
// Spectral stretching
// Synthesis by the aperiodicity
// namespace
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] sp file
// f0         : argv[3] ap file
// spec       : argv[4] f0 file
//-----------------------------------------------------------------------------
// 2016/01/28: Important modification.
// Memory allocation is carried out in advanse.
// This is for compatibility with C language.
// wavread() must be called after GetAudioLength().
//---------------------------------------------------------------------------
// Analysis part
//---------------------------------------------------------------------------
// 2016/02/02
// A new struct is introduced to implement safe program.
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// F0 estimation
// Spectral envelope estimation
// Aperiodicity estimation by D4C
//    for (int i=0; i<world_parameters.f0_length; i++) {
//        printf("%d %F\n", i, world_parameters.f0[i]);
//    }
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// .wav input/output functions were modified for compatibility with C language.
// Since these functions (wavread() and wavwrite()) are roughly implemented,
// we recommend more suitable functions provided by other organizations.
// This file is independent of WORLD project and for the test.cpp.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// CheckHeader() checks the .wav header. This function can only support the
// monaural wave file. This function is only used in waveread().
//-----------------------------------------------------------------------------
// "RIFF"
// "WAVE"
// "fmt "
// 1 0 0 0
// 1 0
// 1 0
//-----------------------------------------------------------------------------
// GetParameters() extracts fp, nbit, wav_length from the .wav file
// This function is only used in wavread().
//-----------------------------------------------------------------------------
// Quantization
// Skip until "data" is found. 2011/03/28
// "data"
// namespace
// Quantization
// "data"
// "data"
//-----------------------------------------------------------------------------
// 
// Author: Zhizheng Wu (wuzhizheng@gmail.com)
// Date: 11-03-2016
//
// To generate waveform given F0, band aperiodicities and spectrum with WORLD vocoder
//
// This is modified based on Msanori Morise's test.cpp. Low-dimensional band aperiodicities are used as suggested by Oliver.
//
// synth FFT_length sampling_rate F0_file spectrogram_file aperiodicity_file output_waveform
//
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Note: win.sln uses an option in Additional Include Directories.
// To compile the program, the option "-I $(SolutionDir)..\src" was set.
// Frame shift [msec]
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
// Modification of the option
// When you You must set the same value.
// If a different value is used, you may suffer a fatal error because of a
// illegal memory access.
// Valuable option.speed represents the ratio for downsampling.
// The signal is downsampled to fs / speed Hz.
// If you want to obtain the accurate result, speed should be set to 1.
// You should not set option.f0_floor to under world::kFloorF0.
// If you want to analyze such low F0 speech, please change world::kFloorF0.
// Processing speed may sacrify, provided that the FFT length changes.
// You can give a positive real number as the threshold.
// Most strict value is 0, but almost all results are counted as unvoiced.
// The value from 0.02 to 0.2 would be reasonable.
// Parameters setting and memory allocation.
// StoneMask is carried out to improve the estimation performance.
// This value may be better one for HMM speech synthesis.
// Default value is -0.09.
// Important notice (2016/02/02)
// You can control a parameter used for the lowest F0 in speech.
// You must not set the f0_floor to 0.
// It will cause a fatal error because fft_size indicates the infinity.
// You must not change the f0_floor after memory allocation.
// You should check the fft_size before excucing the analysis/synthesis.
// The default value (71.0) is strongly recommended.
// On the other hand, setting the lowest F0 of speech is a good choice
// to reduce the fft_size.
// Parameters setting and memory allocation.
// Parameters setting and memory allocation.
// option is not implemented in this version. This is for future update.
// We can use "NULL" as the argument.
// F0 scaling
// Spectral stretching
// Synthesis by the aperiodicity
//  printf("\nSynthesis\n");
//  printf("WORLD: %d [msec]\n", timeGetTime() - elapsed_time);
// namespace
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] Output file
// f0         : argv[3] F0 scaling (a positive number)
// spec       : argv[4] Formant shift (a positive number)
//-----------------------------------------------------------------------------
// compute n bands from fs as in d4c.cpp:325   
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// find number of frames (doubles) in f0 file:  
//  printf("%d\n", f0_length);
// aper
// convert bandaps to full aperiodic spectrum by interpolation (originally in d4c extraction):
// Linear interpolation to convert the coarse aperiodicity into its
// spectral representation.
// -- for interpolating --
// ----
// load band ap values for this frame into  coarse_aperiodicity
//printf("%d %d\n", world_parameters.f0_length, fs);
//---------------------------------------------------------------------------
// Synthesis part
//---------------------------------------------------------------------------
// The length of the output waveform
// Synthesis
// Output
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Test program for WORLD 0.1.2 (2012/08/19)
// Test program for WORLD 0.1.3 (2013/07/26)
// Test program for WORLD 0.1.4 (2014/04/29)
// Test program for WORLD 0.1.4_3 (2015/03/07)
// Test program for WORLD 0.2.0 (2015/05/29)
// Test program for WORLD 0.2.0_1 (2015/05/31)
// Test program for WORLD 0.2.0_2 (2015/06/06)
// Test program for WORLD 0.2.0_3 (2015/07/28)
// Test program for WORLD 0.2.0_4 (2015/11/15)
// Test program for WORLD in GitHub (2015/11/16-)
// Latest update: 2016/03/04
// test.exe input.wav outout.wav f0 spec
// input.wav  : Input file
// output.wav : Output file
// f0         : F0 scaling (a positive number)
// spec       : Formant scaling (a positive number)
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Note: win.sln uses an option in Additional Include Directories.
// To compile the program, the option "-I $(SolutionDir)..\src" was set.
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
// Modification of the option
// When you You must set the same value.
// If a different value is used, you may suffer a fatal error because of a
// illegal memory access.
// Valuable option.speed represents the ratio for downsampling.
// The signal is downsampled to fs / speed Hz.
// If you want to obtain the accurate result, speed should be set to 1.
// You should not set option.f0_floor to under world::kFloorF0.
// If you want to analyze such low F0 speech, please change world::kFloorF0.
// Processing speed may sacrify, provided that the FFT length changes.
// You can give a positive real number as the threshold.
// Most strict value is 0, but almost all results are counted as unvoiced.
// The value from 0.02 to 0.2 would be reasonable.
// Parameters setting and memory allocation.
// StoneMask is carried out to improve the estimation performance.
// This value may be better one for HMM speech synthesis.
// Default value is -0.09.
// Important notice (2016/02/02)
// You can control a parameter used for the lowest F0 in speech.
// You must not set the f0_floor to 0.
// It will cause a fatal error because fft_size indicates the infinity.
// You must not change the f0_floor after memory allocation.
// You should check the fft_size before excucing the analysis/synthesis.
// The default value (71.0) is strongly recommended.
// On the other hand, setting the lowest F0 of speech is a good choice
// to reduce the fft_size.
// Parameters setting and memory allocation.
// Parameters setting and memory allocation.
// option is not implemented in this version. This is for future update.
// We can use "NULL" as the argument.
// F0 scaling
// Spectral stretching
// Synthesis by the aperiodicity
// namespace
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] Output file
// f0         : argv[3] F0 scaling (a positive number)
// spec       : argv[4] Formant shift (a positive number)
//-----------------------------------------------------------------------------
// 2016/01/28: Important modification.
// Memory allocation is carried out in advanse.
// This is for compatibility with C language.
// wavread() must be called after GetAudioLength().
//---------------------------------------------------------------------------
// Analysis part
//---------------------------------------------------------------------------
// 2016/02/02
// A new struct is introduced to implement safe program.
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// F0 estimation
// Spectral envelope estimation
// Aperiodicity estimation by D4C
// Note that F0 must not be changed until all parameters are estimated.
//---------------------------------------------------------------------------
// Synthesis part
//---------------------------------------------------------------------------
// The length of the output waveform
// Synthesis
// Output
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Spectral envelope estimation on the basis of the idea of CheapTrick.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// SmoothingWithRecovery() carries out the spectral smoothing and spectral
// recovery on the Cepstrum domain.
//-----------------------------------------------------------------------------
// We can control q1 as the parameter. 2015/9/22 by M. Morise
// const double q1 = -0.09;  // Please see the reference in CheapTrick.
//-----------------------------------------------------------------------------
// GetPowerSpectrum() calculates the power_spectrum with DC correction.
// DC stands for Direct Current. In this case, the component from 0 to F0 Hz
// is corrected.
//-----------------------------------------------------------------------------
// FFT
// Calculation of the power spectrum.
// DC correction
//-----------------------------------------------------------------------------
// SetParametersForGetWindowedWaveform()
//-----------------------------------------------------------------------------
// Designing of the window function
//-----------------------------------------------------------------------------
// GetWindowedWaveform() windows the waveform by F0-adaptive window
//-----------------------------------------------------------------------------
// F0-adaptive windowing
//-----------------------------------------------------------------------------
// CheapTrickGeneralBody() calculates a spectral envelope at a temporal
// position. This function is only used in CheapTrick().
// Caution:
//   forward_fft is allocated in advance to speed up the processing.
//-----------------------------------------------------------------------------
// F0-adaptive windowing
// Calculate power spectrum with DC correction
// Note: The calculated power spectrum is stored in an array for waveform.
// In this imprementation, power spectrum is transformed by FFT (NOT IFFT).
// However, the same result is obtained.
// This is tricky but important for simple implementation.
// Smoothing of the power (linear axis)
// forward_real_fft.waveform is the power spectrum.
// Smoothing (log axis) and spectral recovery on the cepstrum domain.
// namespace
// q1 is the parameter used for the spectral recovery.
// Since The parameter is optimized, you don't need to change the parameter.
// f0_floor and fs is used to determine fft_size;
// We strongly recommend not to change this value unless you have enough
// knowledge of the signal processing in CheapTrick.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// common.cpp includes functions used in at least two files.
// (1) Common functions
// (2) FFT, IFFT and minimum phase analysis.
//
// In FFT analysis and minimum phase analysis,
// Functions "Initialize*()" allocate the mamory.
// Functions "Destroy*()" free the accolated memory.
// FFT size is used for initialization, and structs are used to keep the memory.
// Functions "GetMinimumPhaseSpectrum()" calculate minimum phase spectrum.
// Forward and inverse FFT do not have the function "Get*()",
// because forward FFT and inverse FFT can run in one step.
//
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// SetParametersForLinearSmoothing() is used in LinearSmoothing()
//-----------------------------------------------------------------------------
// namespace
//-----------------------------------------------------------------------------
// Fundamental functions
//-----------------------------------------------------------------------------
// DCCorrection() corrects the input spectrum from 0 to f0 Hz because the
// general signal does not contain the DC (Direct Current) component.
// It is used in CheapTrick() and D4C().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// LinearSmoothing() carries out the spectral smoothing by rectangular window
// whose length is width Hz and is used in CheapTrick() and D4C().
//-----------------------------------------------------------------------------
// These parameters are set by the other function.
//-----------------------------------------------------------------------------
// NuttallWindow() calculates the coefficients of Nuttall window whose length
// is y_length and is used in Dio() and D4C().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FFT, IFFT and minimum phase analysis
// Mirroring
// This fft_plan carries out "forward" FFT.
// To carriy out the Inverse FFT, the sign of imaginary part
// is inverted after FFT.
// Since x is complex number, calculation of exp(x) is as following.
// Note: This FFT library does not keep the aliasing.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Band-aperiodicity estimation on the basis of the idea of D4C.
//-----------------------------------------------------------------------------
// for std::sort()
//-----------------------------------------------------------------------------
// SetParametersForGetWindowedWaveform()
//-----------------------------------------------------------------------------
// Designing of the window function
// Hanning window
// Blackman window
//-----------------------------------------------------------------------------
// GetWindowedWaveform() windows the waveform by F0-adaptive window
// In the variable window_type, 1: hanning, 2: blackman
//-----------------------------------------------------------------------------
// F0-adaptive windowing
//-----------------------------------------------------------------------------
// GetCentroid() calculates the energy centroid (see the book, time-frequency
// analysis written by L. Cohen).
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetStaticCentroid() calculates the temporally static energy centroid.
// Basic idea was proposed by H. Kawahara.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetSmoothedPowerSpectrum() calculates the smoothed power spectrum.
// The parameters used for smoothing are optimized in davance.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetStaticGroupDelay() calculates the temporally static group delay.
// This is the fundamental parameter in D4C.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetCoarseAperiodicity() calculates the aperiodicity in multiples of 3 kHz.
// The upper limit is given based on the sampling frequency.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// D4CGeneralBody() calculates a spectral envelope at a temporal
// position. This function is only used in D4C().
// Caution:
//   forward_fft is allocated in advance to speed up the processing.
//-----------------------------------------------------------------------------
// Revision of the result based on the F0
// namespace
// Since the window function is common in D4CGeneralBody(),
// it is designed here to speed up.
// Linear interpolation to convert the coarse aperiodicity into its
// spectral representation.
//  printf("Number of bands for aperiodicity: %d\n", number_of_aperiodicities);
// Since the window function is common in D4CGeneralBody(),
// it is designed here to speed up.
// osw
// osw: store coarse aper directly, don't store constant end values 
//        printf("     band number %d\n", j); 
//        printf("     band number %f\n", coarse_aperiodicity[j+1]); 
//         
//     // Linear interpolation to convert the coarse aperiodicity into its
//     // spectral representation.
//     interp1(coarse_frequency_axis, coarse_aperiodicity,
//         number_of_aperiodicities + 2, frequency_axis, fft_size / 2 + 1,
//         aperiodicity[i]);
//     for (int j = 0; j <= fft_size / 2; ++j)
//       aperiodicity[i][j] = pow(10.0, aperiodicity[i][j] / 20.0);
// This struct is dummy.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// F0 estimation based on DIO (Distributed Inline-filter Operation).
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// struct for RawEventByDio()
// "negative" means "zero-crossing point going from positive to negative"
// "positive" means "zero-crossing point going from negative to positive"
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// DesignLowCutFilter() calculates the coefficients the filter.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetDownsampledSignal() calculates the spectrum for estimation.
// This function carries out downsampling to speed up the estimation process
// and calculates the spectrum of the downsampled signal.
//-----------------------------------------------------------------------------
// Initialization
// Downsampling
// Removal of the DC component (y = y - mean value of y)
// Low cut filtering (from 0.1.4)
// Cutoff is 50.0 Hz
//-----------------------------------------------------------------------------
// GetBestF0Contour() calculates the best f0 contour based on stabilities of
// all candidates. The F0 whose stability is minimum is selected.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixStep1() is the 1st step of the postprocessing.
// This function eliminates the unnatural change of f0 based on allowed_range.
//-----------------------------------------------------------------------------
// Initialization
// Processing to prevent the jumping of f0
//-----------------------------------------------------------------------------
// FixStep2() is the 2nd step of the postprocessing.
// This function eliminates the suspected f0 in the anlaut and auslaut.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// CountNumberOfVoicedSections() counts the number of voiced sections.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// SelectOneF0() corrects the f0[current_index] based on
// f0[current_index + sign].
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixStep3() is the 3rd step of the postprocessing.
// This function corrects the f0 candidates from backward to forward.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// BackwardCorrection() is the 4th step of the postprocessing.
// This function corrects the f0 candidates from forward to backward.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixF0Contour() calculates the definitive f0 contour based on all f0
// candidates. There are four steps.
//-----------------------------------------------------------------------------
// memo:
// These are the tentative values. Optimization should be required.
//-----------------------------------------------------------------------------
// GetFilteredSignal() calculates the signal that is the convolution of the
// input signal and low-pass filter.
// This function is only used in RawEventByDio()
//-----------------------------------------------------------------------------
// Nuttall window is used as a low-pass filter.
// Cutoff frequency depends on the window length.
// Convolution
// Compensation of the delay.
//-----------------------------------------------------------------------------
// CheckEvent() returns 1, provided that the input value is over 1.
// This function is for RawEventByDio().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// ZeroCrossingEngine() calculates the zero crossing points from positive to
// negative. Thanks to Custom.Maid http://custom-made.seesaa.net/ (2012/8/19)
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetFourZeroCrossingIntervals() calculates four zero-crossing intervals.
// (1) Zero-crossing going from negative to positive.
// (2) Zero-crossing going from positive to negative.
// (3) Peak, and (4) dip. (3) and (4) are calculated from the zero-crossings of
// the differential of waveform.
//-----------------------------------------------------------------------------
// x_length / 4 (old version) is fixed at 2013/07/14
//-----------------------------------------------------------------------------
// GetF0CandidatesSub() calculates the f0 candidates and deviations.
// This is the sub-function of GetF0Candidates() and assumes the calculation.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetF0Candidates() calculates the F0 candidates based on the zero-crossings.
// Calculation of F0 candidates is carried out in GetF0CandidatesSub().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// DestroyZeroCrossings() frees the memory of array in the struct
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// RawEventByDio() calculates the zero-crossings.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetF0CandidateAndStabilityMap() calculates all f0 candidates and
// their stabilities.
//-----------------------------------------------------------------------------
// Calculation of the acoustics events (zero-crossing)
// A way to avoid zero division
//-----------------------------------------------------------------------------
// DioGeneralBody() estimates the F0 based on Distributed Inline-filter
// Operation.
//-----------------------------------------------------------------------------
// normalization
// Calculation of the spectrum used for the f0 estimation
// f0map represents all F0 candidates. We can modify them.
// Selection of the best value based on fundamental-ness.
// Postprocessing to find the best f0-contour.
// namespace
// You can change default parameters.
// You can use the value from 1 to 12.
// Default value 11 is for the fs of 44.1 kHz.
// The lower value you use, the better performance you can obtain.
// You can give a positive real number as the threshold.
// The most strict value is 0, and there is no upper limit.
// On the other hand, I think that the value from 0.02 to 0.2 is reasonable.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// This file represents the functions about FFT (Fast Fourier Transform)
// implemented by Mr. Ooura, and wrapper functions implemented by M. Morise.
// We can use these wrapper functions as well as the FFTW functions.
// Please see the FFTW web-page to show the usage of the wrapper functions.
// Ooura FFT:
//   (Japanese) http://www.kurims.kyoto-u.ac.jp/~ooura/index-j.html
//   (English) http://www.kurims.kyoto-u.ac.jp/~ooura/index.html
// FFTW:
//   (English) http://www.fftw.org/
// 2012/08/24 by M. Morise
//-----------------------------------------------------------------------------
// c2r
// c2c
// r2c
// c2c
// namespace
// ifft
//-----------------------------------------------------------------------
// The following functions are reffered by
// http://www.kurims.kyoto-u.ac.jp/~ooura/index.html
// -------- child routines --------
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Matlab functions implemented for WORLD
// Since these functions are implemented as the same function of Matlab,
// the source code does not follow the style guide (Names of variables
// and functions).
// Please see the reference of Matlab to show the usage of functions.
// Caution:
//   The functions wavread() and wavwrite() were removed to the /src.
//   they were moved to the test/audioio.cpp. (2016/01/28)
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FilterForDecimate() calculates the coefficients of low-pass filter and
// carries out the filtering. This function is only used for decimate().
//-----------------------------------------------------------------------------
// filter Coefficients
// fs : 44100 (default)
// fs : 48000
// fs : 32000
// fs : 24000 and 22050
// fs : 16000
// fs : 8000
// Filtering on time domain.
// namespace
// Bug was fixed at 2013/07/14 by M. Morise
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// F0 estimation based on instantaneous frequency.
// This method is carried out by using the output of Dio().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetIndexRaw() calculates the temporal positions for windowing.
// Since the result includes negative value and the value that exceeds the
// length of the input signal, it must be modified appropriately.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetMainWindow() generates the window function.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetDiffWindow() generates the differentiated window.
// Diff means differential.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetSpectra() calculates two spectra of the waveform windowed by windows
// (main window and diff window).
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixF0() fixed the F0 by instantaneous frequency.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetTentativeF0() calculates the F0 based on the instantaneous frequency.
// Calculated value is tentative because it is fixed as needed.
// Note: The sixth argument in FixF0() is not optimized.
//-----------------------------------------------------------------------------
// If the fixed value is too large, the result will be rejected.
//-----------------------------------------------------------------------------
// GetMeanF0() calculates the instantaneous frequency.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetRefinedF0() fixes the F0 estimated by Dio(). This function uses
// instantaneous frequency.
//-----------------------------------------------------------------------------
// A safeguard was added (2015/12/02).
// bug fix 2015/11/29
// If amount of correction is overlarge (20 %), initial F0 is employed.
// namespace
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Voice synthesis based on f0, spectrogram and aperiodicity.
// forward_real_fft, inverse_real_fft and minimum_phase are used to speed up.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetAperiodicResponse() calculates an aperiodic response.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetPeriodicResponse() calculates an aperiodic response.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetOneFrameSegment() calculates a periodic and aperiodic response at a time.
//-----------------------------------------------------------------------------
// Synthesis of the periodic response
// Synthesis of the aperiodic response
// namespace
//  printf("%d\n", number_of_pulses);
//    printf("%d %d\n", i, number_of_pulses);
//-----------------------------------------------------------------------------
// 
// Author: Zhizheng Wu (wuzhizheng@gmail.com)
// Date: 11-03-2016
//
// To extract F0, spectrum and band aperiodicities with WORLD vocoder
//
// This is modified based on Msanori Morise's test.cpp. Low-dimensional band aperiodicities are used as suggested by Oliver.
//
// analysis input_waveform F0_file spectrogram_file aperiodicity_file
//
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Note: win.sln uses an option in Additional Include Directories.
// To compile the program, the option "-I $(SolutionDir)..\src" was set.
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
//int number_of_aperiodicities;
// Modification of the option
// When you You must set the same value.
// If a different value is used, you may suffer a fatal error because of a
// illegal memory access.
// Valuable option.speed represents the ratio for downsampling.
// The signal is downsampled to fs / speed Hz.
// If you want to obtain the accurate result, speed should be set to 1.
// You should not set option.f0_floor to under world::kFloorF0.
// If you want to analyze such low F0 speech, please change world::kFloorF0.
// Processing speed may sacrify, provided that the FFT length changes.
// You can give a positive real number as the threshold.
// Most strict value is 0, but almost all results are counted as unvoiced.
// The value from 0.02 to 0.2 would be reasonable.
// Parameters setting and memory allocation.
// StoneMask is carried out to improve the estimation performance.
// This value may be better one for HMM speech synthesis.
// Default value is -0.09.
// Important notice (2016/02/02)
// You can control a parameter used for the lowest F0 in speech.
// You must not set the f0_floor to 0.
// It will cause a fatal error because fft_size indicates the infinity.
// You must not change the f0_floor after memory allocation.
// You should check the fft_size before excucing the analysis/synthesis.
// The default value (71.0) is strongly recommended.
// On the other hand, setting the lowest F0 of speech is a good choice
// to reduce the fft_size.
// Parameters setting and memory allocation.
//int number_of_aperiodicities =
//  static_cast<int>(MyMinDouble(world::kUpperLimit, world_parameters->fs / 2.0 -
//    world::kFrequencyInterval) / world::kFrequencyInterval);
// Parameters setting and memory allocation.
//world_parameters->aperiodicity[i] = new double[number_of_aperiodicities];
//world_parameters->number_of_aperiodicities = number_of_aperiodicities;
// option is not implemented in this version. This is for future update.
// We can use "NULL" as the argument.
// F0 scaling
// Spectral stretching
// Synthesis by the aperiodicity
// namespace
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] sp file
// f0         : argv[3] ap file
// spec       : argv[4] f0 file
//-----------------------------------------------------------------------------
// 2016/01/28: Important modification.
// Memory allocation is carried out in advanse.
// This is for compatibility with C language.
// wavread() must be called after GetAudioLength().
//---------------------------------------------------------------------------
// Analysis part
//---------------------------------------------------------------------------
// 2016/02/02
// A new struct is introduced to implement safe program.
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// F0 estimation
// Spectral envelope estimation
// Aperiodicity estimation by D4C
//fwrite(world_parameters.aperiodicity[i], sizeof(double), world_parameters.number_of_aperiodicities, fap);
//    for (int i=0; i<world_parameters.f0_length; i++) {
//        printf("%d %F\n", i, world_parameters.f0[i]);
//    }
//printf("%d %d %d\n", world_parameters.f0_length, world_parameters.fft_size, world_parameters.number_of_aperiodicities);
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// .wav input/output functions were modified for compatibility with C language.
// Since these functions (wavread() and wavwrite()) are roughly implemented,
// we recommend more suitable functions provided by other organizations.
// This file is independent of WORLD project and for the test.cpp.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// CheckHeader() checks the .wav header. This function can only support the
// monaural wave file. This function is only used in waveread().
//-----------------------------------------------------------------------------
// "RIFF"
// "WAVE"
// "fmt "
// 1 0 0 0
// 1 0
// 1 0
//-----------------------------------------------------------------------------
// GetParameters() extracts fp, nbit, wav_length from the .wav file
// This function is only used in wavread().
//-----------------------------------------------------------------------------
// Quantization
// Skip until "data" is found. 2011/03/28
// "data"
// namespace
// Quantization
// "data"
// "data"
//-----------------------------------------------------------------------------
// 
// Author: Zhizheng Wu (wuzhizheng@gmail.com)
// Date: 11-03-2016
//
// To generate waveform given F0, band aperiodicities and spectrum with WORLD vocoder
//
// This is modified based on Msanori Morise's test.cpp. Low-dimensional band aperiodicities are used as suggested by Oliver.
//
// synth FFT_length sampling_rate F0_file spectrogram_file aperiodicity_file output_waveform
//
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Note: win.sln uses an option in Additional Include Directories.
// To compile the program, the option "-I $(SolutionDir)..\src" was set.
// Frame shift [msec]
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
// Synthesis by the aperiodicity
//  printf("\nSynthesis\n");
//  elapsed_time = timeGetTime();
//  printf("WORLD: %d [msec]\n", timeGetTime() - elapsed_time);
// namespace
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] Output file
// f0         : argv[3] F0 scaling (a positive number)
// spec       : argv[4] Formant shift (a positive number)
//-----------------------------------------------------------------------------
// compute n bands from fs as in d4c.cpp:325   
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// find number of frames (doubles) in f0 file:  
//  printf("%d\n", f0_length);
// aper
// convert bandaps to full aperiodic spectrum by interpolation (originally in d4c extraction):
// Linear interpolation to convert the coarse aperiodicity into its
// spectral representation.
// -- for interpolating --
// ----
// load band ap values for this frame into  coarse_aperiodicity
//printf("%d %d\n", world_parameters.f0_length, fs);
//---------------------------------------------------------------------------
// Synthesis part
//---------------------------------------------------------------------------
// The length of the output waveform
// Synthesis
// Output
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Test program for WORLD 0.1.2 (2012/08/19)
// Test program for WORLD 0.1.3 (2013/07/26)
// Test program for WORLD 0.1.4 (2014/04/29)
// Test program for WORLD 0.1.4_3 (2015/03/07)
// Test program for WORLD 0.2.0 (2015/05/29)
// Test program for WORLD 0.2.0_1 (2015/05/31)
// Test program for WORLD 0.2.0_2 (2015/06/06)
// Test program for WORLD 0.2.0_3 (2015/07/28)
// Test program for WORLD 0.2.0_4 (2015/11/15)
// Test program for WORLD in GitHub (2015/11/16-)
// Latest update: 2016/03/04
// test.exe input.wav outout.wav f0 spec
// input.wav  : Input file
// output.wav : Output file
// f0         : F0 scaling (a positive number)
// spec       : Formant scaling (a positive number)
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Note: win.sln uses an option in Additional Include Directories.
// To compile the program, the option "-I $(SolutionDir)..\src" was set.
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
// Modification of the option
// When you You must set the same value.
// If a different value is used, you may suffer a fatal error because of a
// illegal memory access.
// Valuable option.speed represents the ratio for downsampling.
// The signal is downsampled to fs / speed Hz.
// If you want to obtain the accurate result, speed should be set to 1.
// You should not set option.f0_floor to under world::kFloorF0.
// If you want to analyze such low F0 speech, please change world::kFloorF0.
// Processing speed may sacrify, provided that the FFT length changes.
// You can give a positive real number as the threshold.
// Most strict value is 0, but almost all results are counted as unvoiced.
// The value from 0.02 to 0.2 would be reasonable.
// Parameters setting and memory allocation.
// StoneMask is carried out to improve the estimation performance.
// This value may be better one for HMM speech synthesis.
// Default value is -0.09.
// Important notice (2016/02/02)
// You can control a parameter used for the lowest F0 in speech.
// You must not set the f0_floor to 0.
// It will cause a fatal error because fft_size indicates the infinity.
// You must not change the f0_floor after memory allocation.
// You should check the fft_size before excucing the analysis/synthesis.
// The default value (71.0) is strongly recommended.
// On the other hand, setting the lowest F0 of speech is a good choice
// to reduce the fft_size.
// Parameters setting and memory allocation.
// Parameters setting and memory allocation.
// option is not implemented in this version. This is for future update.
// We can use "NULL" as the argument.
// F0 scaling
// Spectral stretching
// Synthesis by the aperiodicity
// namespace
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] Output file
// f0         : argv[3] F0 scaling (a positive number)
// spec       : argv[4] Formant shift (a positive number)
//-----------------------------------------------------------------------------
// 2016/01/28: Important modification.
// Memory allocation is carried out in advanse.
// This is for compatibility with C language.
// wavread() must be called after GetAudioLength().
//---------------------------------------------------------------------------
// Analysis part
//---------------------------------------------------------------------------
// 2016/02/02
// A new struct is introduced to implement safe program.
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// F0 estimation
// Spectral envelope estimation
// Aperiodicity estimation by D4C
// Note that F0 must not be changed until all parameters are estimated.
//---------------------------------------------------------------------------
// Synthesis part
//---------------------------------------------------------------------------
// The length of the output waveform
// Synthesis
// Output
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
# Install it with pip (it's not the same as 'ConfigParser' (old version))
# Display:
# File setup:
# INPUT:===================================================================================================
# Experiment type:-----------------------------------------------------------------------
#  'demo' (50 training utts) or 'full' (1k training utts)
# Steps:---------------------------------------------------------------------------------
# Downloads wavs and label data.
# Copies downloaded data into the experiment directory. Plus, makes a backup copy of this script.
# Saves new configuration files for Merlin.
# Performs acoustic feature extraction using the MagPhase vocoder
# Converts the state aligned labels to variable rate if running in variable frame rate mode (d_mp_opts['b_const_rate'] = False)
# Merlin: Training of duration model.
# Merlin: Training of acoustic model.
# Merlin: Generation of state durations using the duration model.
# Merlin: Waveform generation for the utterances provided in ./test_synthesis/prompt-lab
# MagPhase Vocoder:-----------------------------------------------------------------------
# Dictionary containing internal options for the MagPhase vocoder (mp).
# Number of coefficients (bins) for magnitude feature M.
# Number of coefficients (bins) for phase features R and I.
# To work in constant frame rate mode.
#  List containing the postfilters to apply during waveform generation.
# You need to choose at least one: 'magphase' (magphase-tailored postfilter), 'merlin' (Merlin's style postfilter), 'no' (no postfilter)
# Acoustic feature extraction done in multiprocessing mode (faster).
# PROCESS:===================================================================================================
# Pre setup:-------------------------------------------------------------------------------
# Build config parsers:-------------------------------------------------------------------
# Duration training config file:
# Duration synthesis:
# Acoustic training:
# Acoustic synth:
# Download Data:--------------------------------------------------------------------------
# Setup Data:-----------------------------------------------------------------------------
# Configure Merlin:-----------------------------------------------------------------------
# Read file list:
#').tolist()
# Acoustic Feature Extraction:-------------------------------------------------------------
# Extract features:
# Labels Conversion to Variable Frame Rate:------------------------------------------------
# NOTE: The script ./script/label_st_align_to_var_rate.py can be also called from comand line directly.
# Run duration training:-------------------------------------------------------------------
# Run acoustic train:----------------------------------------------------------------------
# Run duration syn:------------------------------------------------------------------------
# Run acoustic synth:----------------------------------------------------------------------
#!/usr/bin/python
#URL = 'http://www.cs.toronto.edu/~murray/code/gpu_monitoring/'
# Get ID's of NVIDIA boards. Should do this through a CUDA call, but this is
# a quick and dirty way that works for now:
# /tmp is cleared on reboot on many systems, but it doesn't have to be
# /dev/shm on linux machines is a RAM disk, so is definitely cleared
#    print   id
# On POSIX systems symlink creation is atomic, so this should be a
# robust locking operation:
# On POSIX systems os.rename is an atomic operation, so this is the safe
# way to delete a lock:
# If run as a program:
# Report
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
# Conversion:
#')
# Display:
# Current i/o files:
# Debug:
#'''
# Parsing input arg:
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
#import gnumpy as gnp
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#import theano
# the new class for label composition and normalisation
#from frontend.mlpg_fast import MLParameterGenerationFast
#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
#    sequential_training = True
###################
#validation data is still read block by block
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
#    pretrain_set_x = train_set_x
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
#        dnn_model = DeepRecurrentNetwork(n_in= n_ins, hidden_layer_size = hidden_layer_size, n_out = n_outs, L1_reg = l1_reg, L2_reg = l2_reg, hidden_layer_type = hidden_layer_type)
#        dnn_model = SequentialDNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,
#                        l1_reg = l1_reg, l2_reg = l2_reg,
#                         hidden_layer_sizes = hidden_layer_size)
#    finetune_lr = 0.000125
#, valid_set_y
#            logger.debug('validation loss decreased, so saving model')
#            dbn = best_dnn_model
#        if early_stop > early_stop_epoch:
#            logger.debug('stopping early')
#            break
#    cPickle.dump(best_dnn_model, open(nnets_file_name, 'wb'))
#file_number
### write to cmp file
#file_number
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
# to do - sanity check the label dimension here?
# simple HTS labels
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output acoustic data
#[-0.5, 0.0, 0.5]
#[1.0, -2.0, 1.0]
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
### DNN model training
# not an error - just means directory already exists
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
### generate parameters from DNN
# not an error - just means directory already exists
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
##MCD
##MCD
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
#    if gnp._boardId is not None:
#        import gpu_lock
#        gpu_lock.free_lock(gnp._boardId)
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
#import gnumpy as gnp
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#import theano
# the new class for label composition and normalisation
#from frontend.mlpg_fast import MLParameterGenerationFast
#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
#    sequential_training = True
###################
#validation data is still read block by block
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
#    pretrain_set_x = train_set_x
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
#        dnn_model = DeepRecurrentNetwork(n_in= n_ins, hidden_layer_size = hidden_layer_size, n_out = n_outs, L1_reg = l1_reg, L2_reg = l2_reg, hidden_layer_type = hidden_layer_type)
#    finetune_lr = 0.000125
#            logger.debug('validation loss decreased, so saving model')
#            dbn = best_dnn_model
#        if early_stop > early_stop_epoch:
#            logger.debug('stopping early')
#            break
#    cPickle.dump(best_dnn_model, open(nnets_file_name, 'wb'))
#file_number
### write to cmp file
#file_number
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
# to do - sanity check the label dimension here?
# simple HTS labels
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output acoustic data
#[-0.5, 0.0, 0.5]
#[1.0, -2.0, 1.0]
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
### DNN model training
# not an error - just means directory already exists
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
### generate parameters from DNN
# not an error - just means directory already exists
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
##MCD
##MCD
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
#    if gnp._boardId is not None:
#        import gpu_lock
#        gpu_lock.free_lock(gnp._boardId)
# string constants for various shell calls
##
## make proto
## make vFloors
## make local macro
# get first three lines from local proto
# get remaining lines from vFloors
## make hmmdefs
# ignore
# the header
# the rest
#!MLF!#\n')
# write a CFG for extracting MFCCs
# write a CFG for what we just built
# pass on the previously new one to the old one
# increment
# compute the path for the new one
# make the new directory
# HMMs
# SCP files
# CFG
## save to itself
##increase mixture number
## if multiple_speaker is tuned on. the file_id_list.scp has to reflact this
## for example
## speaker_1/0001
## speaker_2/0001
## This is to do speaker-dependent normalisation
# Copyright (c) 2007 Carnegie Mellon University
#
# You may copy and modify this freely under the same terms as
# Sphinx-III
# has energy
# absolute energy supressed
# has delta coefficients
# has acceleration (delta-delta) coefficients
# is compressed
# has zero mean static coefficients
# has CRC checksum
# has 0th cepstral coefficient
# has VQ data
# has third differential coefficients
# veclen is ignored since it's in the file
# Get coefficients for compressed data
# Uncompress data to floats if required
#        print   self.nSamples, self.veclen
#        print   self.parmKind, self.sampPeriod
#        print   len(data), data.shape
#        if self.parmKind & _K: # Remove and ignore checksum
#            data = data[:-1]
#        print   data.shape
#        data = tmp_data.reshape((len(tmp_data)/self.veclen, self.veclen))
# Uncompress data to floats if required
# HTK datatybes
# Additional 'param kind' options
#has energy
#absolute energy suppressed
#has delta coefficients
#has acceleration coefficients
#is compressed
#has zero mean static coef.
#has CRC checksum
#has 0th cepstral coef.
#has VQ data
#has third differential coef.
# the first 6 bits contain datatype
# HTK header
# number of samples in file (4-byte integer)
# sample period in 100ns units (4-byte integer)
# number of bytes per sample (2-byte integer)
# a code indicating the sample kind (2-byte integer)
#TODO compression
#self.A = struct.unpack('>H', f.read(2))[0]
#self.B = struct.unpack('>H', f.read(2))[0]
#                print   "world"
#                print   "hello"
# forces big-endian byte ordering
# force big-endian byte ordering
#filename_src = "../data/GE001_1.feat"
#print "t", htk.dupa, sys.byteorder
#        self.feature_dimension = feature_dimension
#            print   current_frame_number
#            features = io_funcs.data
#            current_frame_number = io_funcs.n_samples
#            htk_writter = HTK_Parm_IO(n_samples=io_funcs.n_samples, samp_period=io_funcs.samp_period, samp_size=io_funcs.samp_size, param_kind=io_funcs.param_kind, data=norm_features)
#            htk_writter.write_htk(out_file_list[i])
#            io_funcs = HTK_Parm_IO()
#            io_funcs.read_htk(file_name)
#            features = io_funcs.data
#            current_frame_number = io_funcs.n_samples
# setting the print options in this way seems to break subsequent printing of numpy float32 types
# no idea what is going on - removed until this can be solved
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
# setting the print options in this way seems to break subsequent printing of numpy float32 types
# no idea what is going on - removed until this can be solved
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
## remove begining and end double quotes
### if you want to use a particular voice
#out_f1.write("(voice_cstr_edi_fls_multisyn)\n")
#if nof_each_state<1:
#    print 'warning: some states are with zero duration'
#print ph
#break;
#!/usr/bin/env python
### create tcoef dir if not exists ###
#print "vfloor: {0}".format(vfloor)
#set to the frame value if there is no range!
#### User configurable variables ####
#### Train and test file lists ####
#### calculate variance flooring for each feature (from only training files) ####
#### calculate tcoef features ####
### read file by file ###
### read label file ###
### process label file ###
# remove state information [k]
#\n')
#!/usr/bin/env python
# one stream
#remove unvoiced values
#convert to linear scale
#', ignoreSilence=True):
#exclude first line!
#keep as frames or convert to time?! Currently kept in frames
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
# top merlin directory
# input audio directory
# Output features directory
# Expected sample rate
# Magphase directory
# Parsing path:
# Display:
# FILES SETUP:========================================================================
# MULTIPROCESSING EXTRACTION:==========================================================
# For debugging (don't delete):
#for wavfile in l_wavfiles:
#    feat_extraction(wavfile, out_dir)
# top merlin directory
# input audio directory
# Output features directory
# initializations
# tools directory
### STRAIGHT ANALYSIS -- extract vocoder parameters ###
### extract f0, sp, ap ###
### convert f0 to lf0 ###
### convert sp to mgc ###
### convert ap to bap ###
# get wav files list
# do multi-processing
# clean temporal files
# top merlin directory
# input audio directory
# Output features directory
# initializations
# tools directory
#bap order depends on sampling rate.
# If True: Reaper is used for f0 extraction. If False: The vocoder is used for f0 extraction.
# This is to keep compatibility with numpy default dtype.
# Run REAPER:
# Protection - number of frames:
# Save f0 file:
### WORLD ANALYSIS -- extract vocoder parameters ###
### extract sp, ap ###
### Extract f0 using reaper ###
### convert f0 to lf0 ###
### convert sp to mgc ###
### convert bapd to bap ###
# get wav files list
# do multi-processing
# DEBUG:
#for nxf in xrange(len(wav_files)):
#    process(wav_files[nxf])
# clean temporal files
# top merlin directory
# input feat directory
# Output audio directory
# initializations
# file ID list
# feat directories
# tools directory
# set to True if synthesizing generated files
# this coefficient depends on voice
### WORLD Re-synthesis -- reconstruction of parameters ###
### convert lf0 to f0 ###
### post-filtering mgc ###
### convert mgc to sp ###
### convert bapd to bap ###
# Final synthesis using WORLD
# parse the arguments
#!/usr/bin/env python
### Arguments
# tools directory
# Source features directory
# Target features directory
# Source-aligned features directory
# bap dimension
### Define variables
### create outut directories
#################################################################
######## align source feats with target feats using dtw ## ######
#################################################################
### DTW alignment -- align source with target parameters ###
### dtw align src with tgt ###
### load dtw path
# dtw length not matched
### align features
# get mgc files list
# do multi-processing
#!/usr/bin/env python
### Arguments
# tools directory
# Source features directory
# Target features directory
# Source-aligned features directory
# bap dimension
# path to tools
### Define variables
### create outut directories
#################################################################
######## align source feats with target feats using dtw ## ######
#################################################################
# create dummy lab files
### DTW alignment -- align source with target parameters ###
### dtw align src with tgt ###
### load dtw path
# dtw length not matched
### align features
# get mgc files list
# do multi-processing
# clean temporal files
#!/usr/bin/env python
### Arguments
# tools directory
# Source features directory
# Target features directory
# Source-aligned features directory
# bap dimension
#bap_dim = int(sys.argv[5])
# path to tools
### Define variables. TODO: read from config file (void hardcoding)
#src_mag_dir = os.path.join(src_feat_dir, "mag")
#tgt_mag_dir = os.path.join(tgt_feat_dir, "mag")
#src_bap_dir = os.path.join(src_feat_dir, "bap")
#tgt_bap_dir = os.path.join(tgt_feat_dir, "bap")
#src_lf0_dir = os.path.join(src_feat_dir, "lf0")
#tgt_lf0_dir = os.path.join(tgt_feat_dir, "lf0")
### create outut directories
#src_aligned_mag_dir = os.path.join(src_aligned_feat_dir, "mag")
#src_aligned_bap_dir = os.path.join(src_aligned_feat_dir, "bap")
#src_aligned_lf0_dir = os.path.join(src_aligned_feat_dir, "lf0")
#if not os.path.exists(src_aligned_mag_dir):
#    os.mkdir(src_aligned_mag_dir)
#if not os.path.exists(src_aligned_bap_dir):
#    os.mkdir(src_aligned_bap_dir)
#if not os.path.exists(src_aligned_lf0_dir):
#    os.mkdir(src_aligned_lf0_dir)
#################################################################
######## align source feats with target feats using dtw ## ######
#################################################################
# create dummy lab files
### DTW alignment -- align source with target parameters ###
### dtw align src with tgt ###
### load dtw path
# dtw length not matched
### align features
# get mag files list
# do multi-processing
# clean temporal files
#!/usr/bin/env python
#import shutil
### Arguments
# tools directory
# Source features directory
# Target features directory
# Source-aligned features directory
### Define variables
# TODO: Change this (avoid hardcoded)
#src_mag_dir = src_feat_dir
#tgt_mag_dir = tgt_feat_dir
#src_lf0_dir = os.path.join(src_feat_dir, "lf0")
#tgt_lf0_dir = os.path.join(tgt_feat_dir, "lf0")
### create outut directories
#src_aligned_mag_dir = os.path.join(src_aligned_feat_dir, "mag")
#src_aligned_bap_dir = os.path.join(src_aligned_feat_dir, "bap")
#src_aligned_lf0_dir = os.path.join(src_aligned_feat_dir, "lf0")
#if not os.path.exists(src_aligned_mag_dir):
#    os.mkdir(src_aligned_mag_dir)
#if not os.path.exists(src_aligned_bap_dir):
#    os.mkdir(src_aligned_bap_dir)
#if not os.path.exists(src_aligned_lf0_dir):
#    os.mkdir(src_aligned_lf0_dir)
#################################################################
######## align source feats with target feats using dtw ## ######
#################################################################
### DTW alignment -- align source with target parameters ###
### dtw align src with tgt ###
### load dtw path
# dtw length not matched
### align features
# get mag files list
# do multi-processing
# parse the arguments
#print(src_mean_f0, src_std_f0)
#print(tgt_mean_f0, tgt_std_f0)
#!/usr/bin/python
#URL = 'http://www.cs.toronto.edu/~murray/code/gpu_monitoring/'
# Get ID's of NVIDIA boards. Should do this through a CUDA call, but this is
# a quick and dirty way that works for now:
# /tmp is cleared on reboot on many systems, but it doesn't have to be
# /dev/shm on linux machines is a RAM disk, so is definitely cleared
#    print   id
# On POSIX systems symlink creation is atomic, so this should be a
# robust locking operation:
# On POSIX systems os.rename is an atomic operation, so this is the safe
# way to delete a lock:
# If run as a program:
# Report
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
###################################################
########## User configurable variables ############
###################################################
### Input-Output ###
#### define model params ####
### define train, valid, test ###
#### main processess ####
#### Generate only test list ####
###################################################
####### End of user-defined conf variables ########
###################################################
#### Create train, valid and test file lists ####
#### Define keras models class ####
### normalize train data ###
#### define the model ####
#### load the data ####
#### normalize the data ####
#### train the model ####
### Train feedforward model ###
### Train recurrent model ###
#### store the model ####
#### load the model ####
#### load the data ####
#### normalize the data ####
#### compute predictions ####
### Implement each module ###
# create a configuration instance
# and get a short name for this instance
# main function
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# only for socket.getfqdn()
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
#import gnumpy as gnp
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
# the new class for label composition and normalisation
# our custom logging class that can also plot
# as logging
# reference activation weights in layers
## plot activation weights including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
## Model adaptation -- fine tuning the existing model
## We can't just unpickle the old model and use that because fine-tune functions
## depend on opt_l2e option used in construction of initial model. One way around this
## would be to unpickle, manually set unpickled_dnn_model.opt_l2e=True and then call
## unpickled_dnn_model.build_finetne_function() again. This is another way, construct
## new model from scratch with opt_l2e=True, then copy existing weights over:
# assign the existing dnn model parameters to the new dnn model
## Added for LHUC ##
# In LHUC, we keep all the old parameters intact and learn only a small set of new
# parameters
#, batch_size=batch_size
# fixed learning rate 
# exponential decay
# linear decay
# no decay
# if sequential training, the batch size will be the number of frames in an utterance
# batch_size for sequential training is considered only when rnn_batch_training is set to True
## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function
#file_number
### write to cmp file
##generate bottleneck layer as features
### write to cmp file
# split data into a list of num_splits tuples with each tuple representing
# the parameters for perform_acoustic_compositon_on_split
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
# create plot dir if set to True
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# the number can be removed
# Debug:----------------------------------
#generate_wav(gen_dir, file_id_list, cfg)     # generated speech
#-----------------------------------------
# simple HTS labels
###use only training data to find min-max information, then apply on the whole dataset
### enforce silence such that the normalization runs without removing silence: only for final synthesis
### save label normalisation information for unseen testing labels
### make output duration data
### make output acoustic data
#[-0.5, 0.0, 0.5]
#[1.0, -2.0, 1.0]
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
# load mean std values
###calculate mean and std vectors on the training data, and apply on the whole dataset
# for hmpd vocoder we don't need to normalize the 
# pdd values
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
### set configuration variables ###
### call kerasclass and use an instance ###
### call Tensorflowclass and use an instance ###
### DNN model training
# not an error - just means directory already exists
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
# Please only tune on this step when you want to generate bottleneck features from DNN
# not an error - just means directory already exists
### generate parameters from DNN
# not an error - just means directory already exists
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### Perform duration normalization(min. state dur set to 1) ###
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list, cfg)  # reference copy synthesis speech
### setting back to original conditions before calculating objective scores ###
### evaluation: RMSE and CORR for duration
### evaluation: calculate distortion
# for straight or world vocoders
# for magphase vocoder
# for GlottDNN vocoder
# for pulsemodel vocoder
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
##MCD
##MCD
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# Check for the presence of git
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# only for socket.getfqdn()
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
#import gnumpy as gnp
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
# the new class for label composition and normalisation
# our custom logging class that can also plot
# as logging
# reference activation weights in layers
## plot activation weights including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
### extract phone duration array for frame features ###
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
## Model adaptation -- fine tuning the existing model
## We can't just unpickle the old model and use that because fine-tune functions
## depend on opt_l2e option used in construction of initial model. One way around this
## would be to unpickle, manually set unpickled_dnn_model.opt_l2e=True and then call
## unpickled_dnn_model.build_finetne_function() again. This is another way, construct
## new model from scratch with opt_l2e=True, then copy existing weights over:
# assign the existing dnn model parameters to the new dnn model
## Added for LHUC ##
# In LHUC, we keep all the old parameters intact and learn only a small set of new
# parameters
#, batch_size=batch_size
# fixed learning rate 
# exponential decay
# linear decay
# no decay
# if sequential training, the batch size will be the number of frames in an utterance
# batch_size for sequential training is considered only when rnn_batch_training is set to True
## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function
#file_number
### write to cmp file
#file_number
#print b_indices
### write to cmp file
#file_number
### MLU features sub-division ###
### duration array sub-division ###
### additional feature matrix (syllable+phone+frame) ###
### input word feature matrix ###
#print b_indices
### write to cmp file
##generate bottleneck layer as features
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
# create plot dir if set to True
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# the number can be removed
# simple HTS labels
###use only training data to find min-max information, then apply on the whole dataset
### enforce silence such that the normalization runs without removing silence: only for final synthesis
### make duration data for S2S network ###
### save label normalisation information for unseen testing labels
### make output duration data
### make output acoustic data
#[-0.5, 0.0, 0.5]
#[1.0, -2.0, 1.0]
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
# load mean std values
###calculate mean and std vectors on the training data, and apply on the whole dataset
# for hmpd vocoder we don't need to normalize the 
# pdd values
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
### set configuration variables ###
### call kerasclass and use an instance ###
### call Tensorflowclass and use an instance ###
### DNN model training
# not an error - just means directory already exists
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
# Please only tune on this step when you want to generate bottleneck features from DNN
# not an error - just means directory already exists
### generate parameters from DNN
# not an error - just means directory already exists
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### Perform duration normalization(min. state dur set to 1) ###
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list, cfg)  # reference copy synthesis speech
### setting back to original conditions before calculating objective scores ###
### evaluation: RMSE and CORR for duration
### evaluation: calculate distortion
# for straight or world vocoders
# for GlottDNN vocoder
# for pulsemodel vocoder
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
##MCD
##MCD
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# Check for the presence of git
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
###################################################
########## User configurable variables ############
###################################################
### Input-Output ###
#### define model params ####
### Define the work directory###
### define train, valid, test ###
#### main processess ####
#### Generate only test list ####
###################################################
####### End of user-defined conf variables ########
###################################################
#### Create train, valid and test file lists ####
### normalize train data ###
#### load the data ####
#### normalize the data ####
#### define the model ####
#### train the model ####
### Train feedforward model ###
#### load the data ####
#### normalize the data ####
#### compute predictions ####
### Implement each module ###
# create a configuration instance
# and get a short name for this instance
# main function
# except:
#         print "inp stats file is %s"%cfg.inp_stats_file
#        sys.exit(0)
#! /usr/bin/python2 -u
# -*- coding: utf-8 -*-
#
# Copyright 2016 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# doesn't do anything
# get a logger
# this (and only this) logger needs to be configured immediately, otherwise it won't work
# we can't use the full user-supplied configuration mechanism in this particular case,
# because we haven't loaded it yet!
#
# so, just use simple console-only logging
# this level is hardwired here - should change it to INFO
# add a handler & its formatter - will write only to console
# first, set up some default configuration values
# next, load in any user-supplied configuration values
# that might over-ride the default values
# now that we have loaded the user's configuration, we can load the
# separate config file for logging (the name of that file will be specified in the config file)
# finally, set up all remaining configuration values
# that depend upon either default or user-supplied values
# to be called before loading any user specific values
# things to put here are
# 1. variables that the user cannot change
# 2. variables that need to be set before loading the user's config file
# get a logger
# load and parse the provided configFile, if provided
# load the config file
#work_dir must be provided before initialising other directories
# look for those items that are user-configurable, and get their values
# sptk_bindir= ....
# default place for some data
# a list instead of a dict because OrderedDict is not available until 2.7
# and I don't want to import theano here just for that one class
# each entry is a tuple of (variable name, default value, section in config file, option name in config file)
#
# the type of the default value is important and controls the type that the corresponding
# variable will have
#
# to set a default value of 'undefined' use an empty string
# or the special value 'impossible', as appropriate
#
## for glottHMM:
## for glottDNN:
## for sinusoidal:
## For MagPhase Vocoder:
# Containg natural speech waveforms (for acous feat extraction).
# Input-Output
## for joint duration
#+*']                                             ,    'Labels', 'silence_pattern'),
## For MagPhase Vocoder:
#('label_align_orig_const_rate_dir', os.path.join(self.work_dir, 'data/label_state_align'), 'Labels', 'label_align_orig_const_rate'),
## some config variables for token projection DNN
# RNN
# Data
# Keras Processes
## for GlottHMM:
## for GlottDNN:
## for sinusoidal:
## For MagPhase Vocoder:
## for joint dur:-
#            ('use_private_hidden'  , False, 'Streams', 'use_private_hidden'),
# fw_alpha: 'Bark' or 'ERB' allowing deduction of alpha, or explicity float value (e.g. 0.77)
## For MagPhase Vocoder:
#('use_magphase_pf'  ,True                 ,'Waveform'  , 'use_magphase_pf'), # Use MagPhase own Post-Filter (experimemental)
# Acoustic feature extraction
## GlottHMM
## GlottDNN
## sinusoidal
## For MagPhase Vocoder:
## joint dur
# this uses exec(...) which is potentially dangerous since arbitrary code could be executed
# first, look for a user-set value for this variable in the config file
# use default value, if there is one
# to be called after reading any user-specific settings
# because the values set here depend on those user-specific settings
# get a logger
# tools
# set input extension same as output for voice conversion
# check if any hidden layer is recurrent layer 
# switch to tensorflow
## create directories if not exists
# switch to keras
## create directories if not exists
# model files
# input-output normalization stat files
# define model file name
# predicted features directory
# string.lower for some architecture values
# force optimizer to adam if set to sgd
# set sequential training True if using LSTMs
# set default seq length for duration model
# rnn params
### RNN params
# batch training for RNNs
# set/limit batch size to 25
## num. of sentences in this case
###dimensions for the output features
### key name must follow the self.in_dimension_dict.
### If do not want to include dynamic feature, just use the same dimension as that self.in_dimension_dict
### if lf0 is one of the acoustic featues, the out_dimension_dict must have an additional 'vuv' key
### a bit confusing
###need to control the order of the key?
##dimensions for each raw acoustic (output of NN) feature
#            current_stream_hidden_size = 0
#            current_stream_weight = 0.0
#            stream_lr_ratio = 0.0
#                current_stream_hidden_size = self.stream_mgc_hidden_size
#                current_stream_weight      = self.stream_weight_mgc
#                current_stream_hidden_size = self.stream_bap_hidden_size
#                current_stream_weight      = self.stream_weight_bap
#                current_stream_hidden_size = self.stream_lf0_hidden_size
#                current_stream_weight      = self.stream_weight_lf0
#                current_stream_hidden_size = self.stream_vuv_hidden_size
#                current_stream_weight      = self.stream_weight_vuv
#                current_stream_hidden_size = self.stream_stepw_hidden_size
#                current_stream_weight      = self.stream_weight_stepw
#                current_stream_hidden_size = self.stream_sp_hidden_size
#                current_stream_weight      = self.stream_weight_sp
#                current_stream_hidden_size = self.stream_seglf0_hidden_size
#                current_stream_weight      = self.stream_weight_seglf0
## for GlottHMM (start)
#                current_stream_hidden_size = self.stream_F0_hidden_size
#                current_stream_weight      = self.stream_weight_F0
#                current_stream_hidden_size = self.stream_Gain_hidden_size
#                current_stream_weight      = self.stream_weight_Gain
#                current_stream_hidden_size = self.stream_HNR_hidden_size
#                current_stream_weight      = self.stream_weight_HNR
#                current_stream_hidden_size = self.stream_LSF_hidden_size
#                current_stream_weight      = self.stream_weight_LSF
#                current_stream_hidden_size = self.stream_LSFsource_hidden_size
#                current_stream_weight      = self.stream_weight_LSFsource
## for GlottHMM (end)
## for GlottDNN (start)
## for GlottDNN (end)
## for HMPD (start)
## for HMPD (end)
## For MagPhase Vocoder (start):
# Note: 'lf0' is set before. See above.
## For MagPhase Vocoder (end)
## for joint dur (start)
#                current_stream_hidden_size = self.stream_dur_hidden_size
#                current_stream_weight      = self.stream_weight_dur
## for joint dur (end)
#            logger.info('  current_stream_hidden_size: %d' % current_stream_hidden_size)
#            logger.info('  current_stream_weight: %d' % current_stream_weight)
#                if (current_stream_hidden_size <= 0 or current_stream_weight <= 0.0) and self.multistream_switch:
#                    logger.critical('the hidden layer size or stream weight is not corrected setted for %s feature' %(feature_name))
#                    raise
#                if self.multistream_switch:
#                    self.private_hidden_sizes.append(current_stream_hidden_size)
#                    self.stream_weights.append(current_stream_weight)
#        if not self.multistream_switch:
#            self.private_hidden_sizes = []
#            if self.stream_cmp_hidden_size > 0:
#                self.private_hidden_sizes.append(self.stream_cmp_hidden_size)
#            else:
#                self.private_hidden_sizes.append(self.hidden_layer_size[-1])  ## use the same number of hidden layers if multi-stream is not supported
#            self.stream_weights = []
#            self.stream_weights.append(1.0)
#                stream_lr_ratio = 0.5
#                if feature_name == 'lf0':
#                    stream_lr_ratio = self.stream_lf0_lr
#                if feature_name == 'vuv':
#                    stream_lr_ratio = self.stream_vuv_lr
#                self.stream_lr_weights.append(stream_lr_ratio)
### the new cmp is not the one for HTS, it includes all the features, such as that for main tasks and that for additional tasks
#            self.stream_lr_weights.append(0.5)
# to check whether all the input and output features' file extensions are here
## gHMM:
## gDNN
## HMPD
## For MagPhase Vocoder:
# Note: 'lf0' is set before. See above.
## joint dur
## hyper parameters for DNN. need to be setted by the user, as they depend on the architecture
###
#To be recorded in the logging file for reference
# input files
# set up the label processing
# currently must be one of two styles
# xpath_file_name is now obsolete - to remove
# get a logger
# logging configuration, see here for format description
# https://docs.python.org/2/library/logging.config.html#logging-config-fileformat
# what we really want to do is this dicitonary-based configuration, but it's only available from Python 2.7 onwards
#    logging.config.dictConfig(cfg.logging_configuration)
# so we will settle for this file-based configuration procedure instead
# open the logging configuration file
# load the logging configuration file into a string
# this means that cfg.log_config_file does not exist and that no default was provided
# NOTE: currently this will never run
# set up a default level and default handlers
# first, get the root logger - all other loggers will inherit its configuration
# default logging level is DEBUG (a highly-verbose level)
# add a handler to write to console
# and a formatter
# this means that open(...) threw an error
# inject the config lines for the file handler, now that we know the name of the file it will write to
# config file format doesn't allow leading white space on lines, so remove it with dedent
# pass that string as a filehandle
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# configuration for the input labels (features) for the DNN
#
# this currently supports
# * input labels can be any combination of HTS and XML style input labels
# * output features are numerical *only* (all strings are fully expanded into 1-of-n encodings, etc)
#
#
#
# this is all executable python code
#  so we need to define things before using them
#  that means the description is bottom-up
# we need to specify how any non-numerical (e.g., unicode string) features will be converted (mapped) into numerical feature vectors
# (just some examples for now)
# read additional maps from external files and add them to the 'maps' dictionary
#  each such file must define a dictionary of dictionaries called maps, in the same format as above
#  TO DO - avoid full paths here - import them from the main config file
# not sure this will work second time around - may not be able to import under the same module name ??
# how to extract features
# (just a few examples for now)
#
# each feature is a dictionary with various possible entries:
#   xpath: an XPATH that will extract the required feature from a segment target node of an Ossian XML utterance tree
#   hts:   a (list of) HTS pseudo regular expression(s) that match(es) part of an HTS label, resulting in a single boolean feature
#   mapper:   an optional function or dictionary which converts the feature value (e.g., a string) to a (vector of) numerical value(s)
#
# the dictionary describes how to compute that feature
# first, either xpath or hts describes how to extract the feature from a tree or label name
# then, an optional mapping converts the feature via a lookup table (also a dictionary) into a numerical value or vector
#
# if no mapper is provided, then the feature must already be a single numerical or boolean value
#
# some XPATH-based features
# in a future version, we could be more fleixble and allow more than one target_node type at once,
# with a set of XPATHs for each target_node - it would not be very hard to modify the code to do this
# the target nodes within the XML trees that the XPATH expressions apply to
# target_nodes = "//state" ???
# <segment pronunciation="t" cmanner="stop" cplace="alveolar" cvoiced="no" vfront="NA" vheight="NA" vlength="NA" vowel_cons="cons" vround="NA" start="1040" end="1090" has_silence="no">
# and the XPATH expressions to apply
# a composite "vector" of XPATH features
#  this is just an ordered list of features, each of which is a dictionary describing how to compute this feature
#  each feature may be a single numerical value or a vector of numerical values
# some HTS pseudo regular expression-based features
# all of these evaluate to a single boolean value, which will be eventually represented numerically
# note: names of features will need modifying to valid Python variable names (cannot contain "-", for example)
# a composite "vector" of HTS features
# the full feature vector
# + hts_labels
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# configuration for the input labels (features) for the DNN
#
# this currently supports
# * input labels can be any combination of HTS and XML style input labels
# * output features are numerical *only* (all strings are fully expanded into 1-of-n encodings, etc)
#
#
#
# this is all executable python code
#  so we need to define things before using them
#  that means the description is bottom-up
# we need to specify how any non-numerical (e.g., unicode string) features will be converted (mapped) into numerical feature vectors
# (just some examples for now)
## osw -- also make some maps automatically, only specifying list of values for brevity:
## strip special null values:
# read additional maps from external files and add them to the 'maps' dictionary
#  each such file must define a dictionary of dictionaries called maps, in the same format as above
#  TO DO - avoid full paths here - import them from the main config file
# not sure this will work second time around - may not be able to import under the same module name ??
# how to extract features
# (just a few examples for now)
#
# each feature is a dictionary with various possible entries:
#   xpath: an XPATH that will extract the required feature from a segment target node of an Ossian XML utterance tree
#   hts:   a (list of) HTS pseudo regular expression(s) that match(es) part of an HTS label, resulting in a single boolean feature
#   mapper:   an optional function or dictionary which converts the feature value (e.g., a string) to a (vector of) numerical value(s)
#
# the dictionary describes how to compute that feature
# first, either xpath or hts describes how to extract the feature from a tree or label name
# then, an optional mapping converts the feature via a lookup table (also a dictionary) into a numerical value or vector
#
# if no mapper is provided, then the feature must already be a single numerical or boolean value
#
# some XPATH-based features
# in a future version, we could be more fleixble and allow more than one target_node type at once,
# with a set of XPATHs for each target_node - it would not be very hard to modify the code to do this
# the target nodes within the XML trees that the XPATH expressions apply to
# and the XPATH expressions to apply
## NB: first feature is for silence trimming only:
## syll stress
## fine & coarse POS -- 3 word window
## === SIZES and DISTANCES till start/end -- these are numeric and not mapped:
## state in segment -- number states is fixed, so exclude size and only count in 1 direction
## segments in syll
## segments in word
## syll in word
## word in phrase
## syll in phrase
## segment in phrase
## X in utterance
#
# # a composite "vector" of XPATH features
# #  this is just an ordered list of features, each of which is a dictionary describing how to compute this feature
# #  each feature may be a single numerical value or a vector of numerical values
# xpath_labels =[
#
# # ll_segment,
# #  l_segment,
# #  c_segment,
# #  r_segment,
# # rr_segment,
#
# cmanner,
# cplace,
# cvoiced,
#
# vfront,
# vheight,
# vlength,
# vround,
#
# vowel_cons
# ]
#
# some HTS pseudo regular expression-based features
# all of these evaluate to a single boolean value, which will be eventually represented numerically
# note: names of features will need modifying to valid Python variable names (cannot contain "-", for example)
# a composite "vector" of HTS features
# the full feature vector
# + hts_labels
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# instantiate one object of this class
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
### whether dynamic features are needed for each data stream
## we assume static+delta+delta-delta
### merge the data: like the cmp file
### the real function to do the work
### need to be implemented for a specific format
### interpolate F0, if F0 has already been interpolated, nothing will be changed after passing this function
#        delta_win = [-0.5, 0.0, 0.5]
#        acc_win   = [1.0, -2.0, 1.0]
### compute dynamic features for a data matrix
###compute dynamic feature dimension by dimension
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#io_funcs.
###prepare_nn_data(self, in_file_list_dict, out_file_list, in_dimension_dict, out_dimension_dict):
#if os.path.isfile(out_file_name):
#    logger.info('processing file %4d of %4d : %s exists' % (i+1, self.file_number, out_file_name))
#    continue
## F0 added for GlottHMM
### if vuv information to be recorded, store it in corresponding column
### write data to file
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# this only extracts the static lf0 because we need to interpolate it, then add deltas ourselves later
#        delta_win = [-0.5, 0.0, 0.5]
#        acc_win   = [1.0, -2.0, 1.0]
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#        self.dimension_dict = dimension_dict
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# context-dependent printing format for Numpy - should move this out to a utility file somewhere
# a class that can compose input labels according to the user's specification, and convert them to numerical vectors
# what label styles we find in the feature specification
# e.g., 'xpath' , 'hts'
## will be set True if xpaths are compiled
# load in a label specification, provided by the user
# perform some sanity checks on it
#
# make sure 'labels' is defined
#osw# self.logger.debug('looking at feature %s' % feature_specification )
# feature is a dictionary specifying how to construct this part of the input feature vector
# xpath and hts are mutually exclusive label styles
# if there is a mapper, then we will use that to convert the features to numbers
# we need to look at the mapper to deduce the dimensionality of vectors that it will produce
# get an arbitrary item as the reference and measure its dimensionality
# make sure all other entries have the same dimension
#print '   add %s    cum: %s'%( str(l), self.label_dimension)
# without a mapper, features will be single numerical values
#print '   add 1    cum: %s'%( self.label_dimension)
# we have seen at least one feature that will required xpath label files to be loaded
# will become True once implemented
# not yet implemented !
## for frame features -- TODO: decide how to handle this properly
#print '   add 3   cum: %s'%(  self.label_dimension)
# a console handler
# not written test code for actual label processing - too complex and relies on config files
# from logplot.logging_plotting import LoggerPlotter #, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[2]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[3]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[4]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[5]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[6]
#+*'], label_type="state_align"):
## hard-coded silence duration
# remove state information [k]
## hard-coded silence duration
# from logplot.logging_plotting import LoggerPlotter #, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# this class only knows how to deal with a single style of labels (XML or HTS)
# (to deal with composite labels, use LabelComposer instead)
#  -----------------------------
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[2]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[3]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[4]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[5]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[6]
# this subclass support HTS labels, which include time alignments
#            self.question_dict, self.ori_question_dict = self.load_question_set(question_file_name)
###self.dict_size = len(self.question_dict)
## zhizheng's original 5 state features + 4 phoneme features
## the minimal features necessary to go from a state-level to frame-level model
## this is equivalent to a state-based system
## the phoneme level features only
## this is equivalent to a frame-based system without relying on state-features
## this is equivalent to a frame-based system with uniform state-features
## this is equivalent to a frame-based system with minimal features
## this is equivalent to a frame-based positioning system reported in Heiga Zen's work
### if user wants to define their own input, simply set the question set to empty.
### set default feature type to numerical, if not assigned ###
### set default unit size to state, if not assigned ###
### set default feat size to frame or phoneme, if not assigned ###
## phoneme/syllable/word
#', 'sil', 'pau', 'SIL']
# remove state information [k]
### for syllable and word positional information ###
### syllable ending information ###
##pos-bw and c-silences
### word ending information ###
### writing into dur_feature_matrix ###
#': ## removing silence here
# hard coded here 
### writing into dur_feature_matrix ###
# this is not currently used ??? -- it works now :D
#logger.critical('unused function ???')
#raise Exception
## hard coded for now
# to do - support different frame shift - currently hardwired to 5msec
# currently under beta testing: support different frame shift
#label_binary_vector = self.pattern_matching(full_label)
# if there is no CQS question, the label_continuous_vector will become to empty
## features which distinguish frame position in phoneme
# fraction through phone forwards
# fraction through phone backwards
# phone duration
## features which distinguish frame position in phoneme using three continous numerical features
## setting add_frame_features to False performs either state/phoneme level normalisation
# label_feature_matrix = numpy.empty((100000, self.dict_size+self.frame_feature_size))
# remove state information [k]
#                label_binary_vector = self.pattern_matching(full_label)
# if there is no CQS question, the label_continuous_vector will become to empty
## Zhizheng's original 9 subphone features:
## fraction through state (forwards)
## fraction through state (backwards)
## length of state in frames
## state index (counting forwards)
## state index (counting backwards)
## length of phone in frames
## fraction of the phone made up by current state
## fraction through phone (backwards)
## fraction through phone (forwards)
## features which only distinguish state:
## state index (counting forwards)
## features which distinguish frame position in phoneme:
## fraction through phone (counting forwards)
## features which distinguish frame position in phoneme:
## fraction through phone (counting forwards)
## state index (counting forwards)
## features which distinguish frame position in phoneme using three continous numerical features
## features which distinguish state and minimally frame position in state:
## fraction through state (forwards)
## state index (counting forwards)
## state index (counting forwards)
## hard coded for now
# hard coded here 
## fraction through state (forwards)
## fraction through state (backwards)
## length of state in frames
## state index (counting forwards)
## state index (counting backwards)
## length of phone in frames
## fraction of the phone made up by current state
## fraction through phone (forwards)
## fraction through phone (backwards)
### this function is not used now
### this function is not used now
# this function is where most time is spent during label preparation
#
# it might be possible to speed it up by using pre-compiled regular expressions?
# (not trying this now, since we may change to to XML tree format for input instead of HTS labels)
#
#                assert len(ms.group()) == 1
# regex for last question
#                print   line
# last question must only match at end of HTS label string
#save pre-compiled regular expression
#                question_index = question_index + 1
## handle HTK wildcards (and lack of them) at ends of label:
## convert remaining HTK wildcards * and ? to equivalent regex:
## don't use extra features beyond those in questions for duration labels:
## add_frame_features not used in HTSLabelNormalisation -- only in XML version
## remove empty lines
## take last entry -- ignore timings if present
# if there is no CQS question, the label_continuous_vector will become to empty
#  -----------------------------
#output_file_list = ['/afs/inf.ed.ac.uk/group/cstr/projects/blizzard_entries/blizzard2016/straight_voice/Hybrid_duration_experiments/dnn_tts_release/lstm_rnn/data/dur/AMidsummerNightsDream_000_000.dur']
#feature_type="binary"
#unit_size = "phoneme"
#feat_size = "phoneme"
#label_operater.prepare_dur_data(ori_file_list, output_file_list, feature_type, unit_size, feat_size)
#label_operater.prepare_dur_data(ori_file_list, output_file_list, feature_type)
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
## a generic class of linguistic feature extraction
##
##the feature dimensionality of output (should that read 'input' ?)
## the number of utterances to be normalised
## the ori_file_list contains the file paths of the raw linguistic data
## the output_file_list contains the file paths of the normalised linguistic data
##
## the exact function to do the work
## need to be implemented in the specific class
## the function will write the linguistic features directly to the output file
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#    def __init__(self, feature_dimension):
#        self.feature_dimension = feature_dimension
# setting the print options in this way seems to break subsequent printing of numpy float32 types
# no idea what is going on - removed until this can be solved
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
# setting the print options in this way seems to break subsequent printing of numpy float32 types
# no idea what is going on - removed until this can be solved
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# this is the wrong name for this logger because we can also normalise labels here too
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
## If we are to keep some columns unnormalised, use advanced indexing to
## reinstate original values:
#            norm_features = numpy.array(norm_features, 'float32')
#            fid = open(out_file_list[i], 'wb')
#            norm_features.tofile(fid)
#            fid.close()
# print   self.max_vector, self.min_vector
# logger.debug('reshaping fea_max_min_diff from shape %s to (1,%d)' % (fea_max_min_diff.shape, self.feature_dimension) )
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
## use theano to benefit from GPU computation
###assume the delta and acc windows have the same length
#            WDW = dot(dot(WT_static, D_static), W_static) + dot(dot(WT_delta, D_delta), W_delta) + dot(dot(WT_acc, D_acc), W_acc)
#            WDU = dot(dot(WT_static, D_static), U_static) + dot(dot(WT_delta, D_delta), U_delta) + dot(dot(WT_acc, D_acc), U_acc)
#            temp_obs = dot(numpy.linalg.inv(WDW), WDU)
###only theano-dev version support matrix inversion
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# Adding this before the bandmat import lets us import .pyx files without running bandmat's setup.py:
#import pyximport; pyximport.install()
###assume the delta and acc windows have the same length
#        tau_frames.astype('float64')
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
## Added FAST_MLPG as a variable here, in case someone wants to use the slow version, but perhaps we
## should always use the bandmat version?
#io_funcs.
#    pass
# Debug:
#self.inf_float = -50000
# not really necessary to have the logger rembered in the class - can easily obtain it by name instead
# self.logger = logging.getLogger('param_generation')
## hard coding, try removing in future?
#            if feature_name != 'vuv':
#            else:
#                vuv_dimension = dimension_index
#                recorded_vuv = True
### fast version wants variance per frame, not single global one:
#                print  var.shape[1]
#                else:
#                    self.logger.critical("the dimensions do not match for MLPG: %d vs %d" %(var.shape[1], out_dimension_dict[feature_name]))
#                    raise
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#+*'], label_type="state_align", remove_frame_features=True,
# previsouly: continue -- in fact we should keep non-silent data!
## if labels have a few extra frames than audio, this can break the indexing, remove them:
## OSW: rewrote above more succintly
## hard coded for now
# to do - support different frame shift - currently hardwired to 5msec
# currently under beta testing: supports different frame shift
# remove state information [k]
# def load_binary_file(self, file_name, dimension):
#        fid_lab = open(file_name, 'rb')
#        features = numpy.fromfile(fid_lab, dtype=numpy.float32)
#        fid_lab.close()
#        features = features[:(dimension * (features.size / dimension))]
#        features = features.reshape((-1, dimension))
#        return  features
## In case they are different, resize -- keep label fixed as we assume this has
## already been processed. (This problem only arose with STRAIGHT features.)
## label is longer -- pad audio to match by repeating last frame:
## audio is longer -- cut it
# else: -- expected case -- lengths match, so do nothing
#         print silence_flag
## if it's all 0s or 1s, that's ok:
## get the indices where silence_flag == 0 is True (i.e. != 0)
# print silence_flag
## nonzero returns a tuple of arrays, one for each dimension of input array
## every_nth used +as step value in slice
## -1 due to weird error with STRAIGHT features at line 144:
## IndexError: index 445 is out of bounds for axis 0 with size 445
## avoid errors in case there is no silence
## Append to end of utt -- same function used for labels and audio
## means that violation of temporal order doesn't matter -- will be consistent.
## Later, frame shuffling will disperse silent frames evenly across minibatches:
##  ^---- from tuple and back (see nonzero note above)
## advanced integer indexing
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# HTK datatybes
# Additional 'param kind' options
#has energy
#absolute energy suppressed
#has delta coefficients
#has acceleration coefficients
#is compressed
#has zero mean static coef.
#has CRC checksum
#has 0th cepstral coef.
#has VQ data
#has third differential coef.
# the first 6 bits contain datatype
# HTK header
# number of samples in file (4-byte integer)
# sample period in 100ns units (4-byte integer)
# number of bytes per sample (2-byte integer)
# a code indicating the sample kind (2-byte integer)
#TODO compression
#self.A = struct.unpack('>H', f.read(2))[0]
#self.B = struct.unpack('>H', f.read(2))[0]
#                print   "world"
#            if(sys.byteorder=='little'):
#                print   "hello"
#                self.data.byteswap(True) # forces big-endian byte ordering
#if(sys.byteorder=='little'):
#    self.data.byteswap(True) # force big-endian byte ordering
#filename_src = "../data/GE001_1.feat"
#print "t", htk.dupa, sys.byteorder
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# get a logger
# this (and only this) logger needs to be configured immediately, otherwise it won't work
# we can't use the full user-supplied configuration mechanism in this particular case,
# because we haven't loaded it yet!
#
# so, just use simple console-only logging
# this level is hardwired here - should change it to INFO
# add a handler & its formatter - will write only to console
# first, set up some default configuration values
# next, load in any user-supplied configuration values
# that might over-ride the default values
# finally, set up all remaining configuration values
# that depend upon either default or user-supplied values
# to be called before loading any user specific values
# things to put here are
# 1. variables that the user cannot change
# 2. variables that need to be set before loading the user's config file
# get a logger
# load and parse the provided configFile, if provided
# load the config file
#work_dir must be provided before initialising other directories
# default place for some data
# Paths
# Input-Output
# Architecture
# RNN
# Data
# Processes
# this uses exec(...) which is potentially dangerous since arbitrary code could be executed
# default value
# first, look for a user-set value for this variable in the config file
# use default value, if there is one
# to be called after reading any user-specific settings
# because the values set here depend on those user-specific settings
# get a logger
## create directories if not exists
# input-output normalization stat files
# define model file name
# model files
# predicted features directory
# string.lower for some architecture values
# set sequential training True if using LSTMs
# set/limit batch size to 25
## num. of sentences in this case
# rnn params
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
############################
##### Memory variables #####
############################
### read file by file ###
### read file by file ###
### read file by file ###
### read file by file ###
### read file by file ###
### shuffle train id list ###
## shuffle by sentence
## shuffle by a group of sentences
#### normalize training data ####
#### load norm stats ####
#### normalize data ####
#### de-normalize data ####
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# create model
# add hidden layers
# add output layer
# Compile the model
# add hidden layers
# add output layer
# Compile the model
# params
# add hidden layers
#go_backwards=True))
# add output layer
# Compile the model
# serialize model to JSON
# serialize weights to HDF5
#### load the model ####
#### compile the model ####
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#### TODO: Find a good way to pass below params ####
### if batch size is equal to 1 ###
#self.model.fit(temp_train_x, temp_train_y, epochs=1, shuffle=False, verbose=0)
### if batch size more than 1 ###
### Method 1 ###
### Method 2 ###
### Method 3 ###
### Method 3 ###
### Method 4 ###
#### compute predictions ####
### refer Zhizheng and Simon's ICASSP'16 paper for more details
### http://www.zhizheng.org/papers/icassp2016_lstm.pdf
#(1-p) *
# random initialisation
# Input gate weights
# bias
# initial value of hidden and cell state
#
#(1-p) *
# random initialisation
# Input gate weights
# Output gate weights
# bias
# initial value of hidden and cell state and output
#
# ensure sizes have integer type
# ensure sizes have integer type
# random initialisation
# Input gate weights
# random initialisation
# Forget gate weights
# random initialisation
# Output gate weights
# random initialisation
# Cell weights
# bias
### make a layer
# initial value of hidden and cell state
# random initialisation
# Input gate weights
# random initialisation
# Output gate weights
# random initialisation
# Forget gate weights
# random initialisation
# Output gate weights
# random initialisation
# Cell weights
# bias
### make a layer
# initial value of hidden and cell state
#
#
#, i_t, f_t, o_t
#
#
#, i_t, f_t, o_t
#self.w_cf * c_tm1 
#
#f_t *
#
#i_t *
#
#
#i_t *
#self.W_ci,
#self.W_cf,
#self.W_co,
#self.w_cf * c_tm1
##can_h_t = T.tanh(Whx + r_t * T.dot(h_tm1, self.W_hh) + self.b_h)
#self.w_cf * c_tm1
#        c_t = f_t * c_tm1 + (1 - f_t) * T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + self.b_c)
#        h_t = T.tanh(c_t)
# Gated Recurrent Unit
## pre-compute these for fast computation
#
## in order to have the same interface as LSTM
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# assume linear output for mean vectors
#self.sigma = T.nnet.softplus(T.dot(self.input, self.W_sigma)) # + 0.0001
# Zen et al. 2014
# hard variance flooring
# note: sigma contains variances, so var_floor=0.01 means that
# the lowest possible standard deviation is 0.1
# ensure sizes have integer type
# ensure sizes have integer type
# initialize with 0 the weights W as a matrix of shape (n_in, n_out)
# ensure sizes have integer type
# ensure sizes have integer type
# initialize with 0 the weights W as a matrix of shape (n_in, n_out)
# ensure sizes have integer type
# ensure sizes have integer type
# initialize with 0 the weights W as a matrix of shape (n_in, n_out)
## rectifier linear unit
## rectifier smooth unit
# parameters of the model
#        self.output = self.rectifier_linear(lin_output)
# parameters of the model
#self.n_in = n_in
#        self.output = self.rectifier_linear(lin_output)
# parameters of the model
#self.n_in = n_in
# W_values = numpy.asarray(rng.uniform(low=-0.02, high=0.02,
## TODO -- generalise to other n_modes and higher deimneionsal CVs
#initial_W = numpy.asarray( numpy_rng.uniform(
#          low  = -4*numpy.sqrt(6./(n_hidden+n_visible)),
#          high =  4*numpy.sqrt(6./(n_hidden+n_visible)),
#          size = (n_visible, n_hidden)),
#                           dtype = theano.config.floatX)
# first layer, use Gaussian noise
## rectifier linear unit
## rectifier smooth unit
#if corruption_level == 0:
#    tilde_x = self.x
#else:
#    tilde_x = self.get_corrupted_input(self.x, corruption_level)
# tilde_x = self.get_corrupted_input(self.x, corruption_level, 0.5)
# initialize with 0 the weights W as a matrix of shape (n_in, n_out)
# random initialisation 
# Input gate weights
# random initialisation 
# Forget gate weights
# random initialisation 
# Output gate weights
# random initialisation 
# Cell weights
# bias
# scaling factor
### make a layer
# initial value of hidden and cell state
#
# 
#, i_t, f_t, o_t################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#assume linear output for mean vectors
# + 0.0001
#self.sigma = T.exp(T.dot(self.input, self.W_sigma)) # + 0.0001
# initialize with 0 the weights W as a matrix of shape (n_in, n_out)
# initialize with 0 the weights W as a matrix of shape (n_in, n_out)
# randomly initialise the activation weights based on the input size, as advised by the 'tricks of neural network book'
## rectifier linear unit
## rectifier smooth unit
# parameters of the model
#        self.output = self.rectifier_linear(lin_output)
# parameters of the model
# create a Theano random generator that gives symbolic random values
# first layer, use Gaussian noise
# tilde_x = self.get_corrupted_input(self.x, corruption_level, 0.5)
#(1-p) *
# random initialisation
#Wy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_h)), dtype=config.floatX)
#Uy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_out)), dtype=config.floatX)
# identity matrix initialisation
#Wh_value = np.asarray(np.eye(n_h, n_h), dtype=config.floatX)
#Uh_value = np.asarray(np.eye(n_in, n_out), dtype=config.floatX)
# Input gate weights
# Output gate weights
# bias
# initial value of hidden and cell state and output
# simple recurrent decoder params
#self.params = [self.W_xi, self.W_hi, self.W_yi, self.U_hi, self.b_i, self.b]
# recurrent output params and additional input params
#
# simple recurrent decoder
#y_t = T.dot(h_t, self.U_hi) + self.b
# recurrent output and additional input
#(1-p) *
# random initialisation
#Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)
#Wy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_h)), dtype=config.floatX)
#Uh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_out)), dtype=config.floatX)
#Uy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_out)), dtype=config.floatX)
# identity matrix initialisation
# Input gate weights
# Output gate weights
# bias
# initial value of hidden and cell state and output
# hard coded to remove coarse coding features
# recurrent output params and additional input params
#
# random initialisation
# Input gate weights
# random initialisation
# Output gate weights
# random initialisation
# Forget gate weights
# random initialisation
# Output gate weights
# random initialisation
# Cell weights
# bias
### make a layer
# initial value of hidden and cell state
#
#
#, i_t, f_t, o_t
#self.w_cf * c_tm1 
# ensure sizes have integer type
# ensure sizes have integer type
# random initialisation
# Input gate weights
# random initialisation
# Forget gate weights
# random initialisation
# Output gate weights
# random initialisation
# Cell weights
# bias
### make a layer
# initial value of hidden and cell state
# hard coded to remove coarse coding features
#
#
#, i_t, f_t, o_t
#(1-p) *
# random initialisation
# Input gate weights
# bias
# initial value of output
#
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# NOTES
# still to consider: pygal, for HTML5 SVG plotting
# this module provides the base classes that we specialise here
# as logging
# for plotting
# should make this user-configurable - TO DO later
# this line has to come before the import of matplotlib.pyplot
# matplotlib needs to be passed numpy arrays
# for sorting tuples
# TO DO - this needs to be attached to the logging module so that it's available via config options
# class PlotHandler(logging.FileHandler):
#     """A handler for saving plots to disk"""
#     def __init__(self,filename):
#         logging.FileHandler.__init__(self,filename, mode='a', encoding=None, delay=False)
# a generic plot object that contains both the underlying data and the plot itself
# this class needs to be subclassed for each specialised type of plot that we want
# the underlying data for the plot - a dictionary of data series
# each series is a list of data points of arbitrary type (e.g., tuples, arrays, ..)
# the plot generated from these data
# clear the data series
# if there is no data series with this name yet, create an empty one
# append this data point (e.g., it might be a tuple (x,y) )
# don't worry about data type or sorting - that is not our concern here
# only applied if the data points are tuples, such as (x,y) values
# TO DO: first check that each series is a list of tuples, and that they have the same number of elements
# this method checks that all data series
# 1. have the same length
# 2. are sorted in ascending order of x
# 3. have identical values in their x series
# there has to be at least one data series
# check lengths are consistent, sort, then check x values are identical
# print "starting with self.data=",self.data
# sort by ascending x value
# extract a list of just the x values
# print "ending with self.data=",self.data
# raise an exception here?
# a plot with one or more time series sharing a common x axis:
# e.g., the training error and the validation error plotted against epochs
# sort the data series and make sure they are consistent
# if there is a plot already in existence, we will clear it and re-use it;
# this avoids creating extraneous figures which will stay in memory
# (even if we are no longer referencing them)
# create a plot
# TO DO - better filename configuration for plots
## still plotting multiple image in one figure still has problem. the visualization is not good
#, bbox_inches='tight'
#class MultipleLinesPlot(PlotWithData):
#    def generate_plot(self, filename, title='', xlabel='', ylabel=''):
# a dictionary to store all generated plots
# keys are plot names
# values are
# where the plots will be saved - a directory
# default location
# initialise the logging parent class
# (should really use 'super' here I think, but that fails - perhaps because the built in logger class is not derived from 'object' ?)
# add a data point to a named plot
# raise an exception here?
# # the filename to save to is known by the handler, which needs to be assigned to this logger
# # look at the handlers attached to this logger instance
# ph=None
# for h in self.handlers:
#     # we want an instance of a PlotHandler - we'll take the first one we find
#     # (behaviour will be unpredictable if there is more than one handler of this type)
#     if isinstance(h,PlotHandler):
#         ph=h
#         break
# if ph:
# TO DO - need to be sure of safe file names
# else:
#     logger.warn('No handler of type PlotHandler is attached to this logger - cannot save plots')
# colourising formatter adapted from an answer to this question on Stack Overflow
# http://stackoverflow.com/questions/384076/how-can-i-color-python-logging-output
# terminal escape sequences
# pad to fixed width - currently hardwired, should make this dynamic
# maximum width of level names, which is the 8 characters of "CRITICAL"
# The background is set with 40 plus the number of the color, and the foreground with 30
# some simple tests
# tell the built-in logger module to use our custom class when instantiating any new logger
# a console handler
# handler for plotting logger - will write only to console
# # need a handler which will control where to save plots
# ph = PlotHandler("/tmp/plot_test/testing.pdf")
# logger.addHandler(ph)
# the first argument is just a key for referring to this plot within the code
# the second argument says what kind of plot we will be making
# momentum
#+ self.L2_reg * self.L2_sqr
## added for LHUC
# In lhuc the parameters are only scaling parameters which have the name 'c'
# use optimizer
# zip just concatenate two lists
# freeze layers and update weights
#index, batch_size
#[index*batch_size:(index + 1)*batch_size]
#, batch_size
## the function to output activations at a hidden layer
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
###THEANO_FLAGS='cuda.root=/opt/cuda-5.0.35,mode=FAST_RUN,device=gpu0,floatX=float32,exception_verbosity=high' python dnn.py
# allocate symbolic variables for the data
##T.nnet.sigmoid)  #
# add final layer
### MSE
### L1-norm
### L2-norm
# compute number of minibatches for training, validation and testing
# index to a [mini]batch
##top 2 layers use a smaller learning rate
##hard-code now, change it later
# compute list of fine-tuning updates
# compute the gradients with respect to the model parameters
## Theano's default
## Retain learning rate and momentum to make interface backwards compatible,
## even with RPROP where we don't use them, means we have to use on_unused_input='warn'.
# Create a function that scans the entire validation set
#, batch_size
## the function to output activations at a hidden layer
#    test_DBN(train_scp, valid_scp, log_dir, model_dir, n_ins, n_outs, hidden_layers_sizes,
#             finetune_lr, pretraining_epochs, pretrain_lr, training_epochs, batch_size)
###THEANO_FLAGS='cuda.root=/opt/cuda-5.0.35,mode=FAST_RUN,device=gpu0,floatX=float32,exception_verbosity=high' python dnn.py
# as np
#cudamat
#import theano
#import theano.tensor as T
#output layer
#        (train_set_x, train_set_y) = train_xy
# assuming linear output and square error cost function
# final layer is linear output, gradient is one
#output layers
# + self.b_params[i] * self.l2_reg
#update weights and record momentum weights
#        print   self.W_params[0].shape, self.W_params[len(self.W_params)-1].shape
#    def parameter_prediction(self, test_set_x):  #, batch_size
#        n_test_set_x = test_set_x.get_value(borrow=True).shape[0]
#        test_out = theano.function([], self.final_layer.output,
#              givens={self.x: test_set_x[0:n_test_set_x]})
#        predict_parameter = test_out()
#        return predict_parameter
#    test_DBN(train_scp, valid_scp, log_dir, model_dir, n_ins, n_outs, hidden_layer_sizes,
#             finetune_lr, pretraining_epochs, pretrain_lr, training_epochs, batch_size)
### sequence-to-sequence mapping ###
# vanilla encoder-decoder (phone-level features)
# hierarchical encoder-decoder
# hidden layer activation
# momentum
#+ self.L2_reg * self.L2_sqr
## added for LHUC
# In lhuc the parameters are only scaling parameters which have the name 'c'
# use optimizer
# zip just concatenate two lists
# freeze layers and update weights
#index, batch_size
#[index*batch_size:(index + 1)*batch_size]
# momentum
#+ self.L2_reg * self.L2_sqr
# zip just concatenate two lists
# momentum
#+ self.L2_reg * self.L2_sqr
# use optimizer
# zip just concatenate two lists
#, batch_size
#, batch_size
#, batch_size
#, batch_size
#, batch_size
## the function to output activations at a hidden layer
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
###THEANO_FLAGS='cuda.root=/opt/cuda-5.0.35,mode=FAST_RUN,device=gpu0,floatX=float32,exception_verbosity=high' python dnn.py
# allocate symbolic variables for the data
##T.nnet.sigmoid)  #
### Maximum likelihood
#n_component
# normalise by mean_log_det
# lines to compute debugging information for later printing
#self.errors = T.min(T.min(T.log(sigma), axis=1))
#self.errors = T.mean(T.sum(T.log(sigma), axis=1)) # computes mean_log_det
#self.errors = -xEx # (vector quantity) should be about 0.5 * beta * n_outs
#self.errors = point_fit  # (vector quantity) should be about one
#self.errors = T.mean(T.exp(exponent)) / T.exp(T.max(exponent)) # fraction of the data used, should be about efficiency
#self.errors = T.mean(point_fit) # should be about one
#self.errors = log_det_mult # (vector quantity) about zero, or always less if using Rprop
#self.errors = beta_obj # (vector quantity) objective function terms
#self.errors = self.finetune_cost # disable this line below when debugging
#n_component
#self.errors = self.finetune_cost
# disable this line if debugging beta_opt
# compute number of minibatches for training, validation and testing
# index to a [mini]batch
##top 2 layers use a smaller learning rate
# compute list of fine-tuning updates
# compute the gradients with respect to the model parameters
## retain learning rate and momentum to make interface backwards compatible,
## but we won't use them, means we have to use on_unused_input='warn'.
## Otherwise same function for RPROP or otherwise -- can move this block outside if clause.
# Create a function that scans the entire validation set
#, batch_size
#, batch_size
#, batch_size
#    test_DBN(train_scp, valid_scp, log_dir, model_dir, n_ins, n_outs, hidden_layers_sizes,
#             finetune_lr, pretraining_epochs, pretrain_lr, training_epochs, batch_size)
### default seq-to-seq model: tile C as input to all frames ###
### default seq-to-seq model: tile C as input to all frames ###
### Distributed seq-to-seq model: tile C_1-C_n as input to corresponding decoder frames ###
#output layer
#        for i in xrange(25):
#            static_indice.append(i+184)
#            delta_indice.append(i+184+25)
#            acc_indice.append(i+184+50)
#            print   sub_std_mat
#            print   sub_o_std_vec, var_base[sub_dim_start*3:sub_dim_start*3+sub_dim*3] ** 0.5
#            temp_obs_err_vec = gnp.dot(traj_err.T, wuwwu)
#            print   obs_err_vec, temp_obs_err_vec
#            print   obs_err_vec.shape, temp_obs_err_vec.shape
#            print   obs_mu, mlpg_traj, ref_y
#            print   obs_err_vec.shape, sub_o_std_vec.shape, frame_number, wuwwu.shape, traj_err.shape
# final layer is linear output, gradient is one
#output layers
# + self.b_params[i] * self.l2_reg
#update weights and record momentum weights
#        self.errors = gnp.sum((self.final_layer_output - train_set_y) ** 2, axis=1)
#mlpg_traj ref_y
#        for i in xrange(len(self.W_params)):
#############following function for MLPG##################
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# get a logger
# this (and only this) logger needs to be configured immediately, otherwise it won't work
# we can't use the full user-supplied configuration mechanism in this particular case,
# because we haven't loaded it yet!
#
# so, just use simple console-only logging
# this level is hardwired here - should change it to INFO
# add a handler & its formatter - will write only to console
# first, set up some default configuration values
# next, load in any user-supplied configuration values
# that might over-ride the default values
# finally, set up all remaining configuration values
# that depend upon either default or user-supplied values
# to be called before loading any user specific values
# things to put here are
# 1. variables that the user cannot change
# 2. variables that need to be set before loading the user's config file
# get a logger
# load and parse the provided configFile, if provided
# load the config file
#work_dir must be provided before initialising other directories
# default place for some data
# Paths
# Input-Output
# Architecture
# RNN
#encoder_decoder
# Data
# Processes
# this uses exec(...) which is potentially dangerous since arbitrary code could be executed
# default value
# first, look for a user-set value for this variable in the config file
# use default value, if there is one
# to be called after reading any user-specific settings
# because the values set here depend on those user-specific settings
# get a logger
## create directories if not exists
# input-output normalization stat files
# define model file name
# model files
# predicted features directory
# string.lower for some architecture values
# set sequential training True if using LSTMs
# set/limit batch size to 25
## num. of sentences in this case
# rnn params
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
############################
##### Memory variables #####
############################
### read file by file ###
### read file by file ###
### read file by file ###
### read file by file ###
### read file by file ###
### shuffle train id list ###
## shuffle by sentence
## shuffle by a group of sentences
#### normalize training data ####
#### load norm stats ####
#### normalize data ####
#### de-normalize data ####
#!/usr/bin/env python
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#self.session=tf.InteractiveSession()
#self.activation    ={"tanh":tf.nn.tanh,"sigmoid":tf.nn.sigmoid}
#self.saver=tf.train.Saver()
#  stacked_rnn_outputs=tf.reshape(rnn_outputs,[-1,self.n_out])
#  stacked_outputs=tf.layers.dense(stacked_rnn_outputs,self.n_out)
#  output_layer=tf.reshape(stacked_outputs,[-1,utt_length,self.n_out])
##This method is only used when a sequence model is TrainTensorflowModels
#return self._cell(dropout(inputs,self._input_keep_prob,is_training=self.is_training,scope=None),state,scope=None)
#    print pooling_outputs.shape
# print projection_layer.shape
#  print layer_list[-1].shape
#enc_state=(tf.concat(enc_state[0])
# Reset gate and update gate.
# We start with bias of 1.0 to not reset and not update.
#!/usr/bin/env python
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#### TODO: Find a good way to pass below params ####
#rs=sess.run(merged,feed_dict={input_layer:x_batch,output_data:y_batch,is_training_drop:True,is_training_batch:True})
#rs=sess.run(merged,feed_dict={input_layer:x_batch,output_data:y_batch,is_training_batch:True})
#if self.dropout_rate!=0.0:
#        training_loss=loss.eval(feed_dict={input_layer:train_x,output_data:train_y,is_training_drop:False,is_training_batch:False})
#     else:
#        training_loss=loss.eval(feed_dict={input_layer:train_x,output_data:train_y,is_training_batch:False})
#Data Preparation
#Shuffle the data
#overall_loss=tf.summary.scalar("training loss",overall_loss)
#summary_writer.add_summary(overall_loss,epoch)
#if self.dropout_rate!=0.0:
#    if hybrid:
#       training_loss=loss.eval(feed_dict={input_layer:temp_train_x,output_data:temp_train_y,utt_length_placeholder:utt_length,\
#       is_training_drop:False,is_training_batch:False})
#    else:
#       training_loss=loss.eval(feed_dict={input_layer:temp_train_x,output_data:temp_train_y,utt_length_placeholder:utt_length,\
#       is_training_drop:False})
#elif hybrid:
#    training_loss=loss.eval(feed_dict={input_layer:temp_train_x,output_data:temp_train_y,utt_length_placeholder:utt_length,is_training_batch:False})
#else:
#    training_loss=loss.eval(feed_dict={input_layer:temp_train_x,output_data:temp_train_y,utt_length_placeholder:utt_length})
#model_name="sequence_model"+" hybrid.ckpt" if hybrid==1 else "sequence_model.ckpt"
#### compute predictions ####
#if self.cbhg:
#    training_loss=loss.eval(feed_dict={inputs_data:temp_train_x,targets:temp_train_y,target_sequence_length:utt_length})
#else:
#    training_loss=loss.eval(feed_dict={inputs_data:temp_train_x,targets:temp_train_y,inputs_sequence_length:utt_length,target_sequence_length:utt_length})
#### compute predictions ####
#utt_length=[len(utt) for utt in test_x.values()]
#max_step=max(utt_length)
#dec_cell=self.graph.get_collection("decoder_cell")[0]
#  outputs=sess.run(inference_output,{inputs_data:temp_test_x,inputs_sequence_length:utt_length,\
#            target_sequence_length:utt_length})
#   #print _outputs[:,t,:]
#(Decay the first moment running average coefficient)
# (Update biased first moment estimate)
# (Update biased second raw moment estimate)
# (Compute bias-corrected first moment estimate)
# (Compute bias-corrected second raw moment estimate)
#(Update parameters)
#updates.append((m_previous, m))
#updates.append((v_previous, v))
#updates.append((theta_previous, theta) )
#updates.append((m, m_t))
#updates.append((v, v_t))
#updates.append((p, p_t))
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# Force matplotlib to not use any Xwindows backend.
## then update all by default
## 1, 2, 3, 4: Rprop+ Rprop- iRprop+ iRprop-
##    in Igel 2003 'Empirical evaluation of the improved Rprop learning algorithms'
## It would be easier to follow if these things were defined in __init__, but
## they are here to keep all RPROP-specific stuff in one place.
## Also, make some or all
## rprop_init_update is configured during __init__, all of the others are hardcoded here
## for now:-
## first update update_values:
## apply floor/ceiling to updates:
## zero gradients where sign changed: reduce step size but don't change weight
## then update params:
## store previous iteration gradient to check for sign change in next iteration:
# gparam # sign_change_test #  update_changes    #
#m,n = numpy.shape(vals)
## This is generic, and not specific to RPROP:
## only keep bottom one
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#/usr/bin/python -u
#from utils import GlobalCfg
# Logging:
# File setup:
# Feat extraction:
## MagPhase Vocoder:
# TODO: Add WORLD and STRAIGHT
# If vocoder is not supported:
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#            print   reference_vuv
#        f0_mse = numpy.sum((((voiced_ref_data) - (voiced_gen_data)) ** 2))
# ** 0.5
# accept the difference upto two frames
# ** 0.5
# -*- coding: utf-8 -*-
#
# Copyright 2016 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
### save acoustic normalisation information for normalising the features back
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#/usr/bin/python -u
#from utils import GlobalCfg
#import configuration
# cannot have these outside a function - if you do that, they get executed as soon
# as this file is imported, but that can happen before the configuration is set up properly
# SPTK     = cfg.SPTK
# NND      = cfg.NND
# STRAIGHT = cfg.STRAIGHT
# a convenience function instead of calling subprocess directly
# this is so that we can do some logging and catch exceptions
# we don't always want debug logging, even when logging level is DEBUG
# especially if calling a lot of external functions
# so we can disable it by force, where necessary
# the following is only available in later versions of Python
# rval = subprocess.check_output(args)
# bufsize=-1 enables buffering and may improve performance compared to the unbuffered case
# better to use communicate() than read() and write() - this avoids deadlocks
# for critical things, we always log, even if log==False
# not sure under what circumstances this exception would be raised in Python 2.6
# not sure if there is an 'output' attribute under 2.6 ? still need to test this...
# try to kill the subprocess, if it exists
# this means that p was undefined at the moment of the keyboard interrupt
# (and we do nothing)
#    NND      = cfg.NND
## to be moved
### post-filtering
###mgc to sp to wav
### If using world v2, please comment above line and uncomment this
#run_process('{mgc2sp} -a {alpha} -g 0 -m {order} -l {fl} -o 0 {bap} | {sopr} -d 32768.0 -P | {x2x} +fd > {ap}'
#            .format(mgc2sp=SPTK['MGC2SP'], alpha=cfg.fw_alpha, order=cfg.bap_dim, fl=cfg.fl, bap=bap_file_name, sopr=SPTK['SOPR'], x2x=SPTK['X2X'], ap=files['ap']))
# Import MagPhase and libraries:
## STRAIGHT or WORLD vocoders:
## MagPhase Vocoder:
# Add your favorite vocoder here.
# If vocoder is not supported:
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#remove potential empty lines and end of line signs
# get file lengths
# set training algo
# set batch size
# set RNN batch training True
# set params for each training algo
### read file by file ###
## we allow small difference here. may not be correct, but sometimes, there is one/two frames difference
## set sequence length for batch training 
# set seq length to maximum seq length from current batch
# set seq length to maximum seq length from current bucket
# seq length is set based on default/user configuration 
### read file by file ###
# choose utterance from current bucket list
# choose utterance randomly from current file list 
#self.utt_index = numpy.random.randint(self.list_size)
## choose utterance in serial order
# break for any of the below conditions
## we allow small difference here. may not be correct, but sometimes, there is one/two frames difference
# reshape input-output
### MLU features sub-division ###
### duration array sub-division ###
### additional feature matrix (syllable+phone+frame=432) ###
### input word feature matrix ###
### rest of the code similar to S2S ###
### MLU features sub-division ###
### duration array sub-division ###
### additional feature matrix (syllable+phone+frame=432) ###
### input word feature matrix ###
### for batch processing ###
### rest of the code similar to S2S ###
### first check whether there are remaining data from previous utterance
## we allow small difference here. may not be correct, but sometimes, there is one/two frames difference
## if current utterance cannot be stored in the block, then leave the remaining part for the next block
#        temp_set_x = self.make_shared(temp_set_x, 'x')
#        temp_set_y = self.make_shared(temp_set_y, 'y')
##ListDataProvider.__init__(x_file_list, \
##         y_file_list, n_ins=0, n_outs=0, buffer_size = 500000, shuffle=False)
## Put this function at global level so it can be imported for use in dnn_generation
## Turn indexes to words, syllables etc. to one-hot data:
#print projection_indices.tolist()
## Used advanced indexing to turn the relevant features on:
## check conversion???!?!?!
#     print projection_indices.tolist()
#     print '            ^--- proj indices'
#     print
## Effectively remove the index from the original data by setting to 0:
## Turn indexes to words, syllables etc. to one-hot data:
#print projection_indices.tolist()
## check conversion???!?!?!
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#  quick and dirty utility to print out binary files, for debugging
# import numpy
## shall we read the logging config file from command line?
# print features
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
# the new class for label composition and normalisation
#from models.ms_dnn import MultiStreamDNN
#from models.ms_dnn_gv import MultiStreamDNNGv
#from models.sdae import StackedDenoiseAutoEncoder
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
#     private_l2_reg  = float(hyper_params['private_l2_reg'])
#     stream_weights       = hyper_params['stream_weights']
#     private_hidden_sizes = hyper_params['private_hidden_sizes']
#     stream_lr_weights = hyper_params['stream_lr_weights']
#     use_private_hidden = hyper_params['use_private_hidden']
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
# too many consecutive epochs without surpassing the best model
#    visualize_dnn(dnn_model)
#        predicted_parameter = test_out()
### write to cmp file
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
## + cfg.appended_input_dim
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
#    nn_label_norm_mvn_dir = os.path.join(data_dir, 'nn_no_silence_lab_norm_'+suffix)
# to do - sanity check the label dimension here?
# simple HTS labels
## don't remove silences for duration
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output acoustic data
#[-0.5, 0.0, 0.5]
#[1.0, -2.0, 1.0]
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
# save to itself
## don't remove silences for duration
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# logger.debug(' value was\n%s' % cmp_norm_info)
# logger.debug(' value was\n%s' % feature_std_vector)
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
### DNN model training
# not an error - just means directory already exists
# print   'start DNN'
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
### generate parameters from DNN
# not an error - just means directory already exists
#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
##MCD
##MCD
#+*'])
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
# the new class for label composition and normalisation
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
##basic model is ready.
##if corruption levels is set to zero. it becomes normal autoencoder
##model is ready, but the hyper-parameters are not optimised.
## not fully ready
## if pretraining is supported in one model, add the switch here
## be careful to use autoencoder for pretraining here:
## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]
## however, tanh works better and converge fast in finetuning
##
## Will extend this soon...
## in SDAE we do layer-wise pretraining using autoencoders
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
# too many consecutive epochs without surpassing the best model
#    visualize_dnn(dbn)
#        predicted_parameter = test_out()
### write to cmp file
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
#    nn_label_norm_mvn_dir = os.path.join(data_dir, 'nn_no_silence_lab_norm_'+suffix)
# to do - sanity check the label dimension here?
# simple HTS labels
#        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list)
#        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = ['*-#+*'])
#        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output acoustic data
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
#+*'])
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# logger.debug(' value was\n%s' % cmp_norm_info)
# logger.debug(' value was\n%s' % feature_std_vector)
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
#    nnets_file_name = '%s/%s_%s_%d.%d.%d.%d.%d.train.%d.model' \
#                       %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),
#                        len(hidden_layers_sizes), hidden_layers_sizes[0],
#                        lab_dim, cfg.cmp_dim, cfg.train_file_number)
### DNN model training
# not an error - just means directory already exists
# print   'start DNN'
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
### generate parameters from DNN
# not an error - just means directory already exists
# not an error - just means directory already exists
#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
#+*'])
##MCD
##MCD
#+*'])
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
#+*'])
# this can be removed
#
#to calculate distortion of HMM baseline
#+*'])
#+*'])
#+*'])
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
# the new class for label composition and normalisation
#from models.ms_dnn import MultiStreamDNN
#from models.ms_dnn_gv import MultiStreamDNNGv
#from models.sdae import StackedDenoiseAutoEncoder
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
#        dnn_model = DNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,
#                        l1_reg = l1_reg, l2_reg = l2_reg,
#                         hidden_layers_sizes = hidden_layers_sizes,
#                          hidden_activation = hidden_activation,
#                          output_activation = output_activation)
## We can't just unpickle the old model and use that because fine-tune functions
## depend on opt_l2e option used in construction of initial model. One way around this
## would be to unpickle, manually set unpickled_dnn_model.opt_l2e=True and then call
## unpickled_dnn_model.build_finetne_function() again. This is another way, construct
## new model from scratch with opt_l2e=True, then copy existing weights over:
#training_epochs
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
# too many consecutive epochs without surpassing the best model
#    visualize_dnn(dbn)
#        predicted_parameter = test_out()
### write to cmp file
### multiple Gaussian components
#        print   predicted_mu.shape
#        predicted_mu = predicted_mu[aa*n_outs:(aa+1)*n_outs]
#                print   gen_features
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
#    nn_label_norm_mvn_dir = os.path.join(data_dir, 'nn_no_silence_lab_norm_'+suffix)
# to do - sanity check the label dimension here?
# simple HTS labels
#+*'])
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output acoustic data
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
#+*'])
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# logger.debug(' value was\n%s' % cmp_norm_info)
# logger.debug(' value was\n%s' % feature_std_vector)
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
#    nnets_file_name = '%s/%s_%s_%d.%d.%d.%d.%d.train.%d.model' \
#                       %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),
#                        len(hidden_layers_sizes), hidden_layers_sizes[0],
#                        lab_dim, cfg.cmp_dim, cfg.train_file_number)
### DNN model training
# not an error - just means directory already exists
# print   'start DNN'
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
### generate parameters from DNN
# not an error - just means directory already exists
#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)
#        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
#+*'])
##MCD
##MCD
#+*'])
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
#+*'])
# this can be removed
#
#to calculate distortion of HMM baseline
#+*'])
#+*'])
#+*'])
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
#import gnumpy as gnp
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
# the new class for label composition and normalisation
#from frontend.mlpg_fast import MLParameterGenerationFast
#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
#    sequential_training = True
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
#    pretrain_set_x = train_set_x
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
#, batch_size=batch_size
#    finetune_lr = 0.000125
# if sequential training, the batch size will be the number of frames in an utterance
#batch_size = temp_train_set_x.shape[0]
#print train_set_x.eval().shape, train_set_y.eval().shape, this_train_error
## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function
#            logger.debug('validation loss decreased, so saving model')
#            dbn = best_dnn_model
#    cPickle.dump(best_dnn_model, open(nnets_file_name, 'wb'))
#file_number
### write to cmp file
#file_number
#print b_indices
### write to cmp file
#file_number
### write to cmp file
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
# to do - sanity check the label dimension here?
# this means that open(...) threw an error
# simple HTS labels
###use only training data to find min-max information, then apply on the whole dataset
### make duration data for S2S network ###
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output duration data
### make output acoustic data
#[-0.5, 0.0, 0.5]
#[1.0, -2.0, 1.0]
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
### DNN model training
# not an error - just means directory already exists
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
# not an error - just means directory already exists
### generate parameters from DNN
# not an error - just means directory already exists
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### Perform duration normalization(min. state dur set to 1) ###
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list, cfg)  # reference copy synthesis speech
### setting back to original conditions before calculating objective scores ###
### evaluation: RMSE and CORR for duration
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
##MCD
##MCD
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
#    if gnp._boardId is not None:
#        import gpu_lock
#        gpu_lock.free_lock(gnp._boardId)
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
#import gnumpy as gnp
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
# the new class for label composition and normalisation
#from frontend.mlpg_fast import MLParameterGenerationFast
#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
#    sequential_training = True
###################
#validation data is still read block by block
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
#    pretrain_set_x = train_set_x
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
#, batch_size=batch_size
#temporally we use the training set as pretrain_set_x.
#we need to support any data for pretraining
#, batch_size=batch_size
## if pretraining is supported more than one model, add the switch here
## be careful to use autoencoder for pretraining here:
## in SDAE we do layer-wise pretraining using autoencoders
# if sequential training, the batch size will be the number of frames in an utterance
## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function
#    finetune_lr = 0.000125
#            train_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)
#            train_set_y.set_value(numpy.asarray(temp_train_set_y, dtype=theano.config.floatX), borrow=True)
# if sequential training, the batch size will be the number of frames in an utterance
## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function
#            logger.debug('validation loss decreased, so saving model')
#            dbn = best_dnn_model
#    cPickle.dump(best_dnn_model, open(nnets_file_name, 'wb'))
#file_number
### write to cmp file
#file_number
### write to cmp file
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
# to do - sanity check the label dimension here?
# this means that open(...) threw an error
# simple HTS labels
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output duration data
### make output acoustic data
#[-0.5, 0.0, 0.5]
#[1.0, -2.0, 1.0]
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
### DNN model training
# not an error - just means directory already exists
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
# not an error - just means directory already exists
### generate parameters from DNN
# not an error - just means directory already exists
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### Perform duration normalization(min. state dur set to 1) ###
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list, cfg)  # reference copy synthesis speech
### setting back to original conditions before calculating objective scores ###
### evaluation: RMSE and CORR for duration
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
##MCD
##MCD
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
#    if gnp._boardId is not None:
#        import gpu_lock
#        gpu_lock.free_lock(gnp._boardId)
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
#import gnumpy as gnp
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#import theano.tensor as T
# the new class for label composition and normalisation
#from frontend.mlpg_fast import MLParameterGenerationFast
#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
##  W and b for each layer
#print layer_name
#numpy.savetxt(os.path.join(outdir, fname + '.txt'), param_vals[p_ix])
### Input normalisation:-
## output norm
#     norm_info_file = os.path.join(data_dir, 'norm_info' + cfg.combined_feature_name + '_' + str(cfg.cmp_dim) + '_' + cfg.output_feature_normalisation + '.dat')
### normalise input full context label
## + cfg.appended_input_dim
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
## if made with run_lstm:--
## if made with run_dnn:--
## override the name computed from config variables if model_pickle_file specified:
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
# the new class for label composition and normalisation
#from models.ms_dnn import MultiStreamDNN
#from models.ms_dnn_gv import MultiStreamDNNGv
#from models.sdae import StackedDenoiseAutoEncoder
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
#    visualize_dnn(dbn)
#        predicted_parameter = test_out()
### write to cmp file
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
#### parameter setting########
###total file number including training, development, and testing
#total_file_number = len(file_id_list)
#nn_cmp_dir       = os.path.join(data_dir, 'nn' + cfg.combined_feature_name + '_' + str(cfg.cmp_dim))
#nn_cmp_norm_dir   = os.path.join(data_dir, 'nn_norm'  + cfg.combined_feature_name + '_' + str(cfg.cmp_dim))
#in_file_list_dict = {}
#for feature_name in cfg.in_dir_dict.keys():
#    in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)
#nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)
#nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
## need this to find normalisation info:
# simple HTS labels
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# no silence removal for synthesis ...
## minmax norm:
# reload stored minmax values: (TODO -- move reading and writing into MinMaxNormalisation class)
## This doesn't work -- precision is lost -- reads in as float64
#label_norm_info = numpy.fromfile(fid)  ## label_norm_info = numpy.array(label_norm_info, 'float32')
## use struct to enforce float32:
# length in bytes
# = read until bytes run out
## number 32 bit floats
###  apply precompuated min-max to the whole dataset
### make output acoustic data
#    if cfg.MAKECMP:
### retrieve acoustic normalisation information for normalising the features back
### normalise output acoustic data
#    if cfg.NORMCMP:
### DNN model training
#    if cfg.TRAINDNN:
##if cfg.DNNGEN:
# not an error - just means directory already exists
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
## gv_weight hard coded here!
### generate wav ----
#if cfg.GENWAV:
#generate_wav_glottHMM(scaled_dir, file_id_list)
## simple variance scaling (silen et al. 2012, paragraph 3.1)
## TODO: Lots of things like stream names hardcoded here; 3 for delta + delta-delta; ...
#     all_streams = ['cmp','HNR','F0','LSF','Gain','LSFsource']
#     streams_to_scale = ['LSF']
## Try range of interpolation weights for combining global & local variance
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
#
#     # set up logging to use our custom class
#     logging.setLoggerClass(LoggerPlotter)
#
#     # get a logger for this main function
#     logger = logging.getLogger("main")
#logger.critical('usage: run_dnn.sh config_file_name utt_dir')
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
# ListDataProvider
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
# the new class for label composition and normalisation
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## This should always be True -- tidy up later
## including input and output
####parameters#####
######### data providers ##########
####################################
# numpy random generator
############## load existing dnn #####
####################################
## <-------- hard coded !!!!!!!!!!
# 10  ## <-------- hard coded !!!!!!!!!!
#warmup_epoch_3 = epoch + warmup_epoch_3
#inference_epochs += epoch
## osw -- inferring word reps on validation set in a forward pass in a single batch
##        exausts memory when using 20k projected vocab -- also use minibatches
#valid_error = infer_projections_fn(current_finetune_lr, current_momentum)
#this_validation_loss = numpy.mean(valid_error)
#        if plot:
#            ## add dummy validation loss so that plot works:
#            plotlogger.add_plot_point('training convergence','validation set',(epoch,this_validation_loss))
#            plotlogger.add_plot_point('training convergence','training set',(epoch,this_train_valid_loss))
#
#        if cfg.hyper_params['model_type'] == 'TPDNN':
#            if not os.path.isdir(cfg.projection_weights_output_dir):
#                os.mkdir(cfg.projection_weights_output_dir)
#            weights = dnn_model.get_projection_weights()
#            fname = os.path.join(cfg.projection_weights_output_dir, 'proj_INFERENCE_epoch_%s'%(epoch))
#            numpy.savetxt(fname, weights)
#
## always update
##cPickle.dump(best_dnn_model, open(nnets_file_name, 'wb'))
#    if plot:
#        plotlogger.save_plot('training convergence',title='Final training and validation error',xlabel='epochs',ylabel='error')
#
### ========================================================
#    if cfg.hyper_params['model_type'] == 'TPDNN':
#        os.system('python %s %s'%('/afs/inf.ed.ac.uk/user/o/owatts/scripts_NEW/plot_weights_multiple_phases.py', cfg.projection_weights_output_dir))
## 'remove' word representations by randomising them. As model is unpickled and
## not re-saved, this does not throw trained parameters away.
## use randomly chosen training projection -- shuffle in-place = same as sampling wihtout replacement
## shuffle in place along 1st dim (reorder rows)
##  generate utt embeddings uniformly at random within the min-max of the training set (i.e. from a (hyper)-rectangle)
## vector like a row of P with min of its columns
## use mean projection
## stack mean rows
## DEBUG
## DEBUG:=========
#projection_weights_to_use = old_weights # numpy.array(numpy.random.uniform(low=-0.3, high=0.3, size=numpy.shape(old_weights)),  dtype=numpy.float32)
## =============
##  generate utt embeddings from a uniform 10 x 10 grid within the min-max of the training set (i.e. from a rectangle)
## vector like a row of P with min of its columns
## pading to handle 0 index (reserved for defaults)
## points uniformly sampled from between the 1.8 - 2.0 stds of a diagonal covariance gaussian fitted to the data
## vector like a row of P with min of its columns
## points uniformly sampled from between the 1.8 - 2.0 stds of a diagonal covariance gaussian fitted to the data
## vector like a row of P with min of its columns
## points uniformly sampled from between the 1.8 - 2.0 stds of a diagonal covariance gaussian fitted to the data
## vector like a row of P with min of its columns
##  save used weights for future reference:
#features, features_proj = expand_projection_inputs(features, cfg.index_to_project, \
#                                                         cfg.projection_insize)
#temp_set_x = features.tolist()  ## osw - why list conversion necessary?
#        predicted_parameter = test_out()
### write to cmp file
## define a couple of functions for circular rejection sampling:
## if x^2 + y^2 <= 1, point is within unit circle
##
## if x^2 + y^2 <= 1, point is within unit circle
##generate bottleneck layer as festures
### write to cmp file
## Taken from: ~/proj/dnn_tts/script/add_token_index.py
## clear target attribute name from all nodes to be safe:
## all nodes
## 0 is the defualt 'n/a' value -- *some* ancestor of all nodes will have the relevant attibute to fall back on
## Taken from: ~/proj/dnn_tts/script/add_token_index.py
## clear target attribute name from all nodes to be safe:
## all nodes
## 0 is the defualt 'n/a' value -- *some* ancestor of all nodes will have the relevant attibute to fall back on
## TODO -- move reading and writing into MinMaxNormalisation class
# reload stored minmax values:
## This doesn't work -- precision is lost -- reads in as float64
#label_norm_info = numpy.fromfile(fid)  ## label_norm_info = numpy.array(label_norm_info, 'float32')
## use struct to enforce float32:
# length in bytes
# = read until bytes run out
## number of 32 bit floats
## values can be min + max or mean + std, hence non-descript variable names:
## TODO: token_xpath & index_attrib_name   should be in config
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# but for now we need to do it manually
#### parameter setting########
####prepare environment
###synth_utts_input = synth_utts_input[:10]   ### temp!!!!!
## place to put test utts with tokens labelled with projection indices
## was below -- see comment
###normalisation information
### normalise input full context label
# the number can be removed
## need this to find normalisation info:
## always do this in synth:
## if cfg.NORMLAB and (cfg.label_style == 'composed'):
## add_projection_indices was here
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# no silence removal for synthesis ...
## minmax norm:
###  apply precompuated and stored min-max to the whole dataset
### DEBUG
## set up paths -- write CMP data to infer from in outdir:
### make output acoustic data
#    if cfg.MAKECMP:
## skip silence removal for inference -- need to match labels, which are
## not silence removed either
### retrieve acoustic normalisation information for normalising the features back
### normalise output acoustic data
#    if cfg.NORMCMP:
#### DEBUG
###  apply precompuated and stored mean and std to the whole dataset
#            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim)
#            global_mean_vector = min_max_normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number])
#            global_std_vector = min_max_normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector)
#            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim, min_value = 0.01, max_value = 0.99)
#            min_max_normaliser.find_min_max_values(nn_cmp_file_list[0:cfg.train_file_number])
#            min_max_normaliser.normalise_data(nn_cmp_file_list, nn_cmp_norm_file_list)
#            cmp_min_vector = min_max_normaliser.min_vector
#            cmp_max_vector = min_max_normaliser.max_vector
#            cmp_norm_info = numpy.concatenate((cmp_min_vector, cmp_max_vector), axis=0)
### DNN model training
#    if cfg.TRAINDNN: always do this in synth
#### DEBUG
## default, for non-inferring synth methods
## infer control values from TESTING data
## identical lists (our test data) for 'train' and 'valid' -- this is just to
##   keep the infer_projections_fn theano function happy -- operates on
##    validation set. 'Train' set shouldn't be used here.
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
## if cfg.DNNGEN:
# not an error - just means directory already exists
#print nn_label_norm_file_list  ## <-- this WAS mangled in inferred due to copying of file list to trainlist_x etc. which is then shuffled. Now use copy.copy
#print gen_file_list
## DNNGEN
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
## osw: skip MLPG:
#            split_cmp(gen_file_list, ['mgc', 'lf0', 'bap'], cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict)
## Variance scaling:
## gv_weight hardcoded
### generate wav ---- glottHMM only!!!
#if cfg.GENWAV:
# generated speech
## simple variance scaling (silen et al. 2012, paragraph 3.1)
## TODO: Lots of things like stream names hardcoded here; 3 for delta + delta-delta; ...
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
#    if cfg.profile:
#        logger.info('profiling is activated')
#        import cProfile, pstats
#        cProfile.run('main_function(cfg)', 'mainstats')
#        # create a stream for the profiler to write to
#        profiling_output = StringIO.StringIO()
#        p = pstats.Stats('mainstats', stream=profiling_output)
#        # print stats to that stream
#        # here we just report the top 10 functions, sorted by total amount of time spent in each
#        p.strip_dirs().sort_stats('tottime').print_stats(10)
#        # print the result to the log
#        logger.info('---Profiling result follows---\n%s' %  profiling_output.getvalue() )
#        profiling_output.close()
#        logger.info('---End of profiling result---')
#
#    else:
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
##from frontend.mlpg_fast import MLParameterGenerationFast
## osw temp
# the new class for label composition and normalisation
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
### plain DNN case
# def dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):
#     logger = logging.getLogger("dnn_generation")
#     logger.debug('Starting dnn_generation')
#
#     plotlogger = logging.getLogger("plotting")
#
#     dnn_model = cPickle.load(open(nnets_file_name, 'rb'))
#
# #    visualize_dnn(dbn)
#
#     file_number = len(valid_file_list)
#
#     for i in xrange(file_number):
#         logger.info('generating %4d of %4d: %s' % (i+1,file_number,valid_file_list[i]) )
#         fid_lab = open(valid_file_list[i], 'rb')
#         features = numpy.fromfile(fid_lab, dtype=numpy.float32)
#         fid_lab.close()
#         features = features[:(n_ins * (features.size / n_ins))]
#         features = features.reshape((-1, n_ins))
#         temp_set_x = features.tolist()
#         test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))
#
#         predicted_parameter = dnn_model.parameter_prediction(test_set_x=test_set_x)
# #        predicted_parameter = test_out()
#
#         ### write to cmp file
#         predicted_parameter = numpy.array(predicted_parameter, 'float32')
#         temp_parameter = predicted_parameter
#         fid = open(out_file_list[i], 'wb')
#         predicted_parameter.tofile(fid)
#         logger.debug('saved to %s' % out_file_list[i])
#         fid.close()
#
### multiple Gaussian components
## TODO: take this from config
#        print   predicted_mu.shape
#        predicted_mu = predicted_mu[aa*n_outs:(aa+1)*n_outs]
#                print   gen_features
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
###total file number including training, development, and testing
#total_file_number = len(file_id_list)
#nn_cmp_dir       = os.path.join(data_dir, 'nn' + cfg.combined_feature_name + '_' + str(cfg.cmp_dim))
#nn_cmp_norm_dir   = os.path.join(data_dir, 'nn_norm'  + cfg.combined_feature_name + '_' + str(cfg.cmp_dim))
#in_file_list_dict = {}
#for feature_name in cfg.in_dir_dict.keys():
#    in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)
#nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)
#nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
## need this to find normalisation info:
# simple HTS labels
#        logger.info('preparing label data (input) using standard HTS style labels')
#        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list)
#        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = ['*-#+*'])
#        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)
#        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)
#        ###use only training data to find min-max information, then apply on the whole dataset
#        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])
#        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# no silence removal for synthesis ...
## minmax norm:
# reload stored minmax values: (TODO -- move reading and writing into MinMaxNormalisation class)
## This doesn't work -- precision is lost -- reads in as float64
#label_norm_info = numpy.fromfile(fid)  ## label_norm_info = numpy.array(label_norm_info, 'float32')
## use struct to enforce float32:
# length in bytes
# = read until bytes run out
## number 32 bit floats
###  apply precompuated min-max to the whole dataset
### make output acoustic data
#    if cfg.MAKECMP:
### retrieve acoustic normalisation information for normalising the features back
### normalise output acoustic data
#    if cfg.NORMCMP:
### DNN model training
#    if cfg.TRAINDNN:
##if cfg.DNNGEN:
# not an error - just means directory already exists
#gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)
#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)
#        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)
## Variance scaling:
## gv_weight hard coded here!
### generate wav ---- glottHMM only!!!
#if cfg.GENWAV:
# generated speech
## simple variance scaling (silen et al. 2012, paragraph 3.1)
## TODO: Lots of things like stream names hardcoded here; 3 for delta + delta-delta; ...
## Try range of interpolation weights for combining global & local variance
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
#    if cfg.profile:
#        logger.info('profiling is activated')
#        import cProfile, pstats
#        cProfile.run('main_function(cfg)', 'mainstats')
#        # create a stream for the profiler to write to
#        profiling_output = StringIO.StringIO()
#        p = pstats.Stats('mainstats', stream=profiling_output)
#        # print stats to that stream
#        # here we just report the top 10 functions, sorted by total amount of time spent in each
#        p.strip_dirs().sort_stats('tottime').print_stats(10)
#        # print the result to the log
#        logger.info('---Profiling result follows---\n%s' %  profiling_output.getvalue() )
#        profiling_output.close()
#        logger.info('---End of profiling result---')
#
#    else:
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
# the new class for label composition and normalisation
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
#     frames_per_hour = 720000.0
#     tframes = train_set_x.get_value().shape[0]
#     vframes = valid_set_x.get_value().shape[0]
#     print 'Training frames: %s (%s hours)'%(tframes, tframes / frames_per_hour)
#     print 'Validation frames: %s (%s hours)'%(tframes, tframes / frames_per_hour)
#     sys.exit('999')
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
##basic model is ready.
##if corruption levels is set to zero. it becomes normal autoencoder
##model is ready, but the hyper-parameters are not optimised.
## not fully ready
## if pretraining is supported in one model, add the switch here
## be careful to use autoencoder for pretraining here:
## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]
## however, tanh works better and converge fast in finetuning
##
## Will extend this soon...
## in SDAE we do layer-wise pretraining using autoencoders
# =============================================================================
# The original script (run_dnn.py) has a training routine that looks like this:
#
#     foreach epoch:
#        foreach partition:
#            foreach minibatch:
#                train_model
#        validate_performance_and_stop_if_converged
#
# The current script's rountine looks like this:
#
#     foreach epoch:
#        foreach partition:
#            foreach minibatch:
#                train_model
#                if we've seen another hour of data:
#                     validate_performance_and_stop_if_converged
#
# In order to jump out of these multiple loops when converged, we'll use this variable:
#
## Hardcoded checking intervals and framerate: 720000 frames per hour at 5ms frame rate
### calculation validation error in 1 big batch can fail for big data --
### use minibatches
#validation_losses = valid_fn()
#this_validation_loss = numpy.mean(validation_losses)
#print '   validation for batch %s (%s frames): %s'%(minibatch_index, batch_size, v_loss)
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
## It might also be interesting to look at how consistent performance is across minibatches:
# too many consecutive checks without surpassing the best model
#    visualize_dnn(dbn)
#        predicted_parameter = test_out()
### write to cmp file
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
#    nn_label_norm_mvn_dir = os.path.join(data_dir, 'nn_no_silence_lab_norm_'+suffix)
# to do - sanity check the label dimension here?
# simple HTS labels
#+*'])
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output acoustic data
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
#+*'])
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# logger.debug(' value was\n%s' % cmp_norm_info)
# logger.debug(' value was\n%s' % feature_std_vector)
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
#    nnets_file_name = '%s/%s_%s_%d.%d.%d.%d.%d.train.%d.model' \
#                       %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),
#                        len(hidden_layers_sizes), hidden_layers_sizes[0],
#                        lab_dim, cfg.cmp_dim, cfg.train_file_number)
### DNN model training
# not an error - just means directory already exists
# print   'start DNN'
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
### generate parameters from DNN
# not an error - just means directory already exists
#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
#+*'])
##MCD
##MCD
#+*'])
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
#+*'])
# this can be removed
#
#to calculate distortion of HMM baseline
#+*'])
#+*'])
#+*'])
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
# ListDataProvider
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
# the new class for label composition and normalisation
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## This should always be True -- tidy up later
## including input and output
## Function for training projection and non-projection parts at same time
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
##basic model is ready.
##if corruption levels is set to zero. it becomes normal autoencoder
##model is ready, but the hyper-parameters are not optimised.
## not fully ready
## if pretraining is supported in one model, add the switch here
## be careful to use autoencoder for pretraining here:
## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]
## however, tanh works better and converge fast in finetuning
##
## Will extend this soon...
## in SDAE we do layer-wise pretraining using autoencoders
## osw -- getting validation error from a forward pass in a single batch
##        exausts memory when using 20k projected vocab -- also use minibatches
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
# too many consecutive epochs without surpassing the best model
### Save projection values:
## Function for training all model on train data as well as simultaneously
## inferring proj weights on dev data.
# in each epoch do:
#   train_all_fn()
#   infer_projections_fn()    ## <-- updates proj for devset and gives validation loss
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
##basic model is ready.
##if corruption levels is set to zero. it becomes normal autoencoder
##model is ready, but the hyper-parameters are not optimised.
## not fully ready
## if pretraining is supported in one model, add the switch here
## be careful to use autoencoder for pretraining here:
## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]
## however, tanh works better and converge fast in finetuning
##
## Will extend this soon...
## in SDAE we do layer-wise pretraining using autoencoders
##dnn_model.zero_projection_weights()
## infer validation weights before getting validation error:
## osw -- inferring word reps on validation set in a forward pass in a single batch
##        exausts memory when using 20k projected vocab -- also use minibatches
## this function also give us validation loss:
## osw -- getting validation error from a forward pass in a single batch
##        exausts memory when using 20k projected vocab -- also use minibatches
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
# too many consecutive epochs without surpassing the best model
### Save projection values:
## Function for training the non-projection part only
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
##basic model is ready.
##if corruption levels is set to zero. it becomes normal autoencoder
##model is ready, but the hyper-parameters are not optimised.
## not fully ready
## if pretraining is supported in one model, add the switch here
## be careful to use autoencoder for pretraining here:
## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]
## however, tanh works better and converge fast in finetuning
##
## Will extend this soon...
## in SDAE we do layer-wise pretraining using autoencoders
## osw -- getting validation error from a forward pass in a single batch
##        exausts memory when using 20k projected vocab -- also use minibatches
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
# too many consecutive epochs without surpassing the best model
### Save projection values:
### ========== now train the word residual ============
####parameters#####
######### data providers ##########
####################################
# numpy random generator
############## load existing dnn #####
####################################
## 100  ## <-------- hard coded !!!!!!!!!!
# 10  ## <-------- hard coded !!!!!!!!!!
### COULD REMOVE THIS LATER
## osw -- getting validation error from a forward pass in a single batch
##        exausts memory when using 20k projected vocab -- also use minibatches
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
#        if plot:
#            ## add dummy validation loss so that plot works:
#            plotlogger.add_plot_point('training convergence','validation set',(epoch,this_validation_loss))
#            plotlogger.add_plot_point('training convergence','training set',(epoch,this_train_valid_loss))
#
## always update
#    if plot:
#        plotlogger.save_plot('training convergence',title='Final training and validation error',xlabel='epochs',ylabel='error')
#
### ========================================================
### ========== now infer word represntations for out-of-training (dev) data ============
#
#    ### TEMP-- restarted!!! ### ~~~~~~~
#    epoch = 50
#    dnn_model = cPickle.load(open(nnets_file_name, 'rb'))
#    train_all_fn, train_subword_fn, train_word_fn, infer_projections_fn, valid_fn, valid_score_i = \
#                    dnn_model.build_finetune_functions(
#                    (train_set_x, train_set_x_proj, train_set_y),
#                    (valid_set_x, valid_set_x_proj, valid_set_y), batch_size=batch_size)
#    this_train_valid_loss = 198.0 ## approx value
#    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
####parameters#####
######### data providers ##########
####################################
# numpy random generator
############## load existing dnn #####
####################################
#dnn_model.initialise_projection_weights()
## <-------- hard coded !!!!!!!!!!
# 10  ## <-------- hard coded !!!!!!!!!!
#warmup_epoch_3 = epoch + warmup_epoch_3
#inference_epochs += epoch
## osw -- inferring word reps on validation set in a forward pass in a single batch
##        exausts memory when using 20k projected vocab -- also use minibatches
#valid_error = infer_projections_fn(current_finetune_lr, current_momentum)
#this_validation_loss = numpy.mean(valid_error)
#        if plot:
#            ## add dummy validation loss so that plot works:
#            plotlogger.add_plot_point('training convergence','validation set',(epoch,this_validation_loss))
#            plotlogger.add_plot_point('training convergence','training set',(epoch,this_train_valid_loss))
#
## always update
#    if plot:
#        plotlogger.save_plot('training convergence',title='Final training and validation error',xlabel='epochs',ylabel='error')
#
### ========================================================
## 'remove' word representations by randomising them. As model is unpickled and
## no re-saved, this does not throw trained parameters away.
#    visualize_dnn(dbn)
#features, features_proj = expand_projection_inputs(features, cfg.index_to_project, \
#                                                         cfg.projection_insize)
#temp_set_x = features.tolist()  ## osw - why list conversion necessary?
#print temp_set_x
#        predicted_parameter = test_out()
### write to cmp file
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
#    nn_label_norm_mvn_dir = os.path.join(data_dir, 'nn_no_silence_lab_norm_'+suffix)
# to do - sanity check the label dimension here?
# simple HTS labels
#+*'])
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output acoustic data
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
#+*'])
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# logger.debug(' value was\n%s' % cmp_norm_info)
# logger.debug(' value was\n%s' % feature_std_vector)
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
#    nnets_file_name = '%s/%s_%s_%d.%d.%d.%d.%d.train.%d.model' \
#                       %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),
#                        len(hidden_layers_sizes), hidden_layers_sizes[0],
#                        lab_dim, cfg.cmp_dim, cfg.train_file_number)
### DNN model training
# not an error - just means directory already exists
#                train_DNN(train_xy_file_list = (train_x_file_list, train_y_file_list), \
#                          valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \
#                          nnets_file_name = nnets_file_name, \
#                          n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \
#                          hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot)
#                infer_projections(train_xy_file_list = (train_x_file_list, train_y_file_list), \
#                          valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \
#                          nnets_file_name = nnets_file_name, \
#                          n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \
#                          hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot)
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
### generate parameters from DNN (with random token reps and inferred ones -- NOTOKENS & TOKENS)
# not an error - just means directory already exists
## Without words embeddings:
## With word embeddings:
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
## osw: skip MLPG:
#            split_cmp(gen_file_list, ['mgc', 'lf0', 'bap'], cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict)
### generate wav
# generated speech
#           generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
#+*'])
##MCD
##MCD
#+*'])
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
#+*'])
# this can be removed
#
#to calculate distortion of HMM baseline
#+*'])
#+*'])
#+*'])
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
# 128 * 1
#!/usr/bin/env python
#file_paths = []
#filenames = []
#for root, directories, files in os.walk(dir_name):
#    for filename in files:
#        filepath = os.path.join(root, filename)
#        file_paths.append(filepath)
#        filenames.append(filename)
#filenames=filter(os.path.isfile, os.listdir(dir_name))
#for f in os.listdir(dir_name):
#    if os.path.isfile(dir_name+'/'+f):
#        print dir_name+'/'+f+' is a file'
#    else:
#        print dir_name+'/'+f+' is not a file'
#print filenames
#file_paths=[ dir_name+'/'+f for f in os.listdir(dir_name) if os.path.isfile(f) ]
#print in_data_dir
# 128
# 0
# Test the classes used in Merlin pipeline
# TODO run some very simple training on random data)
# Always try to save it and reload it
# Get a logger for these tests
# Build various models
# -*- coding: utf-8 -*-
#
# Copyright 2016 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# pylint: disable=unused-import
# pylint: disable=g-import-not-at-top
# Case 1: GenTestList = False and test_synth_dir = None
# Case 2: GenTestList = True and test_synth_dir = None
# Case 3: GenTestList = True and test_synth_dir = test_synth
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Test program for WORLD 0.1.2 (2012/08/19)
// Test program for WORLD 0.1.3 (2013/07/26)
// Test program for WORLD 0.1.4 (2014/04/29)
// Test program for WORLD 0.1.4_3 (2015/03/07)
// Test program for WORLD 0.2.0 (2015/05/29)
// Test program for WORLD 0.2.0_1 (2015/05/31)
// Test program for WORLD 0.2.0_2 (2015/06/06)
// Test program for WORLD 0.2.0_3 (2015/07/28)
// Test program for WORLD 0.2.0_4 (2015/11/15)
// Test program for WORLD in GitHub (2015/11/16-)
// Latest update: 2016/02/02
// test.exe input.wav outout.wav f0 spec
// input.wav  : Input file
// output.wav : Output file
// f0         : F0 scaling (a positive number)
// spec       : Formant scaling (a positive number)
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
// Modification of the option
// When you You must set the same value.
// If a different value is used, you may suffer a fatal error because of a
// illegal memory access.
// Valuable option.speed represents the ratio for downsampling.
// The signal is downsampled to fs / speed Hz.
// If you want to obtain the accurate result, speed should be set to 1.
// You should not set option.f0_floor to under world_kFloorF0.
// If you want to analyze such low F0 speech, please change world_kFloorF0.
// Processing speed may sacrify, provided that the FFT length changes.
// You can give a positive real number as the threshold.
// Most strict value is 0, but almost all results are counted as unvoiced.
// The value from 0.02 to 0.2 would be reasonable.
// Parameters setting and memory allocation.
// StoneMask is carried out to improve the estimation performance.
// This value may be better one for HMM speech synthesis.
// Default value is -0.09.
// Important notice (2016/02/02)
// You can control a parameter used for the lowest F0 in speech.
// You must not set the f0_floor to 0.
// It will cause a fatal error because fft_size indicates the infinity.
// You must not change the f0_floor after memory allocation.
// You should check the fft_size before excucing the analysis/synthesis.
// The default value (71.0) is strongly recommended.
// On the other hand, setting the lowest F0 of speech is a good choice
// to reduce the fft_size.
// Parameters setting and memory allocation.
// Parameters setting and memory allocation.
// option is not implemented in this version. This is for future update.
// We can use "NULL" as the argument.
// F0 scaling
// Spectral stretching
// Synthesis by the aperiodicity
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] Output file
// f0         : argv[3] F0 scaling (a positive number)
// spec       : argv[4] Formant shift (a positive number)
//-----------------------------------------------------------------------------
// 2016/01/28: Important modification.
// Memory allocation is carried out in advanse.
// This is for compatibility with C language.
// wavread() must be called after GetAudioLength().
//---------------------------------------------------------------------------
// Analysis part
//---------------------------------------------------------------------------
// 2016/02/02
// A new struct is introduced to implement safe program.
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// F0 estimation
// Spectral envelope estimation
// Aperiodicity estimation by D4C
// Note that F0 must not be changed until all parameters are estimated.
//---------------------------------------------------------------------------
// Synthesis part
//---------------------------------------------------------------------------
// The length of the output waveform
// Synthesis
// Output
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Test program for WORLD 0.1.2 (2012/08/19)
// Test program for WORLD 0.1.3 (2013/07/26)
// Test program for WORLD 0.1.4 (2014/04/29)
// Test program for WORLD 0.1.4_3 (2015/03/07)
// Test program for WORLD 0.2.0 (2015/05/29)
// Test program for WORLD 0.2.0_1 (2015/05/31)
// Test program for WORLD 0.2.0_2 (2015/06/06)
// Test program for WORLD 0.2.0_3 (2015/07/28)
// Test program for WORLD 0.2.0_4 (2015/11/15)
// Test program for WORLD in GitHub (2015/11/16-)
// Latest update: 2016/02/02
// test.exe input.wav outout.wav f0 spec
// input.wav  : Input file
// output.wav : Output file
// f0         : F0 scaling (a positive number)
// spec       : Formant scaling (a positive number)
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
// Modification of the option
// When you You must set the same value.
// If a different value is used, you may suffer a fatal error because of a
// illegal memory access.
// Valuable option.speed represents the ratio for downsampling.
// The signal is downsampled to fs / speed Hz.
// If you want to obtain the accurate result, speed should be set to 1.
// You should not set option.f0_floor to under world_kFloorF0.
// If you want to analyze such low F0 speech, please change world_kFloorF0.
// Processing speed may sacrify, provided that the FFT length changes.
// You can give a positive real number as the threshold.
// Most strict value is 0, but almost all results are counted as unvoiced.
// The value from 0.02 to 0.2 would be reasonable.
// Parameters setting and memory allocation.
// StoneMask is carried out to improve the estimation performance.
// This value may be better one for HMM speech synthesis.
// Default value is -0.09.
// Important notice (2016/02/02)
// You can control a parameter used for the lowest F0 in speech.
// You must not set the f0_floor to 0.
// It will cause a fatal error because fft_size indicates the infinity.
// You must not change the f0_floor after memory allocation.
// You should check the fft_size before excucing the analysis/synthesis.
// The default value (71.0) is strongly recommended.
// On the other hand, setting the lowest F0 of speech is a good choice
// to reduce the fft_size.
// Parameters setting and memory allocation.
// Parameters setting and memory allocation.
// option is not implemented in this version. This is for future update.
// We can use "NULL" as the argument.
// F0 scaling
// Spectral stretching
// Synthesis by the aperiodicity
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] Output file
// f0         : argv[3] F0 scaling (a positive number)
// spec       : argv[4] Formant shift (a positive number)
//-----------------------------------------------------------------------------
// 2016/01/28: Important modification.
// Memory allocation is carried out in advanse.
// This is for compatibility with C language.
// wavread() must be called after GetAudioLength().
//---------------------------------------------------------------------------
// Analysis part
//---------------------------------------------------------------------------
// 2016/02/02
// A new struct is introduced to implement safe program.
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// F0 estimation
// Spectral envelope estimation
// Aperiodicity estimation by D4C
// Note that F0 must not be changed until all parameters are estimated.
//---------------------------------------------------------------------------
// Synthesis part
//---------------------------------------------------------------------------
// The length of the output waveform
// Synthesis
// Output
