Credits
=======
  - Showdown v0
    * [Srikanth Ronanki](https://github.com/ronanki)<br/>
      Bug fixing and current Github project maintainer
      
  - Original Project
    * [Zhizheng Wu](http://www.zhizheng.org/)<br/>
      Main contributor
    * [Oliver Watts](http://homepages.inf.ed.ac.uk/owatts/)<br/>
      Main contributor
# INSTALL

To install Merlin, `cd` merlin and run the below steps:

- Install some basic tools in Merlin
```sh
bash tools/compile_tools.sh
```
- Install python dependencies
```sh
pip install -r requirements.txt
```

## More advanced instructions

1. go to `tools/`  and follow INSTALL instructions there.
2. Merlin is coded in python and need third-party python libraries such as:

#### numpy, scipy, matplotlib, lxml 

- Usually shipped with your python packages 
- Available in Ubuntu packages

#### theano

- Can be found on pip
- Need version 0.7 and above
- http://deeplearning.net/software/theano

#### bandmat

- Can be found on pip
- https://pypi.python.org/pypi/bandmat

#### For running on NVIDIA GPU, you will need also CUDA

- https://developer.nvidia.com/cuda-zone

#### and you might want also CUDNN [optionnal]

- https://developer.nvidia.com/cudnn

### Computational efficiency
    
- Computationnal efficiency is obviously greatly improved using GPU.
- It is also improved using the latest versions of theano and numpy.

## Some Linux Instructions

#### For Ubuntu: 
```sh
sudo apt-get install python-numpy python-scipy python-dev python-pip python-nose g++ libopenblas-dev git libc6-dev-i386 glibc-devel.i686 csh
```

#### For Fedora: 
```sh
sudo yum install python-numpy python-scipy python-dev python-pip python-nose g++ libopenblas-dev git libc6-dev-i386 glibc-devel.i686 csh python-lxml libxslt-devel unzip
```

#### Common libraries for both Ubuntu and Fedora:
```sh
sudo env "PATH=$PATH" pip install Theano
sudo env "PATH=$PATH" pip install matplotlib
sudo env "PATH=$PATH" pip install bandmat
sudo env "PATH=$PATH" pip install lxml
```

#### For all stand-alone machines:
- If you are not a sudo user, this [post](https://cstr-edinburgh.github.io/install-merlin/) may help you install Merlin.

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright {yyyy} {name of copyright owner}

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
[![Build Status](https://travis-ci.org/CSTR-Edinburgh/merlin.svg?branch=master)](https://travis-ci.org/CSTR-Edinburgh/merlin)

## Merlin: The Neural Network (NN) based Speech Synthesis System

This repository contains the Neural Network (NN) based Speech Synthesis System  
developed at the Centre for Speech Technology Research (CSTR), University of 
Edinburgh. 

Merlin is a toolkit for building Deep Neural Network models for statistical parametric speech synthesis. 
It must be used in combination with a front-end text processor (e.g., Festival) and a vocoder (e.g., STRAIGHT or WORLD).

The system is written in Python and relies on the Theano numerical computation library.

Merlin comes with recipes (in the spirit of the [Kaldi](https://github.com/kaldi-asr/kaldi) automatic speech recognition toolkit) to show you how to build state-of-the art systems.

Merlin is free software, distributed under an Apache License Version 2.0, allowing unrestricted commercial and non-commercial use alike.

Read the documentation at [cstr-edinburgh.github.io/merlin](https://cstr-edinburgh.github.io/merlin/).

Merlin is compatible with: __Python 2.7-3.6__.

Installation
------------

Merlin uses the following dependencies:

- numpy, scipy
- matplotlib
- bandmat
- theano
- tensorflow (optional, required if you use tensorflow models)
- sklearn, keras, h5py (optional, required if you use keras models)

To install Merlin, `cd` merlin and run the below steps:

- Install some basic tools in Merlin
```sh
bash tools/compile_tools.sh
```
- Install python dependencies
```sh
pip install -r requirements.txt
```

For detailed instructions, to build the toolkit: see [INSTALL](https://github.com/CSTR-Edinburgh/merlin/blob/master/INSTALL.md) and [CSTR blog post](https://cstr-edinburgh.github.io/install-merlin/).  
These instructions are valid for UNIX systems including various flavors of Linux;


Getting started with Merlin
---------------------------

To run the example system builds, see `egs/README.txt`

As a first demo, please follow the scripts in `egs/slt_arctic`

Now, you can also follow Josh Meyer's [blog post](http://jrmeyer.github.io/tts/2017/02/14/Installing-Merlin.html) for detailed instructions <br/> on how to install Merlin and build SLT demo voice.

For a more in-depth tutorial about building voices with Merlin, you can check out:

- [Deep Learning for Text-to-Speech Synthesis, using the Merlin toolkit (Interspeech 2017 tutorial)](http://www.speech.zone/courses/one-off/merlin-interspeech2017)
- [Arctic voices](https://cstr-edinburgh.github.io/merlin/getting-started/slt-arctic-voice)
- [Build your own voice](https://cstr-edinburgh.github.io/merlin/getting-started/build-own-voice)


Synthetic speech samples
------------------------

Listen to [synthetic speech samples](https://cstr-edinburgh.github.io/merlin/demo.html) from our SLT arctic voice.

Development pattern for contributors
------------------------------------

1. [Create a personal fork](https://help.github.com/articles/fork-a-repo/)
of the [main Merlin repository](https://github.com/CSTR-Edinburgh/merlin) in GitHub.
2. Make your changes in a named branch different from `master`, e.g. you create
a branch `my-new-feature`.
3. [Generate a pull request](https://help.github.com/articles/creating-a-pull-request/)
through the Web interface of GitHub.

Contact Us
----------

Post your questions, suggestions, and discussions to [GitHub Issues](https://github.com/CSTR-Edinburgh/merlin/issues).

Citation
--------

If you publish work based on Merlin, please cite: 

Zhizheng Wu, Oliver Watts, Simon King, "[Merlin: An Open Source Neural Network Speech Synthesis System](https://isca-speech.org/archive/SSW_2016/pdfs/ssw9_PS2-13_Wu.pdf)" in Proc. 9th ISCA Speech Synthesis Workshop (SSW9), September 2016, Sunnyvale, CA, USA.

numpy
scipy
scikit-learn
matplotlib
bandmat>=0.5
theano>=0.8
keras>=2.0.5
h5py>=2.7.0
tensorflow>=1.2.0
# Merlin Documentation

The source for Merlin documentation is in this directory under `templates/`. 
Our documentation uses extended Markdown, as implemented by [MkDocs](http://mkdocs.org) and is similar to Keras.

## Building the documentation

- install MkDocs: `pip install mkdocs`
- `cd` to the `docs/` folder and run:
    - `mkdocs serve`    # Starts a local webserver:  [localhost:8000](localhost:8000)
    - `mkdocs build`    # Builds a static site in "site" directory
# Merlin guided unit selection synthesis

coming soon...

## Merlin: The Neural Network (NN) based Speech Synthesis System

This repository contains the Neural Network (NN) based Speech Synthesis System  
developed at the Centre for Speech Technology Research (CSTR), University of 
Edinburgh. 

Merlin is a toolkit for building Deep Neural Network models for statistical parametric speech synthesis. 
It must be used in combination with a front-end text processor (e.g., Festival) and a vocoder (e.g., STRAIGHT or WORLD).

The system is written in Python and relies on the Theano numerical computation library.

Merlin comes with recipes (in the spirit of the [Kaldi](https://github.com/kaldi-asr/kaldi) automatic speech recognition toolkit) to show you how to build state-of-the art systems.

Merlin is free software, distributed under an Apache License Version 2.0, allowing unrestricted commercial and non-commercial use alike.

Read the documentation at [cstr-edinburgh.github.io/merlin](https://cstr-edinburgh.github.io/merlin/).

Merlin is compatible with: __Python 2.7__.

Installation
------------

Merlin uses the following dependencies:

- numpy, scipy
- matplotlib
- bandmat
- theano
- sklearn, keras (optional, required if you use keras models)

To install Merlin, `cd` merlin and run the below steps:

- Install some tools in Merlin
```sh
bash tools/compile_tools.sh
```
- Install python dependencies
```sh
pip install -r requirements.txt
```

For detailed instructions, to build the toolkit: see [`INSTALL`](https://github.com/CSTR-Edinburgh/merlin/blob/master/INSTALL.md).  
These instructions are valid for UNIX
systems including various flavors of Linux;


Getting started with Merlin
---------------------------

To run the example system builds, see `egs/README.txt`

As a first demo, please follow the scripts in `egs/slt_arctic`

Now, you can also follow Josh Meyer [blog post](http://jrmeyer.github.io/merlin/2017/02/14/Installing-Merlin.html) for detailed instructions <br/> on how to install Merlin and build SLT demo voice.

For a more in-depth tutorial about building voices with Merlin, you can check out:

- [Arctic voices](https://cstr-edinburgh.github.io/merlin/slt-arctic-voice)
- [Build your own voice](https://cstr-edinburgh.github.io/merlin/build-own-voice)


Synthetic speech samples
------------------------

Listen to [synthetic speech samples](https://cstr-edinburgh.github.io/merlin/demo.html) from our SLT arctic voice.

Development pattern for contributors
------------------------------------

1. [Create a personal fork](https://help.github.com/articles/fork-a-repo/)
of the [main Merlin repository](https://github.com/CSTR-Edinburgh/merlin) in GitHub.
2. Make your changes in a named branch different from `master`, e.g. you create
a branch `my-new-feature`.
3. [Generate a pull request](https://help.github.com/articles/creating-a-pull-request/)
through the Web interface of GitHub.

Contact Us
----------

Post your questions, suggestions, and discussions to [GitHub Issues](https://github.com/CSTR-Edinburgh/merlin/issues).

Citation
--------

If you publish work based on Merlin, please cite: 

Zhizheng Wu, Oliver Watts, Simon King, "[Merlin: An Open Source Neural Network Speech Synthesis System](http://ssw9.net/papers/ssw9_PS2-13_Wu.pdf)" in Proc. 9th ISCA Speech Synthesis Workshop (SSW9), September 2016, Sunnyvale, CA, USA.

# DNN-based speaker adaptation

coming soon...

# Parallel voice conversion

coming soon...

# Build your own voice

To build your own voice, `cd egs/build_your_own_voice/s1` and follow the below steps:

## Setting up

The first step is to run setup as it creates directories and some text files for testing.

The next steps demonstrate on how to setup voice. 

```sh
./01_setup.sh my_voice
```

It also creates a global config file: `conf/global_settings.cfg`, where default settings are stored.
You need to modify these params as per your own data.

## Prepare labels

To prepare labels
```sh
./02_prepare_labels.sh <path_to_wav_dir> <path_to_text_dir> <path_to_labels_dir>
```

## Prepare acoustic features
 
To prepare acoustic features
```sh
./03_prepare_acoustic_features.sh <path_to_wav_dir> <path_to_feat_dir>
```

## Prepare config files

At this point, we have to prepare two config files to train DNN models
- Acoustic Model
- Duration Model

To prepare config files:
```sh
./04_prepare_conf_files.sh conf/global_settings.cfg
```
Four config files will be generated: two for training, and two for testing. 

## Train duration model

To train duration model:
```sh
./05_train_duration_model.sh <path_to_duration_conf_file>
```

## Train acoustic model

To train acoustic model:
```sh
./06_train_acoustic_model.sh <path_to_acoustic_conf_file>
```
## Synthesize speech

To synthesize speech:
```sh
./07_run_merlin.sh <path_to_text_dir> <path_to_test_dur_conf_file> <path_to_test_synth_conf_file>
```

# Arctic voices

The CMU_ARCTIC databases were constructed at the Language Technologies Institute at 
Carnegie Mellon University as phonetically balanced, 
US English single speaker databases designed for unit selection speech synthesis research.

The databases consist of around 1150 utterances carefully selected from out-of-copyright texts from Project Gutenberg. 
The databses include US English male (bdl), female (slt) speakers (both experinced voice talent) and few other accented speakers.

To run one of these voices, `cd egs/slt_arctic/s1` and follow the below steps:

## Setting up

The first step is to run setup as it creates directories and downloads the required training data files.

To see the list of available voices, run:
```sh
./01_setup.sh
```
The next steps demonstrate on how to setup slt arctic voice. 

- To run on short data(about 50 utterances for training)
```sh
./01_setup.sh slt_arctic_demo
```
- To run on full data(about 1000 sentences for training)
```sh
./01_setup.sh slt_arctic_full
```

It also creates a global config file: `conf/global_settings.cfg`, where default settings are stored.
 
## Prepare config files

At this point, we have to prepare two config files to train DNN models
- Acoustic Model
- Duration Model

To prepare config files:
```sh
./02_prepare_conf_files.sh conf/global_settings.cfg
```
Four config files will be generated: two for training, and two for testing. 

## Train duration model

To train duration model:
```sh
./03_train_duration_model.sh <path_to_duration_conf_file>
```

## Train acoustic model

To train acoustic model:
```sh
./04_train_acoustic_model.sh <path_to_acoustic_conf_file>
```
## Synthesize speech

To synthesize speech:
```sh
./05_run_merlin.sh <path_to_test_dur_conf_file> <path_to_test_synth_conf_file>
```

coming soon...

coming soon...

coming soon...

This directory contains example scripts that demonstrate how to 
use Merlin.  Each subdirectory corresponds to a corpus that we have
example scripts for.

Build your own voice
--------------------

Each subdirectory of this directory contains the scripts for a sequence of experiments.

  s1: To run your_own_voice with WORLD vocoder and radio phoneset.


# Build your own voice


## Requirements

You need to have installed:
* [Merlin](https://github.com/CSTR-Edinburgh/merlin#installation)
* festival: ```bash tools/compile_other_speech_tools.sh```
* htk: ```bash tools/compile_htk.sh```


## Building Steps

To build your own voice, `cd egs/build_your_own_voice/s1` and follow the below steps:

### Setting up

The first step is to run setup as it creates directories and some text files for testing.

The next steps demonstrate on how to setup voice. 

```sh
./01_setup.sh my_voice
```

It also creates a global config file: `conf/global_settings.cfg`, where default settings are stored.
You need to modify these params as per your own data.

### Prepare labels

To prepare labels
```sh
./02_prepare_labels.sh <path_to_wav_dir> <path_to_text_dir> <path_to_labels_dir>
```

### Prepare acoustic features
 
To prepare acoustic features
```sh
./03_prepare_acoustic_features.sh <path_to_wav_dir> <path_to_feat_dir>
```

### Prepare config files

At this point, we have to prepare two config files to train DNN models
- Acoustic Model
- Duration Model

To prepare config files:
```sh
./04_prepare_conf_files.sh conf/global_settings.cfg
```
Four config files will be generated: two for training, and two for testing. 

### Train duration model

To train duration model:
```sh
./05_train_duration_model.sh <path_to_duration_conf_file>
```

### Train acoustic model

To train acoustic model:
```sh
./06_train_acoustic_model.sh <path_to_acoustic_conf_file>
```
### Synthesize speech

To synthesize speech:
```sh
./07_run_merlin.sh <path_to_text_dir> <path_to_test_dur_conf_file> <path_to_test_synth_conf_file>
```

About the Blizzard 2017 corpus
-------------------------------

This corpus is released as part of Blizzard 2017 challenge and was used in Merlin benchmark for Blizzard challenge workshop.
It consists of 7253 sentences -- train(5866), valid(134) and test(253).

1. This data is released under a license for non-commercial use only. 
2. Read and accept the [license](http://www.cstr.ed.ac.uk/projects/blizzard/2017/usborne_blizzard2017/license.html)

For more details about the corpus, please follow below link:
http://www.cstr.ed.ac.uk/projects/blizzard/2018/usborne_blizzard2018/

Each subdirectory of this directory contains the scripts for a sequence of experiments.

  s1: To run fls_blizzard2017 with WORLD vocoder and unilex phoneset
      


Download Merlin
---------------

```bash
git clone https://github.com/CSTR-Edinburgh/merlin.git 
```

Install tools
-------------

```bash
bash merlin/tools/compile_tools.sh
```

Merlin benchmark for Blizzard 2017
--------------

To run full voice, please follow below steps:

```bash
cd merlin/egs/fls_blizzard2017/s1
./run_merlin_benchmark.sh
```

Merlin benchmark for Blizzard 2017 made use of WORLD vocoder and Unilex phoneset, training on 5866 utterances. The training of the voice approximately takes 4 to 6 hours. 

Compare the results in log files to baseline results from [RESULTS.md](https://github.com/CSTR-Edinburgh/merlin/blob/master/egs/fls_blizzard2017/s1/RESULTS.md)

Generate new sentences
----------------------

To generate new sentences, please follow below steps:

```bash
./merlin_synthesis.sh
```

Nick Hurricane Corpus
=====================

Demo data: 
----------
herald_001 -- herald_060 (60 utterances) <br/>
Data distribution: Train: 50; Valid: 5; Test: 5;

Full data: 
----------
herald_001 -- hvd_719 (2542 utterances) <br/>
Data distribution: Train: 2400; Valid: 70; Test: 72;

RESULTS
=======

Baseline results from demo data
-------------------------------

Objective scores from duration model: <br/>

Valid -- RMSE: 6.221 frames/phoneme; CORR: 0.718; <br/>
Test  -- RMSE: 6.902 frames/phoneme; CORR: 0.678;

Objective scores from acoustic model: <br/> 

Valid -- MCD: 5.714 dB; BAP: 2.158 dB; F0:- RMSE: 11.197 Hz; CORR: 0.766; VUV: 6.202%
Test  -- MCD: 5.662 dB; BAP: 2.183 dB; F0:- RMSE: 13.595 Hz; CORR: 0.632; VUV: 7.175%


Baseline results from full data
-------------------------------

Objective scores from duration model: <br/>


Objective scores from acoustic model: <br/> 


# Hybrid speech synthesis

## ATTENTION: still in experimental phase...won't work straight-forward...requires lot of debugging!!

Hybrid speech synthesis is one of the main driving force behind most of the commercial systems that are present today. 

Festival offers a general framework for building speech synthesis systems as well as including examples of various modules. Multisyn is an open-source toolkit for building unit selection voice with any speech corpus. This post gives detailed instructions on how to use Multisyn to build an unit selection model and Festival for final waveform synthesis. 

# Hybrid speech synthesis

## ATTENTION: still in experimental phase...won't work straight-forward...requires lot of debugging!!

Hybrid speech synthesis is one of the main driving force behind most of the commercial systems that are present today. 

Festival offers a general framework for building speech synthesis systems as well as including examples of various modules. Multisyn is an open-source toolkit for building unit selection voice with any speech corpus. This post gives detailed instructions on how to use Multisyn to build an unit selection model and Festival for final waveform synthesis. 

## Tools required

1. [Speech tools](http://www.cstr.ed.ac.uk/projects/speech_tools)
2. [Festival](http://www.cstr.ed.ac.uk/projects/festival)
3. [Multisyn](http://www.cstr.ed.ac.uk/downloads/festival/multisyn_build)
4. [HTK](http://htk.eng.cam.ac.uk)

To build a new voice with Festival Multisyn, follow the step-by-step procedure given below:

## Step-by-step procedure

### Install tools

You might be familiar with most of these tools, but there are some differences in the way we setup these tools. 

- A version of [speech tools](http://www.cstr.ed.ac.uk/downloads/festival/2.4/speech_tools-2.4-with-wrappers.tar.gz) with python wrappers has to be installed in order to work with Multisyn.
- Latest version of [Festival](http://104.131.174.95/downloads/tools/festival-2.4-current.tar.gz) has to be installed in order to use hybrid unit selection. 

Therefore, we recommend installing a fresh copy of these tools following the [scripts provided in Merlin](https://github.com/CSTR-Edinburgh/merlin/blob/master/tools/compile_unit_selection_tools.sh). 

To install speech tools, Festival and Multisyn:

```bash
bash compile_unit_selection_tools.sh
```

To install HTK:

```bash
bash compile_htk.sh
```

Make sure you install all these tools without any errors and check environment variables before proceeding further. 

### Demo data

At this point, make sure you have data ready:

- a [directory containing audio files](http://festvox.org/cmu_arctic/cmu_arctic/cmu_us_slt_arctic/wav/) with file extension `.wav` 
- a [text file](http://festvox.org/cmu_arctic/cmu_arctic/cmu_us_slt_arctic/etc/txt.done.data) with transcriptions in the typical festival format.

For demo purpose, we use [SLT corpus from CMU Arctic Database](http://festvox.org/cmu_arctic/cmu_arctic/packed/cmu_us_slt_arctic-0.95-release.zip). 

### Setting up

The first step is to run setup as it creates directories and some text files for testing.

The next steps demonstrate on how to setup voice. 

```sh
./01_setup.sh conf/global_settings.cfg
```

You need to modify these params as per your own data.

### Build unit-selection model with Multisyn

Choose one of the lexicons:
1. cmulex
2. unilex-rpx
3. combilex-rpx

Choose gender:
1. 'm' for male
2. 'f' for female

If no arguments provided, the script uses default options: unilex and female assuming slt database

```sh
./02_build_unit_selection_model.sh
```

### Build parametric model with Merlin

```sh
./03_build_parametric_model.sh
```

### Build hybrid model

```sh
./04_build_hybrid_model.sh
```

### Synthesis with Festival

The below instructions are for Festival Multisyn voice:

```sh
$FESTDIR/bin/festival
```

Make festival speak "Hello world!" with new voice:

```sh
festival> (voice_cstr_edi_slt_multisyn)
festival> (SayText "Hello world!")
festival> (utt.save.wave (utt.synth (Utterance Text "Hello world!" )) "hello_world.wav")
```

For batch processing:

```sh
./05_run_hybrid_voice.sh <path_to_text_dir> <path_to_wav_dir>
```

For hybrid voice, please use the scm file in `scripts`.
Mandarin Voice
------------------------------------
This is a mandarin speech synthesis demo using only 250 wav from thchs30
dataset(A11 speaker)

A simple mandarin frontend is in https://github.com/jackiexiao/mtts 
Synthetic speech samples: Listen to https://jackiexiao.github.io/MTTS/
generated by different dataset

About THCHS30 Dataset 
------------------------------------
THCHS30 is an open Chinese speech database published by Center for Speech and Language Technology (CSLT) at Tsinghua University.

Actually this dataset is for researchers in the field of speech recognition,
but there is no any mandarin open-source dataset for speech synthesis, so we
use part of this dataset to demostrate mandarin tts.

Directory
------------------------------------
Each subdirectory of this directory contains the scripts for a sequence of experiments.
  s1: To run mandarin_voice with WORLD vocoder
# Mandarin Voice

## To run_demo
```
bash run_demo.sh
```

## To train with your own dataset

(1) Create the following dir and copy your file to dir (suppose current dir is merlin/egs/mandarin_voice/s1/)

* database/wav 
* database/labels/label_phone_align 
* database/prompt-lab 
* copy your own Question file to merlin/misc/questions

(2) modify params as per your own data in 01_setup.sh file, especially

* Voice Name
* QuestionFile
* Labels_Type(phone_align or state_align)
* SamplingFreq
* Train
* Valid
* Test

default setting is 

* QuestionFile=questions-mandarin.hed
* Labels=phone_align
* SamplingFreq=16000
* Train=200
* Valid=25
* Test=25

(3) then run

```
./run_mandarin_voice.sh
```
About the Nick Hurricane corpus
-------------------------------

This corpus is subset of Nick corpus and is used in Merlin demo paper from CSTR in SSW9.
It consists of 2542 sentences -- train(2400), valid(70) and test(72).

For more details about the corpus, please follow below link:
http://datashare.is.ed.ac.uk/handle/10283/347 

Each subdirectory of this directory contains the scripts for a sequence of experiments.

  s1: To run nick_hurricane with STRAIGHT vocoder and combilex phoneset
      The data is not publicly available as it includes features extracted from STRAIGHT and 
      labels using Combilex phoneset (both of them have restricted usage).

      Therefore, the scripts are meant for internal purpose and also for those who have license to both.
      For password, please mail me, only if you have license to both. 


Download Merlin
---------------

git clone https://github.com/CSTR-Edinburgh/merlin.git

Setup
-----

To setup demo voice: 

./01_setup.sh nick_hurricane_demo

    (or)   

To setup full voice:

./01_setup.sh nick_hurricane_full

Demo setup makes use of short amount of data (60 utterances) for training, validation and testing. <br/>
Full setup makes use of whole data (2542 utterances) for training, validation and testing. 

Run Merlin
----------

Once after setup, use below script to create acoustic, duration models and perform final test synthesis:

./02_run_merlin.sh

If demo setup is used, merlin trains only on 50 utterances and should not take more than 5 min. <br/>
Compare the results in log files to baseline results from demo data in [RESULTS.md](https://github.com/CSTR-Edinburgh/merlin/blob/master/egs/nick_hurricane/s1/RESULTS.md)

If full setup is used, merlin utilizes the whole cstr-hurricane data (2542 utterances). The training of the voice approximately takes 1 to 2 hours. <br/>
Compare the results in log files to baseline results from full data in [RESULTS.md](https://github.com/CSTR-Edinburgh/merlin/blob/master/egs/nick_hurricane/s1/RESULTS.md)

Generate new sentences
----------------------

To generate new sentences, please follow [steps] (https://github.com/CSTR-Edinburgh/merlin/issues/28) in below script:

./03_merlin_synthesis.sh

Nick Hurricane Corpus
=====================

Demo data: 
----------
herald_001 -- herald_060 (60 utterances) <br/>
Data distribution: Train: 50; Valid: 5; Test: 5;

Full data: 
----------
herald_001 -- hvd_719 (2542 utterances) <br/>
Data distribution: Train: 2400; Valid: 70; Test: 72;

RESULTS
=======

Baseline results from demo data
-------------------------------

Objective scores from duration model: <br/>

Valid -- RMSE: 6.221 frames/phoneme; CORR: 0.718; <br/>
Test  -- RMSE: 6.902 frames/phoneme; CORR: 0.678;

Objective scores from acoustic model: <br/> 

Valid -- MCD: 5.714 dB; BAP: 2.158 dB; F0:- RMSE: 11.197 Hz; CORR: 0.766; VUV: 6.202%
Test  -- MCD: 5.662 dB; BAP: 2.183 dB; F0:- RMSE: 13.595 Hz; CORR: 0.632; VUV: 7.175%


Baseline results from full data
-------------------------------

Objective scores from duration model: <br/>


Objective scores from acoustic model: <br/> 


About the SLT Arctic corpus

The CMU_ARCTIC databases were constructed at the Language Technologies Institute at Carnegie Mellon University as phonetically balanced, US English single speaker databases designed for unit selection speech synthesis research.

The databases consist of around 1150 utterances carefully selected from out-of-copyright texts from Project Gutenberg. The databses include US English male (bdl) and female (slt) speakers (both experienced voice talent) as well as other accented speakers.

Each subdirectory of this directory contains the
scripts for a sequence of experiments.

  s1: To run slt_arctic_demo with WORLD vocoder.
  s2: To run slt_arctic_demo with MagPhase vocoder (includes acoustic feature extraction).


Download Merlin
---------------

Step 1: git clone https://github.com/CSTR-Edinburgh/merlin.git 

Install tools
-------------

Step 2: cd merlin/tools <br/>
Step 3: ./compile_tools.sh

Demo voice
----------

To run demo voice, please follow below steps:
 
Step 4: cd merlin/egs/slt_arctic/s1 <br/>
Step 5: ./run_demo.sh

Demo voice trains only on 50 utterances and shouldn't take more than 5 min. 

Compare the results in log files to baseline results from demo data in [RESULTS.md](https://github.com/CSTR-Edinburgh/merlin/blob/master/egs/slt_arctic/s1/RESULTS.md)

Full voice
----------

To run full voice, please follow below steps:

Step 6: cd merlin/egs/slt_arctic/s1 <br/>
Step 7: ./run_full_voice.sh

Full voice utilizes the whole arctic data (1132 utterances). The training of the voice approximately takes 1 to 2 hours. 

Compare the results in log files to baseline results from full data in [RESULTS.md](https://github.com/CSTR-Edinburgh/merlin/blob/master/egs/slt_arctic/s1/RESULTS.md)

Generate new sentences
----------------------

To generate new sentences, please follow below steps:

Step 8: Run either demo voice or full voice. <br/>
Step 9: ./merlin_synthesis.sh

SLT Arctic Corpus
=================

Demo data: 
----------
arctic_a0001 -- arctic_a0060 (60 utterances) <br/>
Data distribution: Train: 50; Valid: 5; Test: 5;

Full data: 
----------
arctic_a0001 -- arctic_b0539 (1132 utterances) <br/>
Data distribution: Train: 1000; Valid: 66; Test: 66;

RESULTS
=======

Baseline results from demo data
-------------------------------

Objective scores from duration model: <br/>

Labels: state_align; Network: [416->5] LR 0.002 [4 TANH] [4*512]; <br/>
Valid -- RMSE: 6.826 frames/phoneme; CORR: 0.624; <br/>
Test  -- RMSE: 7.840 frames/phoneme; CORR: 0.562;

Labels: phone_align; Network: [416->1] LR 0.002 [4 TANH] [4*512]; <br/>
Valid -- RMSE: 6.777 frames/phoneme; CORR: 0.633; <br/> 
Test  -- RMSE: 7.665 frames/phoneme; CORR: 0.593;

Objective scores from acoustic model: <br/> 

Labels: state_align; Network: [425->187], LR 0.002 [4 TANH] [4*512]; <br/>
Valid -- MCD: 6.559 dB; BAP: 0.242 dB; F0:- RMSE: 19.573 Hz; CORR: 0.529; VUV: 11.655%  <br/>
Test  -- MCD: 6.586 dB; BAP: 0.259 dB; F0:- RMSE: 15.309 Hz; CORR: 0.701; VUV: 8.821%

Labels: phone_align; Network: [420->187], LR 0.002 [4 TANH] [4*512]; <br/>
Valid -- MCD: 6.762 dB; BAP: 0.246 dB; F0:- RMSE: 19.433 Hz; CORR: 0.538; VUV: 11.403% <br/>
Test  -- MCD: 6.704 dB; BAP: 0.262 dB; F0:- RMSE: 15.264 Hz; CORR: 0.700; VUV: 8.907%


Baseline results from full data
-------------------------------

Objective scores from duration model: <br/>

Labels: state_align; Network: [416->5] LR 0.002 [4 TANH] [4*512]; <br/>
Valid -- RMSE: 6.810 frames/phoneme; CORR: 0.758; <br/>
Test  -- RMSE: 6.330 frames/phoneme; CORR: 0.773;

Labels: state_align; Network: [416->5] LR 0.002 [6 TANH] [6*1024]; <br/>
Valid -- RMSE: 6.564 frames/phoneme; CORR: 0.779; <br/>
Test  -- RMSE: 6.148 frames/phoneme; CORR: 0.788;
 
Labels: phone_align; Network: [416->1] LR 0.002 [4 TANH] [4*512]; <br/>
Valid -- RMSE: 6.953 frames/phoneme; CORR: 0.746; <br/> 
Test  -- RMSE: 6.585 frames/phoneme; CORR: 0.752;

Objective scores from acoustic model: <br/> 

Labels: state_align; Network: [425->187], LR 0.002 [4 TANH] [4*512]; <br/>
Valid -- MCD: 5.067 dB; BAP: 0.228 dB; F0:- RMSE: 11.186 Hz; CORR: 0.761; VUV: 6.312%  <br/>
Test  -- MCD: 5.070 dB; BAP: 0.238 dB; F0:- RMSE: 12.051 Hz; CORR: 0.752; VUV: 5.732%

Labels: state_align; Network: [425->187], LR 0.002 [6 TANH] [6*1024]; <br/>
Valid -- MCD: 4.912 dB; BAP: 0.225 dB; F0:- RMSE: 11.140 Hz; CORR: 0.763; VUV: 6.007% <br/>
Test  -- MCD: 4.912 dB; BAP: 0.232 dB; F0:- RMSE: 11.903 Hz; CORR: 0.759; VUV: 5.348%

Labels: phone_align; Network: [420->187], LR 0.002 [4 TANH] [4*512]; <br/>
Valid -- MCD: 5.255 dB; BAP: 0.235 dB; F0:- RMSE: 11.295 Hz; CORR: 0.754; VUV: 6.822% <br/>
Test  -- MCD: 5.247 dB; BAP: 0.244 dB; F0:- RMSE: 12.003 Hz; CORR: 0.757; VUV: 6.111%

# SLT Arctic TTS Demo using MagPhase Vocoder

## Overview

It is a Text-To-Speech demo using the [new release of the MagPhase vocoder (v2.0)](https://github.com/CSTR-Edinburgh/magphase), which now also supports:

* Constant frame-rate.
* Improved sound quality.
* Two types of post-filter available.
* Selectable number of coefficients for phase features (*real* and *imag*).
* Selectable number of coefficients for the magnitude feature (*mag*).


As a difference with other demos, it also includes **acoustic feature extraction**, thus the recipe works using as input data:
* label state aligned files (.lab).
* wav files (.wav).

Both are downloaded automatically during running the demo script.

## Run Demo Voice

Assuming Merlin is installed, just run:
```
cd merlin/egs/slt_arctic/s2/
python run_demo.py
```
Basically, ```run_demo.py``` script will:

1. Download the input data for you (.lab, .wav).
2. Create the experiment directory in ```./experiments```.
3. Perform acoustic feature extraction with MagPhase vocoder.
4. Build and train duration and acoustic models using Merlin.
5.  Synthesise waveforms using predicted durations. The synthesised waveforms will be stored in: ```/<experiment_dir>/test_synthesis/gen_acous_wav_pf_<postfilter_type>```

## Changing Parameters
Alternatively, you can also experiment by changing the input parameters (See section "INPUT" in *run_demo.py*):

* **exper_type:** Type of experiment. "demo" (50 training utts) or "full" (1k training utts.)

**Steps:**
* **b_download_data:** Download wavs and label data.
* **b_setup_data:** Copy downloaded data into the experiment directory. Plus, make a backup copy of *run_demo.py* script inside the experiment directory.
* **b_config_merlin:** Save new configuration files for Merlin.
* **b_feat_extr:** Perform acoustic feature extraction using the MagPhase vocoder.
* **b_conv_labs_rate:** Convert the state aligned labels to variable rate if running in variable frame rate mode (*b_const_rate=False*).
* **b_dur_train:** Merlin: Training of duration model.
* **b_acous_train:** Merlin: Training of acoustic model.
* **b_dur_syn:** Merlin: Generation of state durations using the duration model.
* **b_acous_syn:** Merlin: Waveform generation for the utterances provided in ```./test_synthesis/prompt-lab```


**MagPhase Vocoder:**

* **mag_dim:** Number of coefficients for magnitude feature (*mag*).
* **phase_dim:** Number of coefficients for phase features (*real* and *imag*).
* **b_const_rate:** To work in constant frame rate mode.
* **l_pf_type:** List containing the postfilters to apply during waveform generation.

* **b_feat_ext_multiproc:** Acoustic feature extraction done in multiprocessing mode (faster).# Speaker adaptation

This directory contains required files to build a speaker adaptation system. The VCTK-Corpus was used for the experiments.
1) Build an average voice model (AVM) over multiple speakers
2) Build a stand alone model of adapt speaker
3) Adapt the AVM for the adapt speaker 

## Download the data
The data is available for free to download. It comprises of a total 108 speakers of which 47 are male speakers. To download and extract the data:

```sh
./download_data.sh
```
Note: the size of tar file is around 11GB and afer extraction it will be around 16GB. Thus you need atleast 27GB of free space.

# Build an average voice model (AVM)
For Demo purpose we are building the AVM over 9 speakers (3 male and 6 female).

## Setting up

The first step is to run setup as it creates directories and some text files for testing.

The next steps demonstrate on how to setup voice. 

```sh
./01_setup.sh vctk_avm
```

It also creates a global config file: `conf/global_settings.cfg`, where default settings are stored. You need to modify these params as per the data.

## Prepare labels

To prepare labels
```sh
./02_prepare_labels.sh <path_to_wav_dir> <path_to_text_dir> <path_to_labels_dir>
```

## Prepare acoustic features
 
To prepare acoustic features
```sh
./03_prepare_acoustic_features.sh <path_to_wav_dir> <path_to_feat_dir>
```

## Prepare config files

At this point, we have to prepare two config files to train DNN models
- Acoustic Model
- Duration Model

To prepare config files:
```sh
./04_prepare_conf_files.sh conf/global_settings.cfg
```
Four config files will be generated: two for training, and two for testing. 

## Train duration model

To train duration model:
```sh
./05_train_duration_model.sh <path_to_duration_conf_file>
```

## Train acoustic model

To train acoustic model:
```sh
./06_train_acoustic_model.sh <path_to_acoustic_conf_file>
```
## Synthesize speech

To synthesize speech:
```sh
./07_run_merlin.sh <path_to_text_dir> <path_to_test_dur_conf_file> <path_to_test_synth_conf_file>
```

# Build voice of speaker p234 without adaptation
Just follow the above steps to creat the voice of p234


# Adapt the speaker `p234` on the AVM

## Setting up
To setup the data and directories

```sh
./08_setup_adapt.sh <voice_name> <average_duration_model> <average_acoustic_model> <adaptation_method>
```

It creates a global config file: `conf/global_settings_adapt.cfg`, where default settings are stored. You need to modify these params as per the data.

## Prepare labels

To prepare labels
```sh
./09_prepare_labels_adapt.sh <path_to_wav_dir> <path_to_text_dir> <path_to_labels_dir>
```

## Prepare acoustic features
 
To prepare acoustic features
```sh
./10_prepare_acoustic_features.sh <path_to_wav_dir> <path_to_feat_dir>
```

## Prepare config files

At this point, we have to prepare two config files to adapt DNN models
- Acoustic Model
- Duration Model

To prepare config files:
```sh
./11_prepare_conf_files.sh conf/global_settings.cfg
```
Four config files will be generated: two for training, and two for testing. 

## Train duration model

To train duration model:
```sh
./12_adapt_duration_model.sh <path_to_duration_conf_file>
```

## Train acoustic model

To train acoustic model:
```sh
./13_adapt_acoustic_model.sh <path_to_acoustic_conf_file>
```
## Synthesize speech
VCTK corpus
=================

Average vocie model (AVM) data (demo): 
--------------------------------
Speakers: 6 female + 3 male, from `p225` to `p233` (3358 utterances) <br/>
Data distribution: Train: 3258; Valid: 50; Test: 50;

Speaker p235 voice model
-------------------------
Speaker: Female (357 utterances) <br/>
Data distribution: Train: 307; Valid: 25; Test: 25;

RESULTS
=======

Baseline results of AVM from demo data
-------------------------------
Duration model tains in about 20 minutes on GPU (Nvidia Titan)

Objective scores from duration model: <br/>

Labels: state_align; Network: [416->5] LR 0.002 [6 TANH] [6*1024]; <br/>
Develop -- RMSE: 5.751 frames/phoneme; CORR: 0.791; <br/>
Test  -- RMSE: 5.502 frames/phoneme; CORR: 0.808;

Acoustic model trains in about 1 hour 30 minutes on GPU (Nvidia Titan)

Objective scores from acoustic model: <br/> 

Labels: state_align; Network: [425->199], LR 0.002 [4 TANH] [4*512]; <br/>
Develop -- MCD: 6.145 dB; BAP: 0.345 dB; F0:- RMSE: 47.039 Hz; CORR: 0.678; VUV: 8.771%  <br/>
Test  -- MCD: 6.097 dB; BAP: 0.329 dB; F0:- RMSE: 46.723 Hz; CORR: 0.627; VUV: 8.474%


Baseline results of speaker `p234` from full data
-------------------------------------------------
Duration model trains in about 5 minutes
Objective scores from duration model: <br/>

Labels: state_align; Network: [416->5] LR 0.002 [6 TANH] [6*1024]; <br/>
Develop -- RMSE: 6.145 frames/phoneme; CORR: 0.664; <br/>
Test  -- RMSE: 7.604 frames/phoneme; CORR: 0.687;

Acoustic model trains in about 10 minutes
Objective scores from acoustic model: <br/> 

Labels: state_align; Network: [425->199], LR 0.002 [6 TANH] [6*1024]; <br/>
Develop: DNN -- MCD: 5.476 dB; BAP: 0.401 dB; F0:- RMSE: 15.202 Hz; CORR: 0.537; VUV: 13.190%
Test   : DNN -- MCD: 5.441 dB; BAP: 0.407 dB; F0:- RMSE: 17.097 Hz; CORR: 0.542; VUV: 16.853%


Adapt speaker `p234` on the average voice model
-----------------------------------------------
Here we used the fine-tune method for the adaptation.

Duration model adapts in about 5 minutes
Objective scores from duration model: <br/>

Labels: state_align; Network: [416->5] LR 0.002 [6 TANH] [6*1024]; <br/>
Develop -- RMSE: 5.214 frames/phoneme; CORR: 0.797; <br/>
Test  -- RMSE: 5.483 frames/phoneme; CORR: 0.851;

Acoustic model adapts in about 10 minutes
Objective scores from acoustic model: <br/> 

Labels: state_align; Network: [425->199], LR 0.002 [6 TANH] [6*1024]; <br/>
Develop: DNN -- MCD: 5.240 dB; BAP: 0.383 dB; F0:- RMSE: 15.034 Hz; CORR: 0.524; VUV: 12.280%  <br/>
Test   : DNN -- MCD: 5.213 dB; BAP: 0.389 dB; F0:- RMSE: 18.190 Hz; CORR: 0.473; VUV: 16.459%
# About voice conversion

Voice conversion aims at transforming the characteristics of a speech
signal uttered by a source speaker in such a way that the transformed
speech sounds like the target speaker. Such a conversion requires 
transformation of spectral and prosody features. 

Most of the current voice conversion techniques need a parallel
database where the source and target speakers record the
same set of utterances. 

CMU ARCTIC databases consists of 7 speakers with each speaker recording 
a set of 1132 phonetically balanced utterances. Therefore, it is an ideal 
choice to perform voice conversion experiments. 

Each subdirectory of this directory contains the
scripts for a sequence of experiments.

- s1: To run voice conversion with either world or straight vocoder.
- s2: To run voice conversion with MagPhase vocoder.

## Demo data

You can download the data from [here](http://104.131.174.95/downloads/voice_conversion/).
- Source: bdl (300 utterances)
- Target: slt (300 utterances)

## About the Arctic corpus

The CMU ARCTIC databases were constructed at the Language Technologies Institute at Carnegie Mellon University as phonetically balanced, US English single speaker databases designed for unit selection speech synthesis research.

The databases consist of around 1150 utterances carefully selected from out-of-copyright texts from Project Gutenberg. The databses include US English male (bdl) and female (slt) speakers (both experinced voice talent) as well as other accented speakers.

For more details, please visit [cmu arctic page](http://www.festvox.org/cmu_arctic/).


# Voice Conversion

To manipulate source speaker's voice to sound like target without changing language content. 

## Install Merlin

Before proceeding any further, you first [install Merlin](https://github.com/CSTR-Edinburgh/merlin#installation) and then run the below steps.

## Dependency tools

Along with Merlin, we need to install few other speech tools in order to run voice conversion. 
- [speech tools](http://www.cstr.ed.ac.uk/downloads/festival/2.4/speech_tools-2.4-release.tar.gz)
- [festvox](http://festvox.org/festvox-2.7/festvox-2.7.0-release.tar.gz)

```sh
bash merlin/tools/compile_other_speech_tools.sh 
```

All these tools are required for only one task i.e., dynamic time warping (DTW) to create parallel data. 
You can check this [tutorial](http://speech.zone/exercises/dtw-in-python) for DTW implementation.

As an alternative, [fastdtw](https://github.com/CSTR-Edinburgh/merlin/blob/master/misc/scripts/voice_conversion/dtw_aligner.py) from python bindings can also be used.  
Please check [step 3](https://github.com/CSTR-Edinburgh/merlin/blob/master/egs/voice_conversion/s1/README.md#align-source-features-with-target) for its usage.
 
To convert source voice to target voice, `cd egs/voice_conversion/s1` and follow the below steps:

## Voice conversion challenge 2016 data

Now, you can run Merlin voice conversion using VC2016 data. 

To download the data:
```sh
./scripts/download_vcc2016_data.sh
```

To run voice conversion between any source-target pair, give the speaker names as arguments:
```sh
./run_vcc2016_benchmark.sh [SOURCE_SPEAKER] [TARGET_SPEAKER]
```

## Demo data

You can download the data from [here](http://104.131.174.95/downloads/voice_conversion/).
- Source: bdl (300 utterances)
- Target: slt (300 utterances)

To run voice conversion on demo data
```sh
./run_demo_vc.sh
```

However, we recommend using step-by-step procedure to correct any errors if raised. 

## Setting up

The first step is to run setup as it creates directories.

The next steps demonstrate on how to setup voice. 

```sh
./01_setup.sh speakerA speakerB
```

It also creates a global config file: `conf/global_settings.cfg`, where default settings are stored.
You need to modify these params as per your own data.

## Prepare acoustic features

To prepare acoustic features
```sh
./02_prepare_acoustic_features.sh <path_to_wav_dir> <path_to_feat_dir>
```

You have to run this script twice, for speakerA and speakerB

## Align source features with target

For voice conversion, we require parallel sentences for training. For this, we use dynamic-time-warping from Festvox 
to align source features with target. 

To align source features with target
```sh
./03_align_src_with_target.sh <path_to_src_feat_dir> <path_to_tgt_feat_dir> <path_to_src_align_dir>
```

Alternatively, [fastdtw](https://github.com/CSTR-Edinburgh/merlin/blob/master/misc/scripts/voice_conversion/dtw_aligner.py) from python bindings can also be used.  

```bash
pip install fastdtw
```

To use fastdtw, replace `dtw_aligner_festvox.py` with `dtw_aligner.py` at line number 60 in `03_align_src_with_target.sh`.

## Prepare config files

At this point, we have to prepare two config files for voice conversion
- Source acoustic model
- Source2Target acoustic Model

To prepare config files:
```sh
./04_prepare_conf_files.sh conf/global_settings.cfg
```
Four config files will be generated: two for training, and two for testing. 

## Prepare source acoustic features 

To prepare acoustic features for SpeakerA
```sh
./05_train_acoustic_model.sh <path_to_acoustic_source_conf_file>
```

## Create a symbolic link for source acoustic features 

At this point, we have to create a symbolic link for source features in the main voice directory.

To prepare symbolic link for source features
```sh
./scripts/create_symbolic_link.sh
```

Input dimension for source features is computed based on sampling rate provided in global conf file. 

## Train acoustic model for voice conversion

To train acoustic model:
```sh
./05_train_acoustic_model.sh <path_to_acoustic_voice_conf_file>
```

## Voice conversion from source to target

To transform voice from speakerA to speakerB:
```sh
./06_run_merlin_vc.sh <path_to_src_wav_dir> <path_to_test_source_conf_file> <path_to_test_synth_conf_file>
```

# Voice Conversion Using MagPhase Vocoder

To manipulate source speaker's voice to sound like target without changing language content. 

MagPhase vocoder for VC has the advantage that in addition to using magnitude spectra derived features, it makes possible to map **phase features** from one speaker to another.

## Install Merlin

Before proceeding any further, you first [install Merlin](https://github.com/CSTR-Edinburgh/merlin#installation) and then run the below steps.

## Dependency tools

Along with Merlin, we need to install few other speech tools in order to run voice conversion. 
- [speech tools](http://www.cstr.ed.ac.uk/downloads/festival/2.4/speech_tools-2.4-release.tar.gz)
- [festvox](http://festvox.org/festvox-2.7/festvox-2.7.0-release.tar.gz)

```sh
bash merlin/tools/compile_other_speech_tools.sh 
```

All these tools are required for only one task i.e., dynamic time warping (DTW) to create parallel data. 
You can check this [tutorial](http://speech.zone/exercises/dtw-in-python) for DTW implementation.

As an alternative, [fastdtw](https://github.com/CSTR-Edinburgh/merlin/blob/master/misc/scripts/voice_conversion/dtw_aligner.py) from python bindings can also be used.  
Please check [step 3](https://github.com/CSTR-Edinburgh/merlin/blob/master/egs/voice_conversion/s1/README.md#align-source-features-with-target) for its usage.
 
To convert source voice to target voice, `cd egs/voice_conversion/s2` and follow the below steps:

## Run with demo data

To do voice conversion on demo data, you can simply run:
```sh
./run_demo_vc.sh
```
Which will download the [data](http://104.131.174.95/downloads/voice_conversion/) and run the whole voice conversion recipe for you. The data consists of:

* Source: bdl (300 utterances)
* Target: slt (300 utterances)

However, we recommend using step-by-step procedure to correct any errors if raised:

### I. Setting up

The first step is to run setup as it creates directories.

The next steps demonstrate on how to setup voice. 

```sh
./01_setup.sh speakerA speakerB
```

It also creates a global config file: `conf/global_settings.cfg`, where default settings are stored.
You need to modify these params as per your own data.

### II. Prepare acoustic features

To prepare acoustic features
```sh
./02_prepare_acoustic_features.sh <path_to_wav_dir> <path_to_feat_dir>
```

You have to run this script twice, for speakerA and speakerB

### III. Align source features with target

For voice conversion, we require parallel sentences for training. For this, we use dynamic-time-warping from Festvox 
to align source features with target. 

To align source features with target
```sh
./03_align_src_with_target.sh <path_to_src_feat_dir> <path_to_tgt_feat_dir> <path_to_src_align_dir>
```

Alternatively, [fastdtw](https://github.com/CSTR-Edinburgh/merlin/blob/master/misc/scripts/voice_conversion/dtw_aligner.py) from python bindings can also be used.  

```bash
pip install fastdtw
```

To use fastdtw, replace `dtw_aligner_festvox.py` with `dtw_aligner.py` at line number 60 in `03_align_src_with_target.sh`.

### IV. Prepare config files

At this point, we have to prepare two config files for voice conversion
- Source acoustic model
- Source2Target acoustic Model

To prepare config files:
```sh
./04_prepare_conf_files.sh conf/global_settings.cfg
```
Four config files will be generated: two for training, and two for testing. 

### V. Prepare source acoustic features

To prepare acoustic features for SpeakerA
```sh
./05_train_acoustic_model.sh <path_to_acoustic_source_conf_file>
```

### VI. Create a symbolic link for source acoustic features

At this point, we have to create a symbolic link for source features in the main voice directory.

To prepare symbolic link for source features
```sh
./scripts/create_symbolic_link.sh
```

Input dimension for source features is computed based on sampling rate provided in global conf file. 

### VII. Train acoustic model for voice conversion

To train acoustic model:
```sh
./05_train_acoustic_model.sh <path_to_acoustic_voice_conf_file>
```

### VIII. Voice conversion from source to target

To transform voice from speakerA to speakerB:
```sh
./06_run_merlin_vc.sh <path_to_src_wav_dir> <path_to_test_source_conf_file> <path_to_test_synth_conf_file>
```

## TODO

* Constant frame rate support.
* Selectable phase and magnitude dimensions.
* Selectable postfilter.

phone (0-348) (438-440)
syl   (348-405) (440-461)
word  (405-438) (461-481)
phone (0-442) (557-559)
syl   (442-524) (559-580)
word  (524-557) (580-600)
forced alignment
----------------

state_align -- create training labels for merlin with HTK tools

phone_align -- create training labels for merlin with festvox tools

To train a new voice with Merlin, either state_align or phone_align labels are required, but not both. So, chose one as per your convenience and set the option accordingly in global_settings.cfg file.
Create training labels for merlin with festvox tools
----------------------------------------------------

Step 0: Install [speech_tools] (http://festvox.org/packed/festival/2.4/speech_tools-2.4-release.tar.gz), [festival] (http://festvox.org/packed/festival/2.4/festival-2.4-release.tar.gz), 
and [festvox] (http://festvox.org/download.html).

Step 1: Run setup.sh -- to download slt data and to create the config file. <br/>
./setup.sh

Step 2: Please configure paths to all tools in config.cfg

Step 3: Run aligner.sh -- which uses ehmm in clustergen setup (from festvox tools) to do forced-alignment. <br/>
./run_aligner.sh config.cfg

If all the above steps performed successfully, you should have your labels ready !! :)

EHMM Aligner
------------

The steps to perform ehmm alignment are adapted from: <br/>
[http://festvox.org/bsv/c3174.html] (http://festvox.org/bsv/c3174.html)


Create training labels for merlin with HTK tools
------------------------------------------------

Step 0: Install [festival](http://festvox.org/packed/festival/2.4/festival-2.4-release.tar.gz), 
and [HTK](http://htk.eng.cam.ac.uk/download.shtml).

Step 1: Run setup.sh -- to download slt data and to create the config file. <br/>
./setup.sh

Step 2: Please configure paths to all tools in config.cfg

Step 3: Run aligner.sh -- which uses HVite (from HTK tools) to do forced-alignment. <br/>
./run_aligner.sh config.cfg

If all the above steps performed successfully, you should have your labels ready !! :)

HTK Aligner
-----------

The steps to perform HTK alignment are adapted from HTS source code: <br/>
[HMM-based Speech Synthesis System (HTS)](http://hts.sp.nitech.ac.jp/)


################################################
#####  Make HTS labels from Festival utts  #####
################################################

FESTDIR="/afs/inf.ed.ac.uk/group/cstr/projects/phd/s1432486/work/test/merlin/tools/festival"

./make_labels ./test/labels/ ./test/utts  $FESTDIR/examples/dumpfeats .


### the args are:
# ./test/labels/     ## Put the newly made labels here.
# ./test/utts      ## Look for existing utterances here.
# dumpfeats         ## This needs to point to Festival's dumpfeats script, usually in .festival/examples
#  .                # Following scirpts must be here: extra_feats.scm label.feats label-full.awk  label-mono.awk
 
#### Change label-full.awk according to the separators you need in output labels. 
Vocoders
--------

a) STRAIGHT - extracts 60-dim MGC, 25-dim BAP, 1-dim LF0 

b) WORLD - extracts 60-dim MGC, variable-dim BAP, 1-dim LF0 <br/>
(BAP dim: 1 for 16Khz, 5 for 48Khz) 

c) MAGPHASE - extracts 60-dim mag, 45-dim real, 45-dim imag, 1-dim LF0 <br/>
(Dimensions of mag, real and imag can be fine-tuned)

d) WORLD_v2 - extracts 60-dim MGC, 5-dim BAP, 1-dim LF0 <br/>
(dimensions of MGC and BAP can be fine-tuned)

We recommend using either STRAIGHT or WORLD. 

WORLD_v2 and MAGPHASE are still under development and require more testing. 
# Build your own voice using the MagPhase vocoder


## Requirements

You need to have installed:
* [Merlin](https://github.com/CSTR-Edinburgh/merlin#installation)
* festival: ```bash tools/compile_other_speech_tools.sh```
* htk: ```bash tools/compile_htk.sh```

## Building Steps

### Generating state-aligned label files
1. Run the recipe **egs/build_your_own_voice/s1** until the step **02_prepare_labels.sh**

    As a result, you will have state aligned label files for your data.

### Acoustic Modelling and Synthesis

2. Edit the script **egs/build_your_own_voice/s2/build_voice.py** according to your data.

3. Run the script:
    ```
    python build_voice.py
    ```

INSTALL
=======

### To install basic tools

Merlin by default requires installation of some basic tools
e.g., SPTK, WORLD

```sh
bash tools/compile_tools.sh
```

### To install other speech tools

When building a new voice, Merlin requires few other tools in order to build labels:
e.g., speech tools, festival and festvox

```sh
bash tools/compile_other_speech_tools.sh
```

If you want to build state align labels, Merlin requires installation of HTK

```sh
bash tools/compile_htk.sh
```
/* ----------------------------------------------------------------- */
/*           WORLD: High-quality speech analysis,                    */
/*           modification and synthesis system                       */
/*           developed by M. Morise                                  */
/*           http://ml.cs.yamanashi.ac.jp/world/                     */
/* ----------------------------------------------------------------- */
/*                                                                   */
/*  Copyright (c) 2010-2016  M. Morise                               */
/*                                                                   */
/* All rights reserved.                                              */
/*                                                                   */
/* Redistribution and use in source and binary forms, with or        */
/* without modification, are permitted provided that the following   */
/* conditions are met:                                               */
/*                                                                   */
/* - Redistributions of source code must retain the above copyright  */
/*   notice, this list of conditions and the following disclaimer.   */
/* - Redistributions in binary form must reproduce the above         */
/*   copyright notice, this list of conditions and the following     */
/*   disclaimer in the documentation and/or other materials provided */
/*   with the distribution.                                          */
/* - Neither the name of the M. Morise nor the names of its          */
/*   contributors may be used to endorse or promote products derived */
/*   from this software without specific prior written permission.   */
/*                                                                   */
/* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND            */
/* CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES,       */
/* INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF          */
/* MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE          */
/* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS */
/* BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,          */
/* EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED   */
/* TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,     */
/* DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON */
/* ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,   */
/* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY    */
/* OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE           */
/* POSSIBILITY OF SUCH DAMAGE.                                       */
/* ----------------------------------------------------------------- */
# World - a high-quality speech analysis, manipulation and synthesis system

WORLD is free software for high-quality speech analysis, manipulation and synthesis.
It can estimate Fundamental frequency (F0), aperiodicity and spectral envelope
and also generate the speech like input speech with only estimated parameters.

This source code is released under the modified-BSD license.
There is no patent in all algorithms in WORLD.
OS 	: Windows 7 64 bit
CPU 	: Core i7-3540M 3.0 GHz
RAM 	: 16 GB

Software
Microsoft Visual Studio 2015WORLD - a high-quality speech analysis, manipulation and synthesis system

WORLD is free software for high-quality speech analysis, manipulation and synthesis.
It can estimate Fundamental frequency (F0), aperiodicity and spectral envelope
and also generate the speech like input speech with only estimated parameters.

2. Usage
Please see test.cpp.

(1) F0 estimation by Dio()
(1-1) F0 is refined by StoneMask() if you need more accurate result.
(2) Spectral envelope estimation by CheapTrick()
(3) Aperiodicity estimation by D4C().
(4) You can manipulation these parameters in this phase. 
(5) Voice synthesis by Synthesis().

English document is written by a Japanese poor editor.
I willingly accept your kind indication on my English text.

3. License
WORLD is free software, and you can redistribute it and 
modify it under the terms of the modified BSD License.
Please see copying.txt for more information.
You can use this program for business, while I hope that 
you contact me after you developed software with WORLD.
This information is crucial to obtain a grant to develop WORLD.

4. Contacts
WORLD was written by Masanori Morise.
You can contact him via e-mail (mmorise [at] yamanashi.ac.jp)
or Twitter: @m_morise͍ WORLD

gɂẮCtest.cppQlɂĂD

WORLD͏CBSDCZXłD

--------------------------------------------------------------------
{IȍlF
́CƉFɂ\łƂ܂D
ŐVłł́CF0CwWCXyNgŕ\܂D
́CVocoderƓ̍lłD

(1) Dio() ɂ艹̊{g𐄒
(1-1) KvɉStoneMask() ɂ{g␳
(2) CheapTrick() ɂ艹̃XyNg𐄒
(3) D4C() ɂ艹̔wW𐄒
(4) Kvɉĉ≹F̐
(5) Synthesis() ɂ艹

--------------------------------------------------------------------
肢F
WORLD𗘗pꍇ́Cp/񏤗pɊւ炸X
񒸂΍Kł (͋`ł͂܂)D
̏́ClXȌlۂɖ𗧂܂D
̌́CWORLD𔭓W邽߂ɗp܂D

҂֘A
mmorise [at] yamanashi.ac.jp
Twitter: @m_morise /* ----------------------------------------------------------------- */
/*           WORLD: High-quality speech analysis,                    */
/*           modification and synthesis system                       */
/*           developed by M. Morise                                  */
/*           http://ml.cs.yamanashi.ac.jp/world/                     */
/* ----------------------------------------------------------------- */
/*                                                                   */
/*  Copyright (c) 2010-2016  M. Morise                               */
/*                                                                   */
/* All rights reserved.                                              */
/*                                                                   */
/* Redistribution and use in source and binary forms, with or        */
/* without modification, are permitted provided that the following   */
/* conditions are met:                                               */
/*                                                                   */
/* - Redistributions of source code must retain the above copyright  */
/*   notice, this list of conditions and the following disclaimer.   */
/* - Redistributions in binary form must reproduce the above         */
/*   copyright notice, this list of conditions and the following     */
/*   disclaimer in the documentation and/or other materials provided */
/*   with the distribution.                                          */
/* - Neither the name of the M. Morise nor the names of its          */
/*   contributors may be used to endorse or promote products derived */
/*   from this software without specific prior written permission.   */
/*                                                                   */
/* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND            */
/* CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES,       */
/* INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF          */
/* MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE          */
/* DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS */
/* BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,          */
/* EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED   */
/* TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,     */
/* DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON */
/* ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,   */
/* OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY    */
/* OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE           */
/* POSSIBILITY OF SUCH DAMAGE.                                       */
/* ----------------------------------------------------------------- */
# World - a high-quality speech analysis, manipulation and synthesis system

WORLD is free software for high-quality speech analysis, manipulation and synthesis.
It can estimate Fundamental frequency (F0), aperiodicity and spectral envelope
and also generate the speech like input speech with only estimated parameters.

This source code is released under the modified-BSD license.
There is no patent in all algorithms in WORLD.
OS 	: Windows 7 64 bit
CPU 	: Core i7-3540M 3.0 GHz
RAM 	: 16 GB

Software
Microsoft Visual Studio 2015WORLD - a high-quality speech analysis, manipulation and synthesis system

WORLD is free software for high-quality speech analysis, manipulation and synthesis.
It can estimate Fundamental frequency (F0), aperiodicity and spectral envelope
and also generate the speech like input speech with only estimated parameters.

2. Usage
Please see test.cpp.

(1) F0 estimation by Dio()
(1-1) F0 is refined by StoneMask() if you need more accurate result.
(2) Spectral envelope estimation by CheapTrick()
(3) Aperiodicity estimation by D4C().
(4) You can manipulation these parameters in this phase. 
(5) Voice synthesis by Synthesis().

English document is written by a Japanese poor editor.
I willingly accept your kind indication on my English text.

3. License
WORLD is free software, and you can redistribute it and 
modify it under the terms of the modified BSD License.
Please see copying.txt for more information.
You can use this program for business, while I hope that 
you contact me after you developed software with WORLD.
This information is crucial to obtain a grant to develop WORLD.

4. Contacts
WORLD was written by Masanori Morise.
You can contact him via e-mail (mmorise [at] yamanashi.ac.jp)
or Twitter: @m_morise͍ WORLD

gɂẮCtest.cppQlɂĂD

WORLD͏CBSDCZXłD

--------------------------------------------------------------------
{IȍlF
́CƉFɂ\łƂ܂D
ŐVłł́CF0CwWCXyNgŕ\܂D
́CVocoderƓ̍lłD

(1) Dio() ɂ艹̊{g𐄒
(1-1) KvɉStoneMask() ɂ{g␳
(2) CheapTrick() ɂ艹̃XyNg𐄒
(3) D4C() ɂ艹̔wW𐄒
(4) Kvɉĉ≹F̐
(5) Synthesis() ɂ艹

--------------------------------------------------------------------
肢F
WORLD𗘗pꍇ́Cp/񏤗pɊւ炸X
񒸂΍Kł (͋`ł͂܂)D
̏́ClXȌlۂɖ𗧂܂D
̌́CWORLD𔭓W邽߂ɗp܂D

҂֘A
mmorise [at] yamanashi.ac.jp
Twitter: @m_morise //-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Spectral envelope estimation on the basis of the idea of CheapTrick.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// SmoothingWithRecovery() carries out the spectral smoothing and spectral
// recovery on the Cepstrum domain.
//-----------------------------------------------------------------------------
// We can control q1 as the parameter. 2015/9/22 by M. Morise
// const double q1 = -0.09;  // Please see the reference in CheapTrick.
//-----------------------------------------------------------------------------
// GetPowerSpectrum() calculates the power_spectrum with DC correction.
// DC stands for Direct Current. In this case, the component from 0 to F0 Hz
// is corrected.
//-----------------------------------------------------------------------------
// FFT
// Calculation of the power spectrum.
// DC correction
//-----------------------------------------------------------------------------
// SetParametersForGetWindowedWaveform()
//-----------------------------------------------------------------------------
// Designing of the window function
//-----------------------------------------------------------------------------
// GetWindowedWaveform() windows the waveform by F0-adaptive window
//-----------------------------------------------------------------------------
// F0-adaptive windowing
//-----------------------------------------------------------------------------
// CheapTrickGeneralBody() calculates a spectral envelope at a temporal
// position. This function is only used in CheapTrick().
// Caution:
//   forward_fft is allocated in advance to speed up the processing.
//-----------------------------------------------------------------------------
// F0-adaptive windowing
// Calculate power spectrum with DC correction
// Note: The calculated power spectrum is stored in an array for waveform.
// In this imprementation, power spectrum is transformed by FFT (NOT IFFT).
// However, the same result is obtained.
// This is tricky but important for simple implementation.
// Smoothing of the power (linear axis)
// forward_real_fft.waveform is the power spectrum.
// Smoothing (log axis) and spectral recovery on the cepstrum domain.
// namespace
// q1 is the parameter used for the spectral recovery.
// Since The parameter is optimized, you don't need to change the parameter.
// f0_floor and fs is used to determine fft_size;
// We strongly recommend not to change this value unless you have enough
// knowledge of the signal processing in CheapTrick.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// common.cpp includes functions used in at least two files.
// (1) Common functions
// (2) FFT, IFFT and minimum phase analysis.
//
// In FFT analysis and minimum phase analysis,
// Functions "Initialize*()" allocate the mamory.
// Functions "Destroy*()" free the accolated memory.
// FFT size is used for initialization, and structs are used to keep the memory.
// Functions "GetMinimumPhaseSpectrum()" calculate minimum phase spectrum.
// Forward and inverse FFT do not have the function "Get*()",
// because forward FFT and inverse FFT can run in one step.
//
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// SetParametersForLinearSmoothing() is used in LinearSmoothing()
//-----------------------------------------------------------------------------
// namespace
//-----------------------------------------------------------------------------
// Fundamental functions
//-----------------------------------------------------------------------------
// DCCorrection() corrects the input spectrum from 0 to f0 Hz because the
// general signal does not contain the DC (Direct Current) component.
// It is used in CheapTrick() and D4C().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// LinearSmoothing() carries out the spectral smoothing by rectangular window
// whose length is width Hz and is used in CheapTrick() and D4C().
//-----------------------------------------------------------------------------
// These parameters are set by the other function.
//-----------------------------------------------------------------------------
// NuttallWindow() calculates the coefficients of Nuttall window whose length
// is y_length and is used in Dio() and D4C().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FFT, IFFT and minimum phase analysis
// Mirroring
// This fft_plan carries out "forward" FFT.
// To carriy out the Inverse FFT, the sign of imaginary part
// is inverted after FFT.
// Since x is complex number, calculation of exp(x) is as following.
// Note: This FFT library does not keep the aliasing.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Band-aperiodicity estimation on the basis of the idea of D4C.
//-----------------------------------------------------------------------------
// for std::sort()
//-----------------------------------------------------------------------------
// SetParametersForGetWindowedWaveform()
//-----------------------------------------------------------------------------
// Designing of the window function
// Hanning window
// Blackman window
//-----------------------------------------------------------------------------
// GetWindowedWaveform() windows the waveform by F0-adaptive window
// In the variable window_type, 1: hanning, 2: blackman
//-----------------------------------------------------------------------------
// F0-adaptive windowing
//-----------------------------------------------------------------------------
// GetCentroid() calculates the energy centroid (see the book, time-frequency
// analysis written by L. Cohen).
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetStaticCentroid() calculates the temporally static energy centroid.
// Basic idea was proposed by H. Kawahara.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetSmoothedPowerSpectrum() calculates the smoothed power spectrum.
// The parameters used for smoothing are optimized in davance.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetStaticGroupDelay() calculates the temporally static group delay.
// This is the fundamental parameter in D4C.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetCoarseAperiodicity() calculates the aperiodicity in multiples of 3 kHz.
// The upper limit is given based on the sampling frequency.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// D4CGeneralBody() calculates a spectral envelope at a temporal
// position. This function is only used in D4C().
// Caution:
//   forward_fft is allocated in advance to speed up the processing.
//-----------------------------------------------------------------------------
// Revision of the result based on the F0
// namespace
// Since the window function is common in D4CGeneralBody(),
// it is designed here to speed up.
// Linear interpolation to convert the coarse aperiodicity into its
// spectral representation.
//  printf("Number of bands for aperiodicity: %d\n", number_of_aperiodicities);
// Since the window function is common in D4CGeneralBody(),
// it is designed here to speed up.
// osw
// osw: store coarse aper directly, don't store constant end values 
//        printf("     band number %d\n", j); 
//        printf("     band number %f\n", coarse_aperiodicity[j+1]); 
//         
//     // Linear interpolation to convert the coarse aperiodicity into its
//     // spectral representation.
//     interp1(coarse_frequency_axis, coarse_aperiodicity,
//         number_of_aperiodicities + 2, frequency_axis, fft_size / 2 + 1,
//         aperiodicity[i]);
//     for (int j = 0; j <= fft_size / 2; ++j)
//       aperiodicity[i][j] = pow(10.0, aperiodicity[i][j] / 20.0);
// This struct is dummy.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// F0 estimation based on DIO (Distributed Inline-filter Operation).
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// struct for RawEventByDio()
// "negative" means "zero-crossing point going from positive to negative"
// "positive" means "zero-crossing point going from negative to positive"
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// DesignLowCutFilter() calculates the coefficients the filter.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetDownsampledSignal() calculates the spectrum for estimation.
// This function carries out downsampling to speed up the estimation process
// and calculates the spectrum of the downsampled signal.
//-----------------------------------------------------------------------------
// Initialization
// Downsampling
// Removal of the DC component (y = y - mean value of y)
// Low cut filtering (from 0.1.4)
// Cutoff is 50.0 Hz
//-----------------------------------------------------------------------------
// GetBestF0Contour() calculates the best f0 contour based on stabilities of
// all candidates. The F0 whose stability is minimum is selected.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixStep1() is the 1st step of the postprocessing.
// This function eliminates the unnatural change of f0 based on allowed_range.
//-----------------------------------------------------------------------------
// Initialization
// Processing to prevent the jumping of f0
//-----------------------------------------------------------------------------
// FixStep2() is the 2nd step of the postprocessing.
// This function eliminates the suspected f0 in the anlaut and auslaut.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// CountNumberOfVoicedSections() counts the number of voiced sections.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// SelectOneF0() corrects the f0[current_index] based on
// f0[current_index + sign].
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixStep3() is the 3rd step of the postprocessing.
// This function corrects the f0 candidates from backward to forward.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// BackwardCorrection() is the 4th step of the postprocessing.
// This function corrects the f0 candidates from forward to backward.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixF0Contour() calculates the definitive f0 contour based on all f0
// candidates. There are four steps.
//-----------------------------------------------------------------------------
// memo:
// These are the tentative values. Optimization should be required.
//-----------------------------------------------------------------------------
// GetFilteredSignal() calculates the signal that is the convolution of the
// input signal and low-pass filter.
// This function is only used in RawEventByDio()
//-----------------------------------------------------------------------------
// Nuttall window is used as a low-pass filter.
// Cutoff frequency depends on the window length.
// Convolution
// Compensation of the delay.
//-----------------------------------------------------------------------------
// CheckEvent() returns 1, provided that the input value is over 1.
// This function is for RawEventByDio().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// ZeroCrossingEngine() calculates the zero crossing points from positive to
// negative. Thanks to Custom.Maid http://custom-made.seesaa.net/ (2012/8/19)
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetFourZeroCrossingIntervals() calculates four zero-crossing intervals.
// (1) Zero-crossing going from negative to positive.
// (2) Zero-crossing going from positive to negative.
// (3) Peak, and (4) dip. (3) and (4) are calculated from the zero-crossings of
// the differential of waveform.
//-----------------------------------------------------------------------------
// x_length / 4 (old version) is fixed at 2013/07/14
//-----------------------------------------------------------------------------
// GetF0CandidatesSub() calculates the f0 candidates and deviations.
// This is the sub-function of GetF0Candidates() and assumes the calculation.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetF0Candidates() calculates the F0 candidates based on the zero-crossings.
// Calculation of F0 candidates is carried out in GetF0CandidatesSub().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// DestroyZeroCrossings() frees the memory of array in the struct
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// RawEventByDio() calculates the zero-crossings.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetF0CandidateAndStabilityMap() calculates all f0 candidates and
// their stabilities.
//-----------------------------------------------------------------------------
// Calculation of the acoustics events (zero-crossing)
// A way to avoid zero division
//-----------------------------------------------------------------------------
// DioGeneralBody() estimates the F0 based on Distributed Inline-filter
// Operation.
//-----------------------------------------------------------------------------
// normalization
// Calculation of the spectrum used for the f0 estimation
// f0map represents all F0 candidates. We can modify them.
// Selection of the best value based on fundamental-ness.
// Postprocessing to find the best f0-contour.
// namespace
// You can change default parameters.
// You can use the value from 1 to 12.
// Default value 11 is for the fs of 44.1 kHz.
// The lower value you use, the better performance you can obtain.
// You can give a positive real number as the threshold.
// The most strict value is 0, and there is no upper limit.
// On the other hand, I think that the value from 0.02 to 0.2 is reasonable.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// This file represents the functions about FFT (Fast Fourier Transform)
// implemented by Mr. Ooura, and wrapper functions implemented by M. Morise.
// We can use these wrapper functions as well as the FFTW functions.
// Please see the FFTW web-page to show the usage of the wrapper functions.
// Ooura FFT:
//   (Japanese) http://www.kurims.kyoto-u.ac.jp/~ooura/index-j.html
//   (English) http://www.kurims.kyoto-u.ac.jp/~ooura/index.html
// FFTW:
//   (English) http://www.fftw.org/
// 2012/08/24 by M. Morise
//-----------------------------------------------------------------------------
// c2r
// c2c
// r2c
// c2c
// namespace
// ifft
//-----------------------------------------------------------------------
// The following functions are reffered by
// http://www.kurims.kyoto-u.ac.jp/~ooura/index.html
// -------- child routines --------
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Matlab functions implemented for WORLD
// Since these functions are implemented as the same function of Matlab,
// the source code does not follow the style guide (Names of variables
// and functions).
// Please see the reference of Matlab to show the usage of functions.
// Caution:
//   The functions wavread() and wavwrite() were removed to the /src.
//   they were moved to the test/audioio.cpp. (2016/01/28)
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FilterForDecimate() calculates the coefficients of low-pass filter and
// carries out the filtering. This function is only used for decimate().
//-----------------------------------------------------------------------------
// filter Coefficients
// fs : 44100 (default)
// fs : 48000
// fs : 32000
// fs : 24000 and 22050
// fs : 16000
// fs : 8000
// Filtering on time domain.
// namespace
// Bug was fixed at 2013/07/14 by M. Morise
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// F0 estimation based on instantaneous frequency.
// This method is carried out by using the output of Dio().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetIndexRaw() calculates the temporal positions for windowing.
// Since the result includes negative value and the value that exceeds the
// length of the input signal, it must be modified appropriately.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetMainWindow() generates the window function.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetDiffWindow() generates the differentiated window.
// Diff means differential.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetSpectra() calculates two spectra of the waveform windowed by windows
// (main window and diff window).
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixF0() fixed the F0 by instantaneous frequency.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetTentativeF0() calculates the F0 based on the instantaneous frequency.
// Calculated value is tentative because it is fixed as needed.
// Note: The sixth argument in FixF0() is not optimized.
//-----------------------------------------------------------------------------
// If the fixed value is too large, the result will be rejected.
//-----------------------------------------------------------------------------
// GetMeanF0() calculates the instantaneous frequency.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetRefinedF0() fixes the F0 estimated by Dio(). This function uses
// instantaneous frequency.
//-----------------------------------------------------------------------------
// A safeguard was added (2015/12/02).
// bug fix 2015/11/29
// If amount of correction is overlarge (20 %), initial F0 is employed.
// namespace
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Voice synthesis based on f0, spectrogram and aperiodicity.
// forward_real_fft, inverse_real_fft and minimum_phase are used to speed up.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetAperiodicResponse() calculates an aperiodic response.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetPeriodicResponse() calculates an aperiodic response.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetOneFrameSegment() calculates a periodic and aperiodic response at a time.
//-----------------------------------------------------------------------------
// Synthesis of the periodic response
// Synthesis of the aperiodic response
// namespace
//  printf("%d\n", number_of_pulses);
//    printf("%d %d\n", i, number_of_pulses);
//-----------------------------------------------------------------------------
// 
// Author: Zhizheng Wu (wuzhizheng@gmail.com)
// Date: 11-03-2016
//
// To extract F0, spectrum and band aperiodicities with WORLD vocoder
//
// This is modified based on Msanori Morise's test.cpp. Low-dimensional band aperiodicities are used as suggested by Oliver.
//
// analysis input_waveform F0_file spectrogram_file aperiodicity_file
//
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Note: win.sln uses an option in Additional Include Directories.
// To compile the program, the option "-I $(SolutionDir)..\src" was set.
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
// Modification of the option
// When you You must set the same value.
// If a different value is used, you may suffer a fatal error because of a
// illegal memory access.
// Valuable option.speed represents the ratio for downsampling.
// The signal is downsampled to fs / speed Hz.
// If you want to obtain the accurate result, speed should be set to 1.
// You should not set option.f0_floor to under world::kFloorF0.
// If you want to analyze such low F0 speech, please change world::kFloorF0.
// Processing speed may sacrify, provided that the FFT length changes.
// You can give a positive real number as the threshold.
// Most strict value is 0, but almost all results are counted as unvoiced.
// The value from 0.02 to 0.2 would be reasonable.
// Parameters setting and memory allocation.
// StoneMask is carried out to improve the estimation performance.
// This value may be better one for HMM speech synthesis.
// Default value is -0.09.
// Important notice (2016/02/02)
// You can control a parameter used for the lowest F0 in speech.
// You must not set the f0_floor to 0.
// It will cause a fatal error because fft_size indicates the infinity.
// You must not change the f0_floor after memory allocation.
// You should check the fft_size before excucing the analysis/synthesis.
// The default value (71.0) is strongly recommended.
// On the other hand, setting the lowest F0 of speech is a good choice
// to reduce the fft_size.
// Parameters setting and memory allocation.
// Parameters setting and memory allocation.
//    world_parameters->aperiodicity[i] =
//      new double[world_parameters->fft_size / 2 + 1];
// option is not implemented in this version. This is for future update.
// We can use "NULL" as the argument.
// F0 scaling
// Spectral stretching
// Synthesis by the aperiodicity
// namespace
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] sp file
// f0         : argv[3] ap file
// spec       : argv[4] f0 file
//-----------------------------------------------------------------------------
// 2016/01/28: Important modification.
// Memory allocation is carried out in advanse.
// This is for compatibility with C language.
// wavread() must be called after GetAudioLength().
//---------------------------------------------------------------------------
// Analysis part
//---------------------------------------------------------------------------
// 2016/02/02
// A new struct is introduced to implement safe program.
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// F0 estimation
// Spectral envelope estimation
// Aperiodicity estimation by D4C
//    for (int i=0; i<world_parameters.f0_length; i++) {
//        printf("%d %F\n", i, world_parameters.f0[i]);
//    }
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// .wav input/output functions were modified for compatibility with C language.
// Since these functions (wavread() and wavwrite()) are roughly implemented,
// we recommend more suitable functions provided by other organizations.
// This file is independent of WORLD project and for the test.cpp.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// CheckHeader() checks the .wav header. This function can only support the
// monaural wave file. This function is only used in waveread().
//-----------------------------------------------------------------------------
// "RIFF"
// "WAVE"
// "fmt "
// 1 0 0 0
// 1 0
// 1 0
//-----------------------------------------------------------------------------
// GetParameters() extracts fp, nbit, wav_length from the .wav file
// This function is only used in wavread().
//-----------------------------------------------------------------------------
// Quantization
// Skip until "data" is found. 2011/03/28
// "data"
// namespace
// Quantization
// "data"
// "data"
//-----------------------------------------------------------------------------
// 
// Author: Zhizheng Wu (wuzhizheng@gmail.com)
// Date: 11-03-2016
//
// To generate waveform given F0, band aperiodicities and spectrum with WORLD vocoder
//
// This is modified based on Msanori Morise's test.cpp. Low-dimensional band aperiodicities are used as suggested by Oliver.
//
// synth FFT_length sampling_rate F0_file spectrogram_file aperiodicity_file output_waveform
//
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Note: win.sln uses an option in Additional Include Directories.
// To compile the program, the option "-I $(SolutionDir)..\src" was set.
// Frame shift [msec]
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
// Modification of the option
// When you You must set the same value.
// If a different value is used, you may suffer a fatal error because of a
// illegal memory access.
// Valuable option.speed represents the ratio for downsampling.
// The signal is downsampled to fs / speed Hz.
// If you want to obtain the accurate result, speed should be set to 1.
// You should not set option.f0_floor to under world::kFloorF0.
// If you want to analyze such low F0 speech, please change world::kFloorF0.
// Processing speed may sacrify, provided that the FFT length changes.
// You can give a positive real number as the threshold.
// Most strict value is 0, but almost all results are counted as unvoiced.
// The value from 0.02 to 0.2 would be reasonable.
// Parameters setting and memory allocation.
// StoneMask is carried out to improve the estimation performance.
// This value may be better one for HMM speech synthesis.
// Default value is -0.09.
// Important notice (2016/02/02)
// You can control a parameter used for the lowest F0 in speech.
// You must not set the f0_floor to 0.
// It will cause a fatal error because fft_size indicates the infinity.
// You must not change the f0_floor after memory allocation.
// You should check the fft_size before excucing the analysis/synthesis.
// The default value (71.0) is strongly recommended.
// On the other hand, setting the lowest F0 of speech is a good choice
// to reduce the fft_size.
// Parameters setting and memory allocation.
// Parameters setting and memory allocation.
// option is not implemented in this version. This is for future update.
// We can use "NULL" as the argument.
// F0 scaling
// Spectral stretching
// Synthesis by the aperiodicity
//  printf("\nSynthesis\n");
//  printf("WORLD: %d [msec]\n", timeGetTime() - elapsed_time);
// namespace
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] Output file
// f0         : argv[3] F0 scaling (a positive number)
// spec       : argv[4] Formant shift (a positive number)
//-----------------------------------------------------------------------------
// compute n bands from fs as in d4c.cpp:325   
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// find number of frames (doubles) in f0 file:  
//  printf("%d\n", f0_length);
// aper
// convert bandaps to full aperiodic spectrum by interpolation (originally in d4c extraction):
// Linear interpolation to convert the coarse aperiodicity into its
// spectral representation.
// -- for interpolating --
// ----
// load band ap values for this frame into  coarse_aperiodicity
//printf("%d %d\n", world_parameters.f0_length, fs);
//---------------------------------------------------------------------------
// Synthesis part
//---------------------------------------------------------------------------
// The length of the output waveform
// Synthesis
// Output
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Test program for WORLD 0.1.2 (2012/08/19)
// Test program for WORLD 0.1.3 (2013/07/26)
// Test program for WORLD 0.1.4 (2014/04/29)
// Test program for WORLD 0.1.4_3 (2015/03/07)
// Test program for WORLD 0.2.0 (2015/05/29)
// Test program for WORLD 0.2.0_1 (2015/05/31)
// Test program for WORLD 0.2.0_2 (2015/06/06)
// Test program for WORLD 0.2.0_3 (2015/07/28)
// Test program for WORLD 0.2.0_4 (2015/11/15)
// Test program for WORLD in GitHub (2015/11/16-)
// Latest update: 2016/03/04
// test.exe input.wav outout.wav f0 spec
// input.wav  : Input file
// output.wav : Output file
// f0         : F0 scaling (a positive number)
// spec       : Formant scaling (a positive number)
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Note: win.sln uses an option in Additional Include Directories.
// To compile the program, the option "-I $(SolutionDir)..\src" was set.
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
// Modification of the option
// When you You must set the same value.
// If a different value is used, you may suffer a fatal error because of a
// illegal memory access.
// Valuable option.speed represents the ratio for downsampling.
// The signal is downsampled to fs / speed Hz.
// If you want to obtain the accurate result, speed should be set to 1.
// You should not set option.f0_floor to under world::kFloorF0.
// If you want to analyze such low F0 speech, please change world::kFloorF0.
// Processing speed may sacrify, provided that the FFT length changes.
// You can give a positive real number as the threshold.
// Most strict value is 0, but almost all results are counted as unvoiced.
// The value from 0.02 to 0.2 would be reasonable.
// Parameters setting and memory allocation.
// StoneMask is carried out to improve the estimation performance.
// This value may be better one for HMM speech synthesis.
// Default value is -0.09.
// Important notice (2016/02/02)
// You can control a parameter used for the lowest F0 in speech.
// You must not set the f0_floor to 0.
// It will cause a fatal error because fft_size indicates the infinity.
// You must not change the f0_floor after memory allocation.
// You should check the fft_size before excucing the analysis/synthesis.
// The default value (71.0) is strongly recommended.
// On the other hand, setting the lowest F0 of speech is a good choice
// to reduce the fft_size.
// Parameters setting and memory allocation.
// Parameters setting and memory allocation.
// option is not implemented in this version. This is for future update.
// We can use "NULL" as the argument.
// F0 scaling
// Spectral stretching
// Synthesis by the aperiodicity
// namespace
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] Output file
// f0         : argv[3] F0 scaling (a positive number)
// spec       : argv[4] Formant shift (a positive number)
//-----------------------------------------------------------------------------
// 2016/01/28: Important modification.
// Memory allocation is carried out in advanse.
// This is for compatibility with C language.
// wavread() must be called after GetAudioLength().
//---------------------------------------------------------------------------
// Analysis part
//---------------------------------------------------------------------------
// 2016/02/02
// A new struct is introduced to implement safe program.
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// F0 estimation
// Spectral envelope estimation
// Aperiodicity estimation by D4C
// Note that F0 must not be changed until all parameters are estimated.
//---------------------------------------------------------------------------
// Synthesis part
//---------------------------------------------------------------------------
// The length of the output waveform
// Synthesis
// Output
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Spectral envelope estimation on the basis of the idea of CheapTrick.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// SmoothingWithRecovery() carries out the spectral smoothing and spectral
// recovery on the Cepstrum domain.
//-----------------------------------------------------------------------------
// We can control q1 as the parameter. 2015/9/22 by M. Morise
// const double q1 = -0.09;  // Please see the reference in CheapTrick.
//-----------------------------------------------------------------------------
// GetPowerSpectrum() calculates the power_spectrum with DC correction.
// DC stands for Direct Current. In this case, the component from 0 to F0 Hz
// is corrected.
//-----------------------------------------------------------------------------
// FFT
// Calculation of the power spectrum.
// DC correction
//-----------------------------------------------------------------------------
// SetParametersForGetWindowedWaveform()
//-----------------------------------------------------------------------------
// Designing of the window function
//-----------------------------------------------------------------------------
// GetWindowedWaveform() windows the waveform by F0-adaptive window
//-----------------------------------------------------------------------------
// F0-adaptive windowing
//-----------------------------------------------------------------------------
// CheapTrickGeneralBody() calculates a spectral envelope at a temporal
// position. This function is only used in CheapTrick().
// Caution:
//   forward_fft is allocated in advance to speed up the processing.
//-----------------------------------------------------------------------------
// F0-adaptive windowing
// Calculate power spectrum with DC correction
// Note: The calculated power spectrum is stored in an array for waveform.
// In this imprementation, power spectrum is transformed by FFT (NOT IFFT).
// However, the same result is obtained.
// This is tricky but important for simple implementation.
// Smoothing of the power (linear axis)
// forward_real_fft.waveform is the power spectrum.
// Smoothing (log axis) and spectral recovery on the cepstrum domain.
// namespace
// q1 is the parameter used for the spectral recovery.
// Since The parameter is optimized, you don't need to change the parameter.
// f0_floor and fs is used to determine fft_size;
// We strongly recommend not to change this value unless you have enough
// knowledge of the signal processing in CheapTrick.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// common.cpp includes functions used in at least two files.
// (1) Common functions
// (2) FFT, IFFT and minimum phase analysis.
//
// In FFT analysis and minimum phase analysis,
// Functions "Initialize*()" allocate the mamory.
// Functions "Destroy*()" free the accolated memory.
// FFT size is used for initialization, and structs are used to keep the memory.
// Functions "GetMinimumPhaseSpectrum()" calculate minimum phase spectrum.
// Forward and inverse FFT do not have the function "Get*()",
// because forward FFT and inverse FFT can run in one step.
//
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// SetParametersForLinearSmoothing() is used in LinearSmoothing()
//-----------------------------------------------------------------------------
// namespace
//-----------------------------------------------------------------------------
// Fundamental functions
//-----------------------------------------------------------------------------
// DCCorrection() corrects the input spectrum from 0 to f0 Hz because the
// general signal does not contain the DC (Direct Current) component.
// It is used in CheapTrick() and D4C().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// LinearSmoothing() carries out the spectral smoothing by rectangular window
// whose length is width Hz and is used in CheapTrick() and D4C().
//-----------------------------------------------------------------------------
// These parameters are set by the other function.
//-----------------------------------------------------------------------------
// NuttallWindow() calculates the coefficients of Nuttall window whose length
// is y_length and is used in Dio() and D4C().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FFT, IFFT and minimum phase analysis
// Mirroring
// This fft_plan carries out "forward" FFT.
// To carriy out the Inverse FFT, the sign of imaginary part
// is inverted after FFT.
// Since x is complex number, calculation of exp(x) is as following.
// Note: This FFT library does not keep the aliasing.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Band-aperiodicity estimation on the basis of the idea of D4C.
//-----------------------------------------------------------------------------
// for std::sort()
//-----------------------------------------------------------------------------
// SetParametersForGetWindowedWaveform()
//-----------------------------------------------------------------------------
// Designing of the window function
// Hanning window
// Blackman window
//-----------------------------------------------------------------------------
// GetWindowedWaveform() windows the waveform by F0-adaptive window
// In the variable window_type, 1: hanning, 2: blackman
//-----------------------------------------------------------------------------
// F0-adaptive windowing
//-----------------------------------------------------------------------------
// GetCentroid() calculates the energy centroid (see the book, time-frequency
// analysis written by L. Cohen).
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetStaticCentroid() calculates the temporally static energy centroid.
// Basic idea was proposed by H. Kawahara.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetSmoothedPowerSpectrum() calculates the smoothed power spectrum.
// The parameters used for smoothing are optimized in davance.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetStaticGroupDelay() calculates the temporally static group delay.
// This is the fundamental parameter in D4C.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetCoarseAperiodicity() calculates the aperiodicity in multiples of 3 kHz.
// The upper limit is given based on the sampling frequency.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// D4CGeneralBody() calculates a spectral envelope at a temporal
// position. This function is only used in D4C().
// Caution:
//   forward_fft is allocated in advance to speed up the processing.
//-----------------------------------------------------------------------------
// Revision of the result based on the F0
// namespace
// Since the window function is common in D4CGeneralBody(),
// it is designed here to speed up.
// Linear interpolation to convert the coarse aperiodicity into its
// spectral representation.
//  printf("Number of bands for aperiodicity: %d\n", number_of_aperiodicities);
// Since the window function is common in D4CGeneralBody(),
// it is designed here to speed up.
// osw
// osw: store coarse aper directly, don't store constant end values 
//        printf("     band number %d\n", j); 
//        printf("     band number %f\n", coarse_aperiodicity[j+1]); 
//         
//     // Linear interpolation to convert the coarse aperiodicity into its
//     // spectral representation.
//     interp1(coarse_frequency_axis, coarse_aperiodicity,
//         number_of_aperiodicities + 2, frequency_axis, fft_size / 2 + 1,
//         aperiodicity[i]);
//     for (int j = 0; j <= fft_size / 2; ++j)
//       aperiodicity[i][j] = pow(10.0, aperiodicity[i][j] / 20.0);
// This struct is dummy.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// F0 estimation based on DIO (Distributed Inline-filter Operation).
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// struct for RawEventByDio()
// "negative" means "zero-crossing point going from positive to negative"
// "positive" means "zero-crossing point going from negative to positive"
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// DesignLowCutFilter() calculates the coefficients the filter.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetDownsampledSignal() calculates the spectrum for estimation.
// This function carries out downsampling to speed up the estimation process
// and calculates the spectrum of the downsampled signal.
//-----------------------------------------------------------------------------
// Initialization
// Downsampling
// Removal of the DC component (y = y - mean value of y)
// Low cut filtering (from 0.1.4)
// Cutoff is 50.0 Hz
//-----------------------------------------------------------------------------
// GetBestF0Contour() calculates the best f0 contour based on stabilities of
// all candidates. The F0 whose stability is minimum is selected.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixStep1() is the 1st step of the postprocessing.
// This function eliminates the unnatural change of f0 based on allowed_range.
//-----------------------------------------------------------------------------
// Initialization
// Processing to prevent the jumping of f0
//-----------------------------------------------------------------------------
// FixStep2() is the 2nd step of the postprocessing.
// This function eliminates the suspected f0 in the anlaut and auslaut.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// CountNumberOfVoicedSections() counts the number of voiced sections.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// SelectOneF0() corrects the f0[current_index] based on
// f0[current_index + sign].
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixStep3() is the 3rd step of the postprocessing.
// This function corrects the f0 candidates from backward to forward.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// BackwardCorrection() is the 4th step of the postprocessing.
// This function corrects the f0 candidates from forward to backward.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixF0Contour() calculates the definitive f0 contour based on all f0
// candidates. There are four steps.
//-----------------------------------------------------------------------------
// memo:
// These are the tentative values. Optimization should be required.
//-----------------------------------------------------------------------------
// GetFilteredSignal() calculates the signal that is the convolution of the
// input signal and low-pass filter.
// This function is only used in RawEventByDio()
//-----------------------------------------------------------------------------
// Nuttall window is used as a low-pass filter.
// Cutoff frequency depends on the window length.
// Convolution
// Compensation of the delay.
//-----------------------------------------------------------------------------
// CheckEvent() returns 1, provided that the input value is over 1.
// This function is for RawEventByDio().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// ZeroCrossingEngine() calculates the zero crossing points from positive to
// negative. Thanks to Custom.Maid http://custom-made.seesaa.net/ (2012/8/19)
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetFourZeroCrossingIntervals() calculates four zero-crossing intervals.
// (1) Zero-crossing going from negative to positive.
// (2) Zero-crossing going from positive to negative.
// (3) Peak, and (4) dip. (3) and (4) are calculated from the zero-crossings of
// the differential of waveform.
//-----------------------------------------------------------------------------
// x_length / 4 (old version) is fixed at 2013/07/14
//-----------------------------------------------------------------------------
// GetF0CandidatesSub() calculates the f0 candidates and deviations.
// This is the sub-function of GetF0Candidates() and assumes the calculation.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetF0Candidates() calculates the F0 candidates based on the zero-crossings.
// Calculation of F0 candidates is carried out in GetF0CandidatesSub().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// DestroyZeroCrossings() frees the memory of array in the struct
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// RawEventByDio() calculates the zero-crossings.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetF0CandidateAndStabilityMap() calculates all f0 candidates and
// their stabilities.
//-----------------------------------------------------------------------------
// Calculation of the acoustics events (zero-crossing)
// A way to avoid zero division
//-----------------------------------------------------------------------------
// DioGeneralBody() estimates the F0 based on Distributed Inline-filter
// Operation.
//-----------------------------------------------------------------------------
// normalization
// Calculation of the spectrum used for the f0 estimation
// f0map represents all F0 candidates. We can modify them.
// Selection of the best value based on fundamental-ness.
// Postprocessing to find the best f0-contour.
// namespace
// You can change default parameters.
// You can use the value from 1 to 12.
// Default value 11 is for the fs of 44.1 kHz.
// The lower value you use, the better performance you can obtain.
// You can give a positive real number as the threshold.
// The most strict value is 0, and there is no upper limit.
// On the other hand, I think that the value from 0.02 to 0.2 is reasonable.
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// This file represents the functions about FFT (Fast Fourier Transform)
// implemented by Mr. Ooura, and wrapper functions implemented by M. Morise.
// We can use these wrapper functions as well as the FFTW functions.
// Please see the FFTW web-page to show the usage of the wrapper functions.
// Ooura FFT:
//   (Japanese) http://www.kurims.kyoto-u.ac.jp/~ooura/index-j.html
//   (English) http://www.kurims.kyoto-u.ac.jp/~ooura/index.html
// FFTW:
//   (English) http://www.fftw.org/
// 2012/08/24 by M. Morise
//-----------------------------------------------------------------------------
// c2r
// c2c
// r2c
// c2c
// namespace
// ifft
//-----------------------------------------------------------------------
// The following functions are reffered by
// http://www.kurims.kyoto-u.ac.jp/~ooura/index.html
// -------- child routines --------
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Matlab functions implemented for WORLD
// Since these functions are implemented as the same function of Matlab,
// the source code does not follow the style guide (Names of variables
// and functions).
// Please see the reference of Matlab to show the usage of functions.
// Caution:
//   The functions wavread() and wavwrite() were removed to the /src.
//   they were moved to the test/audioio.cpp. (2016/01/28)
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FilterForDecimate() calculates the coefficients of low-pass filter and
// carries out the filtering. This function is only used for decimate().
//-----------------------------------------------------------------------------
// filter Coefficients
// fs : 44100 (default)
// fs : 48000
// fs : 32000
// fs : 24000 and 22050
// fs : 16000
// fs : 8000
// Filtering on time domain.
// namespace
// Bug was fixed at 2013/07/14 by M. Morise
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// F0 estimation based on instantaneous frequency.
// This method is carried out by using the output of Dio().
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetIndexRaw() calculates the temporal positions for windowing.
// Since the result includes negative value and the value that exceeds the
// length of the input signal, it must be modified appropriately.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetMainWindow() generates the window function.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetDiffWindow() generates the differentiated window.
// Diff means differential.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetSpectra() calculates two spectra of the waveform windowed by windows
// (main window and diff window).
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// FixF0() fixed the F0 by instantaneous frequency.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetTentativeF0() calculates the F0 based on the instantaneous frequency.
// Calculated value is tentative because it is fixed as needed.
// Note: The sixth argument in FixF0() is not optimized.
//-----------------------------------------------------------------------------
// If the fixed value is too large, the result will be rejected.
//-----------------------------------------------------------------------------
// GetMeanF0() calculates the instantaneous frequency.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetRefinedF0() fixes the F0 estimated by Dio(). This function uses
// instantaneous frequency.
//-----------------------------------------------------------------------------
// A safeguard was added (2015/12/02).
// bug fix 2015/11/29
// If amount of correction is overlarge (20 %), initial F0 is employed.
// namespace
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Voice synthesis based on f0, spectrogram and aperiodicity.
// forward_real_fft, inverse_real_fft and minimum_phase are used to speed up.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetAperiodicResponse() calculates an aperiodic response.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetPeriodicResponse() calculates an aperiodic response.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// GetOneFrameSegment() calculates a periodic and aperiodic response at a time.
//-----------------------------------------------------------------------------
// Synthesis of the periodic response
// Synthesis of the aperiodic response
// namespace
//  printf("%d\n", number_of_pulses);
//    printf("%d %d\n", i, number_of_pulses);
//-----------------------------------------------------------------------------
// 
// Author: Zhizheng Wu (wuzhizheng@gmail.com)
// Date: 11-03-2016
//
// To extract F0, spectrum and band aperiodicities with WORLD vocoder
//
// This is modified based on Msanori Morise's test.cpp. Low-dimensional band aperiodicities are used as suggested by Oliver.
//
// analysis input_waveform F0_file spectrogram_file aperiodicity_file
//
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Note: win.sln uses an option in Additional Include Directories.
// To compile the program, the option "-I $(SolutionDir)..\src" was set.
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
//int number_of_aperiodicities;
// Modification of the option
// When you You must set the same value.
// If a different value is used, you may suffer a fatal error because of a
// illegal memory access.
// Valuable option.speed represents the ratio for downsampling.
// The signal is downsampled to fs / speed Hz.
// If you want to obtain the accurate result, speed should be set to 1.
// You should not set option.f0_floor to under world::kFloorF0.
// If you want to analyze such low F0 speech, please change world::kFloorF0.
// Processing speed may sacrify, provided that the FFT length changes.
// You can give a positive real number as the threshold.
// Most strict value is 0, but almost all results are counted as unvoiced.
// The value from 0.02 to 0.2 would be reasonable.
// Parameters setting and memory allocation.
// StoneMask is carried out to improve the estimation performance.
// This value may be better one for HMM speech synthesis.
// Default value is -0.09.
// Important notice (2016/02/02)
// You can control a parameter used for the lowest F0 in speech.
// You must not set the f0_floor to 0.
// It will cause a fatal error because fft_size indicates the infinity.
// You must not change the f0_floor after memory allocation.
// You should check the fft_size before excucing the analysis/synthesis.
// The default value (71.0) is strongly recommended.
// On the other hand, setting the lowest F0 of speech is a good choice
// to reduce the fft_size.
// Parameters setting and memory allocation.
//int number_of_aperiodicities =
//  static_cast<int>(MyMinDouble(world::kUpperLimit, world_parameters->fs / 2.0 -
//    world::kFrequencyInterval) / world::kFrequencyInterval);
// Parameters setting and memory allocation.
//world_parameters->aperiodicity[i] = new double[number_of_aperiodicities];
//world_parameters->number_of_aperiodicities = number_of_aperiodicities;
// option is not implemented in this version. This is for future update.
// We can use "NULL" as the argument.
// F0 scaling
// Spectral stretching
// Synthesis by the aperiodicity
// namespace
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] sp file
// f0         : argv[3] ap file
// spec       : argv[4] f0 file
//-----------------------------------------------------------------------------
// 2016/01/28: Important modification.
// Memory allocation is carried out in advanse.
// This is for compatibility with C language.
// wavread() must be called after GetAudioLength().
//---------------------------------------------------------------------------
// Analysis part
//---------------------------------------------------------------------------
// 2016/02/02
// A new struct is introduced to implement safe program.
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// F0 estimation
// Spectral envelope estimation
// Aperiodicity estimation by D4C
//fwrite(world_parameters.aperiodicity[i], sizeof(double), world_parameters.number_of_aperiodicities, fap);
//    for (int i=0; i<world_parameters.f0_length; i++) {
//        printf("%d %F\n", i, world_parameters.f0[i]);
//    }
//printf("%d %d %d\n", world_parameters.f0_length, world_parameters.fft_size, world_parameters.number_of_aperiodicities);
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// .wav input/output functions were modified for compatibility with C language.
// Since these functions (wavread() and wavwrite()) are roughly implemented,
// we recommend more suitable functions provided by other organizations.
// This file is independent of WORLD project and for the test.cpp.
//-----------------------------------------------------------------------------
//-----------------------------------------------------------------------------
// CheckHeader() checks the .wav header. This function can only support the
// monaural wave file. This function is only used in waveread().
//-----------------------------------------------------------------------------
// "RIFF"
// "WAVE"
// "fmt "
// 1 0 0 0
// 1 0
// 1 0
//-----------------------------------------------------------------------------
// GetParameters() extracts fp, nbit, wav_length from the .wav file
// This function is only used in wavread().
//-----------------------------------------------------------------------------
// Quantization
// Skip until "data" is found. 2011/03/28
// "data"
// namespace
// Quantization
// "data"
// "data"
//-----------------------------------------------------------------------------
// 
// Author: Zhizheng Wu (wuzhizheng@gmail.com)
// Date: 11-03-2016
//
// To generate waveform given F0, band aperiodicities and spectrum with WORLD vocoder
//
// This is modified based on Msanori Morise's test.cpp. Low-dimensional band aperiodicities are used as suggested by Oliver.
//
// synth FFT_length sampling_rate F0_file spectrogram_file aperiodicity_file output_waveform
//
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Note: win.sln uses an option in Additional Include Directories.
// To compile the program, the option "-I $(SolutionDir)..\src" was set.
// Frame shift [msec]
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
// Synthesis by the aperiodicity
//  printf("\nSynthesis\n");
//  elapsed_time = timeGetTime();
//  printf("WORLD: %d [msec]\n", timeGetTime() - elapsed_time);
// namespace
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] Output file
// f0         : argv[3] F0 scaling (a positive number)
// spec       : argv[4] Formant shift (a positive number)
//-----------------------------------------------------------------------------
// compute n bands from fs as in d4c.cpp:325   
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// find number of frames (doubles) in f0 file:  
//  printf("%d\n", f0_length);
// aper
// convert bandaps to full aperiodic spectrum by interpolation (originally in d4c extraction):
// Linear interpolation to convert the coarse aperiodicity into its
// spectral representation.
// -- for interpolating --
// ----
// load band ap values for this frame into  coarse_aperiodicity
//printf("%d %d\n", world_parameters.f0_length, fs);
//---------------------------------------------------------------------------
// Synthesis part
//---------------------------------------------------------------------------
// The length of the output waveform
// Synthesis
// Output
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Test program for WORLD 0.1.2 (2012/08/19)
// Test program for WORLD 0.1.3 (2013/07/26)
// Test program for WORLD 0.1.4 (2014/04/29)
// Test program for WORLD 0.1.4_3 (2015/03/07)
// Test program for WORLD 0.2.0 (2015/05/29)
// Test program for WORLD 0.2.0_1 (2015/05/31)
// Test program for WORLD 0.2.0_2 (2015/06/06)
// Test program for WORLD 0.2.0_3 (2015/07/28)
// Test program for WORLD 0.2.0_4 (2015/11/15)
// Test program for WORLD in GitHub (2015/11/16-)
// Latest update: 2016/03/04
// test.exe input.wav outout.wav f0 spec
// input.wav  : Input file
// output.wav : Output file
// f0         : F0 scaling (a positive number)
// spec       : Formant scaling (a positive number)
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Note: win.sln uses an option in Additional Include Directories.
// To compile the program, the option "-I $(SolutionDir)..\src" was set.
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
// Modification of the option
// When you You must set the same value.
// If a different value is used, you may suffer a fatal error because of a
// illegal memory access.
// Valuable option.speed represents the ratio for downsampling.
// The signal is downsampled to fs / speed Hz.
// If you want to obtain the accurate result, speed should be set to 1.
// You should not set option.f0_floor to under world::kFloorF0.
// If you want to analyze such low F0 speech, please change world::kFloorF0.
// Processing speed may sacrify, provided that the FFT length changes.
// You can give a positive real number as the threshold.
// Most strict value is 0, but almost all results are counted as unvoiced.
// The value from 0.02 to 0.2 would be reasonable.
// Parameters setting and memory allocation.
// StoneMask is carried out to improve the estimation performance.
// This value may be better one for HMM speech synthesis.
// Default value is -0.09.
// Important notice (2016/02/02)
// You can control a parameter used for the lowest F0 in speech.
// You must not set the f0_floor to 0.
// It will cause a fatal error because fft_size indicates the infinity.
// You must not change the f0_floor after memory allocation.
// You should check the fft_size before excucing the analysis/synthesis.
// The default value (71.0) is strongly recommended.
// On the other hand, setting the lowest F0 of speech is a good choice
// to reduce the fft_size.
// Parameters setting and memory allocation.
// Parameters setting and memory allocation.
// option is not implemented in this version. This is for future update.
// We can use "NULL" as the argument.
// F0 scaling
// Spectral stretching
// Synthesis by the aperiodicity
// namespace
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] Output file
// f0         : argv[3] F0 scaling (a positive number)
// spec       : argv[4] Formant shift (a positive number)
//-----------------------------------------------------------------------------
// 2016/01/28: Important modification.
// Memory allocation is carried out in advanse.
// This is for compatibility with C language.
// wavread() must be called after GetAudioLength().
//---------------------------------------------------------------------------
// Analysis part
//---------------------------------------------------------------------------
// 2016/02/02
// A new struct is introduced to implement safe program.
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// F0 estimation
// Spectral envelope estimation
// Aperiodicity estimation by D4C
// Note that F0 must not be changed until all parameters are estimated.
//---------------------------------------------------------------------------
// Synthesis part
//---------------------------------------------------------------------------
// The length of the output waveform
// Synthesis
// Output
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
# Install it with pip (it's not the same as 'ConfigParser' (old version))
# Display:
# File setup:
# INPUT:===================================================================================================
# Experiment type:-----------------------------------------------------------------------
#  'demo' (50 training utts) or 'full' (1k training utts)
# Steps:---------------------------------------------------------------------------------
# Downloads wavs and label data.
# Copies downloaded data into the experiment directory. Plus, makes a backup copy of this script.
# Saves new configuration files for Merlin.
# Performs acoustic feature extraction using the MagPhase vocoder
# Converts the state aligned labels to variable rate if running in variable frame rate mode (d_mp_opts['b_const_rate'] = False)
# Merlin: Training of duration model.
# Merlin: Training of acoustic model.
# Merlin: Generation of state durations using the duration model.
# Merlin: Waveform generation for the utterances provided in ./test_synthesis/prompt-lab
# MagPhase Vocoder:-----------------------------------------------------------------------
# Dictionary containing internal options for the MagPhase vocoder (mp).
# Number of coefficients (bins) for magnitude feature M.
# Number of coefficients (bins) for phase features R and I.
# To work in constant frame rate mode.
#  List containing the postfilters to apply during waveform generation.
# You need to choose at least one: 'magphase' (magphase-tailored postfilter), 'merlin' (Merlin's style postfilter), 'no' (no postfilter)
# Acoustic feature extraction done in multiprocessing mode (faster).
# PROCESS:===================================================================================================
# Pre setup:-------------------------------------------------------------------------------
# Build config parsers:-------------------------------------------------------------------
# Duration training config file:
# Duration synthesis:
# Acoustic training:
# Acoustic synth:
# Download Data:--------------------------------------------------------------------------
# Setup Data:-----------------------------------------------------------------------------
# Configure Merlin:-----------------------------------------------------------------------
# Read file list:
#').tolist()
# Acoustic Feature Extraction:-------------------------------------------------------------
# Extract features:
# Labels Conversion to Variable Frame Rate:------------------------------------------------
# NOTE: The script ./script/label_st_align_to_var_rate.py can be also called from comand line directly.
# Run duration training:-------------------------------------------------------------------
# Run acoustic train:----------------------------------------------------------------------
# Run duration syn:------------------------------------------------------------------------
# Run acoustic synth:----------------------------------------------------------------------
#!/usr/bin/python
#URL = 'http://www.cs.toronto.edu/~murray/code/gpu_monitoring/'
# Get ID's of NVIDIA boards. Should do this through a CUDA call, but this is
# a quick and dirty way that works for now:
# /tmp is cleared on reboot on many systems, but it doesn't have to be
# /dev/shm on linux machines is a RAM disk, so is definitely cleared
#    print   id
# On POSIX systems symlink creation is atomic, so this should be a
# robust locking operation:
# On POSIX systems os.rename is an atomic operation, so this is the safe
# way to delete a lock:
# If run as a program:
# Report
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
# Conversion:
#')
# Display:
# Current i/o files:
# Debug:
#'''
# Parsing input arg:
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
#import gnumpy as gnp
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#import theano
# the new class for label composition and normalisation
#from frontend.mlpg_fast import MLParameterGenerationFast
#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
#    sequential_training = True
###################
#validation data is still read block by block
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
#    pretrain_set_x = train_set_x
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
#        dnn_model = DeepRecurrentNetwork(n_in= n_ins, hidden_layer_size = hidden_layer_size, n_out = n_outs, L1_reg = l1_reg, L2_reg = l2_reg, hidden_layer_type = hidden_layer_type)
#        dnn_model = SequentialDNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,
#                        l1_reg = l1_reg, l2_reg = l2_reg,
#                         hidden_layer_sizes = hidden_layer_size)
#    finetune_lr = 0.000125
#, valid_set_y
#            logger.debug('validation loss decreased, so saving model')
#            dbn = best_dnn_model
#        if early_stop > early_stop_epoch:
#            logger.debug('stopping early')
#            break
#    cPickle.dump(best_dnn_model, open(nnets_file_name, 'wb'))
#file_number
### write to cmp file
#file_number
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
# to do - sanity check the label dimension here?
# simple HTS labels
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output acoustic data
#[-0.5, 0.0, 0.5]
#[1.0, -2.0, 1.0]
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
### DNN model training
# not an error - just means directory already exists
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
### generate parameters from DNN
# not an error - just means directory already exists
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
##MCD
##MCD
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
#    if gnp._boardId is not None:
#        import gpu_lock
#        gpu_lock.free_lock(gnp._boardId)
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
#import gnumpy as gnp
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#import theano
# the new class for label composition and normalisation
#from frontend.mlpg_fast import MLParameterGenerationFast
#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
#    sequential_training = True
###################
#validation data is still read block by block
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
#    pretrain_set_x = train_set_x
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
#        dnn_model = DeepRecurrentNetwork(n_in= n_ins, hidden_layer_size = hidden_layer_size, n_out = n_outs, L1_reg = l1_reg, L2_reg = l2_reg, hidden_layer_type = hidden_layer_type)
#    finetune_lr = 0.000125
#            logger.debug('validation loss decreased, so saving model')
#            dbn = best_dnn_model
#        if early_stop > early_stop_epoch:
#            logger.debug('stopping early')
#            break
#    cPickle.dump(best_dnn_model, open(nnets_file_name, 'wb'))
#file_number
### write to cmp file
#file_number
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
# to do - sanity check the label dimension here?
# simple HTS labels
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output acoustic data
#[-0.5, 0.0, 0.5]
#[1.0, -2.0, 1.0]
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
### DNN model training
# not an error - just means directory already exists
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
### generate parameters from DNN
# not an error - just means directory already exists
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
##MCD
##MCD
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
#    if gnp._boardId is not None:
#        import gpu_lock
#        gpu_lock.free_lock(gnp._boardId)
# string constants for various shell calls
##
## make proto
## make vFloors
## make local macro
# get first three lines from local proto
# get remaining lines from vFloors
## make hmmdefs
# ignore
# the header
# the rest
#!MLF!#\n')
# write a CFG for extracting MFCCs
# write a CFG for what we just built
# pass on the previously new one to the old one
# increment
# compute the path for the new one
# make the new directory
# HMMs
# SCP files
# CFG
## save to itself
##increase mixture number
## if multiple_speaker is tuned on. the file_id_list.scp has to reflact this
## for example
## speaker_1/0001
## speaker_2/0001
## This is to do speaker-dependent normalisation
# Copyright (c) 2007 Carnegie Mellon University
#
# You may copy and modify this freely under the same terms as
# Sphinx-III
# has energy
# absolute energy supressed
# has delta coefficients
# has acceleration (delta-delta) coefficients
# is compressed
# has zero mean static coefficients
# has CRC checksum
# has 0th cepstral coefficient
# has VQ data
# has third differential coefficients
# veclen is ignored since it's in the file
# Get coefficients for compressed data
# Uncompress data to floats if required
#        print   self.nSamples, self.veclen
#        print   self.parmKind, self.sampPeriod
#        print   len(data), data.shape
#        if self.parmKind & _K: # Remove and ignore checksum
#            data = data[:-1]
#        print   data.shape
#        data = tmp_data.reshape((len(tmp_data)/self.veclen, self.veclen))
# Uncompress data to floats if required
# HTK datatybes
# Additional 'param kind' options
#has energy
#absolute energy suppressed
#has delta coefficients
#has acceleration coefficients
#is compressed
#has zero mean static coef.
#has CRC checksum
#has 0th cepstral coef.
#has VQ data
#has third differential coef.
# the first 6 bits contain datatype
# HTK header
# number of samples in file (4-byte integer)
# sample period in 100ns units (4-byte integer)
# number of bytes per sample (2-byte integer)
# a code indicating the sample kind (2-byte integer)
#TODO compression
#self.A = struct.unpack('>H', f.read(2))[0]
#self.B = struct.unpack('>H', f.read(2))[0]
#                print   "world"
#                print   "hello"
# forces big-endian byte ordering
# force big-endian byte ordering
#filename_src = "../data/GE001_1.feat"
#print "t", htk.dupa, sys.byteorder
#        self.feature_dimension = feature_dimension
#            print   current_frame_number
#            features = io_funcs.data
#            current_frame_number = io_funcs.n_samples
#            htk_writter = HTK_Parm_IO(n_samples=io_funcs.n_samples, samp_period=io_funcs.samp_period, samp_size=io_funcs.samp_size, param_kind=io_funcs.param_kind, data=norm_features)
#            htk_writter.write_htk(out_file_list[i])
#            io_funcs = HTK_Parm_IO()
#            io_funcs.read_htk(file_name)
#            features = io_funcs.data
#            current_frame_number = io_funcs.n_samples
# setting the print options in this way seems to break subsequent printing of numpy float32 types
# no idea what is going on - removed until this can be solved
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
# setting the print options in this way seems to break subsequent printing of numpy float32 types
# no idea what is going on - removed until this can be solved
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
## remove begining and end double quotes
### if you want to use a particular voice
#out_f1.write("(voice_cstr_edi_fls_multisyn)\n")
#if nof_each_state<1:
#    print 'warning: some states are with zero duration'
#print ph
#break;
#!/usr/bin/env python
### create tcoef dir if not exists ###
#print "vfloor: {0}".format(vfloor)
#set to the frame value if there is no range!
#### User configurable variables ####
#### Train and test file lists ####
#### calculate variance flooring for each feature (from only training files) ####
#### calculate tcoef features ####
### read file by file ###
### read label file ###
### process label file ###
# remove state information [k]
#\n')
#!/usr/bin/env python
# one stream
#remove unvoiced values
#convert to linear scale
#', ignoreSilence=True):
#exclude first line!
#keep as frames or convert to time?! Currently kept in frames
#!/usr/bin/env python2
# -*- coding: utf-8 -*-
# top merlin directory
# input audio directory
# Output features directory
# Expected sample rate
# Magphase directory
# Parsing path:
# Display:
# FILES SETUP:========================================================================
# MULTIPROCESSING EXTRACTION:==========================================================
# For debugging (don't delete):
#for wavfile in l_wavfiles:
#    feat_extraction(wavfile, out_dir)
# top merlin directory
# input audio directory
# Output features directory
# initializations
# tools directory
### STRAIGHT ANALYSIS -- extract vocoder parameters ###
### extract f0, sp, ap ###
### convert f0 to lf0 ###
### convert sp to mgc ###
### convert ap to bap ###
# get wav files list
# do multi-processing
# clean temporal files
# top merlin directory
# input audio directory
# Output features directory
# initializations
# tools directory
#bap order depends on sampling rate.
# If True: Reaper is used for f0 extraction. If False: The vocoder is used for f0 extraction.
# This is to keep compatibility with numpy default dtype.
# Run REAPER:
# Protection - number of frames:
# Save f0 file:
### WORLD ANALYSIS -- extract vocoder parameters ###
### extract sp, ap ###
### Extract f0 using reaper ###
### convert f0 to lf0 ###
### convert sp to mgc ###
### convert bapd to bap ###
# get wav files list
# do multi-processing
# DEBUG:
#for nxf in xrange(len(wav_files)):
#    process(wav_files[nxf])
# clean temporal files
# top merlin directory
# input feat directory
# Output audio directory
# initializations
# file ID list
# feat directories
# tools directory
# set to True if synthesizing generated files
# this coefficient depends on voice
### WORLD Re-synthesis -- reconstruction of parameters ###
### convert lf0 to f0 ###
### post-filtering mgc ###
### convert mgc to sp ###
### convert bapd to bap ###
# Final synthesis using WORLD
# parse the arguments
#!/usr/bin/env python
### Arguments
# tools directory
# Source features directory
# Target features directory
# Source-aligned features directory
# bap dimension
### Define variables
### create outut directories
#################################################################
######## align source feats with target feats using dtw ## ######
#################################################################
### DTW alignment -- align source with target parameters ###
### dtw align src with tgt ###
### load dtw path
# dtw length not matched
### align features
# get mgc files list
# do multi-processing
#!/usr/bin/env python
### Arguments
# tools directory
# Source features directory
# Target features directory
# Source-aligned features directory
# bap dimension
# path to tools
### Define variables
### create outut directories
#################################################################
######## align source feats with target feats using dtw ## ######
#################################################################
# create dummy lab files
### DTW alignment -- align source with target parameters ###
### dtw align src with tgt ###
### load dtw path
# dtw length not matched
### align features
# get mgc files list
# do multi-processing
# clean temporal files
#!/usr/bin/env python
### Arguments
# tools directory
# Source features directory
# Target features directory
# Source-aligned features directory
# bap dimension
#bap_dim = int(sys.argv[5])
# path to tools
### Define variables. TODO: read from config file (void hardcoding)
#src_mag_dir = os.path.join(src_feat_dir, "mag")
#tgt_mag_dir = os.path.join(tgt_feat_dir, "mag")
#src_bap_dir = os.path.join(src_feat_dir, "bap")
#tgt_bap_dir = os.path.join(tgt_feat_dir, "bap")
#src_lf0_dir = os.path.join(src_feat_dir, "lf0")
#tgt_lf0_dir = os.path.join(tgt_feat_dir, "lf0")
### create outut directories
#src_aligned_mag_dir = os.path.join(src_aligned_feat_dir, "mag")
#src_aligned_bap_dir = os.path.join(src_aligned_feat_dir, "bap")
#src_aligned_lf0_dir = os.path.join(src_aligned_feat_dir, "lf0")
#if not os.path.exists(src_aligned_mag_dir):
#    os.mkdir(src_aligned_mag_dir)
#if not os.path.exists(src_aligned_bap_dir):
#    os.mkdir(src_aligned_bap_dir)
#if not os.path.exists(src_aligned_lf0_dir):
#    os.mkdir(src_aligned_lf0_dir)
#################################################################
######## align source feats with target feats using dtw ## ######
#################################################################
# create dummy lab files
### DTW alignment -- align source with target parameters ###
### dtw align src with tgt ###
### load dtw path
# dtw length not matched
### align features
# get mag files list
# do multi-processing
# clean temporal files
#!/usr/bin/env python
#import shutil
### Arguments
# tools directory
# Source features directory
# Target features directory
# Source-aligned features directory
### Define variables
# TODO: Change this (avoid hardcoded)
#src_mag_dir = src_feat_dir
#tgt_mag_dir = tgt_feat_dir
#src_lf0_dir = os.path.join(src_feat_dir, "lf0")
#tgt_lf0_dir = os.path.join(tgt_feat_dir, "lf0")
### create outut directories
#src_aligned_mag_dir = os.path.join(src_aligned_feat_dir, "mag")
#src_aligned_bap_dir = os.path.join(src_aligned_feat_dir, "bap")
#src_aligned_lf0_dir = os.path.join(src_aligned_feat_dir, "lf0")
#if not os.path.exists(src_aligned_mag_dir):
#    os.mkdir(src_aligned_mag_dir)
#if not os.path.exists(src_aligned_bap_dir):
#    os.mkdir(src_aligned_bap_dir)
#if not os.path.exists(src_aligned_lf0_dir):
#    os.mkdir(src_aligned_lf0_dir)
#################################################################
######## align source feats with target feats using dtw ## ######
#################################################################
### DTW alignment -- align source with target parameters ###
### dtw align src with tgt ###
### load dtw path
# dtw length not matched
### align features
# get mag files list
# do multi-processing
# parse the arguments
#print(src_mean_f0, src_std_f0)
#print(tgt_mean_f0, tgt_std_f0)
#!/usr/bin/python
#URL = 'http://www.cs.toronto.edu/~murray/code/gpu_monitoring/'
# Get ID's of NVIDIA boards. Should do this through a CUDA call, but this is
# a quick and dirty way that works for now:
# /tmp is cleared on reboot on many systems, but it doesn't have to be
# /dev/shm on linux machines is a RAM disk, so is definitely cleared
#    print   id
# On POSIX systems symlink creation is atomic, so this should be a
# robust locking operation:
# On POSIX systems os.rename is an atomic operation, so this is the safe
# way to delete a lock:
# If run as a program:
# Report
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
###################################################
########## User configurable variables ############
###################################################
### Input-Output ###
#### define model params ####
### define train, valid, test ###
#### main processess ####
#### Generate only test list ####
###################################################
####### End of user-defined conf variables ########
###################################################
#### Create train, valid and test file lists ####
#### Define keras models class ####
### normalize train data ###
#### define the model ####
#### load the data ####
#### normalize the data ####
#### train the model ####
### Train feedforward model ###
### Train recurrent model ###
#### store the model ####
#### load the model ####
#### load the data ####
#### normalize the data ####
#### compute predictions ####
### Implement each module ###
# create a configuration instance
# and get a short name for this instance
# main function
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# only for socket.getfqdn()
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
#import gnumpy as gnp
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
# the new class for label composition and normalisation
# our custom logging class that can also plot
# as logging
# reference activation weights in layers
## plot activation weights including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
## Model adaptation -- fine tuning the existing model
## We can't just unpickle the old model and use that because fine-tune functions
## depend on opt_l2e option used in construction of initial model. One way around this
## would be to unpickle, manually set unpickled_dnn_model.opt_l2e=True and then call
## unpickled_dnn_model.build_finetne_function() again. This is another way, construct
## new model from scratch with opt_l2e=True, then copy existing weights over:
# assign the existing dnn model parameters to the new dnn model
## Added for LHUC ##
# In LHUC, we keep all the old parameters intact and learn only a small set of new
# parameters
#, batch_size=batch_size
# fixed learning rate 
# exponential decay
# linear decay
# no decay
# if sequential training, the batch size will be the number of frames in an utterance
# batch_size for sequential training is considered only when rnn_batch_training is set to True
## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function
#file_number
### write to cmp file
##generate bottleneck layer as features
### write to cmp file
# split data into a list of num_splits tuples with each tuple representing
# the parameters for perform_acoustic_compositon_on_split
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
# create plot dir if set to True
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# the number can be removed
# Debug:----------------------------------
#generate_wav(gen_dir, file_id_list, cfg)     # generated speech
#-----------------------------------------
# simple HTS labels
###use only training data to find min-max information, then apply on the whole dataset
### enforce silence such that the normalization runs without removing silence: only for final synthesis
### save label normalisation information for unseen testing labels
### make output duration data
### make output acoustic data
#[-0.5, 0.0, 0.5]
#[1.0, -2.0, 1.0]
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
# load mean std values
###calculate mean and std vectors on the training data, and apply on the whole dataset
# for hmpd vocoder we don't need to normalize the 
# pdd values
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
### set configuration variables ###
### call kerasclass and use an instance ###
### call Tensorflowclass and use an instance ###
### DNN model training
# not an error - just means directory already exists
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
# Please only tune on this step when you want to generate bottleneck features from DNN
# not an error - just means directory already exists
### generate parameters from DNN
# not an error - just means directory already exists
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### Perform duration normalization(min. state dur set to 1) ###
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list, cfg)  # reference copy synthesis speech
### setting back to original conditions before calculating objective scores ###
### evaluation: RMSE and CORR for duration
### evaluation: calculate distortion
# for straight or world vocoders
# for magphase vocoder
# for GlottDNN vocoder
# for pulsemodel vocoder
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
##MCD
##MCD
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# Check for the presence of git
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# only for socket.getfqdn()
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
#import gnumpy as gnp
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
# the new class for label composition and normalisation
# our custom logging class that can also plot
# as logging
# reference activation weights in layers
## plot activation weights including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
### extract phone duration array for frame features ###
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
## Model adaptation -- fine tuning the existing model
## We can't just unpickle the old model and use that because fine-tune functions
## depend on opt_l2e option used in construction of initial model. One way around this
## would be to unpickle, manually set unpickled_dnn_model.opt_l2e=True and then call
## unpickled_dnn_model.build_finetne_function() again. This is another way, construct
## new model from scratch with opt_l2e=True, then copy existing weights over:
# assign the existing dnn model parameters to the new dnn model
## Added for LHUC ##
# In LHUC, we keep all the old parameters intact and learn only a small set of new
# parameters
#, batch_size=batch_size
# fixed learning rate 
# exponential decay
# linear decay
# no decay
# if sequential training, the batch size will be the number of frames in an utterance
# batch_size for sequential training is considered only when rnn_batch_training is set to True
## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function
#file_number
### write to cmp file
#file_number
#print b_indices
### write to cmp file
#file_number
### MLU features sub-division ###
### duration array sub-division ###
### additional feature matrix (syllable+phone+frame) ###
### input word feature matrix ###
#print b_indices
### write to cmp file
##generate bottleneck layer as features
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
# create plot dir if set to True
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# the number can be removed
# simple HTS labels
###use only training data to find min-max information, then apply on the whole dataset
### enforce silence such that the normalization runs without removing silence: only for final synthesis
### make duration data for S2S network ###
### save label normalisation information for unseen testing labels
### make output duration data
### make output acoustic data
#[-0.5, 0.0, 0.5]
#[1.0, -2.0, 1.0]
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
# load mean std values
###calculate mean and std vectors on the training data, and apply on the whole dataset
# for hmpd vocoder we don't need to normalize the 
# pdd values
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
### set configuration variables ###
### call kerasclass and use an instance ###
### call Tensorflowclass and use an instance ###
### DNN model training
# not an error - just means directory already exists
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
# Please only tune on this step when you want to generate bottleneck features from DNN
# not an error - just means directory already exists
### generate parameters from DNN
# not an error - just means directory already exists
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### Perform duration normalization(min. state dur set to 1) ###
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list, cfg)  # reference copy synthesis speech
### setting back to original conditions before calculating objective scores ###
### evaluation: RMSE and CORR for duration
### evaluation: calculate distortion
# for straight or world vocoders
# for GlottDNN vocoder
# for pulsemodel vocoder
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
##MCD
##MCD
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# Check for the presence of git
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
###################################################
########## User configurable variables ############
###################################################
### Input-Output ###
#### define model params ####
### Define the work directory###
### define train, valid, test ###
#### main processess ####
#### Generate only test list ####
###################################################
####### End of user-defined conf variables ########
###################################################
#### Create train, valid and test file lists ####
### normalize train data ###
#### load the data ####
#### normalize the data ####
#### define the model ####
#### train the model ####
### Train feedforward model ###
#### load the data ####
#### normalize the data ####
#### compute predictions ####
### Implement each module ###
# create a configuration instance
# and get a short name for this instance
# main function
# except:
#         print "inp stats file is %s"%cfg.inp_stats_file
#        sys.exit(0)
#! /usr/bin/python2 -u
# -*- coding: utf-8 -*-
#
# Copyright 2016 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# doesn't do anything
# get a logger
# this (and only this) logger needs to be configured immediately, otherwise it won't work
# we can't use the full user-supplied configuration mechanism in this particular case,
# because we haven't loaded it yet!
#
# so, just use simple console-only logging
# this level is hardwired here - should change it to INFO
# add a handler & its formatter - will write only to console
# first, set up some default configuration values
# next, load in any user-supplied configuration values
# that might over-ride the default values
# now that we have loaded the user's configuration, we can load the
# separate config file for logging (the name of that file will be specified in the config file)
# finally, set up all remaining configuration values
# that depend upon either default or user-supplied values
# to be called before loading any user specific values
# things to put here are
# 1. variables that the user cannot change
# 2. variables that need to be set before loading the user's config file
# get a logger
# load and parse the provided configFile, if provided
# load the config file
#work_dir must be provided before initialising other directories
# look for those items that are user-configurable, and get their values
# sptk_bindir= ....
# default place for some data
# a list instead of a dict because OrderedDict is not available until 2.7
# and I don't want to import theano here just for that one class
# each entry is a tuple of (variable name, default value, section in config file, option name in config file)
#
# the type of the default value is important and controls the type that the corresponding
# variable will have
#
# to set a default value of 'undefined' use an empty string
# or the special value 'impossible', as appropriate
#
## for glottHMM:
## for glottDNN:
## for sinusoidal:
## For MagPhase Vocoder:
# Containg natural speech waveforms (for acous feat extraction).
# Input-Output
## for joint duration
#+*']                                             ,    'Labels', 'silence_pattern'),
## For MagPhase Vocoder:
#('label_align_orig_const_rate_dir', os.path.join(self.work_dir, 'data/label_state_align'), 'Labels', 'label_align_orig_const_rate'),
## some config variables for token projection DNN
# RNN
# Data
# Keras Processes
## for GlottHMM:
## for GlottDNN:
## for sinusoidal:
## For MagPhase Vocoder:
## for joint dur:-
#            ('use_private_hidden'  , False, 'Streams', 'use_private_hidden'),
# fw_alpha: 'Bark' or 'ERB' allowing deduction of alpha, or explicity float value (e.g. 0.77)
## For MagPhase Vocoder:
#('use_magphase_pf'  ,True                 ,'Waveform'  , 'use_magphase_pf'), # Use MagPhase own Post-Filter (experimemental)
# Acoustic feature extraction
## GlottHMM
## GlottDNN
## sinusoidal
## For MagPhase Vocoder:
## joint dur
# this uses exec(...) which is potentially dangerous since arbitrary code could be executed
# first, look for a user-set value for this variable in the config file
# use default value, if there is one
# to be called after reading any user-specific settings
# because the values set here depend on those user-specific settings
# get a logger
# tools
# set input extension same as output for voice conversion
# check if any hidden layer is recurrent layer 
# switch to tensorflow
## create directories if not exists
# switch to keras
## create directories if not exists
# model files
# input-output normalization stat files
# define model file name
# predicted features directory
# string.lower for some architecture values
# force optimizer to adam if set to sgd
# set sequential training True if using LSTMs
# set default seq length for duration model
# rnn params
### RNN params
# batch training for RNNs
# set/limit batch size to 25
## num. of sentences in this case
###dimensions for the output features
### key name must follow the self.in_dimension_dict.
### If do not want to include dynamic feature, just use the same dimension as that self.in_dimension_dict
### if lf0 is one of the acoustic featues, the out_dimension_dict must have an additional 'vuv' key
### a bit confusing
###need to control the order of the key?
##dimensions for each raw acoustic (output of NN) feature
#            current_stream_hidden_size = 0
#            current_stream_weight = 0.0
#            stream_lr_ratio = 0.0
#                current_stream_hidden_size = self.stream_mgc_hidden_size
#                current_stream_weight      = self.stream_weight_mgc
#                current_stream_hidden_size = self.stream_bap_hidden_size
#                current_stream_weight      = self.stream_weight_bap
#                current_stream_hidden_size = self.stream_lf0_hidden_size
#                current_stream_weight      = self.stream_weight_lf0
#                current_stream_hidden_size = self.stream_vuv_hidden_size
#                current_stream_weight      = self.stream_weight_vuv
#                current_stream_hidden_size = self.stream_stepw_hidden_size
#                current_stream_weight      = self.stream_weight_stepw
#                current_stream_hidden_size = self.stream_sp_hidden_size
#                current_stream_weight      = self.stream_weight_sp
#                current_stream_hidden_size = self.stream_seglf0_hidden_size
#                current_stream_weight      = self.stream_weight_seglf0
## for GlottHMM (start)
#                current_stream_hidden_size = self.stream_F0_hidden_size
#                current_stream_weight      = self.stream_weight_F0
#                current_stream_hidden_size = self.stream_Gain_hidden_size
#                current_stream_weight      = self.stream_weight_Gain
#                current_stream_hidden_size = self.stream_HNR_hidden_size
#                current_stream_weight      = self.stream_weight_HNR
#                current_stream_hidden_size = self.stream_LSF_hidden_size
#                current_stream_weight      = self.stream_weight_LSF
#                current_stream_hidden_size = self.stream_LSFsource_hidden_size
#                current_stream_weight      = self.stream_weight_LSFsource
## for GlottHMM (end)
## for GlottDNN (start)
## for GlottDNN (end)
## for HMPD (start)
## for HMPD (end)
## For MagPhase Vocoder (start):
# Note: 'lf0' is set before. See above.
## For MagPhase Vocoder (end)
## for joint dur (start)
#                current_stream_hidden_size = self.stream_dur_hidden_size
#                current_stream_weight      = self.stream_weight_dur
## for joint dur (end)
#            logger.info('  current_stream_hidden_size: %d' % current_stream_hidden_size)
#            logger.info('  current_stream_weight: %d' % current_stream_weight)
#                if (current_stream_hidden_size <= 0 or current_stream_weight <= 0.0) and self.multistream_switch:
#                    logger.critical('the hidden layer size or stream weight is not corrected setted for %s feature' %(feature_name))
#                    raise
#                if self.multistream_switch:
#                    self.private_hidden_sizes.append(current_stream_hidden_size)
#                    self.stream_weights.append(current_stream_weight)
#        if not self.multistream_switch:
#            self.private_hidden_sizes = []
#            if self.stream_cmp_hidden_size > 0:
#                self.private_hidden_sizes.append(self.stream_cmp_hidden_size)
#            else:
#                self.private_hidden_sizes.append(self.hidden_layer_size[-1])  ## use the same number of hidden layers if multi-stream is not supported
#            self.stream_weights = []
#            self.stream_weights.append(1.0)
#                stream_lr_ratio = 0.5
#                if feature_name == 'lf0':
#                    stream_lr_ratio = self.stream_lf0_lr
#                if feature_name == 'vuv':
#                    stream_lr_ratio = self.stream_vuv_lr
#                self.stream_lr_weights.append(stream_lr_ratio)
### the new cmp is not the one for HTS, it includes all the features, such as that for main tasks and that for additional tasks
#            self.stream_lr_weights.append(0.5)
# to check whether all the input and output features' file extensions are here
## gHMM:
## gDNN
## HMPD
## For MagPhase Vocoder:
# Note: 'lf0' is set before. See above.
## joint dur
## hyper parameters for DNN. need to be setted by the user, as they depend on the architecture
###
#To be recorded in the logging file for reference
# input files
# set up the label processing
# currently must be one of two styles
# xpath_file_name is now obsolete - to remove
# get a logger
# logging configuration, see here for format description
# https://docs.python.org/2/library/logging.config.html#logging-config-fileformat
# what we really want to do is this dicitonary-based configuration, but it's only available from Python 2.7 onwards
#    logging.config.dictConfig(cfg.logging_configuration)
# so we will settle for this file-based configuration procedure instead
# open the logging configuration file
# load the logging configuration file into a string
# this means that cfg.log_config_file does not exist and that no default was provided
# NOTE: currently this will never run
# set up a default level and default handlers
# first, get the root logger - all other loggers will inherit its configuration
# default logging level is DEBUG (a highly-verbose level)
# add a handler to write to console
# and a formatter
# this means that open(...) threw an error
# inject the config lines for the file handler, now that we know the name of the file it will write to
# config file format doesn't allow leading white space on lines, so remove it with dedent
# pass that string as a filehandle
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# configuration for the input labels (features) for the DNN
#
# this currently supports
# * input labels can be any combination of HTS and XML style input labels
# * output features are numerical *only* (all strings are fully expanded into 1-of-n encodings, etc)
#
#
#
# this is all executable python code
#  so we need to define things before using them
#  that means the description is bottom-up
# we need to specify how any non-numerical (e.g., unicode string) features will be converted (mapped) into numerical feature vectors
# (just some examples for now)
# read additional maps from external files and add them to the 'maps' dictionary
#  each such file must define a dictionary of dictionaries called maps, in the same format as above
#  TO DO - avoid full paths here - import them from the main config file
# not sure this will work second time around - may not be able to import under the same module name ??
# how to extract features
# (just a few examples for now)
#
# each feature is a dictionary with various possible entries:
#   xpath: an XPATH that will extract the required feature from a segment target node of an Ossian XML utterance tree
#   hts:   a (list of) HTS pseudo regular expression(s) that match(es) part of an HTS label, resulting in a single boolean feature
#   mapper:   an optional function or dictionary which converts the feature value (e.g., a string) to a (vector of) numerical value(s)
#
# the dictionary describes how to compute that feature
# first, either xpath or hts describes how to extract the feature from a tree or label name
# then, an optional mapping converts the feature via a lookup table (also a dictionary) into a numerical value or vector
#
# if no mapper is provided, then the feature must already be a single numerical or boolean value
#
# some XPATH-based features
# in a future version, we could be more fleixble and allow more than one target_node type at once,
# with a set of XPATHs for each target_node - it would not be very hard to modify the code to do this
# the target nodes within the XML trees that the XPATH expressions apply to
# target_nodes = "//state" ???
# <segment pronunciation="t" cmanner="stop" cplace="alveolar" cvoiced="no" vfront="NA" vheight="NA" vlength="NA" vowel_cons="cons" vround="NA" start="1040" end="1090" has_silence="no">
# and the XPATH expressions to apply
# a composite "vector" of XPATH features
#  this is just an ordered list of features, each of which is a dictionary describing how to compute this feature
#  each feature may be a single numerical value or a vector of numerical values
# some HTS pseudo regular expression-based features
# all of these evaluate to a single boolean value, which will be eventually represented numerically
# note: names of features will need modifying to valid Python variable names (cannot contain "-", for example)
# a composite "vector" of HTS features
# the full feature vector
# + hts_labels
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# configuration for the input labels (features) for the DNN
#
# this currently supports
# * input labels can be any combination of HTS and XML style input labels
# * output features are numerical *only* (all strings are fully expanded into 1-of-n encodings, etc)
#
#
#
# this is all executable python code
#  so we need to define things before using them
#  that means the description is bottom-up
# we need to specify how any non-numerical (e.g., unicode string) features will be converted (mapped) into numerical feature vectors
# (just some examples for now)
## osw -- also make some maps automatically, only specifying list of values for brevity:
## strip special null values:
# read additional maps from external files and add them to the 'maps' dictionary
#  each such file must define a dictionary of dictionaries called maps, in the same format as above
#  TO DO - avoid full paths here - import them from the main config file
# not sure this will work second time around - may not be able to import under the same module name ??
# how to extract features
# (just a few examples for now)
#
# each feature is a dictionary with various possible entries:
#   xpath: an XPATH that will extract the required feature from a segment target node of an Ossian XML utterance tree
#   hts:   a (list of) HTS pseudo regular expression(s) that match(es) part of an HTS label, resulting in a single boolean feature
#   mapper:   an optional function or dictionary which converts the feature value (e.g., a string) to a (vector of) numerical value(s)
#
# the dictionary describes how to compute that feature
# first, either xpath or hts describes how to extract the feature from a tree or label name
# then, an optional mapping converts the feature via a lookup table (also a dictionary) into a numerical value or vector
#
# if no mapper is provided, then the feature must already be a single numerical or boolean value
#
# some XPATH-based features
# in a future version, we could be more fleixble and allow more than one target_node type at once,
# with a set of XPATHs for each target_node - it would not be very hard to modify the code to do this
# the target nodes within the XML trees that the XPATH expressions apply to
# and the XPATH expressions to apply
## NB: first feature is for silence trimming only:
## syll stress
## fine & coarse POS -- 3 word window
## === SIZES and DISTANCES till start/end -- these are numeric and not mapped:
## state in segment -- number states is fixed, so exclude size and only count in 1 direction
## segments in syll
## segments in word
## syll in word
## word in phrase
## syll in phrase
## segment in phrase
## X in utterance
#
# # a composite "vector" of XPATH features
# #  this is just an ordered list of features, each of which is a dictionary describing how to compute this feature
# #  each feature may be a single numerical value or a vector of numerical values
# xpath_labels =[
#
# # ll_segment,
# #  l_segment,
# #  c_segment,
# #  r_segment,
# # rr_segment,
#
# cmanner,
# cplace,
# cvoiced,
#
# vfront,
# vheight,
# vlength,
# vround,
#
# vowel_cons
# ]
#
# some HTS pseudo regular expression-based features
# all of these evaluate to a single boolean value, which will be eventually represented numerically
# note: names of features will need modifying to valid Python variable names (cannot contain "-", for example)
# a composite "vector" of HTS features
# the full feature vector
# + hts_labels
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# instantiate one object of this class
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
### whether dynamic features are needed for each data stream
## we assume static+delta+delta-delta
### merge the data: like the cmp file
### the real function to do the work
### need to be implemented for a specific format
### interpolate F0, if F0 has already been interpolated, nothing will be changed after passing this function
#        delta_win = [-0.5, 0.0, 0.5]
#        acc_win   = [1.0, -2.0, 1.0]
### compute dynamic features for a data matrix
###compute dynamic feature dimension by dimension
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#io_funcs.
###prepare_nn_data(self, in_file_list_dict, out_file_list, in_dimension_dict, out_dimension_dict):
#if os.path.isfile(out_file_name):
#    logger.info('processing file %4d of %4d : %s exists' % (i+1, self.file_number, out_file_name))
#    continue
## F0 added for GlottHMM
### if vuv information to be recorded, store it in corresponding column
### write data to file
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# this only extracts the static lf0 because we need to interpolate it, then add deltas ourselves later
#        delta_win = [-0.5, 0.0, 0.5]
#        acc_win   = [1.0, -2.0, 1.0]
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#        self.dimension_dict = dimension_dict
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# context-dependent printing format for Numpy - should move this out to a utility file somewhere
# a class that can compose input labels according to the user's specification, and convert them to numerical vectors
# what label styles we find in the feature specification
# e.g., 'xpath' , 'hts'
## will be set True if xpaths are compiled
# load in a label specification, provided by the user
# perform some sanity checks on it
#
# make sure 'labels' is defined
#osw# self.logger.debug('looking at feature %s' % feature_specification )
# feature is a dictionary specifying how to construct this part of the input feature vector
# xpath and hts are mutually exclusive label styles
# if there is a mapper, then we will use that to convert the features to numbers
# we need to look at the mapper to deduce the dimensionality of vectors that it will produce
# get an arbitrary item as the reference and measure its dimensionality
# make sure all other entries have the same dimension
#print '   add %s    cum: %s'%( str(l), self.label_dimension)
# without a mapper, features will be single numerical values
#print '   add 1    cum: %s'%( self.label_dimension)
# we have seen at least one feature that will required xpath label files to be loaded
# will become True once implemented
# not yet implemented !
## for frame features -- TODO: decide how to handle this properly
#print '   add 3   cum: %s'%(  self.label_dimension)
# a console handler
# not written test code for actual label processing - too complex and relies on config files
# from logplot.logging_plotting import LoggerPlotter #, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[2]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[3]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[4]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[5]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[6]
#+*'], label_type="state_align"):
## hard-coded silence duration
# remove state information [k]
## hard-coded silence duration
# from logplot.logging_plotting import LoggerPlotter #, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# this class only knows how to deal with a single style of labels (XML or HTS)
# (to deal with composite labels, use LabelComposer instead)
#  -----------------------------
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[2]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[3]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[4]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[5]
#-p+l=i:1_4/A/0_0_0/B/1-1-4:1-1&1-4#1-3$1-4>0-1<0-1|i/C/1+1+3/D/0_0/E/content+1:1+3&1+2#0+1/F/content_1/G/0_0/H/4=3:1=1&L-L%/I/0_0/J/4+3-1[6]
# this subclass support HTS labels, which include time alignments
#            self.question_dict, self.ori_question_dict = self.load_question_set(question_file_name)
###self.dict_size = len(self.question_dict)
## zhizheng's original 5 state features + 4 phoneme features
## the minimal features necessary to go from a state-level to frame-level model
## this is equivalent to a state-based system
## the phoneme level features only
## this is equivalent to a frame-based system without relying on state-features
## this is equivalent to a frame-based system with uniform state-features
## this is equivalent to a frame-based system with minimal features
## this is equivalent to a frame-based positioning system reported in Heiga Zen's work
### if user wants to define their own input, simply set the question set to empty.
### set default feature type to numerical, if not assigned ###
### set default unit size to state, if not assigned ###
### set default feat size to frame or phoneme, if not assigned ###
## phoneme/syllable/word
#', 'sil', 'pau', 'SIL']
# remove state information [k]
### for syllable and word positional information ###
### syllable ending information ###
##pos-bw and c-silences
### word ending information ###
### writing into dur_feature_matrix ###
#': ## removing silence here
# hard coded here 
### writing into dur_feature_matrix ###
# this is not currently used ??? -- it works now :D
#logger.critical('unused function ???')
#raise Exception
## hard coded for now
# to do - support different frame shift - currently hardwired to 5msec
# currently under beta testing: support different frame shift
#label_binary_vector = self.pattern_matching(full_label)
# if there is no CQS question, the label_continuous_vector will become to empty
## features which distinguish frame position in phoneme
# fraction through phone forwards
# fraction through phone backwards
# phone duration
## features which distinguish frame position in phoneme using three continous numerical features
## setting add_frame_features to False performs either state/phoneme level normalisation
# label_feature_matrix = numpy.empty((100000, self.dict_size+self.frame_feature_size))
# remove state information [k]
#                label_binary_vector = self.pattern_matching(full_label)
# if there is no CQS question, the label_continuous_vector will become to empty
## Zhizheng's original 9 subphone features:
## fraction through state (forwards)
## fraction through state (backwards)
## length of state in frames
## state index (counting forwards)
## state index (counting backwards)
## length of phone in frames
## fraction of the phone made up by current state
## fraction through phone (backwards)
## fraction through phone (forwards)
## features which only distinguish state:
## state index (counting forwards)
## features which distinguish frame position in phoneme:
## fraction through phone (counting forwards)
## features which distinguish frame position in phoneme:
## fraction through phone (counting forwards)
## state index (counting forwards)
## features which distinguish frame position in phoneme using three continous numerical features
## features which distinguish state and minimally frame position in state:
## fraction through state (forwards)
## state index (counting forwards)
## state index (counting forwards)
## hard coded for now
# hard coded here 
## fraction through state (forwards)
## fraction through state (backwards)
## length of state in frames
## state index (counting forwards)
## state index (counting backwards)
## length of phone in frames
## fraction of the phone made up by current state
## fraction through phone (forwards)
## fraction through phone (backwards)
### this function is not used now
### this function is not used now
# this function is where most time is spent during label preparation
#
# it might be possible to speed it up by using pre-compiled regular expressions?
# (not trying this now, since we may change to to XML tree format for input instead of HTS labels)
#
#                assert len(ms.group()) == 1
# regex for last question
#                print   line
# last question must only match at end of HTS label string
#save pre-compiled regular expression
#                question_index = question_index + 1
## handle HTK wildcards (and lack of them) at ends of label:
## convert remaining HTK wildcards * and ? to equivalent regex:
## don't use extra features beyond those in questions for duration labels:
## add_frame_features not used in HTSLabelNormalisation -- only in XML version
## remove empty lines
## take last entry -- ignore timings if present
# if there is no CQS question, the label_continuous_vector will become to empty
#  -----------------------------
#output_file_list = ['/afs/inf.ed.ac.uk/group/cstr/projects/blizzard_entries/blizzard2016/straight_voice/Hybrid_duration_experiments/dnn_tts_release/lstm_rnn/data/dur/AMidsummerNightsDream_000_000.dur']
#feature_type="binary"
#unit_size = "phoneme"
#feat_size = "phoneme"
#label_operater.prepare_dur_data(ori_file_list, output_file_list, feature_type, unit_size, feat_size)
#label_operater.prepare_dur_data(ori_file_list, output_file_list, feature_type)
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
## a generic class of linguistic feature extraction
##
##the feature dimensionality of output (should that read 'input' ?)
## the number of utterances to be normalised
## the ori_file_list contains the file paths of the raw linguistic data
## the output_file_list contains the file paths of the normalised linguistic data
##
## the exact function to do the work
## need to be implemented in the specific class
## the function will write the linguistic features directly to the output file
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#    def __init__(self, feature_dimension):
#        self.feature_dimension = feature_dimension
# setting the print options in this way seems to break subsequent printing of numpy float32 types
# no idea what is going on - removed until this can be solved
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
# setting the print options in this way seems to break subsequent printing of numpy float32 types
# no idea what is going on - removed until this can be solved
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# this is the wrong name for this logger because we can also normalise labels here too
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
## If we are to keep some columns unnormalised, use advanced indexing to
## reinstate original values:
#            norm_features = numpy.array(norm_features, 'float32')
#            fid = open(out_file_list[i], 'wb')
#            norm_features.tofile(fid)
#            fid.close()
# print   self.max_vector, self.min_vector
# logger.debug('reshaping fea_max_min_diff from shape %s to (1,%d)' % (fea_max_min_diff.shape, self.feature_dimension) )
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
# po=numpy.get_printoptions()
# numpy.set_printoptions(precision=2, threshold=20, linewidth=1000, edgeitems=4)
# restore the print options
# numpy.set_printoptions(po)
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
## use theano to benefit from GPU computation
###assume the delta and acc windows have the same length
#            WDW = dot(dot(WT_static, D_static), W_static) + dot(dot(WT_delta, D_delta), W_delta) + dot(dot(WT_acc, D_acc), W_acc)
#            WDU = dot(dot(WT_static, D_static), U_static) + dot(dot(WT_delta, D_delta), U_delta) + dot(dot(WT_acc, D_acc), U_acc)
#            temp_obs = dot(numpy.linalg.inv(WDW), WDU)
###only theano-dev version support matrix inversion
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# Adding this before the bandmat import lets us import .pyx files without running bandmat's setup.py:
#import pyximport; pyximport.install()
###assume the delta and acc windows have the same length
#        tau_frames.astype('float64')
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
## Added FAST_MLPG as a variable here, in case someone wants to use the slow version, but perhaps we
## should always use the bandmat version?
#io_funcs.
#    pass
# Debug:
#self.inf_float = -50000
# not really necessary to have the logger rembered in the class - can easily obtain it by name instead
# self.logger = logging.getLogger('param_generation')
## hard coding, try removing in future?
#            if feature_name != 'vuv':
#            else:
#                vuv_dimension = dimension_index
#                recorded_vuv = True
### fast version wants variance per frame, not single global one:
#                print  var.shape[1]
#                else:
#                    self.logger.critical("the dimensions do not match for MLPG: %d vs %d" %(var.shape[1], out_dimension_dict[feature_name]))
#                    raise
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#+*'], label_type="state_align", remove_frame_features=True,
# previsouly: continue -- in fact we should keep non-silent data!
## if labels have a few extra frames than audio, this can break the indexing, remove them:
## OSW: rewrote above more succintly
## hard coded for now
# to do - support different frame shift - currently hardwired to 5msec
# currently under beta testing: supports different frame shift
# remove state information [k]
# def load_binary_file(self, file_name, dimension):
#        fid_lab = open(file_name, 'rb')
#        features = numpy.fromfile(fid_lab, dtype=numpy.float32)
#        fid_lab.close()
#        features = features[:(dimension * (features.size / dimension))]
#        features = features.reshape((-1, dimension))
#        return  features
## In case they are different, resize -- keep label fixed as we assume this has
## already been processed. (This problem only arose with STRAIGHT features.)
## label is longer -- pad audio to match by repeating last frame:
## audio is longer -- cut it
# else: -- expected case -- lengths match, so do nothing
#         print silence_flag
## if it's all 0s or 1s, that's ok:
## get the indices where silence_flag == 0 is True (i.e. != 0)
# print silence_flag
## nonzero returns a tuple of arrays, one for each dimension of input array
## every_nth used +as step value in slice
## -1 due to weird error with STRAIGHT features at line 144:
## IndexError: index 445 is out of bounds for axis 0 with size 445
## avoid errors in case there is no silence
## Append to end of utt -- same function used for labels and audio
## means that violation of temporal order doesn't matter -- will be consistent.
## Later, frame shuffling will disperse silent frames evenly across minibatches:
##  ^---- from tuple and back (see nonzero note above)
## advanced integer indexing
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# HTK datatybes
# Additional 'param kind' options
#has energy
#absolute energy suppressed
#has delta coefficients
#has acceleration coefficients
#is compressed
#has zero mean static coef.
#has CRC checksum
#has 0th cepstral coef.
#has VQ data
#has third differential coef.
# the first 6 bits contain datatype
# HTK header
# number of samples in file (4-byte integer)
# sample period in 100ns units (4-byte integer)
# number of bytes per sample (2-byte integer)
# a code indicating the sample kind (2-byte integer)
#TODO compression
#self.A = struct.unpack('>H', f.read(2))[0]
#self.B = struct.unpack('>H', f.read(2))[0]
#                print   "world"
#            if(sys.byteorder=='little'):
#                print   "hello"
#                self.data.byteswap(True) # forces big-endian byte ordering
#if(sys.byteorder=='little'):
#    self.data.byteswap(True) # force big-endian byte ordering
#filename_src = "../data/GE001_1.feat"
#print "t", htk.dupa, sys.byteorder
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# get a logger
# this (and only this) logger needs to be configured immediately, otherwise it won't work
# we can't use the full user-supplied configuration mechanism in this particular case,
# because we haven't loaded it yet!
#
# so, just use simple console-only logging
# this level is hardwired here - should change it to INFO
# add a handler & its formatter - will write only to console
# first, set up some default configuration values
# next, load in any user-supplied configuration values
# that might over-ride the default values
# finally, set up all remaining configuration values
# that depend upon either default or user-supplied values
# to be called before loading any user specific values
# things to put here are
# 1. variables that the user cannot change
# 2. variables that need to be set before loading the user's config file
# get a logger
# load and parse the provided configFile, if provided
# load the config file
#work_dir must be provided before initialising other directories
# default place for some data
# Paths
# Input-Output
# Architecture
# RNN
# Data
# Processes
# this uses exec(...) which is potentially dangerous since arbitrary code could be executed
# default value
# first, look for a user-set value for this variable in the config file
# use default value, if there is one
# to be called after reading any user-specific settings
# because the values set here depend on those user-specific settings
# get a logger
## create directories if not exists
# input-output normalization stat files
# define model file name
# model files
# predicted features directory
# string.lower for some architecture values
# set sequential training True if using LSTMs
# set/limit batch size to 25
## num. of sentences in this case
# rnn params
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
############################
##### Memory variables #####
############################
### read file by file ###
### read file by file ###
### read file by file ###
### read file by file ###
### read file by file ###
### shuffle train id list ###
## shuffle by sentence
## shuffle by a group of sentences
#### normalize training data ####
#### load norm stats ####
#### normalize data ####
#### de-normalize data ####
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# create model
# add hidden layers
# add output layer
# Compile the model
# add hidden layers
# add output layer
# Compile the model
# params
# add hidden layers
#go_backwards=True))
# add output layer
# Compile the model
# serialize model to JSON
# serialize weights to HDF5
#### load the model ####
#### compile the model ####
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#### TODO: Find a good way to pass below params ####
### if batch size is equal to 1 ###
#self.model.fit(temp_train_x, temp_train_y, epochs=1, shuffle=False, verbose=0)
### if batch size more than 1 ###
### Method 1 ###
### Method 2 ###
### Method 3 ###
### Method 3 ###
### Method 4 ###
#### compute predictions ####
### refer Zhizheng and Simon's ICASSP'16 paper for more details
### http://www.zhizheng.org/papers/icassp2016_lstm.pdf
#(1-p) *
# random initialisation
# Input gate weights
# bias
# initial value of hidden and cell state
#
#(1-p) *
# random initialisation
# Input gate weights
# Output gate weights
# bias
# initial value of hidden and cell state and output
#
# ensure sizes have integer type
# ensure sizes have integer type
# random initialisation
# Input gate weights
# random initialisation
# Forget gate weights
# random initialisation
# Output gate weights
# random initialisation
# Cell weights
# bias
### make a layer
# initial value of hidden and cell state
# random initialisation
# Input gate weights
# random initialisation
# Output gate weights
# random initialisation
# Forget gate weights
# random initialisation
# Output gate weights
# random initialisation
# Cell weights
# bias
### make a layer
# initial value of hidden and cell state
#
#
#, i_t, f_t, o_t
#
#
#, i_t, f_t, o_t
#self.w_cf * c_tm1 
#
#f_t *
#
#i_t *
#
#
#i_t *
#self.W_ci,
#self.W_cf,
#self.W_co,
#self.w_cf * c_tm1
##can_h_t = T.tanh(Whx + r_t * T.dot(h_tm1, self.W_hh) + self.b_h)
#self.w_cf * c_tm1
#        c_t = f_t * c_tm1 + (1 - f_t) * T.tanh(Wcx + T.dot(h_tm1, self.W_hc) + self.b_c)
#        h_t = T.tanh(c_t)
# Gated Recurrent Unit
## pre-compute these for fast computation
#
## in order to have the same interface as LSTM
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# assume linear output for mean vectors
#self.sigma = T.nnet.softplus(T.dot(self.input, self.W_sigma)) # + 0.0001
# Zen et al. 2014
# hard variance flooring
# note: sigma contains variances, so var_floor=0.01 means that
# the lowest possible standard deviation is 0.1
# ensure sizes have integer type
# ensure sizes have integer type
# initialize with 0 the weights W as a matrix of shape (n_in, n_out)
# ensure sizes have integer type
# ensure sizes have integer type
# initialize with 0 the weights W as a matrix of shape (n_in, n_out)
# ensure sizes have integer type
# ensure sizes have integer type
# initialize with 0 the weights W as a matrix of shape (n_in, n_out)
## rectifier linear unit
## rectifier smooth unit
# parameters of the model
#        self.output = self.rectifier_linear(lin_output)
# parameters of the model
#self.n_in = n_in
#        self.output = self.rectifier_linear(lin_output)
# parameters of the model
#self.n_in = n_in
# W_values = numpy.asarray(rng.uniform(low=-0.02, high=0.02,
## TODO -- generalise to other n_modes and higher deimneionsal CVs
#initial_W = numpy.asarray( numpy_rng.uniform(
#          low  = -4*numpy.sqrt(6./(n_hidden+n_visible)),
#          high =  4*numpy.sqrt(6./(n_hidden+n_visible)),
#          size = (n_visible, n_hidden)),
#                           dtype = theano.config.floatX)
# first layer, use Gaussian noise
## rectifier linear unit
## rectifier smooth unit
#if corruption_level == 0:
#    tilde_x = self.x
#else:
#    tilde_x = self.get_corrupted_input(self.x, corruption_level)
# tilde_x = self.get_corrupted_input(self.x, corruption_level, 0.5)
# initialize with 0 the weights W as a matrix of shape (n_in, n_out)
# random initialisation 
# Input gate weights
# random initialisation 
# Forget gate weights
# random initialisation 
# Output gate weights
# random initialisation 
# Cell weights
# bias
# scaling factor
### make a layer
# initial value of hidden and cell state
#
# 
#, i_t, f_t, o_t################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#assume linear output for mean vectors
# + 0.0001
#self.sigma = T.exp(T.dot(self.input, self.W_sigma)) # + 0.0001
# initialize with 0 the weights W as a matrix of shape (n_in, n_out)
# initialize with 0 the weights W as a matrix of shape (n_in, n_out)
# randomly initialise the activation weights based on the input size, as advised by the 'tricks of neural network book'
## rectifier linear unit
## rectifier smooth unit
# parameters of the model
#        self.output = self.rectifier_linear(lin_output)
# parameters of the model
# create a Theano random generator that gives symbolic random values
# first layer, use Gaussian noise
# tilde_x = self.get_corrupted_input(self.x, corruption_level, 0.5)
#(1-p) *
# random initialisation
#Wy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_h)), dtype=config.floatX)
#Uy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_out)), dtype=config.floatX)
# identity matrix initialisation
#Wh_value = np.asarray(np.eye(n_h, n_h), dtype=config.floatX)
#Uh_value = np.asarray(np.eye(n_in, n_out), dtype=config.floatX)
# Input gate weights
# Output gate weights
# bias
# initial value of hidden and cell state and output
# simple recurrent decoder params
#self.params = [self.W_xi, self.W_hi, self.W_yi, self.U_hi, self.b_i, self.b]
# recurrent output params and additional input params
#
# simple recurrent decoder
#y_t = T.dot(h_t, self.U_hi) + self.b
# recurrent output and additional input
#(1-p) *
# random initialisation
#Wh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_h)), dtype=config.floatX)
#Wy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_h)), dtype=config.floatX)
#Uh_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_h), size=(n_h, n_out)), dtype=config.floatX)
#Uy_value = np.asarray(rng.normal(0.0, 1.0/np.sqrt(n_out), size=(n_out, n_out)), dtype=config.floatX)
# identity matrix initialisation
# Input gate weights
# Output gate weights
# bias
# initial value of hidden and cell state and output
# hard coded to remove coarse coding features
# recurrent output params and additional input params
#
# random initialisation
# Input gate weights
# random initialisation
# Output gate weights
# random initialisation
# Forget gate weights
# random initialisation
# Output gate weights
# random initialisation
# Cell weights
# bias
### make a layer
# initial value of hidden and cell state
#
#
#, i_t, f_t, o_t
#self.w_cf * c_tm1 
# ensure sizes have integer type
# ensure sizes have integer type
# random initialisation
# Input gate weights
# random initialisation
# Forget gate weights
# random initialisation
# Output gate weights
# random initialisation
# Cell weights
# bias
### make a layer
# initial value of hidden and cell state
# hard coded to remove coarse coding features
#
#
#, i_t, f_t, o_t
#(1-p) *
# random initialisation
# Input gate weights
# bias
# initial value of output
#
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# NOTES
# still to consider: pygal, for HTML5 SVG plotting
# this module provides the base classes that we specialise here
# as logging
# for plotting
# should make this user-configurable - TO DO later
# this line has to come before the import of matplotlib.pyplot
# matplotlib needs to be passed numpy arrays
# for sorting tuples
# TO DO - this needs to be attached to the logging module so that it's available via config options
# class PlotHandler(logging.FileHandler):
#     """A handler for saving plots to disk"""
#     def __init__(self,filename):
#         logging.FileHandler.__init__(self,filename, mode='a', encoding=None, delay=False)
# a generic plot object that contains both the underlying data and the plot itself
# this class needs to be subclassed for each specialised type of plot that we want
# the underlying data for the plot - a dictionary of data series
# each series is a list of data points of arbitrary type (e.g., tuples, arrays, ..)
# the plot generated from these data
# clear the data series
# if there is no data series with this name yet, create an empty one
# append this data point (e.g., it might be a tuple (x,y) )
# don't worry about data type or sorting - that is not our concern here
# only applied if the data points are tuples, such as (x,y) values
# TO DO: first check that each series is a list of tuples, and that they have the same number of elements
# this method checks that all data series
# 1. have the same length
# 2. are sorted in ascending order of x
# 3. have identical values in their x series
# there has to be at least one data series
# check lengths are consistent, sort, then check x values are identical
# print "starting with self.data=",self.data
# sort by ascending x value
# extract a list of just the x values
# print "ending with self.data=",self.data
# raise an exception here?
# a plot with one or more time series sharing a common x axis:
# e.g., the training error and the validation error plotted against epochs
# sort the data series and make sure they are consistent
# if there is a plot already in existence, we will clear it and re-use it;
# this avoids creating extraneous figures which will stay in memory
# (even if we are no longer referencing them)
# create a plot
# TO DO - better filename configuration for plots
## still plotting multiple image in one figure still has problem. the visualization is not good
#, bbox_inches='tight'
#class MultipleLinesPlot(PlotWithData):
#    def generate_plot(self, filename, title='', xlabel='', ylabel=''):
# a dictionary to store all generated plots
# keys are plot names
# values are
# where the plots will be saved - a directory
# default location
# initialise the logging parent class
# (should really use 'super' here I think, but that fails - perhaps because the built in logger class is not derived from 'object' ?)
# add a data point to a named plot
# raise an exception here?
# # the filename to save to is known by the handler, which needs to be assigned to this logger
# # look at the handlers attached to this logger instance
# ph=None
# for h in self.handlers:
#     # we want an instance of a PlotHandler - we'll take the first one we find
#     # (behaviour will be unpredictable if there is more than one handler of this type)
#     if isinstance(h,PlotHandler):
#         ph=h
#         break
# if ph:
# TO DO - need to be sure of safe file names
# else:
#     logger.warn('No handler of type PlotHandler is attached to this logger - cannot save plots')
# colourising formatter adapted from an answer to this question on Stack Overflow
# http://stackoverflow.com/questions/384076/how-can-i-color-python-logging-output
# terminal escape sequences
# pad to fixed width - currently hardwired, should make this dynamic
# maximum width of level names, which is the 8 characters of "CRITICAL"
# The background is set with 40 plus the number of the color, and the foreground with 30
# some simple tests
# tell the built-in logger module to use our custom class when instantiating any new logger
# a console handler
# handler for plotting logger - will write only to console
# # need a handler which will control where to save plots
# ph = PlotHandler("/tmp/plot_test/testing.pdf")
# logger.addHandler(ph)
# the first argument is just a key for referring to this plot within the code
# the second argument says what kind of plot we will be making
# momentum
#+ self.L2_reg * self.L2_sqr
## added for LHUC
# In lhuc the parameters are only scaling parameters which have the name 'c'
# use optimizer
# zip just concatenate two lists
# freeze layers and update weights
#index, batch_size
#[index*batch_size:(index + 1)*batch_size]
#, batch_size
## the function to output activations at a hidden layer
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
###THEANO_FLAGS='cuda.root=/opt/cuda-5.0.35,mode=FAST_RUN,device=gpu0,floatX=float32,exception_verbosity=high' python dnn.py
# allocate symbolic variables for the data
##T.nnet.sigmoid)  #
# add final layer
### MSE
### L1-norm
### L2-norm
# compute number of minibatches for training, validation and testing
# index to a [mini]batch
##top 2 layers use a smaller learning rate
##hard-code now, change it later
# compute list of fine-tuning updates
# compute the gradients with respect to the model parameters
## Theano's default
## Retain learning rate and momentum to make interface backwards compatible,
## even with RPROP where we don't use them, means we have to use on_unused_input='warn'.
# Create a function that scans the entire validation set
#, batch_size
## the function to output activations at a hidden layer
#    test_DBN(train_scp, valid_scp, log_dir, model_dir, n_ins, n_outs, hidden_layers_sizes,
#             finetune_lr, pretraining_epochs, pretrain_lr, training_epochs, batch_size)
###THEANO_FLAGS='cuda.root=/opt/cuda-5.0.35,mode=FAST_RUN,device=gpu0,floatX=float32,exception_verbosity=high' python dnn.py
# as np
#cudamat
#import theano
#import theano.tensor as T
#output layer
#        (train_set_x, train_set_y) = train_xy
# assuming linear output and square error cost function
# final layer is linear output, gradient is one
#output layers
# + self.b_params[i] * self.l2_reg
#update weights and record momentum weights
#        print   self.W_params[0].shape, self.W_params[len(self.W_params)-1].shape
#    def parameter_prediction(self, test_set_x):  #, batch_size
#        n_test_set_x = test_set_x.get_value(borrow=True).shape[0]
#        test_out = theano.function([], self.final_layer.output,
#              givens={self.x: test_set_x[0:n_test_set_x]})
#        predict_parameter = test_out()
#        return predict_parameter
#    test_DBN(train_scp, valid_scp, log_dir, model_dir, n_ins, n_outs, hidden_layer_sizes,
#             finetune_lr, pretraining_epochs, pretrain_lr, training_epochs, batch_size)
### sequence-to-sequence mapping ###
# vanilla encoder-decoder (phone-level features)
# hierarchical encoder-decoder
# hidden layer activation
# momentum
#+ self.L2_reg * self.L2_sqr
## added for LHUC
# In lhuc the parameters are only scaling parameters which have the name 'c'
# use optimizer
# zip just concatenate two lists
# freeze layers and update weights
#index, batch_size
#[index*batch_size:(index + 1)*batch_size]
# momentum
#+ self.L2_reg * self.L2_sqr
# zip just concatenate two lists
# momentum
#+ self.L2_reg * self.L2_sqr
# use optimizer
# zip just concatenate two lists
#, batch_size
#, batch_size
#, batch_size
#, batch_size
#, batch_size
## the function to output activations at a hidden layer
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
###THEANO_FLAGS='cuda.root=/opt/cuda-5.0.35,mode=FAST_RUN,device=gpu0,floatX=float32,exception_verbosity=high' python dnn.py
# allocate symbolic variables for the data
##T.nnet.sigmoid)  #
### Maximum likelihood
#n_component
# normalise by mean_log_det
# lines to compute debugging information for later printing
#self.errors = T.min(T.min(T.log(sigma), axis=1))
#self.errors = T.mean(T.sum(T.log(sigma), axis=1)) # computes mean_log_det
#self.errors = -xEx # (vector quantity) should be about 0.5 * beta * n_outs
#self.errors = point_fit  # (vector quantity) should be about one
#self.errors = T.mean(T.exp(exponent)) / T.exp(T.max(exponent)) # fraction of the data used, should be about efficiency
#self.errors = T.mean(point_fit) # should be about one
#self.errors = log_det_mult # (vector quantity) about zero, or always less if using Rprop
#self.errors = beta_obj # (vector quantity) objective function terms
#self.errors = self.finetune_cost # disable this line below when debugging
#n_component
#self.errors = self.finetune_cost
# disable this line if debugging beta_opt
# compute number of minibatches for training, validation and testing
# index to a [mini]batch
##top 2 layers use a smaller learning rate
# compute list of fine-tuning updates
# compute the gradients with respect to the model parameters
## retain learning rate and momentum to make interface backwards compatible,
## but we won't use them, means we have to use on_unused_input='warn'.
## Otherwise same function for RPROP or otherwise -- can move this block outside if clause.
# Create a function that scans the entire validation set
#, batch_size
#, batch_size
#, batch_size
#    test_DBN(train_scp, valid_scp, log_dir, model_dir, n_ins, n_outs, hidden_layers_sizes,
#             finetune_lr, pretraining_epochs, pretrain_lr, training_epochs, batch_size)
### default seq-to-seq model: tile C as input to all frames ###
### default seq-to-seq model: tile C as input to all frames ###
### Distributed seq-to-seq model: tile C_1-C_n as input to corresponding decoder frames ###
#output layer
#        for i in xrange(25):
#            static_indice.append(i+184)
#            delta_indice.append(i+184+25)
#            acc_indice.append(i+184+50)
#            print   sub_std_mat
#            print   sub_o_std_vec, var_base[sub_dim_start*3:sub_dim_start*3+sub_dim*3] ** 0.5
#            temp_obs_err_vec = gnp.dot(traj_err.T, wuwwu)
#            print   obs_err_vec, temp_obs_err_vec
#            print   obs_err_vec.shape, temp_obs_err_vec.shape
#            print   obs_mu, mlpg_traj, ref_y
#            print   obs_err_vec.shape, sub_o_std_vec.shape, frame_number, wuwwu.shape, traj_err.shape
# final layer is linear output, gradient is one
#output layers
# + self.b_params[i] * self.l2_reg
#update weights and record momentum weights
#        self.errors = gnp.sum((self.final_layer_output - train_set_y) ** 2, axis=1)
#mlpg_traj ref_y
#        for i in xrange(len(self.W_params)):
#############following function for MLPG##################
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# get a logger
# this (and only this) logger needs to be configured immediately, otherwise it won't work
# we can't use the full user-supplied configuration mechanism in this particular case,
# because we haven't loaded it yet!
#
# so, just use simple console-only logging
# this level is hardwired here - should change it to INFO
# add a handler & its formatter - will write only to console
# first, set up some default configuration values
# next, load in any user-supplied configuration values
# that might over-ride the default values
# finally, set up all remaining configuration values
# that depend upon either default or user-supplied values
# to be called before loading any user specific values
# things to put here are
# 1. variables that the user cannot change
# 2. variables that need to be set before loading the user's config file
# get a logger
# load and parse the provided configFile, if provided
# load the config file
#work_dir must be provided before initialising other directories
# default place for some data
# Paths
# Input-Output
# Architecture
# RNN
#encoder_decoder
# Data
# Processes
# this uses exec(...) which is potentially dangerous since arbitrary code could be executed
# default value
# first, look for a user-set value for this variable in the config file
# use default value, if there is one
# to be called after reading any user-specific settings
# because the values set here depend on those user-specific settings
# get a logger
## create directories if not exists
# input-output normalization stat files
# define model file name
# model files
# predicted features directory
# string.lower for some architecture values
# set sequential training True if using LSTMs
# set/limit batch size to 25
## num. of sentences in this case
# rnn params
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
############################
##### Memory variables #####
############################
### read file by file ###
### read file by file ###
### read file by file ###
### read file by file ###
### read file by file ###
### shuffle train id list ###
## shuffle by sentence
## shuffle by a group of sentences
#### normalize training data ####
#### load norm stats ####
#### normalize data ####
#### de-normalize data ####
#!/usr/bin/env python
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#self.session=tf.InteractiveSession()
#self.activation    ={"tanh":tf.nn.tanh,"sigmoid":tf.nn.sigmoid}
#self.saver=tf.train.Saver()
#  stacked_rnn_outputs=tf.reshape(rnn_outputs,[-1,self.n_out])
#  stacked_outputs=tf.layers.dense(stacked_rnn_outputs,self.n_out)
#  output_layer=tf.reshape(stacked_outputs,[-1,utt_length,self.n_out])
##This method is only used when a sequence model is TrainTensorflowModels
#return self._cell(dropout(inputs,self._input_keep_prob,is_training=self.is_training,scope=None),state,scope=None)
#    print pooling_outputs.shape
# print projection_layer.shape
#  print layer_list[-1].shape
#enc_state=(tf.concat(enc_state[0])
# Reset gate and update gate.
# We start with bias of 1.0 to not reset and not update.
#!/usr/bin/env python
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#### TODO: Find a good way to pass below params ####
#rs=sess.run(merged,feed_dict={input_layer:x_batch,output_data:y_batch,is_training_drop:True,is_training_batch:True})
#rs=sess.run(merged,feed_dict={input_layer:x_batch,output_data:y_batch,is_training_batch:True})
#if self.dropout_rate!=0.0:
#        training_loss=loss.eval(feed_dict={input_layer:train_x,output_data:train_y,is_training_drop:False,is_training_batch:False})
#     else:
#        training_loss=loss.eval(feed_dict={input_layer:train_x,output_data:train_y,is_training_batch:False})
#Data Preparation
#Shuffle the data
#overall_loss=tf.summary.scalar("training loss",overall_loss)
#summary_writer.add_summary(overall_loss,epoch)
#if self.dropout_rate!=0.0:
#    if hybrid:
#       training_loss=loss.eval(feed_dict={input_layer:temp_train_x,output_data:temp_train_y,utt_length_placeholder:utt_length,\
#       is_training_drop:False,is_training_batch:False})
#    else:
#       training_loss=loss.eval(feed_dict={input_layer:temp_train_x,output_data:temp_train_y,utt_length_placeholder:utt_length,\
#       is_training_drop:False})
#elif hybrid:
#    training_loss=loss.eval(feed_dict={input_layer:temp_train_x,output_data:temp_train_y,utt_length_placeholder:utt_length,is_training_batch:False})
#else:
#    training_loss=loss.eval(feed_dict={input_layer:temp_train_x,output_data:temp_train_y,utt_length_placeholder:utt_length})
#model_name="sequence_model"+" hybrid.ckpt" if hybrid==1 else "sequence_model.ckpt"
#### compute predictions ####
#if self.cbhg:
#    training_loss=loss.eval(feed_dict={inputs_data:temp_train_x,targets:temp_train_y,target_sequence_length:utt_length})
#else:
#    training_loss=loss.eval(feed_dict={inputs_data:temp_train_x,targets:temp_train_y,inputs_sequence_length:utt_length,target_sequence_length:utt_length})
#### compute predictions ####
#utt_length=[len(utt) for utt in test_x.values()]
#max_step=max(utt_length)
#dec_cell=self.graph.get_collection("decoder_cell")[0]
#  outputs=sess.run(inference_output,{inputs_data:temp_test_x,inputs_sequence_length:utt_length,\
#            target_sequence_length:utt_length})
#   #print _outputs[:,t,:]
#(Decay the first moment running average coefficient)
# (Update biased first moment estimate)
# (Update biased second raw moment estimate)
# (Compute bias-corrected first moment estimate)
# (Compute bias-corrected second raw moment estimate)
#(Update parameters)
#updates.append((m_previous, m))
#updates.append((v_previous, v))
#updates.append((theta_previous, theta) )
#updates.append((m, m_t))
#updates.append((v, v_t))
#updates.append((p, p_t))
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
# Force matplotlib to not use any Xwindows backend.
## then update all by default
## 1, 2, 3, 4: Rprop+ Rprop- iRprop+ iRprop-
##    in Igel 2003 'Empirical evaluation of the improved Rprop learning algorithms'
## It would be easier to follow if these things were defined in __init__, but
## they are here to keep all RPROP-specific stuff in one place.
## Also, make some or all
## rprop_init_update is configured during __init__, all of the others are hardcoded here
## for now:-
## first update update_values:
## apply floor/ceiling to updates:
## zero gradients where sign changed: reduce step size but don't change weight
## then update params:
## store previous iteration gradient to check for sign change in next iteration:
# gparam # sign_change_test #  update_changes    #
#m,n = numpy.shape(vals)
## This is generic, and not specific to RPROP:
## only keep bottom one
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#/usr/bin/python -u
#from utils import GlobalCfg
# Logging:
# File setup:
# Feat extraction:
## MagPhase Vocoder:
# TODO: Add WORLD and STRAIGHT
# If vocoder is not supported:
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#            print   reference_vuv
#        f0_mse = numpy.sum((((voiced_ref_data) - (voiced_gen_data)) ** 2))
# ** 0.5
# accept the difference upto two frames
# ** 0.5
# -*- coding: utf-8 -*-
#
# Copyright 2016 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
### save acoustic normalisation information for normalising the features back
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#/usr/bin/python -u
#from utils import GlobalCfg
#import configuration
# cannot have these outside a function - if you do that, they get executed as soon
# as this file is imported, but that can happen before the configuration is set up properly
# SPTK     = cfg.SPTK
# NND      = cfg.NND
# STRAIGHT = cfg.STRAIGHT
# a convenience function instead of calling subprocess directly
# this is so that we can do some logging and catch exceptions
# we don't always want debug logging, even when logging level is DEBUG
# especially if calling a lot of external functions
# so we can disable it by force, where necessary
# the following is only available in later versions of Python
# rval = subprocess.check_output(args)
# bufsize=-1 enables buffering and may improve performance compared to the unbuffered case
# better to use communicate() than read() and write() - this avoids deadlocks
# for critical things, we always log, even if log==False
# not sure under what circumstances this exception would be raised in Python 2.6
# not sure if there is an 'output' attribute under 2.6 ? still need to test this...
# try to kill the subprocess, if it exists
# this means that p was undefined at the moment of the keyboard interrupt
# (and we do nothing)
#    NND      = cfg.NND
## to be moved
### post-filtering
###mgc to sp to wav
### If using world v2, please comment above line and uncomment this
#run_process('{mgc2sp} -a {alpha} -g 0 -m {order} -l {fl} -o 0 {bap} | {sopr} -d 32768.0 -P | {x2x} +fd > {ap}'
#            .format(mgc2sp=SPTK['MGC2SP'], alpha=cfg.fw_alpha, order=cfg.bap_dim, fl=cfg.fl, bap=bap_file_name, sopr=SPTK['SOPR'], x2x=SPTK['X2X'], ap=files['ap']))
# Import MagPhase and libraries:
## STRAIGHT or WORLD vocoders:
## MagPhase Vocoder:
# Add your favorite vocoder here.
# If vocoder is not supported:
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#remove potential empty lines and end of line signs
# get file lengths
# set training algo
# set batch size
# set RNN batch training True
# set params for each training algo
### read file by file ###
## we allow small difference here. may not be correct, but sometimes, there is one/two frames difference
## set sequence length for batch training 
# set seq length to maximum seq length from current batch
# set seq length to maximum seq length from current bucket
# seq length is set based on default/user configuration 
### read file by file ###
# choose utterance from current bucket list
# choose utterance randomly from current file list 
#self.utt_index = numpy.random.randint(self.list_size)
## choose utterance in serial order
# break for any of the below conditions
## we allow small difference here. may not be correct, but sometimes, there is one/two frames difference
# reshape input-output
### MLU features sub-division ###
### duration array sub-division ###
### additional feature matrix (syllable+phone+frame=432) ###
### input word feature matrix ###
### rest of the code similar to S2S ###
### MLU features sub-division ###
### duration array sub-division ###
### additional feature matrix (syllable+phone+frame=432) ###
### input word feature matrix ###
### for batch processing ###
### rest of the code similar to S2S ###
### first check whether there are remaining data from previous utterance
## we allow small difference here. may not be correct, but sometimes, there is one/two frames difference
## if current utterance cannot be stored in the block, then leave the remaining part for the next block
#        temp_set_x = self.make_shared(temp_set_x, 'x')
#        temp_set_y = self.make_shared(temp_set_y, 'y')
##ListDataProvider.__init__(x_file_list, \
##         y_file_list, n_ins=0, n_outs=0, buffer_size = 500000, shuffle=False)
## Put this function at global level so it can be imported for use in dnn_generation
## Turn indexes to words, syllables etc. to one-hot data:
#print projection_indices.tolist()
## Used advanced indexing to turn the relevant features on:
## check conversion???!?!?!
#     print projection_indices.tolist()
#     print '            ^--- proj indices'
#     print
## Effectively remove the index from the original data by setting to 0:
## Turn indexes to words, syllables etc. to one-hot data:
#print projection_indices.tolist()
## check conversion???!?!?!
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://github.com/CSTR-Edinburgh/merlin
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#  quick and dirty utility to print out binary files, for debugging
# import numpy
## shall we read the logging config file from command line?
# print features
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
# the new class for label composition and normalisation
#from models.ms_dnn import MultiStreamDNN
#from models.ms_dnn_gv import MultiStreamDNNGv
#from models.sdae import StackedDenoiseAutoEncoder
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
#     private_l2_reg  = float(hyper_params['private_l2_reg'])
#     stream_weights       = hyper_params['stream_weights']
#     private_hidden_sizes = hyper_params['private_hidden_sizes']
#     stream_lr_weights = hyper_params['stream_lr_weights']
#     use_private_hidden = hyper_params['use_private_hidden']
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
# too many consecutive epochs without surpassing the best model
#    visualize_dnn(dnn_model)
#        predicted_parameter = test_out()
### write to cmp file
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
## + cfg.appended_input_dim
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
#    nn_label_norm_mvn_dir = os.path.join(data_dir, 'nn_no_silence_lab_norm_'+suffix)
# to do - sanity check the label dimension here?
# simple HTS labels
## don't remove silences for duration
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output acoustic data
#[-0.5, 0.0, 0.5]
#[1.0, -2.0, 1.0]
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
# save to itself
## don't remove silences for duration
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# logger.debug(' value was\n%s' % cmp_norm_info)
# logger.debug(' value was\n%s' % feature_std_vector)
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
### DNN model training
# not an error - just means directory already exists
# print   'start DNN'
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
### generate parameters from DNN
# not an error - just means directory already exists
#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
##MCD
##MCD
#+*'])
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
# the new class for label composition and normalisation
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
##basic model is ready.
##if corruption levels is set to zero. it becomes normal autoencoder
##model is ready, but the hyper-parameters are not optimised.
## not fully ready
## if pretraining is supported in one model, add the switch here
## be careful to use autoencoder for pretraining here:
## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]
## however, tanh works better and converge fast in finetuning
##
## Will extend this soon...
## in SDAE we do layer-wise pretraining using autoencoders
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
# too many consecutive epochs without surpassing the best model
#    visualize_dnn(dbn)
#        predicted_parameter = test_out()
### write to cmp file
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
#    nn_label_norm_mvn_dir = os.path.join(data_dir, 'nn_no_silence_lab_norm_'+suffix)
# to do - sanity check the label dimension here?
# simple HTS labels
#        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list)
#        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = ['*-#+*'])
#        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output acoustic data
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
#+*'])
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# logger.debug(' value was\n%s' % cmp_norm_info)
# logger.debug(' value was\n%s' % feature_std_vector)
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
#    nnets_file_name = '%s/%s_%s_%d.%d.%d.%d.%d.train.%d.model' \
#                       %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),
#                        len(hidden_layers_sizes), hidden_layers_sizes[0],
#                        lab_dim, cfg.cmp_dim, cfg.train_file_number)
### DNN model training
# not an error - just means directory already exists
# print   'start DNN'
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
### generate parameters from DNN
# not an error - just means directory already exists
# not an error - just means directory already exists
#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
#+*'])
##MCD
##MCD
#+*'])
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
#+*'])
# this can be removed
#
#to calculate distortion of HMM baseline
#+*'])
#+*'])
#+*'])
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
################################################################################
#           The Neural Network (NN) based Speech Synthesis System
#                https://svn.ecdf.ed.ac.uk/repo/inf/dnn_tts/
#
#                Centre for Speech Technology Research
#                     University of Edinburgh, UK
#                      Copyright (c) 2014-2015
#                        All Rights Reserved.
#
# The system as a whole and most of the files in it are distributed
# under the following copyright and conditions
#
#  Permission is hereby granted, free of charge, to use and distribute
#  this software and its documentation without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of this work, and to
#  permit persons to whom this work is furnished to do so, subject to
#  the following conditions:
#
#   - Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   - Redistributions in binary form must reproduce the above
#     copyright notice, this list of conditions and the following
#     disclaimer in the documentation and/or other materials provided
#     with the distribution.
#   - The authors' names may not be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
#  THE UNIVERSITY OF EDINBURGH AND THE CONTRIBUTORS TO THIS WORK
#  DISCLAIM ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
#  ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT
#  SHALL THE UNIVERSITY OF EDINBURGH NOR THE CONTRIBUTORS BE LIABLE
#  FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#  WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
#  AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION,
#  ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF
#  THIS SOFTWARE.
################################################################################
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
# the new class for label composition and normalisation
#from models.ms_dnn import MultiStreamDNN
#from models.ms_dnn_gv import MultiStreamDNNGv
#from models.sdae import StackedDenoiseAutoEncoder
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
#        dnn_model = DNN(numpy_rng=numpy_rng, n_ins=n_ins, n_outs = n_outs,
#                        l1_reg = l1_reg, l2_reg = l2_reg,
#                         hidden_layers_sizes = hidden_layers_sizes,
#                          hidden_activation = hidden_activation,
#                          output_activation = output_activation)
## We can't just unpickle the old model and use that because fine-tune functions
## depend on opt_l2e option used in construction of initial model. One way around this
## would be to unpickle, manually set unpickled_dnn_model.opt_l2e=True and then call
## unpickled_dnn_model.build_finetne_function() again. This is another way, construct
## new model from scratch with opt_l2e=True, then copy existing weights over:
#training_epochs
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
# too many consecutive epochs without surpassing the best model
#    visualize_dnn(dbn)
#        predicted_parameter = test_out()
### write to cmp file
### multiple Gaussian components
#        print   predicted_mu.shape
#        predicted_mu = predicted_mu[aa*n_outs:(aa+1)*n_outs]
#                print   gen_features
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
#    nn_label_norm_mvn_dir = os.path.join(data_dir, 'nn_no_silence_lab_norm_'+suffix)
# to do - sanity check the label dimension here?
# simple HTS labels
#+*'])
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output acoustic data
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
#+*'])
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# logger.debug(' value was\n%s' % cmp_norm_info)
# logger.debug(' value was\n%s' % feature_std_vector)
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
#    nnets_file_name = '%s/%s_%s_%d.%d.%d.%d.%d.train.%d.model' \
#                       %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),
#                        len(hidden_layers_sizes), hidden_layers_sizes[0],
#                        lab_dim, cfg.cmp_dim, cfg.train_file_number)
### DNN model training
# not an error - just means directory already exists
# print   'start DNN'
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
### generate parameters from DNN
# not an error - just means directory already exists
#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)
#        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
#+*'])
##MCD
##MCD
#+*'])
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
#+*'])
# this can be removed
#
#to calculate distortion of HMM baseline
#+*'])
#+*'])
#+*'])
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
#import gnumpy as gnp
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
# the new class for label composition and normalisation
#from frontend.mlpg_fast import MLParameterGenerationFast
#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
#    sequential_training = True
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
#    pretrain_set_x = train_set_x
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
#, batch_size=batch_size
#    finetune_lr = 0.000125
# if sequential training, the batch size will be the number of frames in an utterance
#batch_size = temp_train_set_x.shape[0]
#print train_set_x.eval().shape, train_set_y.eval().shape, this_train_error
## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function
#            logger.debug('validation loss decreased, so saving model')
#            dbn = best_dnn_model
#    cPickle.dump(best_dnn_model, open(nnets_file_name, 'wb'))
#file_number
### write to cmp file
#file_number
#print b_indices
### write to cmp file
#file_number
### write to cmp file
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
# to do - sanity check the label dimension here?
# this means that open(...) threw an error
# simple HTS labels
###use only training data to find min-max information, then apply on the whole dataset
### make duration data for S2S network ###
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output duration data
### make output acoustic data
#[-0.5, 0.0, 0.5]
#[1.0, -2.0, 1.0]
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
### DNN model training
# not an error - just means directory already exists
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
# not an error - just means directory already exists
### generate parameters from DNN
# not an error - just means directory already exists
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### Perform duration normalization(min. state dur set to 1) ###
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list, cfg)  # reference copy synthesis speech
### setting back to original conditions before calculating objective scores ###
### evaluation: RMSE and CORR for duration
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
##MCD
##MCD
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
#    if gnp._boardId is not None:
#        import gpu_lock
#        gpu_lock.free_lock(gnp._boardId)
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
#import gnumpy as gnp
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
# the new class for label composition and normalisation
#from frontend.mlpg_fast import MLParameterGenerationFast
#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
#    sequential_training = True
###################
#validation data is still read block by block
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
#    pretrain_set_x = train_set_x
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
#, batch_size=batch_size
#temporally we use the training set as pretrain_set_x.
#we need to support any data for pretraining
#, batch_size=batch_size
## if pretraining is supported more than one model, add the switch here
## be careful to use autoencoder for pretraining here:
## in SDAE we do layer-wise pretraining using autoencoders
# if sequential training, the batch size will be the number of frames in an utterance
## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function
#    finetune_lr = 0.000125
#            train_set_x.set_value(numpy.asarray(temp_train_set_x, dtype=theano.config.floatX), borrow=True)
#            train_set_y.set_value(numpy.asarray(temp_train_set_y, dtype=theano.config.floatX), borrow=True)
# if sequential training, the batch size will be the number of frames in an utterance
## send a batch to the shared variable, rather than pass the batch size and batch index to the finetune function
#            logger.debug('validation loss decreased, so saving model')
#            dbn = best_dnn_model
#    cPickle.dump(best_dnn_model, open(nnets_file_name, 'wb'))
#file_number
### write to cmp file
#file_number
### write to cmp file
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
# to do - sanity check the label dimension here?
# this means that open(...) threw an error
# simple HTS labels
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output duration data
### make output acoustic data
#[-0.5, 0.0, 0.5]
#[1.0, -2.0, 1.0]
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
### DNN model training
# not an error - just means directory already exists
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
# not an error - just means directory already exists
### generate parameters from DNN
# not an error - just means directory already exists
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### Perform duration normalization(min. state dur set to 1) ###
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list, cfg)  # reference copy synthesis speech
### setting back to original conditions before calculating objective scores ###
### evaluation: RMSE and CORR for duration
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
##MCD
##MCD
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
#    if gnp._boardId is not None:
#        import gpu_lock
#        gpu_lock.free_lock(gnp._boardId)
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
#import gnumpy as gnp
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#import theano.tensor as T
# the new class for label composition and normalisation
#from frontend.mlpg_fast import MLParameterGenerationFast
#from frontend.mlpg_fast_layer import MLParameterGenerationFastLayer
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
##  W and b for each layer
#print layer_name
#numpy.savetxt(os.path.join(outdir, fname + '.txt'), param_vals[p_ix])
### Input normalisation:-
## output norm
#     norm_info_file = os.path.join(data_dir, 'norm_info' + cfg.combined_feature_name + '_' + str(cfg.cmp_dim) + '_' + cfg.output_feature_normalisation + '.dat')
### normalise input full context label
## + cfg.appended_input_dim
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
## if made with run_lstm:--
## if made with run_dnn:--
## override the name computed from config variables if model_pickle_file specified:
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
# the new class for label composition and normalisation
#from models.ms_dnn import MultiStreamDNN
#from models.ms_dnn_gv import MultiStreamDNNGv
#from models.sdae import StackedDenoiseAutoEncoder
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
#    visualize_dnn(dbn)
#        predicted_parameter = test_out()
### write to cmp file
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
#### parameter setting########
###total file number including training, development, and testing
#total_file_number = len(file_id_list)
#nn_cmp_dir       = os.path.join(data_dir, 'nn' + cfg.combined_feature_name + '_' + str(cfg.cmp_dim))
#nn_cmp_norm_dir   = os.path.join(data_dir, 'nn_norm'  + cfg.combined_feature_name + '_' + str(cfg.cmp_dim))
#in_file_list_dict = {}
#for feature_name in cfg.in_dir_dict.keys():
#    in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)
#nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)
#nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
## need this to find normalisation info:
# simple HTS labels
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# no silence removal for synthesis ...
## minmax norm:
# reload stored minmax values: (TODO -- move reading and writing into MinMaxNormalisation class)
## This doesn't work -- precision is lost -- reads in as float64
#label_norm_info = numpy.fromfile(fid)  ## label_norm_info = numpy.array(label_norm_info, 'float32')
## use struct to enforce float32:
# length in bytes
# = read until bytes run out
## number 32 bit floats
###  apply precompuated min-max to the whole dataset
### make output acoustic data
#    if cfg.MAKECMP:
### retrieve acoustic normalisation information for normalising the features back
### normalise output acoustic data
#    if cfg.NORMCMP:
### DNN model training
#    if cfg.TRAINDNN:
##if cfg.DNNGEN:
# not an error - just means directory already exists
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
## gv_weight hard coded here!
### generate wav ----
#if cfg.GENWAV:
#generate_wav_glottHMM(scaled_dir, file_id_list)
## simple variance scaling (silen et al. 2012, paragraph 3.1)
## TODO: Lots of things like stream names hardcoded here; 3 for delta + delta-delta; ...
#     all_streams = ['cmp','HNR','F0','LSF','Gain','LSFsource']
#     streams_to_scale = ['LSF']
## Try range of interpolation weights for combining global & local variance
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
#
#     # set up logging to use our custom class
#     logging.setLoggerClass(LoggerPlotter)
#
#     # get a logger for this main function
#     logger = logging.getLogger("main")
#logger.critical('usage: run_dnn.sh config_file_name utt_dir')
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
# ListDataProvider
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
# the new class for label composition and normalisation
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## This should always be True -- tidy up later
## including input and output
####parameters#####
######### data providers ##########
####################################
# numpy random generator
############## load existing dnn #####
####################################
## <-------- hard coded !!!!!!!!!!
# 10  ## <-------- hard coded !!!!!!!!!!
#warmup_epoch_3 = epoch + warmup_epoch_3
#inference_epochs += epoch
## osw -- inferring word reps on validation set in a forward pass in a single batch
##        exausts memory when using 20k projected vocab -- also use minibatches
#valid_error = infer_projections_fn(current_finetune_lr, current_momentum)
#this_validation_loss = numpy.mean(valid_error)
#        if plot:
#            ## add dummy validation loss so that plot works:
#            plotlogger.add_plot_point('training convergence','validation set',(epoch,this_validation_loss))
#            plotlogger.add_plot_point('training convergence','training set',(epoch,this_train_valid_loss))
#
#        if cfg.hyper_params['model_type'] == 'TPDNN':
#            if not os.path.isdir(cfg.projection_weights_output_dir):
#                os.mkdir(cfg.projection_weights_output_dir)
#            weights = dnn_model.get_projection_weights()
#            fname = os.path.join(cfg.projection_weights_output_dir, 'proj_INFERENCE_epoch_%s'%(epoch))
#            numpy.savetxt(fname, weights)
#
## always update
##cPickle.dump(best_dnn_model, open(nnets_file_name, 'wb'))
#    if plot:
#        plotlogger.save_plot('training convergence',title='Final training and validation error',xlabel='epochs',ylabel='error')
#
### ========================================================
#    if cfg.hyper_params['model_type'] == 'TPDNN':
#        os.system('python %s %s'%('/afs/inf.ed.ac.uk/user/o/owatts/scripts_NEW/plot_weights_multiple_phases.py', cfg.projection_weights_output_dir))
## 'remove' word representations by randomising them. As model is unpickled and
## not re-saved, this does not throw trained parameters away.
## use randomly chosen training projection -- shuffle in-place = same as sampling wihtout replacement
## shuffle in place along 1st dim (reorder rows)
##  generate utt embeddings uniformly at random within the min-max of the training set (i.e. from a (hyper)-rectangle)
## vector like a row of P with min of its columns
## use mean projection
## stack mean rows
## DEBUG
## DEBUG:=========
#projection_weights_to_use = old_weights # numpy.array(numpy.random.uniform(low=-0.3, high=0.3, size=numpy.shape(old_weights)),  dtype=numpy.float32)
## =============
##  generate utt embeddings from a uniform 10 x 10 grid within the min-max of the training set (i.e. from a rectangle)
## vector like a row of P with min of its columns
## pading to handle 0 index (reserved for defaults)
## points uniformly sampled from between the 1.8 - 2.0 stds of a diagonal covariance gaussian fitted to the data
## vector like a row of P with min of its columns
## points uniformly sampled from between the 1.8 - 2.0 stds of a diagonal covariance gaussian fitted to the data
## vector like a row of P with min of its columns
## points uniformly sampled from between the 1.8 - 2.0 stds of a diagonal covariance gaussian fitted to the data
## vector like a row of P with min of its columns
##  save used weights for future reference:
#features, features_proj = expand_projection_inputs(features, cfg.index_to_project, \
#                                                         cfg.projection_insize)
#temp_set_x = features.tolist()  ## osw - why list conversion necessary?
#        predicted_parameter = test_out()
### write to cmp file
## define a couple of functions for circular rejection sampling:
## if x^2 + y^2 <= 1, point is within unit circle
##
## if x^2 + y^2 <= 1, point is within unit circle
##generate bottleneck layer as festures
### write to cmp file
## Taken from: ~/proj/dnn_tts/script/add_token_index.py
## clear target attribute name from all nodes to be safe:
## all nodes
## 0 is the defualt 'n/a' value -- *some* ancestor of all nodes will have the relevant attibute to fall back on
## Taken from: ~/proj/dnn_tts/script/add_token_index.py
## clear target attribute name from all nodes to be safe:
## all nodes
## 0 is the defualt 'n/a' value -- *some* ancestor of all nodes will have the relevant attibute to fall back on
## TODO -- move reading and writing into MinMaxNormalisation class
# reload stored minmax values:
## This doesn't work -- precision is lost -- reads in as float64
#label_norm_info = numpy.fromfile(fid)  ## label_norm_info = numpy.array(label_norm_info, 'float32')
## use struct to enforce float32:
# length in bytes
# = read until bytes run out
## number of 32 bit floats
## values can be min + max or mean + std, hence non-descript variable names:
## TODO: token_xpath & index_attrib_name   should be in config
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# but for now we need to do it manually
#### parameter setting########
####prepare environment
###synth_utts_input = synth_utts_input[:10]   ### temp!!!!!
## place to put test utts with tokens labelled with projection indices
## was below -- see comment
###normalisation information
### normalise input full context label
# the number can be removed
## need this to find normalisation info:
## always do this in synth:
## if cfg.NORMLAB and (cfg.label_style == 'composed'):
## add_projection_indices was here
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# no silence removal for synthesis ...
## minmax norm:
###  apply precompuated and stored min-max to the whole dataset
### DEBUG
## set up paths -- write CMP data to infer from in outdir:
### make output acoustic data
#    if cfg.MAKECMP:
## skip silence removal for inference -- need to match labels, which are
## not silence removed either
### retrieve acoustic normalisation information for normalising the features back
### normalise output acoustic data
#    if cfg.NORMCMP:
#### DEBUG
###  apply precompuated and stored mean and std to the whole dataset
#            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim)
#            global_mean_vector = min_max_normaliser.compute_mean(nn_cmp_file_list[0:cfg.train_file_number])
#            global_std_vector = min_max_normaliser.compute_std(nn_cmp_file_list[0:cfg.train_file_number], global_mean_vector)
#            min_max_normaliser = MinMaxNormalisation(feature_dimension = cfg.cmp_dim, min_value = 0.01, max_value = 0.99)
#            min_max_normaliser.find_min_max_values(nn_cmp_file_list[0:cfg.train_file_number])
#            min_max_normaliser.normalise_data(nn_cmp_file_list, nn_cmp_norm_file_list)
#            cmp_min_vector = min_max_normaliser.min_vector
#            cmp_max_vector = min_max_normaliser.max_vector
#            cmp_norm_info = numpy.concatenate((cmp_min_vector, cmp_max_vector), axis=0)
### DNN model training
#    if cfg.TRAINDNN: always do this in synth
#### DEBUG
## default, for non-inferring synth methods
## infer control values from TESTING data
## identical lists (our test data) for 'train' and 'valid' -- this is just to
##   keep the infer_projections_fn theano function happy -- operates on
##    validation set. 'Train' set shouldn't be used here.
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
## if cfg.DNNGEN:
# not an error - just means directory already exists
#print nn_label_norm_file_list  ## <-- this WAS mangled in inferred due to copying of file list to trainlist_x etc. which is then shuffled. Now use copy.copy
#print gen_file_list
## DNNGEN
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
## osw: skip MLPG:
#            split_cmp(gen_file_list, ['mgc', 'lf0', 'bap'], cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict)
## Variance scaling:
## gv_weight hardcoded
### generate wav ---- glottHMM only!!!
#if cfg.GENWAV:
# generated speech
## simple variance scaling (silen et al. 2012, paragraph 3.1)
## TODO: Lots of things like stream names hardcoded here; 3 for delta + delta-delta; ...
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
#    if cfg.profile:
#        logger.info('profiling is activated')
#        import cProfile, pstats
#        cProfile.run('main_function(cfg)', 'mainstats')
#        # create a stream for the profiler to write to
#        profiling_output = StringIO.StringIO()
#        p = pstats.Stats('mainstats', stream=profiling_output)
#        # print stats to that stream
#        # here we just report the top 10 functions, sorted by total amount of time spent in each
#        p.strip_dirs().sort_stats('tottime').print_stats(10)
#        # print the result to the log
#        logger.info('---Profiling result follows---\n%s' %  profiling_output.getvalue() )
#        profiling_output.close()
#        logger.info('---End of profiling result---')
#
#    else:
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
##from frontend.mlpg_fast import MLParameterGenerationFast
## osw temp
# the new class for label composition and normalisation
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
### plain DNN case
# def dnn_generation(valid_file_list, nnets_file_name, n_ins, n_outs, out_file_list):
#     logger = logging.getLogger("dnn_generation")
#     logger.debug('Starting dnn_generation')
#
#     plotlogger = logging.getLogger("plotting")
#
#     dnn_model = cPickle.load(open(nnets_file_name, 'rb'))
#
# #    visualize_dnn(dbn)
#
#     file_number = len(valid_file_list)
#
#     for i in xrange(file_number):
#         logger.info('generating %4d of %4d: %s' % (i+1,file_number,valid_file_list[i]) )
#         fid_lab = open(valid_file_list[i], 'rb')
#         features = numpy.fromfile(fid_lab, dtype=numpy.float32)
#         fid_lab.close()
#         features = features[:(n_ins * (features.size / n_ins))]
#         features = features.reshape((-1, n_ins))
#         temp_set_x = features.tolist()
#         test_set_x = theano.shared(numpy.asarray(temp_set_x, dtype=theano.config.floatX))
#
#         predicted_parameter = dnn_model.parameter_prediction(test_set_x=test_set_x)
# #        predicted_parameter = test_out()
#
#         ### write to cmp file
#         predicted_parameter = numpy.array(predicted_parameter, 'float32')
#         temp_parameter = predicted_parameter
#         fid = open(out_file_list[i], 'wb')
#         predicted_parameter.tofile(fid)
#         logger.debug('saved to %s' % out_file_list[i])
#         fid.close()
#
### multiple Gaussian components
## TODO: take this from config
#        print   predicted_mu.shape
#        predicted_mu = predicted_mu[aa*n_outs:(aa+1)*n_outs]
#                print   gen_features
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
###total file number including training, development, and testing
#total_file_number = len(file_id_list)
#nn_cmp_dir       = os.path.join(data_dir, 'nn' + cfg.combined_feature_name + '_' + str(cfg.cmp_dim))
#nn_cmp_norm_dir   = os.path.join(data_dir, 'nn_norm'  + cfg.combined_feature_name + '_' + str(cfg.cmp_dim))
#in_file_list_dict = {}
#for feature_name in cfg.in_dir_dict.keys():
#    in_file_list_dict[feature_name] = prepare_file_path_list(file_id_list, cfg.in_dir_dict[feature_name], cfg.file_extension_dict[feature_name], False)
#nn_cmp_file_list         = prepare_file_path_list(file_id_list, nn_cmp_dir, cfg.cmp_ext)
#nn_cmp_norm_file_list    = prepare_file_path_list(file_id_list, nn_cmp_norm_dir, cfg.cmp_ext)
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
## need this to find normalisation info:
# simple HTS labels
#        logger.info('preparing label data (input) using standard HTS style labels')
#        label_normaliser.perform_normalisation(in_label_align_file_list, binary_label_file_list)
#        remover = SilenceRemover(n_cmp = lab_dim, silence_pattern = ['*-#+*'])
#        remover.remove_silence(binary_label_file_list, in_label_align_file_list, nn_label_file_list)
#        min_max_normaliser = MinMaxNormalisation(feature_dimension = lab_dim, min_value = 0.01, max_value = 0.99)
#        ###use only training data to find min-max information, then apply on the whole dataset
#        min_max_normaliser.find_min_max_values(nn_label_file_list[0:cfg.train_file_number])
#        min_max_normaliser.normalise_data(nn_label_file_list, nn_label_norm_file_list)
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# no silence removal for synthesis ...
## minmax norm:
# reload stored minmax values: (TODO -- move reading and writing into MinMaxNormalisation class)
## This doesn't work -- precision is lost -- reads in as float64
#label_norm_info = numpy.fromfile(fid)  ## label_norm_info = numpy.array(label_norm_info, 'float32')
## use struct to enforce float32:
# length in bytes
# = read until bytes run out
## number 32 bit floats
###  apply precompuated min-max to the whole dataset
### make output acoustic data
#    if cfg.MAKECMP:
### retrieve acoustic normalisation information for normalising the features back
### normalise output acoustic data
#    if cfg.NORMCMP:
### DNN model training
#    if cfg.TRAINDNN:
##if cfg.DNNGEN:
# not an error - just means directory already exists
#gen_file_list = prepare_file_path_list(gen_file_id_list, gen_dir, cfg.cmp_ext)
#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)
#        dnn_generation(test_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)
## Variance scaling:
## gv_weight hard coded here!
### generate wav ---- glottHMM only!!!
#if cfg.GENWAV:
# generated speech
## simple variance scaling (silen et al. 2012, paragraph 3.1)
## TODO: Lots of things like stream names hardcoded here; 3 for delta + delta-delta; ...
## Try range of interpolation weights for combining global & local variance
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
#    if cfg.profile:
#        logger.info('profiling is activated')
#        import cProfile, pstats
#        cProfile.run('main_function(cfg)', 'mainstats')
#        # create a stream for the profiler to write to
#        profiling_output = StringIO.StringIO()
#        p = pstats.Stats('mainstats', stream=profiling_output)
#        # print stats to that stream
#        # here we just report the top 10 functions, sorted by total amount of time spent in each
#        p.strip_dirs().sort_stats('tottime').print_stats(10)
#        # print the result to the log
#        logger.info('---Profiling result follows---\n%s' %  profiling_output.getvalue() )
#        profiling_output.close()
#        logger.info('---End of profiling result---')
#
#    else:
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
# the new class for label composition and normalisation
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## including input and output
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
#     frames_per_hour = 720000.0
#     tframes = train_set_x.get_value().shape[0]
#     vframes = valid_set_x.get_value().shape[0]
#     print 'Training frames: %s (%s hours)'%(tframes, tframes / frames_per_hour)
#     print 'Validation frames: %s (%s hours)'%(tframes, tframes / frames_per_hour)
#     sys.exit('999')
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
##basic model is ready.
##if corruption levels is set to zero. it becomes normal autoencoder
##model is ready, but the hyper-parameters are not optimised.
## not fully ready
## if pretraining is supported in one model, add the switch here
## be careful to use autoencoder for pretraining here:
## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]
## however, tanh works better and converge fast in finetuning
##
## Will extend this soon...
## in SDAE we do layer-wise pretraining using autoencoders
# =============================================================================
# The original script (run_dnn.py) has a training routine that looks like this:
#
#     foreach epoch:
#        foreach partition:
#            foreach minibatch:
#                train_model
#        validate_performance_and_stop_if_converged
#
# The current script's rountine looks like this:
#
#     foreach epoch:
#        foreach partition:
#            foreach minibatch:
#                train_model
#                if we've seen another hour of data:
#                     validate_performance_and_stop_if_converged
#
# In order to jump out of these multiple loops when converged, we'll use this variable:
#
## Hardcoded checking intervals and framerate: 720000 frames per hour at 5ms frame rate
### calculation validation error in 1 big batch can fail for big data --
### use minibatches
#validation_losses = valid_fn()
#this_validation_loss = numpy.mean(validation_losses)
#print '   validation for batch %s (%s frames): %s'%(minibatch_index, batch_size, v_loss)
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
## It might also be interesting to look at how consistent performance is across minibatches:
# too many consecutive checks without surpassing the best model
#    visualize_dnn(dbn)
#        predicted_parameter = test_out()
### write to cmp file
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
#    nn_label_norm_mvn_dir = os.path.join(data_dir, 'nn_no_silence_lab_norm_'+suffix)
# to do - sanity check the label dimension here?
# simple HTS labels
#+*'])
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output acoustic data
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
#+*'])
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# logger.debug(' value was\n%s' % cmp_norm_info)
# logger.debug(' value was\n%s' % feature_std_vector)
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
#    nnets_file_name = '%s/%s_%s_%d.%d.%d.%d.%d.train.%d.model' \
#                       %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),
#                        len(hidden_layers_sizes), hidden_layers_sizes[0],
#                        lab_dim, cfg.cmp_dim, cfg.train_file_number)
### DNN model training
# not an error - just means directory already exists
# print   'start DNN'
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
### generate parameters from DNN
# not an error - just means directory already exists
#        dnn_generation(valid_x_file_list, nnets_file_name, lab_dim, cfg.cmp_dim, gen_file_list)
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
### generate wav
# generated speech
#       generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
#+*'])
##MCD
##MCD
#+*'])
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
#+*'])
# this can be removed
#
#to calculate distortion of HMM baseline
#+*'])
#+*'])
#+*'])
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
#  numpy & theano imports need to be done in this order (only for some numpy installations, not sure why)
# we need to explicitly import this in some cases, not sure why this doesn't get imported with numpy itself
# and only after that can we import theano
# ListDataProvider
#from frontend.acoustic_normalisation import CMPNormalisation
#from frontend.feature_normalisation_base import FeatureNormBase
# the new class for label composition and normalisation
#import matplotlib.pyplot as plt
# our custom logging class that can also plot
#from logplot.logging_plotting import LoggerPlotter, MultipleTimeSeriesPlot, SingleWeightMatrixPlot
# as logging
## This should always be True -- tidy up later
## including input and output
## Function for training projection and non-projection parts at same time
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
##basic model is ready.
##if corruption levels is set to zero. it becomes normal autoencoder
##model is ready, but the hyper-parameters are not optimised.
## not fully ready
## if pretraining is supported in one model, add the switch here
## be careful to use autoencoder for pretraining here:
## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]
## however, tanh works better and converge fast in finetuning
##
## Will extend this soon...
## in SDAE we do layer-wise pretraining using autoencoders
## osw -- getting validation error from a forward pass in a single batch
##        exausts memory when using 20k projected vocab -- also use minibatches
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
# too many consecutive epochs without surpassing the best model
### Save projection values:
## Function for training all model on train data as well as simultaneously
## inferring proj weights on dev data.
# in each epoch do:
#   train_all_fn()
#   infer_projections_fn()    ## <-- updates proj for devset and gives validation loss
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
##basic model is ready.
##if corruption levels is set to zero. it becomes normal autoencoder
##model is ready, but the hyper-parameters are not optimised.
## not fully ready
## if pretraining is supported in one model, add the switch here
## be careful to use autoencoder for pretraining here:
## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]
## however, tanh works better and converge fast in finetuning
##
## Will extend this soon...
## in SDAE we do layer-wise pretraining using autoencoders
##dnn_model.zero_projection_weights()
## infer validation weights before getting validation error:
## osw -- inferring word reps on validation set in a forward pass in a single batch
##        exausts memory when using 20k projected vocab -- also use minibatches
## this function also give us validation loss:
## osw -- getting validation error from a forward pass in a single batch
##        exausts memory when using 20k projected vocab -- also use minibatches
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
# too many consecutive epochs without surpassing the best model
### Save projection values:
## Function for training the non-projection part only
# get loggers for this function
# this one writes to both console and file
# this one takes care of plotting duties
# create an (empty) plot of training convergence, ready to receive data points
####parameters#####
## use a switch to turn on pretraining
## pretraining may not help too much, if this case, we turn it off to save time
###################
##temporally we use the training set as pretrain_set_x.
##we need to support any data for pretraining
# numpy random generator
## not all the model support pretraining right now
## valid_fn and valid_model are the same. reserve to computer multi-stream distortion
##basic model is ready.
##if corruption levels is set to zero. it becomes normal autoencoder
##model is ready, but the hyper-parameters are not optimised.
## not fully ready
## if pretraining is supported in one model, add the switch here
## be careful to use autoencoder for pretraining here:
## for SDAE, currently only sigmoid function is supported in the hidden layers, as our input is scaled to [0, 1]
## however, tanh works better and converge fast in finetuning
##
## Will extend this soon...
## in SDAE we do layer-wise pretraining using autoencoders
## osw -- getting validation error from a forward pass in a single batch
##        exausts memory when using 20k projected vocab -- also use minibatches
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
# too many consecutive epochs without surpassing the best model
### Save projection values:
### ========== now train the word residual ============
####parameters#####
######### data providers ##########
####################################
# numpy random generator
############## load existing dnn #####
####################################
## 100  ## <-------- hard coded !!!!!!!!!!
# 10  ## <-------- hard coded !!!!!!!!!!
### COULD REMOVE THIS LATER
## osw -- getting validation error from a forward pass in a single batch
##        exausts memory when using 20k projected vocab -- also use minibatches
# this has a possible bias if the minibatches were not all of identical size
# but it should not be siginficant if minibatches are small
#        if plot:
#            ## add dummy validation loss so that plot works:
#            plotlogger.add_plot_point('training convergence','validation set',(epoch,this_validation_loss))
#            plotlogger.add_plot_point('training convergence','training set',(epoch,this_train_valid_loss))
#
## always update
#    if plot:
#        plotlogger.save_plot('training convergence',title='Final training and validation error',xlabel='epochs',ylabel='error')
#
### ========================================================
### ========== now infer word represntations for out-of-training (dev) data ============
#
#    ### TEMP-- restarted!!! ### ~~~~~~~
#    epoch = 50
#    dnn_model = cPickle.load(open(nnets_file_name, 'rb'))
#    train_all_fn, train_subword_fn, train_word_fn, infer_projections_fn, valid_fn, valid_score_i = \
#                    dnn_model.build_finetune_functions(
#                    (train_set_x, train_set_x_proj, train_set_y),
#                    (valid_set_x, valid_set_x_proj, valid_set_y), batch_size=batch_size)
#    this_train_valid_loss = 198.0 ## approx value
#    ### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
####parameters#####
######### data providers ##########
####################################
# numpy random generator
############## load existing dnn #####
####################################
#dnn_model.initialise_projection_weights()
## <-------- hard coded !!!!!!!!!!
# 10  ## <-------- hard coded !!!!!!!!!!
#warmup_epoch_3 = epoch + warmup_epoch_3
#inference_epochs += epoch
## osw -- inferring word reps on validation set in a forward pass in a single batch
##        exausts memory when using 20k projected vocab -- also use minibatches
#valid_error = infer_projections_fn(current_finetune_lr, current_momentum)
#this_validation_loss = numpy.mean(valid_error)
#        if plot:
#            ## add dummy validation loss so that plot works:
#            plotlogger.add_plot_point('training convergence','validation set',(epoch,this_validation_loss))
#            plotlogger.add_plot_point('training convergence','training set',(epoch,this_train_valid_loss))
#
## always update
#    if plot:
#        plotlogger.save_plot('training convergence',title='Final training and validation error',xlabel='epochs',ylabel='error')
#
### ========================================================
## 'remove' word representations by randomising them. As model is unpickled and
## no re-saved, this does not throw trained parameters away.
#    visualize_dnn(dbn)
#features, features_proj = expand_projection_inputs(features, cfg.index_to_project, \
#                                                         cfg.projection_insize)
#temp_set_x = features.tolist()  ## osw - why list conversion necessary?
#print temp_set_x
#        predicted_parameter = test_out()
### write to cmp file
##generate bottleneck layer as festures
### write to cmp file
# get a logger for this main function
# get another logger to handle plotting duties
# later, we might do this via a handler that is created, attached and configured
# using the standard config mechanism of the logging module
# but for now we need to do it manually
#### parameter setting########
####prepare environment
# this means that open(...) threw an error
###total file number including training, development, and testing
###normalisation information
### normalise input full context label
# currently supporting two different forms of lingustic features
# later, we should generalise this
# no longer supported - use new "composed" style labels instead
# label_normaliser = XMLLabelNormalisation(xpath_file_name=cfg.xpath_file_name)
# the number can be removed
#    nn_label_norm_mvn_dir = os.path.join(data_dir, 'nn_no_silence_lab_norm_'+suffix)
# to do - sanity check the label dimension here?
# simple HTS labels
#+*'])
###use only training data to find min-max information, then apply on the whole dataset
# new flexible label preprocessor
# logger.info('%s' % label_composer.configuration.labels )
# there are now a set of parallel input label files (e.g, one set of HTS and another set of Ossian trees)
# create all the lists of these, ready to pass to the label composer
# now iterate through the files, one at a time, constructing the labels for them
# iterate through the required label styles and open each corresponding label file
# a dictionary of file descriptors, pointing at the required files
# the files will be a parallel set of files for a single utterance
# e.g., the XML tree and an HTS label file
# now close all opened files
# silence removal
## use first feature in label -- hardcoded for now
## Binary labels have 2 roles: both the thing trimmed and the instructions for trimming:
# start from the labels we have just produced, not trimmed versions
###use only training data to find min-max information, then apply on the whole dataset
### save label normalisation information for unseen testing labels
### make output acoustic data
## do this to get lab_dim:
## use first feature in label -- hardcoded for now
## overwrite the untrimmed audio with the trimmed version:
## back off to previous method using HTS labels:
#+*'])
# save to itself
### save acoustic normalisation information for normalising the features back
### normalise output acoustic data
###calculate mean and std vectors on the training data, and apply on the whole dataset
# logger.debug(' value was\n%s' % cmp_norm_info)
# logger.debug(' value was\n%s' % feature_std_vector)
# we need to know the label dimension before training the DNN
# computing that requires us to look at the labels
#
# currently, there are two ways to do this
#    nnets_file_name = '%s/%s_%s_%d.%d.%d.%d.%d.train.%d.model' \
#                       %(model_dir, cfg.model_type, cfg.combined_feature_name, int(cfg.multistream_switch),
#                        len(hidden_layers_sizes), hidden_layers_sizes[0],
#                        lab_dim, cfg.cmp_dim, cfg.train_file_number)
### DNN model training
# not an error - just means directory already exists
#                train_DNN(train_xy_file_list = (train_x_file_list, train_y_file_list), \
#                          valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \
#                          nnets_file_name = nnets_file_name, \
#                          n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \
#                          hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot)
#                infer_projections(train_xy_file_list = (train_x_file_list, train_y_file_list), \
#                          valid_xy_file_list = (valid_x_file_list, valid_y_file_list), \
#                          nnets_file_name = nnets_file_name, \
#                          n_ins = lab_dim, n_outs = cfg.cmp_dim, ms_outs = cfg.multistream_outs, \
#                          hyper_params = cfg.hyper_params, buffer_size = cfg.buffer_size, plot = cfg.plot)
# Could 'raise' the exception further, but that causes a deep traceback to be printed
# which we don't care about for a keyboard interrupt. So, just bail out immediately
### generate parameters from DNN (with random token reps and inferred ones -- NOTOKENS & TOKENS)
# not an error - just means directory already exists
## Without words embeddings:
## With word embeddings:
##perform MLPG to smooth parameter trajectory
## lf0 is included, the output features much have vuv.
## osw: skip MLPG:
#            split_cmp(gen_file_list, ['mgc', 'lf0', 'bap'], cfg.cmp_dim, cfg.out_dimension_dict, cfg.file_extension_dict)
### generate wav
# generated speech
#           generate_wav(nn_cmp_dir, gen_file_id_list)  # reference copy synthesis speech
### evaluation: calculate distortion
## get lab_dim:
## use first feature in label -- hardcoded for now
## Use these to trim silence:
#+*'])
##MCD
##MCD
#+*'])
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
##Cassia's bap is computed from 10*log|S(w)|. if use HTS/SPTK style, do the same as MGC
#+*'])
# this can be removed
#
#to calculate distortion of HMM baseline
#+*'])
#+*'])
#+*'])
# these things should be done even before trying to parse the command line
# create a configuration instance
# and get a short name for this instance
# set up logging to use our custom class
# get a logger for this main function
# create a stream for the profiler to write to
# print stats to that stream
# here we just report the top 10 functions, sorted by total amount of time spent in each
# print the result to the log
# 128 * 1
#!/usr/bin/env python
#file_paths = []
#filenames = []
#for root, directories, files in os.walk(dir_name):
#    for filename in files:
#        filepath = os.path.join(root, filename)
#        file_paths.append(filepath)
#        filenames.append(filename)
#filenames=filter(os.path.isfile, os.listdir(dir_name))
#for f in os.listdir(dir_name):
#    if os.path.isfile(dir_name+'/'+f):
#        print dir_name+'/'+f+' is a file'
#    else:
#        print dir_name+'/'+f+' is not a file'
#print filenames
#file_paths=[ dir_name+'/'+f for f in os.listdir(dir_name) if os.path.isfile(f) ]
#print in_data_dir
# 128
# 0
# Test the classes used in Merlin pipeline
# TODO run some very simple training on random data)
# Always try to save it and reload it
# Get a logger for these tests
# Build various models
# -*- coding: utf-8 -*-
#
# Copyright 2016 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# pylint: disable=unused-import
# pylint: disable=g-import-not-at-top
# Case 1: GenTestList = False and test_synth_dir = None
# Case 2: GenTestList = True and test_synth_dir = None
# Case 3: GenTestList = True and test_synth_dir = test_synth
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Test program for WORLD 0.1.2 (2012/08/19)
// Test program for WORLD 0.1.3 (2013/07/26)
// Test program for WORLD 0.1.4 (2014/04/29)
// Test program for WORLD 0.1.4_3 (2015/03/07)
// Test program for WORLD 0.2.0 (2015/05/29)
// Test program for WORLD 0.2.0_1 (2015/05/31)
// Test program for WORLD 0.2.0_2 (2015/06/06)
// Test program for WORLD 0.2.0_3 (2015/07/28)
// Test program for WORLD 0.2.0_4 (2015/11/15)
// Test program for WORLD in GitHub (2015/11/16-)
// Latest update: 2016/02/02
// test.exe input.wav outout.wav f0 spec
// input.wav  : Input file
// output.wav : Output file
// f0         : F0 scaling (a positive number)
// spec       : Formant scaling (a positive number)
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
// Modification of the option
// When you You must set the same value.
// If a different value is used, you may suffer a fatal error because of a
// illegal memory access.
// Valuable option.speed represents the ratio for downsampling.
// The signal is downsampled to fs / speed Hz.
// If you want to obtain the accurate result, speed should be set to 1.
// You should not set option.f0_floor to under world_kFloorF0.
// If you want to analyze such low F0 speech, please change world_kFloorF0.
// Processing speed may sacrify, provided that the FFT length changes.
// You can give a positive real number as the threshold.
// Most strict value is 0, but almost all results are counted as unvoiced.
// The value from 0.02 to 0.2 would be reasonable.
// Parameters setting and memory allocation.
// StoneMask is carried out to improve the estimation performance.
// This value may be better one for HMM speech synthesis.
// Default value is -0.09.
// Important notice (2016/02/02)
// You can control a parameter used for the lowest F0 in speech.
// You must not set the f0_floor to 0.
// It will cause a fatal error because fft_size indicates the infinity.
// You must not change the f0_floor after memory allocation.
// You should check the fft_size before excucing the analysis/synthesis.
// The default value (71.0) is strongly recommended.
// On the other hand, setting the lowest F0 of speech is a good choice
// to reduce the fft_size.
// Parameters setting and memory allocation.
// Parameters setting and memory allocation.
// option is not implemented in this version. This is for future update.
// We can use "NULL" as the argument.
// F0 scaling
// Spectral stretching
// Synthesis by the aperiodicity
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] Output file
// f0         : argv[3] F0 scaling (a positive number)
// spec       : argv[4] Formant shift (a positive number)
//-----------------------------------------------------------------------------
// 2016/01/28: Important modification.
// Memory allocation is carried out in advanse.
// This is for compatibility with C language.
// wavread() must be called after GetAudioLength().
//---------------------------------------------------------------------------
// Analysis part
//---------------------------------------------------------------------------
// 2016/02/02
// A new struct is introduced to implement safe program.
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// F0 estimation
// Spectral envelope estimation
// Aperiodicity estimation by D4C
// Note that F0 must not be changed until all parameters are estimated.
//---------------------------------------------------------------------------
// Synthesis part
//---------------------------------------------------------------------------
// The length of the output waveform
// Synthesis
// Output
//-----------------------------------------------------------------------------
// Copyright 2012-2016 Masanori Morise. All Rights Reserved.
// Author: mmorise [at] yamanashi.ac.jp (Masanori Morise)
//
// Test program for WORLD 0.1.2 (2012/08/19)
// Test program for WORLD 0.1.3 (2013/07/26)
// Test program for WORLD 0.1.4 (2014/04/29)
// Test program for WORLD 0.1.4_3 (2015/03/07)
// Test program for WORLD 0.2.0 (2015/05/29)
// Test program for WORLD 0.2.0_1 (2015/05/31)
// Test program for WORLD 0.2.0_2 (2015/06/06)
// Test program for WORLD 0.2.0_3 (2015/07/28)
// Test program for WORLD 0.2.0_4 (2015/11/15)
// Test program for WORLD in GitHub (2015/11/16-)
// Latest update: 2016/02/02
// test.exe input.wav outout.wav f0 spec
// input.wav  : Input file
// output.wav : Output file
// f0         : F0 scaling (a positive number)
// spec       : Formant scaling (a positive number)
//-----------------------------------------------------------------------------
// For .wav input/output functions.
// WORLD core functions.
// Linux porting section: implement timeGetTime() by gettimeofday(),
//-----------------------------------------------------------------------------
// struct for WORLD
// This struct is an option.
// Users are NOT forced to use this struct.
//-----------------------------------------------------------------------------
// Modification of the option
// When you You must set the same value.
// If a different value is used, you may suffer a fatal error because of a
// illegal memory access.
// Valuable option.speed represents the ratio for downsampling.
// The signal is downsampled to fs / speed Hz.
// If you want to obtain the accurate result, speed should be set to 1.
// You should not set option.f0_floor to under world_kFloorF0.
// If you want to analyze such low F0 speech, please change world_kFloorF0.
// Processing speed may sacrify, provided that the FFT length changes.
// You can give a positive real number as the threshold.
// Most strict value is 0, but almost all results are counted as unvoiced.
// The value from 0.02 to 0.2 would be reasonable.
// Parameters setting and memory allocation.
// StoneMask is carried out to improve the estimation performance.
// This value may be better one for HMM speech synthesis.
// Default value is -0.09.
// Important notice (2016/02/02)
// You can control a parameter used for the lowest F0 in speech.
// You must not set the f0_floor to 0.
// It will cause a fatal error because fft_size indicates the infinity.
// You must not change the f0_floor after memory allocation.
// You should check the fft_size before excucing the analysis/synthesis.
// The default value (71.0) is strongly recommended.
// On the other hand, setting the lowest F0 of speech is a good choice
// to reduce the fft_size.
// Parameters setting and memory allocation.
// Parameters setting and memory allocation.
// option is not implemented in this version. This is for future update.
// We can use "NULL" as the argument.
// F0 scaling
// Spectral stretching
// Synthesis by the aperiodicity
//-----------------------------------------------------------------------------
// Test program.
// test.exe input.wav outout.wav f0 spec flag
// input.wav  : argv[1] Input file
// output.wav : argv[2] Output file
// f0         : argv[3] F0 scaling (a positive number)
// spec       : argv[4] Formant shift (a positive number)
//-----------------------------------------------------------------------------
// 2016/01/28: Important modification.
// Memory allocation is carried out in advanse.
// This is for compatibility with C language.
// wavread() must be called after GetAudioLength().
//---------------------------------------------------------------------------
// Analysis part
//---------------------------------------------------------------------------
// 2016/02/02
// A new struct is introduced to implement safe program.
// You must set fs and frame_period before analysis/synthesis.
// 5.0 ms is the default value.
// Generally, the inverse of the lowest F0 of speech is the best.
// However, the more elapsed time is required.
// F0 estimation
// Spectral envelope estimation
// Aperiodicity estimation by D4C
// Note that F0 must not be changed until all parameters are estimated.
//---------------------------------------------------------------------------
// Synthesis part
//---------------------------------------------------------------------------
// The length of the output waveform
// Synthesis
// Output
