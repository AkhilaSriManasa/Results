/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//invoked by benchmarking framework
/*
//www.apache.org/licenses/LICENSE-2.0
//invoked by benchmarking framework
// Do NOT make any field final (even if it is not annotated with @Param)! See also
// http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/JMHSample_10_ConstantFold.java
// we cannot use individual @Params as some will lead to invalid combinations which do not let the benchmark terminate. JMH offers no
// support to constrain the combinations of benchmark parameters and we do not want to rely on OptionsBuilder as each benchmark would
// need its own main method and we cannot execute more than one class with a main method per JAR.
// indices| shards| replicas| nodes
/*
//www.apache.org/licenses/LICENSE-2.0
// noop
// noop
// noop
/*
//www.apache.org/licenses/LICENSE-2.0
//invoked by benchmarking framework
/*
//www.apache.org/licenses/LICENSE-2.0
//invoked by benchmarking framework
// benchmark an accessor that does not contain a timezone
// this used to throw an exception earlier and thus was very very slow
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// end the stream
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/gradle/gradle/issues/2363">not available</a> for
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// for testkit tests, until BwcVersions is extracted into an extension
// We only care about the last 2 majors when it comes to BWC.
// It might take us time to remove the older ones from versionLines, so we allow them to exist.
/**
// We have Gradle projects set up to check out and build unreleased versions based on the our branching
// conventions described in this classes javadoc
// based on the rules described in this classes javadoc, figure out the branch on which an unreleased version
// lives.
// We do this based on the Gradle project path because there's a direct correlation, so we dont have to duplicate
// the logic from there
// The .x branch will always point to the latest minor (for that major), so a "minor" project will be on the .x branch
// unless there is more recent (higher) minor.
// The current version is being worked, is always unreleased
// the tip of the previous major is unreleased for sure, be it a minor or a bugfix
// if the previous major is a x.y.0 release, then the tip of the minor before that (y-1) is also unreleased
// the last bugfix for this minor series is always unreleased
// we found an unreleased minor
// we found that the previous minor is staged but not yet released
// in this case, the minor before that has a bugfix, should there be such a minor
// Current is an unreleased major: x.0.0 so we have to look for other unreleased versions in the previous major
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** List of files to concatenate */
/** line to add at the top of the target file */
// To remove duplicate lines
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// this is needed for isInternal
// pkg private for tests
// for the distribution as a file, just depend on the artifact directly
// no extraction allowed for rpm, deb or docker
// for the distribution extracted, add a root level task that does the extraction, and depend on that
// extracted configuration as an artifact consisting of the extracted distribution directory
// ensure a root level download task exists
// NOTE: this is *horrendous*, but seems to be the only way to check for the existence of a registered task
// already setup this version
// fall through: register the task
// add task for extraction, delaying resolving config until runtime
// this header is not a credential but we hack the capability to send this header to avoid polluting our download stats
// all other repos should ignore the special group name
//artifacts.elastic.co", FAKE_IVY_GROUP);
// external, so add snapshot repo as well
//snapshots.elastic.co", FAKE_SNAPSHOT_IVY_GROUP);
/**
// non-external project, so depend on local build
/*
//www.apache.org/licenses/LICENSE-2.0
// package private to tests can use
// pkg private so plugin can configure
// pkg private so plugin can configure
// ensure the version parses, but don't store as Version since that removes -SNAPSHOT
// TODO: remove this when distro tests are per distribution
// internal, make this distribution's configuration unmodifiable
// defaults for archive, set here instead of via convention so integ-test-zip can verify they are not set
// rpm, deb or docker
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// This will make sure the task is not considered up to date if the resources are changed.
/*
//www.apache.org/licenses/LICENSE-2.0
// pkg private, for internal use
// internal, make this jdks configuration unmodifiable
/*
//www.apache.org/licenses/LICENSE-2.0
// depend on the jdk directory "artifact" from the root project
// root project
// ensure a root level jdk download task exists
// all other repos should ignore the special jdk artifacts
// NOTE: this is *horrendous*, but seems to be the only way to check for the existence of a registered task
// already setup this version
// fall through: register the task
// decompose the bundled jdk version, broken into elements as: [feature, interim, update, build]
// Note the "patch" version is not yet handled here, as it has not yet been used by java.
// add fake ivy repo for jdk url
//artifactory.elstc.co/artifactory/oss-jdk-local/");
// current pattern since 12.0.1
//download.oracle.com");
// simpler legacy pattern from JDK 9 to JDK 12 that we are advocating to Oracle to bring back
//download.oracle.com");
// add the jdk as a "dependency"
// add task for extraction
// delay resolving jdkConfig until runtime
// TODO: look into doing this as an artifact transform, which are cacheable starting in gradle 5.3
// remove extra unnecessary directory levels
/*
/*
//github.com/gradle/gradle/issues/3982 and https://discuss.gradle.org/t/tar-and-untar-losing-symbolic-links/2039
/*
// this happens on the top-level directories in the archive, which we are removing
// finally remove the top-level directories from the output path
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The starting length of the buffer */
/** The buffer of bytes sent to the stream */
/** Offset of the start of unwritten data in the buffer */
/** Offset of the end (semi-open) of unwritten data in the buffer */
// always flush with newlines instead of adding to the buffer
// first try shifting the used buffer back to the beginning to make space
// otherwise extend the buffer
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// ensure the input directory exists
// start the reaper
// same jvm as gradle
// no need for a big heap, just need to read some files and execute
// be explicit for stdin, we use closing of the pipe to signal shutdown to the reaper
// when running inside the Elasticsearch build just pull find the jar in the runtime classpath
// copy the reaper jar
// Track system property keys as an input so our build cache key will change if we add properties but values are still ignored
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//" + host + "/_cluster/health?wait_for_nodes=>=" + numberOfNodes + "&wait_for_status=yellow"));
// found trusted cert entry
// enforce Java version
/**
/**
/**
// gradle/groovy does not properly escape the double quote for windows
// Initialize global build parameters
// Make sure than any task execution generates and prints build info
// if JAVA_HOME is not set,so we use the JDK that Gradle was run with.
//github.com/elastic/elasticsearch/issues/31399 details.",
// Since it costs IO to compute this, and is done at configuration time we want to cache this if possible
// It's safe to store this in a static variable since it's just a primitive so leaking memory isn't an issue
// Count physical cores on any Linux distro ( don't count hyper-threading )
// the ID of the CPU socket
// Number of cores not including hyper-threading
// Ask macOS to count physical CPUs for us
/*
//git-scm.com/docs/gitrepository-layout and https://git-scm.com/docs/git-worktree.
// this is a git repository, we can read HEAD directly
// this is a git worktree, follow the pointer to the repository
// this is the common case
// Check packed references for commit ID
// we are in detached HEAD state
// for now, do not be lenient until we have better understanding of real-world scenarios where this happens
// Since all tasks depend on this task, and it always runs for every build, this makes sure that lifecycle tasks will still
// correctly report as UP-TO-DATE, since the convention is a lifecycle task (i.e. assemble, build, etc) will only be marked as
// UP-TO-DATE if all upstream tasks were also UP-TO-DATE.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Other plugins this plugin extends through SPI */
/** True if the plugin requires the elasticsearch keystore to exist, false otherwise. */
/** A license file that should be included in the built plugin zip. */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: we should be able to default this to eg compile deps, but we need to move the licenses
// check from distribution to core (ie this should only be run on java projects)
/** A collection of jar files that should be checked. */
/** The directory to find the license and sha files in. */
/** A map of patterns to prefix, used to find the LICENSE and NOTICE file. */
/** Names of dependencies whose shas should not exist. */
/**
/**
// no dependencies to check
// TODO: why do we support suffix of LICENSE *and* LICENSE.txt??
// local deps should not have sha files!
// order is the same for keys and values iteration since we use a linked hashmap
// TODO: shouldn't have to trim, sha files should not have trailing newline
// try the other suffix...TODO: get rid of this, just support ending in .txt
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// we always include all source files, and exclude what should not be checked
// exclude sh files that might have the executable bit set
/*.sh");
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
// we always include all source files, and exclude what should not be checked
// exclude known binary extensions
/*.gz")
/*.ico")
/*.jar")
/*.zip")
/*.jks")
/*.crt")
/*.keystore")
/*.png");
/*
// add mandatory rules
// TODO: fail if pattern contains a newline, it won't work (currently)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We use compile classpath normalization here because class implementation changes are irrelevant for the purposes of jar hell.
// We only care about the runtime classpath ABI here.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Don't pick up all source sets like the java9 ones as logger-check doesn't support the class format
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Run only after everything is compiled
// some test projects don't have a main source set
// Don't load base classes if we don't have any tests.
// This allows defaults to be configured for projects that don't have any tests
//
// TODO when base classes are set, check for classes that extend them
// TODO: check for non public classes that seem like tests
// TODO: check for abstract classes that implement the naming conventions
// No empty enabled tasks
// TODO: check that the testing tasks are included in the right task based on the name ( from the rule )
// Include the message to get more info to get more a more useful message when running Gradle without -s
// Loading the classes depends on the classpath, so we could make this an input annotated with @Classpath.
// The reason we don't is that test classes are already inputs and while the dependencies are needed to load
// the classes these don't influence the checks done by this task.
// A side effect is that we could mark as up-to-date with missing dependencies, but these will be found when
// running the tests.
// First we visit the root directory
// And it package is empty string regardless of the directory name
// Go up one package by jumping back to the second to last '.'
// Don't initialize the class to save time. Not needed for this test and this doesn't share a VM with any other tests.
/*
//www.apache.org/licenses/LICENSE-2.0
// These are SelfResolvingDependency, and some of them backed by file collections, like the Gradle API files,
// or dependencies added as `files(...)`, we can't be sure if those are third party or not.
// err on the side of scanning these to make sure we don't miss anything
// don't scan provided dependencies that we already scanned, e.x. don't scan cores dependencies for every plugin
// Mark successful third party audit check
// We need to clean up to make sure old dependencies don't linger
// exclude classes from multi release jars
/**");
// Deal with multi release jars:
// The order is important, we iterate here so we don't depend on the order in which Gradle executes the spec
// We extract multi release jar classes ( if these exist ) going from 9 - the first to support them, to the
// current `targetCompatibility` version.
// Each extract will overwrite the top level classes that existed before it, the result is that we end up
// with a single version of the class in `jarExpandDir`.
// This will be the closes version to `targetCompatibility`, the same class that would be loaded in a JVM
// that has `targetCompatibility` version.
// This means we only scan classes that would be loaded into `targetCompatibility`, and don't look at any
// pther version specific implementation of said classes.
/**");
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The parent dependency licenses task to use configuration from */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
// we get invoked with stubbed details, there is no way to introspect this other than catching this exception
// we get invoked with stubbed details, there is no way to introspect this other than catching this exception
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// ensure the target extraction path is empty
// copy the file from the archive using a small buffer to avoid heaping
// check if the underlying file system supports POSIX permissions
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// all distributions used by distro tests. this is temporary until tests are per distribution
// TODO: it would be useful to also have the SYSTEM_JAVA_HOME setup in the root project, so that running from GCP only needs
// a java for gradle to run, and the tests are self sufficient and consistent with the java they use
// this condition ensures windows boxes get windows distributions, and linux boxes get linux distributions
// Only VM sub-projects that are specifically opted-in to testing Docker should
// have the Docker task added as a dependency. Although we control whether Docker
// is installed in the VM via `Vagrantfile` and we could auto-detect its presence
// in the VM, the test tasks e.g. `destructiveDistroTest.default-docker` are defined
// on the host during Gradle's configuration phase and not in the VM, so
// auto-detection doesn't work.
//
// The shouldTestDocker property could be null, hence we use Boolean.TRUE.equals()
// bats doesn't run on windows
// was not passed in, so randomly choose one from bwc versions
// Upgrade tests will go from current to current when the BWC tests are disabled to skip real BWC tests
// setup jdks used by the distro tests, and by gradle executing
// setup VM used by these tests
// pass these along to get correct build scans
// temporary, until we have tasks per distribution
// temporary, until we have tasks per distribution
// write bwc version, and append -SNAPSHOT if it is an unreleased version
// TODO: this is serializable, need to think how to represent this as an input
// inputs.property("bwc_versions", bwcVersions);
// this is always true, but bats tests rely on it. It is just temporary until bats is removed.
// temporary, until we have tasks per distribution
// All our Docker images include a bundled JDK so it doesn't make sense to test without one
// We don't configure distributions for prior versions for Docker. This is because doing
// so prompts Gradle to try and resolve the Docker dependencies, which doesn't work as
// they can't be downloaded via Ivy (configured in DistributionDownloadPlugin). Since we
// need these for the BATS upgrade tests, and those tests only cover .rpm and .deb, it's
// OK to omit creating such distributions in the first place. We may need to revisit
// this in the future, so allow upgrade testing using Docker containers.
// upgrade version is always bundled jdk
// NOTE: this is mimicking the old VagrantTestPlugin upgrade behavior. It will eventually be replaced
// witha dedicated upgrade test from every bwc version like other bwc tests
// temporary until distro tests have one test per distro
// return true if the project is for a windows VM, false otherwise
// remove optional leading and trailing quotes and whitespace
/**
// Not yet supported.
// Assume that Docker for Mac is installed, since Docker is part of the dev workflow.
// We don't attempt to check the current flavor and version of Linux unless we're
// running in CI, because we don't want to stop people running the Docker tests in
// their own environments if they really want to.
// Only some hosts in CI are configured with Docker. We attempt to work out the OS
// and version, so that we know whether to expect to find Docker. We don't attempt
// to probe for whether Docker is available, because that doesn't tell us whether
// Docker is unavailable when it should be.
// Check if this is output from the test suite itself (e.g. afterTest or beforeTest)
// Hold on to any repro messages so we can report them immediately on test case failure
// if the test suite failed, report all captured output
// It's not explicit what the threading guarantees are for TestListener method execution so we'll
// be explicitly safe here to avoid interleaving output from multiple test suites
// make sure we've flushed everything to disk before reading
// go back and fetch the reproduction line for this test failure
// include test failure exception stacktraces in test suite output log
/**
// there's no need to keep this stuff on disk after suite execution
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// configure the cluster name eagerly so nodes know about it
// Can only configure master nodes if we have node names defined
/*
//www.apache.org/licenses/LICENSE-2.0
// package private just so test clusters plugin can access to wire up task dependencies
// we change the underlying distribution when changing the test distribution of the cluster.
/**
// make sure we always start fresh
/**
//TODO: Remove this when system modules are available
// only install modules that are not already bundled with the integ-test distribution
// ES_PATH_CONF is also set as an environment variable and for a reference to ${ES_PATH_CONF}
// to work ES_JAVA_OPTS, we need to make sure that ES_PATH_CONF before ES_JAVA_OPTS. Instead,
// we replace the reference with the actual value in other environment variables
// Support passing in additional JVM arguments
// Windows requires this as it defaults to `c:\windows` despite ES_TMPDIR
// Override the system hostname variables for testing
// Don't inherit anything from the environment for as that would lack reproducibility
// don't buffer all in memory, make sure we don't block on the default pipes
// This is a special case. If start() throws an exception the plugin will still call stop
// Another exception here would eat the orriginal.
// Test clusters are not reused, don't spend time on a graceful shutdown
// Clean up the ports file in case this is started again.
// No-op if the process has already exited by itself.
// Stop all children last - if the ML processes are killed before the ES JVM then
// they'll be recorded as having failed and won't restart when the cluster restarts.
// ES could actually be a child when there's some wrapper process like on Windows,
// and in that case the ML processes will be grandchildren of the wrapper.
// check to see if the previous message (possibly combined from multiple lines) was an error or
// warning as we want to show all of them
// We combine multi line log messages to make sure we never break exceptions apart
// Start configuration from scratch in case of a restart
/**
// Throw away the first name as the archives have everything in a single top level folder we are not interested in
// Note does not work for network drives, e.g. Vagrant
// Default the watermarks to absurdly low to prevent the tests from failing on nodes without enough disk space
// increase script compilation limit since tests can rapid-fire script compilations
// Temporarily disable the real memory usage circuit breaker. It depends on real memory usage which we have no full control
// over and the REST client will not retry on circuit breaking exceptions yet (see #31986 for details). Once the REST client
// can retry on circuit breaking exceptions, we can revert again to the default configuration.
// Don't wait for state, just start up quickly. This will also allow new and old nodes in the BWC case to become the master
// Make sure no duplicate config keys
// TODO: We may be able to simplify this with Gradle 5.6
// https://docs.gradle.org/nightly/release-notes.html#improved-handling-of-zip-archives-on-classpaths
/*.jar"));
/*.jar"));
/*.jar"));
/*.jar"));
// Installing plugins at config time and loading them when nods start requires additional time we need to
// account for
/**
/*
// no data was ready to be consumed and rather than continuously spinning, pause
// for some time to avoid excessive CPU usage. Ideally we would use the JDK
// WatchService to receive change notifications but the WatchService does not have
// a native MacOS implementation and instead relies upon polling with possible
// delays up to 10s before a notification is received. See JDK-7133447.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// enable the DSL to describe clusters
// provide a task to be able to list defined clusters.
// When we know what tasks will run, we claim the clusters of those task to differentiate between clusters
// that are defined in the build script and the ones that will actually be used in this invocation of gradle
// we use this information to determine when the last task that required the cluster executed so that we can
// terminate the cluster right away and free up resources.
// Before each task, we determine if a cluster needs to be started for that task.
// After each task we determine if there are clusters that are no longer needed.
// Create an extensions that allows describing clusters
// Once we know all the tasks that need to execute, we claim all the clusters that belong to those and count the
// claims so we'll know when it's safe to stop them.
// we only start the cluster before the actions, so we'll not start it if the task is up-to-date
// always unclaim the cluster, even if _this_ task is up-to-date, as others might not have been
// and caused the cluster to start.
// If the task fails, and other tasks use this cluster, the other task will likely never be
// executed at all, so we will never be called again to un-claim and terminate it.
// task failed or this is the last one to stop
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Check for exclusive access
/*
//www.apache.org/licenses/LICENSE-2.0
// if only one fixture is used, that's this one, but without a compose file that's not a valid configuration
// Configure ports for the tests as system properties.
// We only know these at execution time so we need to do it in doFirst
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Check if the Docker binary exists
/**
// Check if the Docker binary exists
// Since we use a multi-stage Docker build, check the Docker version since 17.05
// Check that we can execute a privileged command
/**
/**
/**
/**
/**
/**
/**
/*
// Some other problem, print the error
/**
// The redundant cast is to silence a compiler warning.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//testanything.org) is used by BATS for its output format.
// haven't reached start of bats test yet, pass through whatever we see
/* These might be failure report lines or comments or whatever. Its hard
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// most significant version is good
// else equal, so check next element
/**
/* nothing to do */}
/* nothing to do */ }
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: should verify this against the Vagrantfile, but would need to do so in afterEvaluate once vagrantfile is unmodifiable
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// pkg private so plugin can set this after construction
// pass through env
// output from vagrant needs to be manually curated because --machine-readable isn't actually "readable"
// start the configuration VM if it hasn't been started yet
// Destroying before every execution can be annoying while iterating on tests locally. Therefore, we provide a flag that defaults
// to true that can be used to control whether or not to destroy any test boxes before test execution.
// register box to be shutdown if gradle dies
// We lock the provider to virtualbox because the Vagrantfile specifies lots of boxes that only work
// properly in virtualbox. Virtualbox is vagrant's default but its possible to change that default and folks do.
// stops the VM if refs are down to 0, or force was called
// convert the given path from an elasticsearch repo path to a VM path
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/* We don't want to try to be a full terminal emulator but we want to
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// catch if we have a failure to even run the script at all above, equivalent to set -e, sort of
// start inline bash script
// end inline bash script
/*getLogger().error("Failed command, dumping dmesg", e);
/*
//www.apache.org/licenses/LICENSE-2.0
// system.parent = extensions loader.
// note: for jigsaw, this evilness will need modifications (e.g. use jrt filesystem!)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// use an initial dummy delegate to avoid doing a conditional on every write
/**
// We use an anonymous inner class here because Gradle cannot properly snapshot this input for the purposes of
// incremental build if we use a lambda. This ensures LoggedExec tasks that declare output can be UP-TO-DATE.
// the file may not exist if the command never output anything
/**
/**
/**
/**
// currently snapshot is not taken into account
/**
// fall trough
/**
/**
// Since we are mutating private static fields from a public static inner class we need to suppress
// accessibility controls here.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//s3.amazonaws.com/artifacts.elastic.co/maven";
// add an insecure maven repository to the build.gradle
//s3.amazonaws.com/artifacts.elastic.co/ivy";
// add an insecure ivy repository to the build.gradle
/*
//www.apache.org/licenses/LICENSE-2.0
//s3.amazonaws.com/artifacts.elastic.co/maven");
//s3.amazonaws.com/artifacts.elastic.co/maven");
//artifacts.elastic.co/maven");
/*
//www.apache.org/licenses/LICENSE-2.0
// unreleased major and two unreleased minors ( minor in feature freeze )
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: check reuse of root task across projects MOVE TO UNIT TEST
// TODO: future: check integ-test-zip to maven, snapshots to snapshot service for external project
// for debugging
/*
//www.apache.org/licenses/LICENSE-2.0
// create a new project in each iteration, so that we know we are resolving the only additional project being created
// note: no non bundled jdk for bwc
// note: no non bundled jdk for bwc
// create a distro and finalize its configuration
// check the download plugin can be fully configured
/*
//www.apache.org/licenses/LICENSE-2.0
// generate a new temporary folder and make sure it does not exists
// cleanup
// generate a new temporary folder and make sure it does not exists
// cleanup
/*
//www.apache.org/licenses/LICENSE-2.0
// This is a side effect of compile time reference
/*
//www.apache.org/licenses/LICENSE-2.0
// 3 import configs, 3 export configs
// for debugging
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//localhost/"));
//localhost/"));
// FIXME: distribution download plugin doesn't support running externally
//github.com/elastic/elasticsearch/issues/47123")
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//todo", "// some stuff, toDo");
/*.java");
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Build the sample jars
/*
//www.apache.org/licenses/LICENSE-2.0
// wait for "Connection worker" to die
// add expectThrows from junit 5
//linux.oracle.com/\"",
//bugzilla.oracle.com/\"",
//bugzilla.oracle.com/");
//linux.oracle.com/");
/**
/**
// Use / on Windows too, the build script is not happy with \
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Skip any methods that have overrieds/ shadows.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// consider 40% of all iterations as warmup iterations
//GC between trials to reduce the likelihood of a GC occurring in the middle of a trial.
/**
// request a full GC ...
// ... and give GC a chance to run
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// throughput calculation is based on the total (Wall clock) time it took to generate all samples
// convert ns -> ms without losing precision
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// when the generator is done, there are no more data -> shutdown client
//We need to wait until the queue is drained
//no op
// reset data structures
// also send the last bulk:
//measure only service time, latency is not that interesting for a throughput benchmark
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// busy spin
// no op
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// short circuit the call to the transport layer
/*
//www.apache.org/licenses/LICENSE-2.0
// simulate at least a realistic amount of data that gets serialized
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
//default buffer limit is 100MB
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
/**
// Intentionally does nothing
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing.
// The superclass implementation of method will clear the credentials from the cache, but we don't
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// warn code
// warn agent
// warn agent
// warn agent
// quoted warning value, captured
// quoted RFC 1123 date format
// opening quote
// weekday
// 2-digit day
// month
// 4-digit year
// (two-digit hour):(two-digit minute):(two-digit second)
// GMT
// closing quote (optional, since an older version can still send a warn-date)
/**
// start of line, leading space
// quoted RFC 1123 date format
// opening quote
// day of week, atomic group to prevent backtracking
// 2-digit day
// month, atomic group to prevent backtracking
// 4-digit year
// (two-digit hour):(two-digit minute):(two-digit second)
// GMT
// closing quote (optional, since an older version can still send a warn-date), end of line
/**
/**
/**
/*
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We don't rely on default headers supported by HttpAsyncClient as those cannot be replaced.
// These are package private for tests.
/**
//cloud.elastic.co and will resemble a string like the following
// there is an optional first portion of the cloudId that is a human readable string, but it is not used.
// once decoded the parts are separated by a $ character
/**
/**
/**
// TODO should we throw an IAE if we have two nodes with the same host?
/**
/**
//mark host dead and retry against next one
//mark host alive and don't retry, as the error should be a request problem
/**
/**
/**
/*
/*
/*
/*
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
//ignore is a special parameter supported by the clients, shouldn't be sent to es
// request headers override default headers, so we don't add default headers if they exist as request headers
//we stream the request body if the entity allows for it
//404 never causes error if returned for a HEAD request
//404 never causes error if returned for a HEAD request
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// best effort to ensure that it looks like "/base/path" rather than "/base/path/"
/**
/**
/**
//default timeouts are all infinite
//default settings for connection pooling may be too constraining
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// This class extends RuntimeException in order to deal with wrapping that is done in FutureUtils on exception.
// if the exception is not of type ElasticsearchException or RuntimeException it will be wrapped in a UncategorizedExecutionException
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// this test requires a strictly increasing timer. This ensures that even if we call this time supplier in a very tight
// loop we always notice time moving forward. This does not happen for real timer implementations
// (e.g. on Linux <code>clock_gettime</code> provides microsecond resolution).
//check that from here on the timeout does not increase
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//maximum buffer that this test ends up allocating is 50MB
//everything goes well
//we use reflection to make sure that the class can be instantiated from the outside, and the constructor is public
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//1]", new Node(new HttpHost("1")).toString());
//1, attributes={foo=[bar], baz=[bort, zoom]}]",
//1, roles=mdi]", new Node(new HttpHost("1"),
//1, version=ver]", new Node(new HttpHost("1"),
//1, name=nam]", new Node(new HttpHost("1"),
//1, bound=[http://1, http://2]]", new Node(new HttpHost("1"),
//1, bound=[http://1, http://2], name=nam, version=ver, roles=m, attributes={foo=[bar], baz=[bort, zoom]}]",
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Evil entity without a charset
//check that the body is still readable as most entities are not repeatable
//test a non repeatable entity
// Evil entity without a charset
//check that the body is still readable as most entities are not repeatable
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Test that adding parameters with a null value is ok.
// Test that adding a duplicate parameter fails
// Mutate request or method but keep everything else constant
// randomRequest can't produce this value
/*
//www.apache.org/licenses/LICENSE-2.0
//test a non repeatable entity
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/elastic/elasticsearch/issues/49094", inFipsJvm());
// Build a keystore of default type programmatically since we can't use JKS keystores to
// init a KeyManagerFactory in FIPS 140 JVMs.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//");
//");
/**
//github.com/elastic/elasticsearch/issues/24069
//this way we get notified if the default ever changes
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//returns a different status code depending on the path
//verify that shutting down some hosts doesn't matter as long as one working host is left behind
//we don't test status codes that are subject to retries as they interfere with hosts being stopped
//we don't test status codes that are subject to retries as they interfere with hosts being stopped
//github.com/elastic/elasticsearch/issues/45577")
//we wait for the request to get to the server-side otherwise we almost always cancel
// the request artificially on the client-side before even sending it
/**
/*
// Windows isn't consistent here. Sometimes the message is even null!
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
//no exception gets thrown although we got a 404
//first request causes all the hosts to be blacklisted, the returned exception holds one suppressed exception each
//first request causes all the hosts to be blacklisted, the returned exception holds one suppressed exception each
//check that one different host is resurrected at each new attempt
//after the first request, all hosts are blacklisted, a single one gets resurrected each time
//after the first request, all hosts are blacklisted, a single one gets resurrected each time
//mark one host back alive through a successful request and check that all requests after that are sent to it
//let the selected host catch up on number of failures, it gets selected a consecutive number of times as it's the one
//selected to be retried earlier (due to lower number of failures) till all the hosts have the same number of failures
/*
/*
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//returns a different status code depending on the path
//copy request body to response body so we can verify it was sent
//copy request headers to response headers so we can verify they were sent
// provide the username/password for every request
// disable preemptive auth by ignoring any authcache
// don't use the "persistent credentials strategy"
/**
//github.com/elastic/elasticsearch/issues/24069
/**
//calling abort before the request is sent is a no-op
//expected
//expected
/**
/**
/**
/**
/**
/**
//a trailing slash gets automatically added if a pathPrefix is configured
//pathPrefix is not required to start with '/', will be added automatically
//a trailing slash gets automatically added if a pathPrefix is configured
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Call the callback asynchronous to better simulate how async http client works
//return the desired status code or exception depending on the path
//return the same body that was sent
//return the same headers that were sent
/**
/**
/**
/**
//error status codes should cause an exception to be thrown
//no exception gets thrown although we got an error status code, as it was configured to be ignored
//IOExceptions should be let bubble up
// And we do all that so the thrown exception has our method in the stacktrace
// And we do all that so the thrown exception has our method in the stacktrace
// And we do all that so the thrown exception has our method in the stacktrace
// And we do all that so the thrown exception has our method in the stacktrace
// And we do all that so the thrown exception has our method in the stacktrace
// And we do all that so the thrown exception has our method in the stacktrace
// And we do all that so the thrown exception has our method in the stacktrace
/**
/**
/**
//randomly add some ignore parameter, which doesn't get sent as part of the request
// request level headers override default headers
//all good
//randomize between sync and async methods
/**
// 0 is getStackTrace
// 1 is this method
// 2 is the caller, what we want
/*
//www.apache.org/licenses/LICENSE-2.0
///"), new ResponseListener() {
///", exception.getMessage());
/*", emptyMap);
/*", uri.getPath());
/*", emptyMap);
// Normal cases where the node selector doesn't reject all living nodes
/*
//1, version=1], [host=http://2, version=2], "
//3, version=3]] and dead []";
// Mark all the nodes dead for a few test cases
/*
/*
/*
//1, version=1], [host=http://2, version=2], "
//3, version=3]]";
/*
/*
/*
// Calling it again rotates the set of results
/**
//test the transition from negative to positive values
//test the highest positive values up to MAX_VALUE
//test the transition from MAX_VALUE to MIN_VALUE
//this is the only time where there is most likely going to be a jump from a node
//to another one that's not necessarily the next one.
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// tag::example[]
// end::example[]
// tag::rest-client-options-singleton
// <1>
// <2>
// end::rest-client-options-singleton
//tag::rest-client-init
//end::rest-client-init
//tag::rest-client-close
//end::rest-client-close
//tag::rest-client-init-default-headers
// <1>
//end::rest-client-init-default-headers
//tag::rest-client-init-node-selector
// <1>
//end::rest-client-init-node-selector
//tag::rest-client-init-allocation-aware-selector
// <1>
/*
//end::rest-client-init-allocation-aware-selector
//tag::rest-client-init-failure-listener
// <1>
//end::rest-client-init-failure-listener
//tag::rest-client-init-request-config-callback
// <1>
//end::rest-client-init-request-config-callback
//tag::rest-client-init-client-config-callback
// <1>
//end::rest-client-init-client-config-callback
//tag::rest-client-sync
// <1>
// <2>
//end::rest-client-sync
//tag::rest-client-async
// <1>
// <2>
// <3>
// <4>
//end::rest-client-async
//tag::rest-client-parameters
//end::rest-client-parameters
//tag::rest-client-body
//end::rest-client-body
//tag::rest-client-body-shorter
//end::rest-client-body-shorter
//tag::rest-client-options-set-singleton
//end::rest-client-options-set-singleton
//tag::rest-client-options-customize-header
//end::rest-client-options-customize-header
//tag::rest-client-async-example
//let's assume that the documents are stored in an HttpEntity array
// <1>
// <2>
//end::rest-client-async-example
//tag::rest-client-async-cancel
// <1>
// <2>
//end::rest-client-async-cancel
//tag::rest-client-response2
// <1>
// <2>
// <3>
// <4>
// <5>
//end::rest-client-response2
//tag::rest-client-config-timeouts
//end::rest-client-config-timeouts
//tag::rest-client-config-threads
//end::rest-client-config-threads
//tag::rest-client-config-basic-auth
//end::rest-client-config-basic-auth
//tag::rest-client-config-disable-preemptive-auth
// <1>
//end::rest-client-config-disable-preemptive-auth
//tag::rest-client-config-encrypted-communication
//end::rest-client-config-encrypted-communication
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-apis.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-put-follow.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-put-follow.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-pause-follow.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-pause-follow.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-resume-follow.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-resume-follow.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-unfollow.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-unfollow.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-forget-follower.html">the docs</a> for more details
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-forget-follower.html">the docs</a> for more details
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-put-auto-follow-pattern.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-put-auto-follow-pattern.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-delete-auto-follow-pattern.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-delete-auto-follow-pattern.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-auto-follow-pattern.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-auto-follow-pattern.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-pause-auto-follow-pattern.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-pause-auto-follow-pattern.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-resume-auto-follow-pattern.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-resume-auto-follow-pattern.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-stats.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-stats.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-follow-stats.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-follow-stats.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-follow-info.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-follow-info.html">
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html">Cluster API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html"> Cluster Update Settings
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html"> Cluster Update Settings
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/cluster-get-settings.html"> Cluster Get Settings
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/cluster-get-settings.html"> Cluster Get Settings
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html"> Cluster Health API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html"> Cluster Health API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/cluster-remote-info.html"> Remote cluster info
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/cluster-remote-info.html"> Remote cluster info
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/enrich-apis.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/put-enrich-policy-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/put-enrich-policy-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/delete-enrich-policy-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/delete-enrich-policy-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-enrich-policy-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-enrich-policy-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/enrich-stats-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/enrich-stats-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/execute-enrich-policy-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/execute-enrich-policy-api.html">
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/graph-explore-api.html">Graph API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/graph-explore-api.html">Graph API
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-high-ilm-ilm-get-lifecycle-policy.html">
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-high-ilm-ilm-get-lifecycle-policy.html">
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-high-ilm-ilm-put-lifecycle-policy.html">
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-high-ilm-ilm-put-lifecycle-policy.html">
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/**
//www.elastic.co/guide/en/elasticsearch/client/java-rest/current/
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices.html">Indices API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-delete-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-delete-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-mapping.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-mapping.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-field-mapping.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-field-mapping.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-open-close.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-open-close.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-open-close.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-open-close.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-refresh.html"> Refresh API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-refresh.html"> Refresh API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-flush.html"> Flush API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-flush.html"> Flush API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/indices-synced-flush-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/indices-synced-flush-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-settings.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-settings.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-forcemerge.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-forcemerge.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-forcemerge.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-forcemerge.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-clearcache.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-clearcache.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-exists.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-exists.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-shrink-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-shrink-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-shrink-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-shrink-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-split-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-split-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-split-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-split-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-clone-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-clone-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-clone-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-clone-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-rollover-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-rollover-index.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html"> Indices Aliases API on
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html"> Indices Aliases API on
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html"> Update Indices Settings
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html"> Update Indices Settings
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html"> Index Templates API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html"> Index Templates API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-validate.html"> Validate Query API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-validate.html"> Validate Query API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html"> Index Templates API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html"> Index Templates API
/**
/**
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html">Analyze API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html">Analyze API on elastic.co</a>
/**
/**
/**
/**
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html"> Index Templates API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html"> Index Templates API
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html">Ingest API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html"> Put Pipeline API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html"> Put Pipeline API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-pipeline-api.html"> Get Pipeline API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/get-pipeline-api.html"> Get Pipeline API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/delete-pipeline-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/delete-pipeline-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/simulate-pipeline-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/simulate-pipeline-api.html">
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/licensing-apis.html">
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// No changes is required
// Need to convert into JSON
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-apis.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-job.html">ML PUT job documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-job.html">ML PUT job documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-job.html">ML GET job documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-job.html">ML GET job documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-job-stats.html">Get job stats docs</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-job-stats.html">Get job stats docs</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-expired-data.html">ML Delete Expired Data
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-expired-data.html">ML Delete Expired Data
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-job.html">ML Delete job documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-job.html">ML Delete Job documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-open-job.html">ML Open Job documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-open-job.html">ML Open Job documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-close-job.html">ML Close Job documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-close-job.html">ML Close Job documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-flush-job.html">Flush ML job documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-flush-job.html">Flush ML job documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/ml-forecast.html">Forecast ML Job Documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/ml-forecast.html">Forecast ML Job Documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-forecast.html">Delete Job Forecast
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-forecast.html">Delete Job Forecast
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-snapshot.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-snapshot.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-revert-snapshot.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-revert-snapshot.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-datafeed.html">ML PUT datafeed documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-datafeed.html">ML PUT datafeed documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-datafeed.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-datafeed.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-datafeed.html">ML GET datafeed documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-datafeed.html">ML GET datafeed documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-datafeed.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-datafeed.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-start-datafeed.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-start-datafeed.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-stop-datafeed.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-stop-datafeed.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-datafeed-stats.html">Get datafeed stats docs</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-preview-datafeed.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-datafeed-stats.html">Get datafeed stats docs</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-preview-datafeed.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-job.html">ML Update Job Documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-job.html">ML Update Job Documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-bucket.html">ML GET buckets documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-bucket.html">ML GET buckets documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-category.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-category.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-snapshot.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-snapshot.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-snapshot.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-snapshot.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-overall-buckets.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-overall-buckets.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-record.html">ML GET records documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-record.html">ML GET records documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-post-data.html">ML POST Data documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-post-data.html">ML POST Data documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-calendar.html">ML GET calendars documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-calendar.html">ML GET calendars documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-influencer.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-influencer.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-calendar.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-calendar.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-calendar-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-calendar-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-calendar-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-calendar-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-calendar.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-calendar.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-calendar-event.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-calendar-event.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-post-calendar-event.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-post-calendar-event.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-calendar-event.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-calendar-event.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-filter.html">ML PUT Filter documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-filter.html">ML PUT Filter documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-filter.html">ML GET Filter documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-filter.html">ML GET Filter documentation</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-filter.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-filter.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-filter.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-filter.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-ml-info.html">Machine Learning info</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-ml-info.html">Machine Learning info</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-find-file-structure.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-find-file-structure.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-set-upgrade-mode.html">Set Upgrade Mode</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-set-upgrade-mode.html">Set Upgrade Mode</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/put-dfanalytics.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/put-dfanalytics.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-dfanalytics.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-dfanalytics.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-dfanalytics-stats.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-dfanalytics-stats.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/start-dfanalytics.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/start-dfanalytics.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/stop-dfanalytics.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/stop-dfanalytics.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/delete-dfanalytics.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/delete-dfanalytics.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/evaluate-dfanalytics.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/evaluate-dfanalytics.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/explain-dfanalytics.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/explain-dfanalytics.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-inference.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-inference.html">
/**
/**
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-inference-stats.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-inference-stats.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/delete-inference.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/delete-inference.html">
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/migration-api.html">
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** the total number of nodes that the operation was carried on */
/** the number of nodes that the operation has failed on */
/** the number of nodes that the operation was successful on */
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Contains only status utility methods
// Bulk API only supports newline delimited JSON or Smile. Before executing
// the bulk, we need to check that all requests have the same content-type
// and this content-type is supported by the Bulk API.
/*
// Version params are not currently supported by the source exists API so are not passed
// The Java API allows update requests with different content types
// set for the partial document and the upsert document. This client
// only accepts update requests that have the same content types set
// for both doc and upsert.
/**
// we set "group_by" to "none" because this is the response format we can parse back
/**
/**
/**
// the default in AbstractBulkByScrollRequest is Float.POSITIVE_INFINITY,
// but we don't want to add that to the URL parameters, instead we use -1
// Always explicitly place the ignore_unavailable value.
/**
/**
//encode each part (e.g. index, type and id) separately before merging them into the path
//we prepend "/" to the path part to make this path absolute, otherwise there can be issues with
//paths that start with `-` or contain `:`
//the authority must be an empty string and not null, else paths that being with slashes could have them
//misinterpreted as part of the authority.
//manually encode any slash that each part may contain
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/indices.html">Indices API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html">Cluster API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html">Ingest API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html">Snapshot API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-apis.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ccr-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/tasks.html">Task Management API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/info-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/graph-explore-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/licensing-apis.html">
/**
//FILL-ME-IN-WE-HAVE-NO-DOCS-YET.com"> X-Pack APIs
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/migration-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-apis.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/transform-apis.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html">Bulk API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html">Bulk API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html">Reindex API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html">Reindex API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html">Reindex API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-update-by-query.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-update-by-query.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-delete-by-query.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-delete-by-query.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-delete-by-query.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-delete-by-query.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-delete-by-query.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-update-by-query.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-update-by-query.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html#docs-reindex-rethrottle">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html#docs-reindex-rethrottle">
/**
/**
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-get.html">Get API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-get.html">Get API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-multi-get.html">Multi Get API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-multi-get.html">Multi Get API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-multi-get.html">Multi Get API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-multi-get.html">Multi Get API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-get.html">Get API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-get.html">Get API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-get.html#_source">Source exists API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-get.html#_source">Source exists API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html">Index API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html">Index API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-count.html">Count API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-count.html">Count API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html">Update API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html">Update API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-delete.html">Delete API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-delete.html">Delete API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html">Search API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html">Search API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-multi-search.html">Multi search API on
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-multi-search.html">Multi search API on
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-multi-search.html">Multi search API on
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-multi-search.html">Multi search API on
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/search-request-body.html#request-body-search-scroll">Search
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/search-request-body.html#request-body-search-scroll">Search
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/search-request-body.html#request-body-search-scroll">Search
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/search-request-body.html#request-body-search-scroll">Search
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/search-request-body.html#_clear_scroll_api">
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/search-request-body.html#_clear_scroll_api">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html">Search Template API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html">Search Template API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-explain.html">Explain API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-explain.html">Explain API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-termvectors.html">Term Vectors API on
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-termvectors.html">Term Vectors API on
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-multi-termvectors.html">Multi Term Vectors API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/docs-multi-termvectors.html">Multi Term Vectors API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-rank-eval.html">Ranking Evaluation API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/multi-search-template.html">Multi Search Template API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/multi-search-template.html">Multi Search Template API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-rank-eval.html">Ranking Evaluation API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-field-caps.html">Field Capabilities API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-using.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-using.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-using.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-using.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-using.html"> Scripting API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-using.html"> Scripting API
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-field-caps.html">Field Capabilities API
/**
/**
/**
/**
/**
// the exception is ignored as we now try to parse the response as an error.
// this covers cases like get where 404 can either be a valid document not found response,
// or an error for which parsing is completely different. We try to consider the 404 response as a valid one
// first. If parsing of the response breaks, we fall back to parsing it as an error.
/**
/**
/**
/**
/**
/**
// the exception is ignored as we now try to parse the response as an error.
// this covers cases like get where 404 can either be a valid document not found response,
// or an error for which parsing is completely different. We try to consider the 404 response as a valid one
// first. If parsing of the response breaks, we fall back to parsing it as an error.
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-apis.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-put-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-put-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-start-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-start-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-stop-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-stop-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-delete-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-delete-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-put-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-put-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-search.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-search.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/rollup-get-rollup-caps.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rollup-put-job.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/rollup-get-rollup-index-caps.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/rollup-get-rollup-index-caps.html">
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api.html">Security APIs on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-user.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-user.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-users.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-users.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-user.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-user.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-role-mapping.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-role-mapping.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-role-mapping.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-role-mapping.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-enable-user.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-enable-user.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-enable-user.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-enable-user.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-disable-user.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-disable-user.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-disable-user.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-disable-user.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-authenticate.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-authenticate.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-has-privileges.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-has-privileges.html">
/**
/**
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-cache.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-cache.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-role-cache.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-role-cache.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-ssl.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-ssl.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-change-password.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-change-password.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-change-password.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-change-password.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-role-mapping.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-role.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-role.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-role.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-role.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-role-mapping.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-role.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-role.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-token.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-token.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-invalidate-token.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-invalidate-token.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-builtin-privileges.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-builtin-privileges.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-privileges.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-privileges.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-privileges.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-privileges.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-privilege.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-privilege.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-api-key.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-api-key.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-api-key.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-api-key.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-invalidate-api-key.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-invalidate-api-key.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delegate-pki-authentication.html"> the
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delegate-pki-authentication.html"> the
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html">Snapshot API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html"> Snapshot and Restore
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/tasks.html">Task Management API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/tasks.html"> Task Management API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/tasks.html"> Task Management API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/tasks.html"> Task Management API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/tasks.html"> Task Management API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/tasks.html"> Task Management API on elastic.co</a>
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/tasks.html"> Task Management API on elastic.co</a>
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/put-transform.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/put-transform.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/update-transform.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/update-transform.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-transform-stats.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-transform-stats.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/delete-transform.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/delete-transform.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/preview-transform.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/preview-transform.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/start-transform.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/start-transform.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/stop-transform.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/stop-transform.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-transform.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/get-transform.html">
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-start.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-start.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-start.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-start.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-put-watch.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-put-watch.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-get-watch.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-get-watch.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-deactivate-watch.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-deactivate-watch.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-delete-watch.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-delete-watch.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-ack-watch.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-ack-watch.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-activate-watch.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-activate-watch.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-execute-watch.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-execute-watch.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-stats.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-stats.html">
/*
//www.apache.org/licenses/LICENSE-2.0
// will ignore if ID is null
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/info-api.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/info-api.html">
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: deprecate and remove this field in favor of initialConnectionTimeout field that is of type TimeValue.
// When rest api versioning exists then change org.elasticsearch.transport.RemoteConnectionInfo to properly serialize
// the initialConnectionTimeout field so that we can properly parse initialConnectionTimeout as TimeValue
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
// total
// successful
// skipped
// failed
// failures
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
//BWC @see org.elasticsearch.action.search.SearchResponse
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Indexer is running, but not actively indexing data (e.g. it's idle). */
/** Indexer is actively indexing data. */
/**
/** Indexer is "paused" and ignoring scheduled triggers. */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// as the response comes from server, we are sure that args[0] will be a list of TermVectorsResponse
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// set values only when different from defaults
/*
//www.apache.org/licenses/LICENSE-2.0
// as the response comes from server, we are sure that args[5] will be a list of TermVector
/**
/**
/**
/**
/**
/**
// as the response comes from server, we are sure that args[1] will be a list of Term
/**
/**
/**
// Class containing a general field statistics for the field
/*
/**
/**
// as the response comes from server, we are sure that args[4] will be a list of Token
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
//When deserializing from XContent we need to wait for all vertices to be loaded before
// Connection objects can be created that reference them. This class provides the interim
// state for connections.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// reverse-engineer if detailed stats were requested -
// mainly here for testing framework's equality tests
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// otherwise inherit settings from parent
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-significantterms-aggregation.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-significantterms-aggregation.html">
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// check to make sure that step details are either all null or all set.
/*
//www.apache.org/licenses/LICENSE-2.0
// ILM
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//package private for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Needs to be declared but not used in constructing the response object
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//package private for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// exactly one of these two members is not null
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// EMPTY is safe here because we never call namedObject
//move to the first alias
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Request the mappings of specific fields */
/**
/** @param fields a list of fields to retrieve the mapping for */
/** Indicates whether default mapping settings should be returned */
/*
//www.apache.org/licenses/LICENSE-2.0
/** Response object for {@link GetFieldMappingsRequest} API */
/**
/**
/**
//pkg-private for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// to have deterministic order
/**
/**
// We start at START_OBJECT since parseIndexEntry ensures that
// We start at START_OBJECT since fromXContent ensures that
// This is just an internal container to make stuff easier for returning
// we assume this is an index entry
// make the order deterministic
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// To compare results we need to make sure the templates are listed in the same order
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Indicates whether default mapping settings should be returned */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// EMPTY is safe here because we never call namedObject
//move to the first alias
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//the index name "_na_" is never read back, what matters are settings, mappings and aliases
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// todo consolidate this parsing with the parsing in PutLicenseResponse
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// parse out the key/value pairs
// only add indices that contain deprecation issues
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Filter out null values returned by {@link EvaluateDataFrameResponse::parseMetric}.
// Metric name not recognized. Return {@code null} value here and filter it out later.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// This one is plural in FileStructure, but singular in FileStructureOverrides
// Sample is not included in the X-Content representation
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
//We leave out the content for server side parity
//We leave out the content for server side parity
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Evaluations
// Evaluation metrics
// Evaluation metrics results
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** List of per-class results. */
/** Fraction of documents for which predicted class equals the actual class. */
/** Name of the class. */
/** Fraction of documents that are either true positives or true negatives wrt {@code className}. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** List of per-class results. */
/** Average of per-class precisions. */
/** Name of the class. */
/** Fraction of documents predicted as belonging to the {@code predictedClass} class predicted correctly. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** List of per-class results. */
/** Average of per-class recalls. */
/** Name of the class. */
/** Fraction of documents actually belonging to the {@code actualClass} class predicted correctly. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// create static hash code from name as there are currently no unique fields per class instance
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// create static hash code from name as there are currently no unique fields per class instance
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// 1 gb maximum
/*
//www.apache.org/licenses/LICENSE-2.0
// PreProcessing
// Model
// Aggregating output
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// We have reached the maximum, signal stream completion.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/* Noop does not matter client side*/ },
/* Does not matter client side*/ },
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/google/cld3/blob/06f695f1c8ee530104416aab5dcf2d6a1414a56a/src/embedding_network.cc
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/* Noop does not matter client side */ },
/* Noop does not matter client side */ },
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// allocate space in the root node and set to a leaf
/**
// allocate space for the child nodes
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// This one is nasty - the syntax for analyzers takes either names or objects at many levels, hence it's not
// possible to simply declare whether the field is a string or object and a completely custom parser is required
/**
/**
/**
/**
/**
/**
// remove empty strings
// This cannot be builder.field(CATEGORIZATION_ANALYZER.getPreferredName(), categorizationAnalyzerConfig, params);
// because that always writes categorizationAnalyzerConfig as an object, and in the case of a global analyzer it
// gets written as a single string.
// We always assign sequential IDs to the detectors that are correct for this analysis config
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Exactly one of these two members is not null
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/ml-put-job.html">create anomaly detection
//www.elastic.co/guide/en/elastic-stack-overview/current/ml-functions.html">detector functions</a>.
/**
// negative means unknown
/**
/**
/**
/**
/**
/**
/**
/**
// negative means unknown
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// For QueryPage
// Don't include TYPE as it's fixed
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// EQ was considered but given the oddity of such a
// condition and the fact that it would be a numerically
// unstable condition, it was rejected.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Stored snapshot documents created prior to 6.3.0 will have no value for min_version.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Used for QueryPage
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Used for QueryPage
/**
// Can't use emptyList as might be appended to
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Used for QueryPage
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
// Used for QueryPage
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Used for QueryPage
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// We're careful about the type in the list
/**
/**
/**
/**
/**
/**
/**
// We're careful of the contents
// Optional to accommodate old versions of state, not used
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// From DateHistogramAggregationBuilder in core, transplanted and modified to a set
// so we don't need to import a dependency on the class
/**
// validate fixed time
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// As we do not yet support the nanosecond precision when we serialize to JSON,
// here creating the 'Instant' of milliseconds precision.
// This Instant can then be used for date comparison.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// pkg scope for testing
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-user-privileges.html">the API docs</a>
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// pkg scope for testing
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// error count is parsed but ignored as we have list of errors
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we parse but do not use the count of errors as we implicitly have this in the size of the Exceptions list
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// parse extraneous wrapper
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** returns a list of nodes in which the cache was cleared */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// As we do not yet support the nanosecond precision when we serialize to JSON,
// here creating the 'Instant' of milliseconds precision.
// This Instant can then be used for date comparison.
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// find the start of the DSL object
// parseArray requires that the parser is positioned
// at the START_ARRAY token
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// null or singleton '*' means all fields are granted, empty means no fields are granted
// null or empty means no fields are denied
// unspecified granted fields means no restriction
// unspecified denied fields means no restriction
/**
/**
// we treat just '*' as no FLS since that's what the UI defaults to
// The role parser will reject a field_security object that doesn't have a "granted" field
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Don't ignore unknown fields. It is dangerous if the object we parse is also
// part of a request that we build later on, and the fields that we now ignore will
// end up being implicitly set to null in that request.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// parser is still placed on the field name, advance to next token (field value)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// When categories change, adapting this field should suffice. Categories are NOT
// opaque "named_objects", we wish to maintain control over these namespaces
// ignore_unknown_fields is irrelevant here anyway, but let's keep it to false
// because this conveys strictness (woop woop)
// same data as in privileges but broken down by categories; internally, it is
// easier to work with this structure
/**
// duplicates are just ignored
// all operations for a specific category
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// missing query means all documents, i.e. no restrictions
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Don't ignore unknown fields. It is dangerous if the object we parse is also
// part of a request that we build later on, and the fields that we now ignore
// will end up being implicitly set to null in that request.
// no cluster privileges are granted unless otherwise specified
// no indices privileges are granted unless otherwise specified
// no application resource privileges are granted unless otherwise specified
// no run as privileges are granted unless otherwise specified
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-user-privileges.html">the API docs</a>
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Package visible for testing
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Any additional metadata object added by the metadataToXContent method is ignored
// and skipped, so that the parser does not fail on unknown fields. The parser only
// support metadata key-pairs and metadata arrays of values.
// Parse the array and add each item to the corresponding list of metadata.
// Arrays of objects are not supported yet and just ignored and skipped.
// Adds root causes as suppressed exception. This way they are not lost
// after parsing and can be retrieved using getSuppressed() method.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// First populate all tasks
// Now go through all task group builders and add children to their parents
// we found parent in the list of tasks - add it to the parent list
// we got zombie or the parent was filtered out - add it to the top task list
// top level task - add it to the top task list
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// already provided in constructor: triggering a no-op
// already provided in constructor: triggering a no-op
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Discard the count field which is the size of the transforms array
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Discard the count field which is the size of the transforms array
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// default handling: if the user does not specify a query, we default to match_all
/**
/**
// Using Arrays.hashCode as Objects.hash does not deeply hash nested arrays. Since we are doing Array.equals, this is necessary
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// types of transforms
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// From DateHistogramAggregationBuilder in core, transplanted and modified to a set
// so we don't need to import a dependency on the class
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// be parsing friendly, whether the token needs to be advanced or not (similar to what ObjectParser does)
// leniently skip over key-value and array fields in the root of the object
// need to consume up to dest field end obj
// not a valid group source. Consume up to the dest field end object
// destination field end_object
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// EMPTY is safe here because we never use namedObject
/*
//www.apache.org/licenses/LICENSE-2.0
// awaiting execution of the watch
// initial phase, watch execution has started, but the input is not yet processed
// input is being executed
// condition phase is being executed
// transform phase (optional, depends if a global transform was configured in the watch)
// actions phase, all actions, including specific action transforms
// missing watch, failed execution of input/condition/transform,
// successful run
/*
//www.apache.org/licenses/LICENSE-2.0
// the condition of the watch was not met
// Execution has been throttled due to time-based throttling - this might only affect a single action though
// Execution has been throttled due to ack-based throttling/muting of an action - this might only affect a single action though
// regular execution
// an error in the condition or the execution of the input
// a rejection due to a filled up threadpool
// the execution was scheduled, but in between the watch was deleted
// even though the execution was scheduled, it was not executed, because the watch was already queued in the thread pool
// this can happen when a watch was executed, but not completely finished (the triggered watch entry was not deleted), and then
// watcher is restarted (manually or due to host switch) - the triggered watch will be executed but the history entry already
// exists
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Prevent instantiation.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO move this constant to License.java once we move License.java to the protocol jar
/**
/**
/**
// This is how constructing object parser works
/*
/**
// Matches up with declaration below
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Return a map from feature name to usage information for that feature. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//let's make sure that the bulk action limit trips, one single execution will index all the documents
//let's make sure that this bulk won't be automatically flushed
//we really need an explicit flush as none of the bulk thresholds was reached
//set interval and size to high values
//with concurrent requests > 1 we can't rely on the order of the bulk requests
//we do want to check that we don't get duplicate ids back
//let's make sure that the bulk action limit trips, one single execution will index all the documents
// check if we can call it multiple times
//set interval and size to high values
//with concurrent requests > 1 we can't rely on the order of the bulk requests
//we do want to check that we don't get duplicate ids back
//with concurrent requests > 1 we can't rely on the order of the bulk requests
//we do want to check that we don't get duplicate ids back
// tag::bulk-processor-mix-parameters
// <1>
// <2>
// end::bulk-processor-mix-parameters
//let's make sure that the bulk action limit trips, one single execution will index all the documents
/*
//www.apache.org/licenses/LICENSE-2.0
// ignored, we exceeded the write queue size when dispatching the initial bulk request
// we're not expecting any other errors
// we're not expecting that we overwhelmed it even once when we maxed out the number of retries
/**
// this is intentionally *not* static final. We will only ever have one instance of this class per test case and want the
// thread local to be eligible for garbage collection right after the test to avoid leaks.
// Assumption: This method is called from the same thread as the last call to the internal iterator's #hasNext() / #next()
// see also Retry.AbstractRetryHandler#onResponse().
// did we ever retry?
// we should correlate any iterator only once
// update on every invocation as we might get rescheduled on a different thread. Unfortunately, there is a chance that
// we pollute the thread local map with stale values. Due to the implementation of Retry and the life cycle of the
// enclosing class CorrelatingBackoffPolicy this should not pose a major problem though.
// update on every invocation
/*
//www.apache.org/licenses/LICENSE-2.0
// global pipeline was not applied
// tag::bulk-request-mix-pipeline
// <1>
// <2>
// end::bulk-request-mix-pipeline
// will take global index
/*
//www.apache.org/licenses/LICENSE-2.0
// Need to pause prior to unfollowing it:
// Need to close index prior to unfollowing it:
// Cleanup:
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// including another index that we do not assert on, to ensure that we are not
// accidentally asserting on entire cluster state
/*
//www.apache.org/licenses/LICENSE-2.0
// If Master Timeout wasn't set it uses the same value as Timeout
/*
//www.apache.org/licenses/LICENSE-2.0
// Testing deletion
// Testing non existing document
// Testing version conflict
// Testing version type
// Testing version type with a wrong version
// Testing routing
// Prepare
//parameters are encoded by the low-level client but let's test that everything works the same when we use the high-level one
// Not entirely sure if _termvectors belongs to CRUD, and in the absence of a better place, will have it here
// prepare : index docs
// test _termvectors on real documents
// test _termvectors on artificial documents
// Not entirely sure if _termvectors belongs to CRUD, and in the absence of a better place, will have it here
// Not entirely sure if _mtermvectors belongs to CRUD, and in the absence of a better place, will have it here
// prepare : index docs
// test _mtermvectors where MultiTermVectorsRequest is constructed with ids and a template
// test _mtermvectors where MultiTermVectorsRequest constructed with adding each separate request
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Configure local cluster as remote cluster:
// TODO: replace with nodes info highlevel rest client code when it is available:
// Select node info of first node (we don't know the node id):
/*
//www.apache.org/licenses/LICENSE-2.0
// do not add elements at the top-level as any element at this level is parsed as a new index
// do not add new alias
// do not insert random data into AliasMetaData#filter
/*
//www.apache.org/licenses/LICENSE-2.0
// Create chain of doc IDs across indices 1->2->3
//Explore indices where lack of fielddata=true on one index leads to partial failures 
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// The injected Unfollow step will run pretty rapidly here, so we need
// to wait for it to settle into the "stable" step of waiting to be
// ready to roll over
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Index present
// Index doesn't exist
// One index exists, one doesn't
// Create index
// Create index with mappings, aliases and settings
// default settings should be null
//noinspection unchecked
//noinspection unchecked
// Delete index if exists
// Return 404 if index doesn't exist
//without the refresh the rollover may not happen as the number of docs seen may be off
/*
/*
// Failed to validate because index patterns are missing
// Create-only specified but an template exists already
// Rejected due to unknown settings
// New API returns a MappingMetaData class rather than CompressedXContent for the mapping
/*
//www.apache.org/licenses/LICENSE-2.0
// the request object will not have include_defaults present unless it is set to
// true
// the request object will not have include_defaults present unless it is set to
// true
// the HEAD endpoint requires at least an alias or an index
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// todo add case where we successfully start a trial - see note above
// case where we don't acknowledge trial license conditions
// case where we acknowledge, but the trial is already started at cluster startup
// use a hard-coded trial license for 20 yrs to be able to roll back from another licenses
// 2018-10-16 07:02:48 UTC
// 2038-10-11 07:02:48 UTC, 20 yrs later
// we don't test the case where we successfully start a basic because the integ test cluster generates one on startup
// and we don't have a good way to prevent that / work around it in this test project
// case where we don't acknowledge basic license conditions
// case where we acknowledge and the basic is started successfully
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// 2018-08-01T00:00:00Z
// 10 days of hourly buckets
// Also index an interim bucket
// Index a number of model snapshots, one of which contains the new model_size_stats fields
// 'model_bytes_exceeded' and 'model_bytes_memory_limit' that were introduced in 7.2.0.
// We want to verify that we can parse the snapshots whether or not these fields are present.
// index some model_snapshot results
// request a non-existent snapshotId
// index some category results
// request a non-existent category
// Make sure we get all buckets
// Let's index matching buckets with the score being 10.0 higher
// As the second job has scores that are -10 from the first, the overall buckets should be +5 from the initial job
// Let us index a few influencer docs
// Last one is interim
// Last one score is higher
// score < 50.0
// score < 75.0
// score > 75.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Test getting specific jobs
// Test getting all jobs explicitly
// Test getting all jobs implicitly
// Test getting specific
// Test getting all explicitly
// Test getting all implicitly
// Test getting all with wildcard
// Test when allow_no_jobs is false
// Test getting specific datafeeds
// Test getting a single one
// Test getting all datafeeds explicitly
// Test getting all datafeeds implicitly
// Test get missing pattern with allow_no_datafeeds set to true
// Test get missing pattern with allow_no_datafeeds set to false
// Set up the index and docs
// create the job and the datafeed
// Should only process two documents
// Process all documents and end the stream
// Set up the index
// create the job and the datafeed
// Set up the index
// create the job and the datafeed
// Test getting specific
// Test getting all explicitly
// Test getting all implicitly
// Test getting all with wildcard
// Test when allow_no_jobs is false
// Set up the index and docs
// create the job and the datafeed
// Tests that nothing goes wrong when there's nothing to delete
// Set up the index and docs
// Index a randomly named unused state document
// Check that the current timestamp component, in seconds, differs from previously.
// Note that we used to use an 'awaitBusy(() -> false, 1, TimeUnit.SECONDS);'
// for the same purpose but the new approach...
// a) explicitly checks that the timestamps, in seconds, are actually different and
// b) is slightly more efficient since we may not need to wait an entire second for the timestamp to increment
// Update snapshot timestamp to force it out of snapshot retention window
// Wait for the forecast to expire
// FIXME: We should wait for something specific to change, rather than waiting for time to pass.
// Run up to now
// Verify .ml-state contains the expected unused state document
// Wait for the forecast to expire
// FIXME: We should wait for something specific to change, rather than waiting for time to pass.
// Verify .ml-state doesn't contain unused state documents
// calendar is missing
// default value
// default value
// default value
// default value
// default value
// default value
// default value
// default value
// default value
// Verify that the destination index does not exist. Otherwise, analytics' reindexing step would fail.
// Wait for the analytics to stop.
// Verify that the destination index got created.
//github.com/elastic/elasticsearch/issues/43924")
// Verify that the destination index does not exist. Otherwise, analytics' reindexing step would fail.
// #0
// #1
// #2
// #3
// #4
// #5
// #6
// #7
// #8
// #9
// Precision is 3/5=0.6 as there were 3 true examples (#7, #8, #9) among the 5 positive examples (#3, #4, #7, #8, #9)
// Precision is 2/3=0.(6) as there were 2 true examples (#8, #9) among the 3 positive examples (#4, #8, #9)
// Precision is 2/3=0.(6) as there were 2 true examples (#8, #9) among the 3 positive examples (#4, #8, #9)
// Recall is 2/5=0.4 as there were 2 true positive examples (#8, #9) among the 5 true examples (#5, #6, #7, #8, #9)
// Recall is 2/5=0.4 as there were 2 true positive examples (#8, #9) among the 5 true examples (#5, #6, #7, #8, #9)
// docs #8 and #9
// doc #4
// docs #0, #1, #2 and #3
// docs #5, #6 and #7
// #0
// #1
// #2
// #3
// #4
// #5
// #6
// #7
// #8
// #9
// Request only "blue" subset to be evaluated
// docs #0, #1, #2 and #3
// docs #4 and #5
// #0
// #1
// #2
// #3
// #4
// #5
// #6
// #7
// #8
// #9
// Accuracy
// 9 out of 10 examples were classified correctly
// 6 out of 10 examples were classified correctly
// 8 out of 10 examples were classified correctly
// 6 out of 10 examples were classified correctly
// Precision
// 3 out of 5 examples labeled as "cat" were classified correctly
// 3 out of 4 examples labeled as "dog" were classified correctly
// Recall
// 3 out of 5 examples labeled as "cat" were classified correctly
// 3 out of 4 examples labeled as "dog" were classified correctly
// no examples labeled as "ant" were classified correctly
// No size provided for MulticlassConfusionMatrixMetric, default used instead
// Explicit size provided for MulticlassConfusionMatrixMetric metric
// We are pretty liberal here as this test does not aim at verifying concrete numbers but rather end-to-end user workflow.
// Data Frame has 10 rows, expect that the returned estimates fall within (1kB, 1GB) range.
// Data Frame now has 100 rows, expect that the returned estimates will be greater than or equal to the previous ones.
//should not be random, see:https://github.com/elastic/ml-cpp/issues/208
//should not be random, see:https://github.com/elastic/ml-cpp/issues/208
/*
//www.apache.org/licenses/LICENSE-2.0
// a test like this cannot test actual deprecations
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Request with config
// Request with id
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//localhost:9200], URI [/_blah], " +
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// compare with what the low level client outputs
// only check node name existence, might be a different one from what was hit by low level client in multi-node cluster
// TODO: reconsider this leniency now that the transport client is gone
/*
//www.apache.org/licenses/LICENSE-2.0
// add another index to test basic multi index support
/**
// the expected Prec@ for the first query is 5/7 and the expected Prec@ for the second is 1/7, divided by 2 to get the average
// now try this when test2 is closed
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Prepare
// reindex one document with id 1 from source to destination
// Prepare
// tag::submit-reindex-task
// <1>
// <2>
// <3>
// end::submit-reindex-task
// Prepare
// test1: create one doc in dest
// test2: update using script
// test update-by-query rethrottling
// this following settings are supposed to halt reindexing after first document
// any rethrottling after the update-by-query is done performed with the same taskId should result in a failure
// Prepare
// test1: delete one doc
// test delete-by-query rethrottling
// this following settings are supposed to halt reindexing after first document
// any rethrottling after the delete-by-query is done performed with the same taskId should result in a failure
// Prepare
// tag::submit-delete_by_query-task
// end::submit-delete_by_query-task
// The parent task hasn't started yet
/*
//www.apache.org/licenses/LICENSE-2.0
// test illegal RethrottleRequest values
// There is some logic around _create endpoint and version/version type
// if* params are passed in the body
// rarely skip setting the search source completely
// frequently set the search source to have some content, otherwise leave it
// empty but still set it
//as we create SearchSourceBuilder in CountRequest constructor
// No need to return a very complex SearchSourceBuilder here, that is tested
// elsewhere
// scroll is not supported in the current msearch api, so unset it:
// only expand_wildcards, ignore_unavailable and allow_no_indices can be
// specified from msearch api, so unset other options:
// Create a random request.
// Verify that the resulting REST request looks as expected.
// Create a simple request.
// Setting simulate true means the template should only be rendered.
// Verify that the resulting REST request looks as expected.
// Create a random request.
// scroll is not supported in the current msearch or msearchtemplate api, so unset it:
// batched reduce size is currently not set-able on a per-request basis as it is a query string parameter only
// Create a random request.
// Verify that the resulting REST request looks as expected.
// Note that we don't check the field param value explicitly, as field names are
// passed through
// a hash set before being added to the request, and can appear in a
// non-deterministic order.
//foo");
///foo");
//foo/bar");
//foo@bar");
//part2").addPathPart("///part3");
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// core
// security
// license
// taken from the server side MainResponse
//localhost:9200, " +
//although we got an exception, we turn it into a successful response because the status code was provided among ignores
//localhost:9200, " +
//although we got an exception, we turn it into a successful response because the status code was provided among ignores
//response parsing throws exception while handling ignores. same as when GetResponse#fromXContent throws error when trying
//to parse a 404 response which contains an error rather than a valid document not found response.
//response parsing throws exception while handling ignores. same as when GetResponse#fromXContent throws error when trying
//to parse a 404 response which contains an error rather than a valid document not found response.
//this list should be empty once the high-level client is feature complete
//These API are not required for high-level client feature completeness
// TODO remove in 8.0 - we will undeprecate indices.get_template because the current getIndexTemplate
// impl will replace the existing getTemplate method.
// The above general-purpose code ignores all deprecated methods which in this case leaves `getTemplate`
// looking like it doesn't have a valid implementatation when it does.
// Synced flush is deprecated
//we convert all the method names to snake case, hence we need to look for the '_async' suffix rather than 'Async'
//TODO xpack api are currently ignored, we need to load xpack yaml spec too
// IndicesClientIT.getIndexTemplate should be renamed "getTemplate" in version 8.0 when we
// can get rid of 7.0's deprecated "getTemplate"
//we decided not to support cat API in the high-level REST client, they are supposed to be used from a low-level client
//A few methods return a boolean rather than a response object
// It's acceptable for 404s to be represented as empty Optionals
//a few methods don't accept a request object as argument
// This is no longer true for all methods. Some methods can contain these 2 args backwards because of deprecation
// This is no longer true for all methods. Some methods can contain these 2 args backwards because of deprecation
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO expand this to also test with histogram and terms?
// stop the job
// TODO expand this to also test with histogram and terms?
// wait for the PutJob api to create the index w/ metadata
// TODO expand this to also test with histogram and terms?
// wait for the PutJob api to create the index w/ metadata
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Setting simulate true causes the template to only be rendered.
// The whole HTTP request should fail if no nested search requests are valid
// Check the capabilities for the 'rating' field.
// Check the capabilities for the 'field' field.
// add logging to get more info about why https://github.com/elastic/elasticsearch/issues/35644 is failing
// TODO remove this once #35644 is fixed
/*
//www.apache.org/licenses/LICENSE-2.0
// create user
// assert user created
// update user
// assert user not created
// cleanup
// create user
// assert user created
// get user
// assert user was correctly retrieved
// test fixture: put enabled user
// authenticate correctly
// get user
// delete user
// authentication no longer works
// delete non-existing user
// create random role
// assert role created
// assert role is equal
// assert role updated
// assert role deleted
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// assumes the repository already exists
// If we don't wait for the snapshot to complete we have to cancel it to not leak the snapshot task
// check that the request went ok without parsing JSON here. When using the high level client, check acknowledgement instead.
// check that the request went ok without parsing JSON here. When using the high level client, check acknowledgement instead.
// check that the request went ok without parsing JSON here. When using the high level client, check acknowledgement instead.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// total shard failure
// shard copy failure
// Building the shardRouting map here.
// This was an index entry.
/*
//www.apache.org/licenses/LICENSE-2.0
// It's possible that there are other tasks except 'cluster:monitor/tasks/lists[n]' and 'action":"cluster:monitor/tasks/lists'
// Run a Reindex to create a task
// (need to use low level client because currently high level client
// doesn't support async return of task id - needs
// https://github.com/elastic/elasticsearch/pull/35202 )
// Check 404s are returned as empty Optionals
// in this case, probably no task will actually be cancelled.
// this is ok, that case is covered in TasksIT.testTasksCancellation
// Since the task may or may not have been cancelled, assert that we received a response only
// The actual testing of task cancellation is covered by TasksIT.testTasksCancellation
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// The second delete should fail
// Since we are non-continuous, the transform could auto-stop between being started earlier and us gathering the statistics
// Calling stop with wait_for_completion assures that we will be in the `STOPPED` state for the transform task
// TODO add tests to cover continuous situations
// start the transform
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Deactivate a watch that exists
// Deactivate a watch that does not exist
// delete watch that exists
// delete watch that does not exist
// TODO: use the high-level REST client here once it supports 'execute watch'.
// exception when activating a not existing watcher
// exception when activating a not existing watcher
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// x-content loses the exception
// sort by index name, then shard ID
// x-content loses the exception
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// sort by index name, then shard ID
// x-content loses the exception
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Name isn't serialized, because it specified in url path, so no need to randomly generate it here.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Still needed for StopRollupJobResponseTests and StartRollupJobResponseTests test classes
// This method can't be moved to these classes because getFieldName() method isn't accessible from these test classes.
/*
//www.apache.org/licenses/LICENSE-2.0
// failures are grouped
/*
//www.apache.org/licenses/LICENSE-2.0
// similar to SearchRequestTests as CountRequest inline several members (and functionality) from SearchRequest
// In RestCountAction the request body is parsed as QueryBuilder (top level query field),
// so that is why this is chosen as server side instance.
// query is the only property that is serialized as xcontent:
// query is the only property that is serialized as xcontent:
/*
//www.apache.org/licenses/LICENSE-2.0
// Not comparing XContent for equivalence as we cannot compare the ShardSearchFailure#cause, because it will be wrapped in an outer
// ElasticSearchException. Best effort: try to check that the original message appears somewhere in the rendered xContent
// For more see ShardSearchFailureTests.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// build fields_statistics
// build terms
// build term_statistics
// build tokens
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Create leader index:
// tag::ccr-put-follow-request
// <1>
// <2>
// <3>
// <4>
// end::ccr-put-follow-request
// tag::ccr-put-follow-execute
// end::ccr-put-follow-execute
// tag::ccr-put-follow-response
// <1>
// <2>
// <3>
// end::ccr-put-follow-response
// Pause following and delete follower index, so that we can execute put follow api again:
// tag::ccr-put-follow-execute-listener
// <1>
// <2>
// end::ccr-put-follow-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ccr-put-follow-execute-async
// <1>
// end::ccr-put-follow-execute-async
// Create leader index:
// Follow index, so that it can be paused:
// tag::ccr-pause-follow-request
// <1>
// end::ccr-pause-follow-request
// tag::ccr-pause-follow-execute
// end::ccr-pause-follow-execute
// tag::ccr-pause-follow-response
// <1>
// end::ccr-pause-follow-response
// tag::ccr-pause-follow-execute-listener
// <1>
// <2>
// end::ccr-pause-follow-execute-listener
// Resume follow index, so that it can be paused again:
// Replace the empty listener by a blocking listener in test
// tag::ccr-pause-follow-execute-async
// <1>
// end::ccr-pause-follow-execute-async
// Create leader index:
// Follow index, so that it can be paused:
// Pause follow index, so that it can be resumed:
// tag::ccr-resume-follow-request
// <1>
// end::ccr-resume-follow-request
// tag::ccr-resume-follow-execute
// end::ccr-resume-follow-execute
// tag::ccr-resume-follow-response
// <1>
// end::ccr-resume-follow-response
// Pause follow index, so that it can be resumed again:
// tag::ccr-resume-follow-execute-listener
// <1>
// <2>
// end::ccr-resume-follow-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ccr-resume-follow-execute-async
// <1>
// end::ccr-resume-follow-execute-async
// Cleanup:
// Create leader index:
// Follow index, pause and close, so that it can be unfollowed:
// tag::ccr-unfollow-request
// <1>
// end::ccr-unfollow-request
// tag::ccr-unfollow-execute
// end::ccr-unfollow-execute
// tag::ccr-unfollow-response
// <1>
// end::ccr-unfollow-response
// Delete, put follow index, pause and close, so that it can be unfollowed again:
// tag::ccr-unfollow-execute-listener
// <1>
// <2>
// end::ccr-unfollow-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ccr-unfollow-execute-async
// <1>
// end::ccr-unfollow-execute-async
// create leader index
// tag::ccr-forget-follower-request
// <1>
// <2>
// <3>
// <4>
// <5>
// end::ccr-forget-follower-request
// tag::ccr-forget-follower-execute
// end::ccr-forget-follower-execute
// tag::ccr-forget-follower-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// end::ccr-forget-follower-response
// tag::ccr-forget-follower-execute-listener
// <1>
// <2>
// end::ccr-forget-follower-execute-listener
// replace the empty listener by a blocking listener in test
// tag::ccr-forget-follower-execute-async
// <1>
// end::ccr-forget-follower-execute-async
// tag::ccr-put-auto-follow-pattern-request
// <1>
// <2>
// <3>
// <4>
// end::ccr-put-auto-follow-pattern-request
// tag::ccr-put-auto-follow-pattern-execute
// end::ccr-put-auto-follow-pattern-execute
// tag::ccr-put-auto-follow-pattern-response
// <1>
// end::ccr-put-auto-follow-pattern-response
// Delete auto follow pattern, so that we can store it again:
// tag::ccr-put-auto-follow-pattern-execute-listener
// <1>
// <2>
// end::ccr-put-auto-follow-pattern-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ccr-put-auto-follow-pattern-execute-async
// <1>
// end::ccr-put-auto-follow-pattern-execute-async
// Cleanup:
// Put auto follow pattern, so that we can delete it:
// tag::ccr-delete-auto-follow-pattern-request
// <1>
// end::ccr-delete-auto-follow-pattern-request
// tag::ccr-delete-auto-follow-pattern-execute
// end::ccr-delete-auto-follow-pattern-execute
// tag::ccr-delete-auto-follow-pattern-response
// <1>
// end::ccr-delete-auto-follow-pattern-response
// Put auto follow pattern, so that we can delete it again:
// tag::ccr-delete-auto-follow-pattern-execute-listener
// <1>
// <2>
// end::ccr-delete-auto-follow-pattern-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ccr-delete-auto-follow-pattern-execute-async
// <1>
// end::ccr-delete-auto-follow-pattern-execute-async
// Put auto follow pattern, so that we can get it:
// tag::ccr-get-auto-follow-pattern-request
// <1>
// end::ccr-get-auto-follow-pattern-request
// tag::ccr-get-auto-follow-pattern-execute
// end::ccr-get-auto-follow-pattern-execute
// tag::ccr-get-auto-follow-pattern-response
// <1>
// end::ccr-get-auto-follow-pattern-response
// tag::ccr-get-auto-follow-pattern-execute-listener
// <1>
// <2>
// end::ccr-get-auto-follow-pattern-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ccr-get-auto-follow-pattern-execute-async
// <1>
// end::ccr-get-auto-follow-pattern-execute-async
// Cleanup:
// tag::ccr-pause-auto-follow-pattern-request
// <1>
// end::ccr-pause-auto-follow-pattern-request
// tag::ccr-pause-auto-follow-pattern-execute
// end::ccr-pause-auto-follow-pattern-execute
// tag::ccr-pause-auto-follow-pattern-response
// <1>
// end::ccr-pause-auto-follow-pattern-response
// tag::ccr-pause-auto-follow-pattern-execute-listener
// <1>
// <2>
// end::ccr-pause-auto-follow-pattern-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ccr-pause-auto-follow-pattern-execute-async
// <1>
// end::ccr-pause-auto-follow-pattern-execute-async
// Cleanup:
// tag::ccr-resume-auto-follow-pattern-request
// <1>
// end::ccr-resume-auto-follow-pattern-request
// tag::ccr-resume-auto-follow-pattern-execute
// end::ccr-resume-auto-follow-pattern-execute
// tag::ccr-resume-auto-follow-pattern-response
// <1>
// end::ccr-resume-auto-follow-pattern-response
// tag::ccr-resume-auto-follow-pattern-execute-listener
// <1>
// <2>
// end::ccr-resume-auto-follow-pattern-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ccr-resume-auto-follow-pattern-execute-async
// <1>
// end::ccr-resume-auto-follow-pattern-execute-async
// Cleanup:
// tag::ccr-get-stats-request
// <1>
// end::ccr-get-stats-request
// tag::ccr-get-stats-execute
// end::ccr-get-stats-execute
// tag::ccr-get-stats-response
// <1>
// <2>
// end::ccr-get-stats-response
// tag::ccr-get-stats-execute-listener
// <1>
// <2>
// end::ccr-get-stats-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ccr-get-stats-execute-async
// <1>
// end::ccr-get-stats-execute-async
// Create leader index:
// Follow index, so that we can query for follow stats:
// tag::ccr-get-follow-stats-request
// <1>
// end::ccr-get-follow-stats-request
// tag::ccr-get-follow-stats-execute
// end::ccr-get-follow-stats-execute
// tag::ccr-get-follow-stats-response
// <1>
// end::ccr-get-follow-stats-response
// tag::ccr-get-follow-stats-execute-listener
// <1>
// <2>
// end::ccr-get-follow-stats-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ccr-get-follow-stats-execute-async
// <1>
// end::ccr-get-follow-stats-execute-async
// Create leader index:
// Follow index, so that we can query for follow stats:
// tag::ccr-get-follow-info-request
// <1>
// end::ccr-get-follow-info-request
// tag::ccr-get-follow-info-execute
// end::ccr-get-follow-info-execute
// tag::ccr-get-follow-info-response
// <1>
// end::ccr-get-follow-info-response
// tag::ccr-get-follow-info-execute-listener
// <1>
// <2>
// end::ccr-get-follow-info-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ccr-get-follow-info-execute-async
// <1>
// end::ccr-get-follow-info-execute-async
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// tag::put-settings-request
// end::put-settings-request
// tag::put-settings-create-settings
// <1>
// <2>
// end::put-settings-create-settings
// tag::put-settings-request-cluster-settings
// <1>
// <2>
// end::put-settings-request-cluster-settings
// tag::put-settings-settings-builder
// <1>
// end::put-settings-settings-builder
// tag::put-settings-settings-map
// <1>
// end::put-settings-settings-map
// tag::put-settings-settings-source
// <1>
// end::put-settings-settings-source
// tag::put-settings-request-timeout
// <1>
// <2>
// end::put-settings-request-timeout
// tag::put-settings-request-masterTimeout
// <1>
// <2>
// end::put-settings-request-masterTimeout
// tag::put-settings-execute
// end::put-settings-execute
// tag::put-settings-response
// <1>
// <2>
// <3>
// end::put-settings-response
// tag::put-settings-request-reset-transient
// <1>
// tag::put-settings-request-reset-transient
// tag::put-settings-execute-listener
// <1>
// <2>
// end::put-settings-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-settings-execute-async
// <1>
// end::put-settings-execute-async
// tag::get-settings-request
// end::get-settings-request
// tag::get-settings-request-includeDefaults
// <1>
// end::get-settings-request-includeDefaults
// tag::get-settings-request-local
// <1>
// end::get-settings-request-local
// tag::get-settings-request-masterTimeout
// <1>
// <2>
// end::get-settings-request-masterTimeout
// tag::get-settings-execute
// <1>
// end::get-settings-execute
// tag::get-settings-response
// <1>
// <2>
// <3>
// <4>
// end::get-settings-response
// tag::get-settings-execute-listener
// <1>
// <2>
// end::get-settings-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-settings-execute-async
// <1>
// end::get-settings-execute-async
// tag::health-request
// end::health-request
// tag::health-request-indices-ctr
// end::health-request-indices-ctr
// tag::health-request-indices-setter
// end::health-request-indices-setter
// tag::health-request-timeout
// <1>
// <2>
// end::health-request-timeout
// tag::health-request-master-timeout
// <1>
// <2>
// end::health-request-master-timeout
// tag::health-request-wait-status
// <1>
// <2>
// end::health-request-wait-status
// tag::health-request-wait-events
// <1>
// end::health-request-wait-events
// tag::health-request-level
// <1>
// end::health-request-level
// tag::health-request-wait-relocation
// <1>
// end::health-request-wait-relocation
// tag::health-request-wait-initializing
// <1>
// end::health-request-wait-initializing
// tag::health-request-wait-nodes
// <1>
// <2>
// <3>
// end::health-request-wait-nodes
// tag::health-request-wait-active
// <1>
// <2>
// end::health-request-wait-active
// tag::health-request-local
// <1>
// end::health-request-local
// tag::health-execute
// end::health-execute
// tag::health-response-general
// <1>
// <2>
// end::health-response-general
// tag::health-response-request-status
// <1>
// <2>
// end::health-response-request-status
// tag::health-response-nodes
// <1>
// <2>
// end::health-response-nodes
// tag::health-response-shards
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// end::health-response-shards
// tag::health-response-task
// <1>
// <2>
// <3>
// end::health-response-task
// tag::health-response-indices
// <1>
// end::health-response-indices
// tag::health-response-index
// <1>
// end::health-response-index
// tag::health-response-shard-details
// <1>
// end::health-response-shard-details
// tag::health-execute-listener
// <1>
// <2>
// end::health-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::health-execute-async
// <1>
// end::health-execute-async
// tag::remote-info-request
// end::remote-info-request
// tag::remote-info-execute
// <1>
// end::remote-info-execute
// tag::remote-info-response
// end::remote-info-response
// tag::remote-info-request
// end::remote-info-request
// tag::remote-info-execute-listener
// <1>
// <2>
// end::remote-info-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::health-execute-async
// <1>
// end::health-execute-async
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//tag::index-request-map
// <1>
//end::index-request-map
//tag::index-request-xcontent
// <1>
//end::index-request-xcontent
//tag::index-request-shortcut
// <1>
//end::index-request-shortcut
//tag::index-request-string
// <1>
// <2>
// <3>
//end::index-request-string
// tag::index-execute
// end::index-execute
// tag::index-response
// <1>
// <2>
// <3>
// <4>
// end::index-response
// tag::index-request-routing
// <1>
// end::index-request-routing
// tag::index-request-timeout
// <1>
// <2>
// end::index-request-timeout
// tag::index-request-refresh
// <1>
// <2>
// end::index-request-refresh
// tag::index-request-version
// <1>
// end::index-request-version
// tag::index-request-version-type
// <1>
// end::index-request-version-type
// tag::index-request-op-type
// <1>
// <2>
// end::index-request-op-type
// tag::index-request-pipeline
// <1>
// end::index-request-pipeline
// tag::index-conflict
// <1>
// end::index-conflict
// tag::index-optype
// <1>
// end::index-optype
// tag::index-execute-listener
// <1>
// <2>
// end::index-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::index-execute-async
// <1>
// end::index-execute-async
//tag::update-request
// <1>
// <2>
//end::update-request
//tag::update-request-with-inline-script
// <1>
// <2>
// <3>
//end::update-request-with-inline-script
//tag::update-request-with-stored-script
// <1>
// <2>
//end::update-request-with-stored-script
//tag::update-request-with-doc-as-map
// <1>
//end::update-request-with-doc-as-map
//tag::update-request-with-doc-as-xcontent
// <1>
//end::update-request-with-doc-as-xcontent
//tag::update-request-shortcut
// <1>
//end::update-request-shortcut
//tag::update-request-with-doc-as-string
// <1>
//end::update-request-with-doc-as-string
// tag::update-execute
// end::update-execute
// tag::update-response
// <1>
// <2>
// <3>
// <4>
// end::update-response
// tag::update-getresult
// <1>
// <2>
// <3>
// <4>
// <5>
// end::update-getresult
// tag::update-failure
// <1>
// <2>
// end::update-failure
//tag::update-docnotfound
// <1>
//end::update-docnotfound
// tag::update-conflict
// <1>
// end::update-conflict
//tag::update-request-no-source
// <1>
//end::update-request-no-source
//tag::update-request-source-include
// <1>
//end::update-request-source-include
//tag::update-request-source-exclude
// <1>
//end::update-request-source-exclude
// tag::update-request-routing
// <1>
// end::update-request-routing
// tag::update-request-timeout
// <1>
// <2>
// end::update-request-timeout
// tag::update-request-retry
// <1>
// end::update-request-retry
// tag::update-request-refresh
// <1>
// <2>
// end::update-request-refresh
// tag::update-request-cas
// <1>
// <2>
// end::update-request-cas
// tag::update-request-detect-noop
// <1>
// end::update-request-detect-noop
// tag::update-request-upsert
// <1>
// end::update-request-upsert
// tag::update-request-scripted-upsert
// <1>
// end::update-request-scripted-upsert
// tag::update-request-doc-upsert
// <1>
// end::update-request-doc-upsert
// tag::update-request-active-shards
// <1>
// <2>
// end::update-request-active-shards
// tag::update-execute-listener
// <1>
// <2>
// end::update-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::update-execute-async
// <1>
// end::update-execute-async
// tag::delete-request
// <1>
// <2>
// end::delete-request
// tag::delete-execute
// end::delete-execute
// tag::delete-response
// <1>
// <2>
// end::delete-response
// tag::delete-request-routing
// <1>
// end::delete-request-routing
// tag::delete-request-timeout
// <1>
// <2>
// end::delete-request-timeout
// tag::delete-request-refresh
// <1>
// <2>
// end::delete-request-refresh
// tag::delete-request-version
// <1>
// end::delete-request-version
// tag::delete-request-version-type
// <1>
// end::delete-request-version-type
// tag::delete-notfound
// <1>
// end::delete-notfound
// tag::delete-conflict
// <1>
// end::delete-conflict
// tag::delete-execute-listener
// <1>
// <2>
// end::delete-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-execute-async
// <1>
// end::delete-execute-async
// tag::bulk-request
// <1>
// <2>
// <3>
// <4>
// end::bulk-request
// tag::bulk-execute
// end::bulk-execute
// tag::bulk-request-with-mixed-operations
// <1>
// <2>
// <3>
// end::bulk-request-with-mixed-operations
// tag::bulk-response
// <1>
// <2>
// <3>
// <4>
// <5>
// end::bulk-response
// tag::bulk-has-failures
// <1>
// end::bulk-has-failures
// tag::bulk-errors
// <1>
// <2>
// end::bulk-errors
// tag::bulk-request-timeout
// <1>
// <2>
// end::bulk-request-timeout
// tag::bulk-request-refresh
// <1>
// <2>
// end::bulk-request-refresh
// tag::bulk-request-active-shards
// <1>
// <2>
// end::bulk-request-active-shards
// tag::bulk-request-pipeline
// <1>
// end::bulk-request-pipeline
// tag::bulk-request-routing
// <1>
// end::bulk-request-routing
// tag::bulk-request-index-type
// <1>
// end::bulk-request-index-type
// tag::bulk-execute-listener
// <1>
// <2>
// end::bulk-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::bulk-execute-async
// <1>
// end::bulk-execute-async
// tag::reindex-request
// <1>
// <2>
// <3>
// end::reindex-request
// tag::reindex-request-versionType
// <1>
// end::reindex-request-versionType
// tag::reindex-request-opType
// <1>
// end::reindex-request-opType
// tag::reindex-request-conflicts
// <1>
// end::reindex-request-conflicts
// tag::reindex-request-maxDocs
// <1>
// end::reindex-request-maxDocs
// tag::reindex-request-sourceSize
// <1>
// end::reindex-request-sourceSize
// tag::reindex-request-pipeline
// <1>
// end::reindex-request-pipeline
// tag::reindex-request-script
// <1>
// end::reindex-request-script
// tag::reindex-request-remote
// <1>
// end::reindex-request-remote
// tag::reindex-request-timeout
// <1>
// end::reindex-request-timeout
// tag::reindex-request-refresh
// <1>
// end::reindex-request-refresh
// tag::reindex-request-scroll
// <1>
// end::reindex-request-scroll
// tag::reindex-execute
// end::reindex-execute
// tag::reindex-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// <8>
// <9>
// <10>
// <11>
// <12>
// <13>
// <14>
// <15>
// end::reindex-response
// These cannot be set with a remote set, so its set here instead for the docs
// tag::reindex-request-query
// <1>
// end::reindex-request-query
// tag::reindex-request-slices
// <1>
// end::reindex-request-slices
// tag::reindex-execute-listener
// <1>
// <2>
// end::reindex-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::reindex-execute-async
// <1>
// end::reindex-execute-async
// tag::rethrottle-disable-request
// <1>
// end::rethrottle-disable-request
// tag::rethrottle-request
// <1>
// end::rethrottle-request
// tag::rethrottle-request-execution
// <1>
// <2>
// <3>
// end::rethrottle-request-execution
// tag::rethrottle-request-async-listener
// <1>
// <2>
// end::rethrottle-request-async-listener
// Replace the empty listener by a blocking listener in test
// tag::rethrottle-execute-async
// <1>
// <2>
// <3>
// end::rethrottle-execute-async
// tag::update-by-query-request
// <1>
// end::update-by-query-request
// tag::update-by-query-request-conflicts
// <1>
// end::update-by-query-request-conflicts
// tag::update-by-query-request-query
// <1>
// end::update-by-query-request-query
// tag::update-by-query-request-maxDocs
// <1>
// end::update-by-query-request-maxDocs
// tag::update-by-query-request-scrollSize
// <1>
// end::update-by-query-request-scrollSize
// tag::update-by-query-request-pipeline
// <1>
// end::update-by-query-request-pipeline
// tag::update-by-query-request-script
// <1>
// end::update-by-query-request-script
// tag::update-by-query-request-timeout
// <1>
// end::update-by-query-request-timeout
// tag::update-by-query-request-refresh
// <1>
// end::update-by-query-request-refresh
// tag::update-by-query-request-slices
// <1>
// end::update-by-query-request-slices
// tag::update-by-query-request-scroll
// <1>
// end::update-by-query-request-scroll
// tag::update-by-query-request-routing
// <1>
// end::update-by-query-request-routing
// tag::update-by-query-request-indicesOptions
// <1>
// end::update-by-query-request-indicesOptions
// tag::update-by-query-execute
// end::update-by-query-execute
// tag::update-by-query-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// <8>
// <9>
// <10>
// <11>
// <12>
// <13>
// <14>
// end::update-by-query-response
// tag::update-by-query-execute-listener
// <1>
// <2>
// end::update-by-query-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::update-by-query-execute-async
// <1>
// end::update-by-query-execute-async
// tag::delete-by-query-request
// <1>
// end::delete-by-query-request
// tag::delete-by-query-request-conflicts
// <1>
// end::delete-by-query-request-conflicts
// tag::delete-by-query-request-query
// <1>
// end::delete-by-query-request-query
// tag::delete-by-query-request-maxDocs
// <1>
// end::delete-by-query-request-maxDocs
// tag::delete-by-query-request-scrollSize
// <1>
// end::delete-by-query-request-scrollSize
// tag::delete-by-query-request-timeout
// <1>
// end::delete-by-query-request-timeout
// tag::delete-by-query-request-refresh
// <1>
// end::delete-by-query-request-refresh
// tag::delete-by-query-request-slices
// <1>
// end::delete-by-query-request-slices
// tag::delete-by-query-request-scroll
// <1>
// end::delete-by-query-request-scroll
// tag::delete-by-query-request-routing
// <1>
// end::delete-by-query-request-routing
// tag::delete-by-query-request-indicesOptions
// <1>
// end::delete-by-query-request-indicesOptions
// tag::delete-by-query-execute
// end::delete-by-query-execute
// tag::delete-by-query-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// <8>
// <9>
// <10>
// <11>
// <12>
// <13>
// end::delete-by-query-response
// tag::delete-by-query-execute-listener
// <1>
// <2>
// end::delete-by-query-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-by-query-execute-async
// <1>
// end::delete-by-query-execute-async
//tag::get-request
// <1>
// <2>
//end::get-request
//tag::get-execute
//end::get-execute
//tag::get-response
// <1>
// <2>
// <3>
// <4>
//end::get-response
//tag::get-request-no-source
// <1>
//end::get-request-no-source
//tag::get-request-source-include
// <1>
//end::get-request-source-include
//tag::get-request-source-exclude
// <1>
//end::get-request-source-exclude
//tag::get-request-stored
// <1>
// <2>
//end::get-request-stored
//tag::get-request-routing
// <1>
//end::get-request-routing
//tag::get-request-preference
// <1>
//end::get-request-preference
//tag::get-request-realtime
// <1>
//end::get-request-realtime
//tag::get-request-refresh
// <1>
//end::get-request-refresh
//tag::get-request-version
// <1>
//end::get-request-version
//tag::get-request-version-type
// <1>
//end::get-request-version-type
// tag::get-execute-listener
// <1>
// <2>
// end::get-execute-listener
// Replace the empty listener by a blocking listener in test
//tag::get-execute-async
// <1>
//end::get-execute-async
//tag::get-indexnotfound
// <1>
//end::get-indexnotfound
// tag::get-conflict
// <1>
// end::get-conflict
// tag::exists-request
// <1>
// <2>
// <3>
// <4>
// end::exists-request
// tag::exists-execute
// end::exists-execute
// tag::exists-execute-listener
// <1>
// <2>
// end::exists-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::exists-execute-async
// <1>
// end::exists-execute-async
// tag::bulk-processor-init
// <1>
// <2>
// <3>
// <4>
// <5>
// end::bulk-processor-init
// tag::bulk-processor-add
// end::bulk-processor-add
// tag::bulk-processor-await
// <1>
// end::bulk-processor-await
// tag::bulk-processor-close
// end::bulk-processor-close
// tag::bulk-processor-listener
// <1>
// <2>
// <3>
// end::bulk-processor-listener
// tag::bulk-processor-options
// <1>
// <2>
// <3>
// <4>
// <5>
// end::bulk-processor-options
// Not entirely sure if _termvectors belongs to CRUD, and in the absence of a better place, will have it here
// tag::term-vectors-request
// end::term-vectors-request
// tag::term-vectors-request-artificial
// <1>
// end::term-vectors-request-artificial
// tag::term-vectors-request-optional-arguments
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// <8>
// <9>
// end::term-vectors-request-optional-arguments
// tag::term-vectors-execute
// end::term-vectors-execute
// tag::term-vectors-response
// <1>
// <2>
// <3>
// end::term-vectors-response
// tag::term-vectors-term-vectors
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// <8>
// <9>
// <10>
// <11>
// <12>
// <13>
// <14>
// <15>
// end::term-vectors-term-vectors
// tag::term-vectors-execute-listener
// <1>
// <2>
// end::term-vectors-execute-listener
// tag::term-vectors-execute-async
// <1>
// end::term-vectors-execute-async
// Not entirely sure if _mtermvectors belongs to CRUD, and in the absence of a better place, will have it here
// tag::multi-term-vectors-request
// <1>
// <2>
// <3>
// end::multi-term-vectors-request
// tag::multi-term-vectors-request-template
// <1>
// <2>
// end::multi-term-vectors-request-template
// tag::multi-term-vectors-execute
// end::multi-term-vectors-execute
// tag::multi-term-vectors-response
// <1>
// end::multi-term-vectors-response
// tag::multi-term-vectors-execute-listener
// <1>
// <2>
// end::multi-term-vectors-execute-listener
// tag::multi-term-vectors-execute-async
// <1>
// end::multi-term-vectors-execute-async
// tag::multi-get-request
// <1>
// <2>
// <3>
// end::multi-get-request
// Add a missing index so we can test it.
// tag::multi-get-request-item-extras
// <1>
// <2>
// <3>
// end::multi-get-request-item-extras
// tag::multi-get-request-top-level-extras
// <1>
// <2>
// <3>
// end::multi-get-request-top-level-extras
// tag::multi-get-execute
// end::multi-get-execute
// tag::multi-get-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// end::multi-get-response
// tag::multi-get-indexnotfound
// <1>
// <2>
// <3>
// TODO status is broken! fix in a followup
// assertEquals(RestStatus.NOT_FOUND, ee.status());        // <4>
// <5>
// end::multi-get-indexnotfound
// tag::multi-get-execute-listener
// <1>
// <2>
// end::multi-get-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::multi-get-execute-async
// <1>
// end::multi-get-execute-async
// tag::multi-get-request-no-source
// <1>
// end::multi-get-request-no-source
// tag::multi-get-request-source-include
// <1>
// end::multi-get-request-source-include
// tag::multi-get-request-source-exclude
// <1>
// end::multi-get-request-source-exclude
// tag::multi-get-request-stored
// <1>
// <2>
// end::multi-get-request-stored
// tag::multi-get-conflict
// <1>
// <2>
// <3>
// TODO status is broken! fix in a followup
// assertEquals(RestStatus.CONFLICT, ee.status());          // <4>
// <5>
// end::multi-get-conflict
/*
//www.apache.org/licenses/LICENSE-2.0
// ignore... it is ok if policy has already been removed
// tag::enrich-put-policy-request
// end::enrich-put-policy-request
// tag::enrich-put-policy-execute
// end::enrich-put-policy-execute
// tag::enrich-put-policy-response
// <1>
// end::enrich-put-policy-response
// tag::enrich-put-policy-execute-listener
// <1>
// <2>
// end::enrich-put-policy-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::enrich-put-policy-execute-async
// <1>
// end::enrich-put-policy-execute-async
// Add a policy, so that it can be deleted:
// tag::enrich-delete-policy-request
// end::enrich-delete-policy-request
// tag::enrich-delete-policy-execute
// end::enrich-delete-policy-execute
// tag::enrich-delete-policy-response
// <1>
// end::enrich-delete-policy-response
// tag::enrich-delete-policy-execute-listener
// <1>
// <2>
// end::enrich-delete-policy-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::enrich-delete-policy-execute-async
// <1>
// end::enrich-delete-policy-execute-async
// tag::enrich-get-policy-request
// end::enrich-get-policy-request
// tag::enrich-get-policy-execute
// end::enrich-get-policy-execute
// tag::enrich-get-policy-response
// <1>
// end::enrich-get-policy-response
// tag::enrich-get-policy-execute-listener
// <1>
// <2>
// end::enrich-get-policy-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::enrich-get-policy-execute-async
// <1>
// end::enrich-get-policy-execute-async
// tag::enrich-stats-request
// end::enrich-stats-request
// tag::enrich-stats-execute
// end::enrich-stats-execute
// tag::enrich-stats-response
// <1>
// <2>
// end::enrich-stats-response
// tag::enrich-stats-execute-listener
// <1>
// <2>
// end::enrich-stats-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::enrich-stats-execute-async
// <1>
// end::enrich-stats-execute-async
// tag::enrich-execute-policy-request
// end::enrich-execute-policy-request
// tag::enrich-execute-policy-execute
// end::enrich-execute-policy-execute
// tag::enrich-execute-policy-response
// end::enrich-execute-policy-response
// tag::enrich-execute-policy-execute-listener
// <1>
// <2>
// end::enrich-execute-policy-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::enrich-execute-policy-execute-async
// <1>
// end::enrich-execute-policy-execute-async
/*
//www.apache.org/licenses/LICENSE-2.0
// Create chain of doc IDs across indices 1->2->3
// tag::x-pack-graph-explore-request
// <1>
// <2>
// <3>
// <4>
// end::x-pack-graph-explore-request
// tag::x-pack-graph-explore-response
// <1>
// <2>
// end::x-pack-graph-explore-response
// tag::x-pack-graph-explore-expand
// <1>
// <2>
// <3>
// end::x-pack-graph-explore-expand        
/*
//www.apache.org/licenses/LICENSE-2.0
// tag::ilm-put-lifecycle-policy-request
// <1>
// <2>
// <3>
// end::ilm-put-lifecycle-policy-request
// tag::ilm-put-lifecycle-policy-execute
// end::ilm-put-lifecycle-policy-execute
// tag::ilm-put-lifecycle-policy-response
// <1>
// end::ilm-put-lifecycle-policy-response
// Delete the policy so it can be added again
// tag::ilm-put-lifecycle-policy-execute-listener
// <1>
// <2>
// end::ilm-put-lifecycle-policy-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ilm-put-lifecycle-policy-execute-async
// <1>
// end::ilm-put-lifecycle-policy-execute-async
// Set up a policy so we have something to delete
// tag::ilm-delete-lifecycle-policy-request
// <1>
// end::ilm-delete-lifecycle-policy-request
// tag::ilm-delete-lifecycle-policy-execute
// end::ilm-delete-lifecycle-policy-execute
// tag::ilm-delete-lifecycle-policy-response
// <1>
// end::ilm-delete-lifecycle-policy-response
// Put the policy again so we can delete it again
// tag::ilm-delete-lifecycle-policy-execute-listener
// <1>
// <2>
// end::ilm-delete-lifecycle-policy-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ilm-delete-lifecycle-policy-execute-async
// <1>
// end::ilm-delete-lifecycle-policy-execute-async
// Set up some policies so we have something to get
// tag::ilm-get-lifecycle-policy-request
// <1>
// <2>
// end::ilm-get-lifecycle-policy-request
// tag::ilm-get-lifecycle-policy-execute
// end::ilm-get-lifecycle-policy-execute
// tag::ilm-get-lifecycle-policy-response
// <1>
// <2>
// end::ilm-get-lifecycle-policy-response
// tag::ilm-get-lifecycle-policy-execute-listener
// <1>
// <2>
// end::ilm-get-lifecycle-policy-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ilm-get-lifecycle-policy-execute-async
// <1>
// end::ilm-get-lifecycle-policy-execute-async
// create a policy & index
// wait for the policy to become active
// tag::ilm-explain-lifecycle-request
// <1>
// end::ilm-explain-lifecycle-request
// tag::ilm-explain-lifecycle-execute
// end::ilm-explain-lifecycle-execute
// tag::ilm-explain-lifecycle-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// end::ilm-explain-lifecycle-response
// tag::ilm-explain-lifecycle-execute-listener
// <1>
// <2>
// end::ilm-explain-lifecycle-execute-listener
// tag::ilm-explain-lifecycle-execute-async
// <1>
// end::ilm-explain-lifecycle-execute-async
// tag::ilm-status-request
// end::ilm-status-request
// Check that ILM has stopped
// tag::ilm-status-execute
// end::ilm-status-execute
// tag::ilm-status-response
// <1>
// end::ilm-status-response
// tag::ilm-status-execute-listener
// <1>
// <2>
// end::ilm-status-execute-listener
// tag::ilm-status-execute-async
// <1>
// end::ilm-status-execute-async
// Check that ILM is running again
// tag::ilm-stop-ilm-request
// end::ilm-stop-ilm-request
// tag::ilm-stop-ilm-execute
// end::ilm-stop-ilm-execute
// tag::ilm-stop-ilm-response
// <1>
// end::ilm-stop-ilm-response
// tag::ilm-stop-ilm-execute-listener
// <1>
// <2>
// end::ilm-stop-ilm-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ilm-stop-ilm-execute-async
// <1>
// end::ilm-stop-ilm-execute-async
// tag::ilm-start-ilm-request
// end::ilm-start-ilm-request
// tag::ilm-start-ilm-execute
// end::ilm-start-ilm-execute
// tag::ilm-start-ilm-response
// <1>
// end::ilm-start-ilm-response
// tag::ilm-start-ilm-execute-listener
// <1>
// <2>
// end::ilm-start-ilm-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ilm-start-ilm-execute-async
// <1>
// end::ilm-start-ilm-execute-async
// setup policy to immediately fail on index
// tag::ilm-retry-lifecycle-policy-request
// <1>
// end::ilm-retry-lifecycle-policy-request
// tag::ilm-retry-lifecycle-policy-execute
// end::ilm-retry-lifecycle-policy-execute
// tag::ilm-retry-lifecycle-policy-response
// <1>
// end::ilm-retry-lifecycle-policy-response
// tag::ilm-retry-lifecycle-policy-execute-listener
// <1>
// <2>
// end::ilm-retry-lifecycle-policy-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ilm-retry-lifecycle-policy-execute-async
// <1>
// end::ilm-retry-lifecycle-policy-execute-async
// setup policy for index
// tag::ilm-remove-lifecycle-policy-from-index-request
// <1>
// end::ilm-remove-lifecycle-policy-from-index-request
// tag::ilm-remove-lifecycle-policy-from-index-execute
// end::ilm-remove-lifecycle-policy-from-index-execute
// tag::ilm-remove-lifecycle-policy-from-index-response
// <1>
// <2>
// end::ilm-remove-lifecycle-policy-from-index-response
// re-apply policy on index
// tag::ilm-remove-lifecycle-policy-from-index-execute-listener
// <1>
// <2>
// end::ilm-remove-lifecycle-policy-from-index-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::ilm-remove-lifecycle-policy-from-index-execute-async
// <1>
// end::ilm-remove-lifecycle-policy-from-index-execute-async
//////// PUT
// tag::slm-put-snapshot-lifecycle-policy-request
// end::slm-put-snapshot-lifecycle-policy-request
// tag::slm-put-snapshot-lifecycle-policy-execute
// end::slm-put-snapshot-lifecycle-policy-execute
// tag::slm-put-snapshot-lifecycle-policy-response
// <1>
// end::slm-put-snapshot-lifecycle-policy-response
// tag::slm-put-snapshot-lifecycle-policy-execute-listener
// <1>
// <2>
// end::slm-put-snapshot-lifecycle-policy-execute-listener
// tag::slm-put-snapshot-lifecycle-policy-execute-async
// <1>
// end::slm-put-snapshot-lifecycle-policy-execute-async
//////// GET
// tag::slm-get-snapshot-lifecycle-policy-request
// <1>
// <2>
// end::slm-get-snapshot-lifecycle-policy-request
// tag::slm-get-snapshot-lifecycle-policy-execute
// end::slm-get-snapshot-lifecycle-policy-execute
// tag::slm-get-snapshot-lifecycle-policy-execute-listener
// <1>
// <2>
// end::slm-get-snapshot-lifecycle-policy-execute-listener
// tag::slm-get-snapshot-lifecycle-policy-execute-async
// <1>
// end::slm-get-snapshot-lifecycle-policy-execute-async
// tag::slm-get-snapshot-lifecycle-policy-response
// <1>
// <2>
// end::slm-get-snapshot-lifecycle-policy-response
//////// EXECUTE
// tag::slm-execute-snapshot-lifecycle-policy-request
// <1>
// end::slm-execute-snapshot-lifecycle-policy-request
// tag::slm-execute-snapshot-lifecycle-policy-execute
// end::slm-execute-snapshot-lifecycle-policy-execute
// tag::slm-execute-snapshot-lifecycle-policy-response
// <1>
// end::slm-execute-snapshot-lifecycle-policy-response
// tag::slm-execute-snapshot-lifecycle-policy-execute-listener
// <1>
// <2>
// end::slm-execute-snapshot-lifecycle-policy-execute-listener
// We need a listener that will actually wait for the snapshot to be created
// Ignore
// tag::slm-execute-snapshot-lifecycle-policy-execute-async
// <1>
// end::slm-execute-snapshot-lifecycle-policy-execute-async
// tag::slm-get-snapshot-lifecycle-stats
// end::slm-get-snapshot-lifecycle-stats
// tag::slm-get-snapshot-lifecycle-stats-execute
// end::slm-get-snapshot-lifecycle-stats-execute
//////// DELETE
// tag::slm-delete-snapshot-lifecycle-policy-request
// <1>
// end::slm-delete-snapshot-lifecycle-policy-request
// tag::slm-delete-snapshot-lifecycle-policy-execute
// end::slm-delete-snapshot-lifecycle-policy-execute
// tag::slm-delete-snapshot-lifecycle-policy-response
// <1>
// end::slm-delete-snapshot-lifecycle-policy-response
// tag::slm-delete-snapshot-lifecycle-policy-execute-listener
// <1>
// <2>
// end::slm-delete-snapshot-lifecycle-policy-execute-listener
// tag::slm-delete-snapshot-lifecycle-policy-execute-async
// <1>
// end::slm-delete-snapshot-lifecycle-policy-execute-async
//////// EXECUTE RETENTION
// tag::slm-execute-snapshot-lifecycle-retention-request
// end::slm-execute-snapshot-lifecycle-retention-request
// tag::slm-execute-snapshot-lifecycle-retention-execute
// end::slm-execute-snapshot-lifecycle-retention-execute
// tag::slm-execute-snapshot-lifecycle-retention-response
// end::slm-execute-snapshot-lifecycle-retention-response
// tag::slm-execute-snapshot-lifecycle-retention-execute-listener
// <1>
// <2>
// end::slm-execute-snapshot-lifecycle-retention-execute-listener
// tag::slm-execute-snapshot-lifecycle-retention-execute-async
// <1>
// end::slm-execute-snapshot-lifecycle-retention-execute-async
// tag::slm-status-request
// end::slm-status-request
// Check that SLM has stopped
// tag::slm-status-execute
// end::slm-status-execute
// tag::slm-status-response
// <1>
// end::slm-status-response
// tag::slm-status-execute-listener
// <1>
// <2>
// end::slm-status-execute-listener
// tag::slm-status-execute-async
// <1>
// end::slm-status-execute-async
// Check that SLM is running again
// tag::slm-stop-slm-request
// end::slm-stop-slm-request
// tag::slm-stop-slm-execute
// end::slm-stop-slm-execute
// tag::slm-stop-slm-response
// <1>
// end::slm-stop-slm-response
// tag::slm-stop-slm-execute-listener
// <1>
// <2>
// end::slm-stop-slm-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::slm-stop-slm-execute-async
// <1>
// end::slm-stop-slm-execute-async
// tag::slm-start-slm-request
// end::slm-start-slm-request
// tag::slm-start-slm-execute
// end::slm-start-slm-execute
// tag::slm-start-slm-response
// <1>
// end::slm-start-slm-response
// tag::slm-start-slm-execute-listener
// <1>
// <2>
// end::slm-start-slm-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::slm-start-slm-execute-async
// <1>
// end::slm-start-slm-execute-async
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// tag::example
// end::example
// tag::indices-exists-request
// <1>
// end::indices-exists-request
// tag::indices-exists-request-optionals
// <1>
// <2>
// <3>
// <4>
// end::indices-exists-request-optionals
// tag::indices-exists-execute
// end::indices-exists-execute
// tag::indices-exists-execute-listener
// <1>
// <2>
// end::indices-exists-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::indices-exists-execute-async
// <1>
// end::indices-exists-execute-async
// tag::delete-index-request
// <1>
// end::delete-index-request
// tag::delete-index-request-timeout
// <1>
// <2>
// end::delete-index-request-timeout
// tag::delete-index-request-masterTimeout
// <1>
// <2>
// end::delete-index-request-masterTimeout
// tag::delete-index-request-indicesOptions
// <1>
// end::delete-index-request-indicesOptions
// tag::delete-index-execute
// end::delete-index-execute
// tag::delete-index-response
// <1>
// end::delete-index-response
// tag::delete-index-notfound
// <1>
// end::delete-index-notfound
// tag::delete-index-execute-listener
// <1>
// <2>
// end::delete-index-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-index-execute-async
// <1>
// end::delete-index-execute-async
// tag::create-index-request
// <1>
// end::create-index-request
// tag::create-index-request-settings
// <1>
// end::create-index-request-settings
// tag::create-index-request-mappings
// <1>
// <2>
// end::create-index-request-mappings
//tag::create-index-mappings-map
// <1>
//end::create-index-mappings-map
//tag::create-index-mappings-xcontent
// <1>
//end::create-index-mappings-xcontent
// tag::create-index-request-aliases
// <1>
// end::create-index-request-aliases
// tag::create-index-request-timeout
// <1>
// end::create-index-request-timeout
// tag::create-index-request-masterTimeout
// <1>
// end::create-index-request-masterTimeout
// tag::create-index-request-waitForActiveShards
// <1>
// <2>
// end::create-index-request-waitForActiveShards
// tag::create-index-whole-source
// <1>
// end::create-index-whole-source
// tag::create-index-execute
// end::create-index-execute
// tag::create-index-response
// <1>
// <2>
// end::create-index-response
// tag::create-index-execute-listener
// <1>
// <2>
// end::create-index-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::create-index-execute-async
// <1>
// end::create-index-execute-async
// tag::put-mapping-request
// <1>
// end::put-mapping-request
// tag::put-mapping-request-source
// <1>
// end::put-mapping-request-source
//tag::put-mapping-map
// <1>
//end::put-mapping-map
//tag::put-mapping-xcontent
// <1>
//end::put-mapping-xcontent
// tag::put-mapping-request-timeout
// <1>
// end::put-mapping-request-timeout
// tag::put-mapping-request-masterTimeout
// <1>
// end::put-mapping-request-masterTimeout
// tag::put-mapping-execute
// end::put-mapping-execute
// tag::put-mapping-response
// <1>
// end::put-mapping-response
// tag::put-mapping-execute-listener
// <1>
// <2>
// end::put-mapping-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-mapping-execute-async
// <1>
// end::put-mapping-execute-async
// tag::get-mappings-request
// <1>
// <2>
// end::get-mappings-request
// tag::get-mappings-request-masterTimeout
// <1>
// end::get-mappings-request-masterTimeout
// tag::get-mappings-request-indicesOptions
// <1>
// end::get-mappings-request-indicesOptions
// tag::get-mappings-execute
// end::get-mappings-execute
// tag::get-mappings-response
// <1>
// <2>
// <3>
// end::get-mappings-response
// tag::get-mappings-execute-listener
// <1>
// <2>
// end::get-mappings-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-mappings-execute-async
// <1>
// end::get-mappings-execute-async
// <1>
// tag::get-field-mappings-request
// <1>
// <2>
// <3>
// end::get-field-mappings-request
// tag::get-field-mappings-request-indicesOptions
// <1>
// end::get-field-mappings-request-indicesOptions
// tag::get-field-mappings-request-local
// <1>
// end::get-field-mappings-request-local
// tag::get-field-mappings-execute
// end::get-field-mappings-execute
// tag::get-field-mappings-response
// <1>
// <2>
// <3>
// <4>
// <5>
// end::get-field-mappings-response
// tag::get-field-mappings-execute-listener
// <1>
// <2>
// end::get-field-mappings-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-field-mappings-execute-async
// <1>
// end::get-field-mappings-execute-async
// tag::open-index-request
// <1>
// end::open-index-request
// tag::open-index-request-timeout
// <1>
// <2>
// end::open-index-request-timeout
// tag::open-index-request-masterTimeout
// <1>
// <2>
// end::open-index-request-masterTimeout
// tag::open-index-request-waitForActiveShards
// <1>
// <2>
// end::open-index-request-waitForActiveShards
// tag::open-index-request-indicesOptions
// <1>
// end::open-index-request-indicesOptions
// tag::open-index-execute
// end::open-index-execute
// tag::open-index-response
// <1>
// <2>
// end::open-index-response
// tag::open-index-execute-listener
// <1>
// <2>
// end::open-index-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::open-index-execute-async
// <1>
// end::open-index-execute-async
// tag::open-index-notfound
// <1>
// end::open-index-notfound
// tag::refresh-request
// <1>
// <2>
// <3>
// end::refresh-request
// tag::refresh-request-indicesOptions
// <1>
// end::refresh-request-indicesOptions
// tag::refresh-execute
// end::refresh-execute
// tag::refresh-response
// <1>
// <2>
// <3>
// <4>
// end::refresh-response
// tag::refresh-execute-listener
// <1>
// <2>
// end::refresh-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::refresh-execute-async
// <1>
// end::refresh-execute-async
// tag::refresh-notfound
// <1>
// end::refresh-notfound
// tag::flush-request
// <1>
// <2>
// <3>
// end::flush-request
// tag::flush-request-indicesOptions
// <1>
// end::flush-request-indicesOptions
// tag::flush-request-wait
// <1>
// end::flush-request-wait
// tag::flush-request-force
// <1>
// end::flush-request-force
// tag::flush-execute
// end::flush-execute
// tag::flush-response
// <1>
// <2>
// <3>
// <4>
// end::flush-response
// tag::flush-execute-listener
// <1>
// <2>
// end::flush-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::flush-execute-async
// <1>
// end::flush-execute-async
// tag::flush-notfound
// <1>
// end::flush-notfound
// tag::flush-synced-request
// <1>
// <2>
// <3>
// end::flush-synced-request
// tag::flush-synced-request-indicesOptions
// <1>
// end::flush-synced-request-indicesOptions
// tag::flush-synced-execute
// end::flush-synced-execute
// tag::flush-synced-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// <8>
// <9>
// <10>
// end::flush-synced-response
// tag::flush-synced-execute-listener
// <1>
// <2>
// end::flush-synced-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::flush-synced-execute-async
// <1>
// end::flush-synced-execute-async
// tag::flush-synced-notfound
// <1>
// end::flush-synced-notfound
// tag::get-settings-request
// <1>
// end::get-settings-request
// tag::get-settings-request-names
// <1>
// end::get-settings-request-names
// tag::get-settings-request-indicesOptions
// <1>
// end::get-settings-request-indicesOptions
// tag::get-settings-execute
// end::get-settings-execute
// tag::get-settings-response
// <1>
// <2>
// <3>
// end::get-settings-response
// tag::get-settings-execute-listener
// <1>
// <2>
// end::get-settings-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-settings-execute-async
// <1>
// end::get-settings-execute-async
// tag::get-settings-request-include-defaults
// <1>
// end::get-settings-request-include-defaults
// tag::get-settings-defaults-response
// <1>
// <2>
// end::get-settings-defaults-response
// Replace the empty listener by a blocking listener in test
// tag::get-index-request
// <1>
// end::get-index-request
// tag::get-index-request-indicesOptions
// <1>
// end::get-index-request-indicesOptions
// tag::get-index-request-includeDefaults
// <1>
// end::get-index-request-includeDefaults
// tag::get-index-execute
// end::get-index-execute
// tag::get-index-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// end::get-index-response
// tag::get-index-execute-listener
// <1>
// <2>
// end::get-index-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-index-execute-async
// <1>
// end::get-index-execute-async
// tag::force-merge-request
// <1>
// <2>
// <3>
// end::force-merge-request
// tag::force-merge-request-indicesOptions
// <1>
// end::force-merge-request-indicesOptions
// tag::force-merge-request-segments-num
// <1>
// end::force-merge-request-segments-num
// tag::force-merge-request-only-expunge-deletes
// <1>
// end::force-merge-request-only-expunge-deletes
// set only expunge deletes back to its default value
// as it is mutually exclusive with max. num. segments
// tag::force-merge-request-flush
// <1>
// end::force-merge-request-flush
// tag::force-merge-execute
// end::force-merge-execute
// tag::force-merge-response
// <1>
// <2>
// <3>
// <4>
// end::force-merge-response
// tag::force-merge-execute-listener
// <1>
// <2>
// end::force-merge-execute-listener
// tag::force-merge-execute-async
// <1>
// end::force-merge-execute-async
// tag::force-merge-notfound
// <1>
// end::force-merge-notfound
// tag::clear-cache-request
// <1>
// <2>
// <3>
// end::clear-cache-request
// tag::clear-cache-request-indicesOptions
// <1>
// end::clear-cache-request-indicesOptions
// tag::clear-cache-request-query
// <1>
// end::clear-cache-request-query
// tag::clear-cache-request-request
// <1>
// end::clear-cache-request-request
// tag::clear-cache-request-fielddata
// <1>
// end::clear-cache-request-fielddata
// tag::clear-cache-request-fields
// <1>
// end::clear-cache-request-fields
// tag::clear-cache-execute
// end::clear-cache-execute
// tag::clear-cache-response
// <1>
// <2>
// <3>
// <4>
// end::clear-cache-response
// tag::clear-cache-execute-listener
// <1>
// <2>
// end::clear-cache-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::clear-cache-execute-async
// <1>
// end::clear-cache-execute-async
// tag::clear-cache-notfound
// <1>
// end::clear-cache-notfound
// tag::close-index-request
// <1>
// end::close-index-request
// tag::close-index-request-timeout
// <1>
// end::close-index-request-timeout
// tag::close-index-request-masterTimeout
// <1>
// end::close-index-request-masterTimeout
// tag::close-index-request-indicesOptions
// <1>
// end::close-index-request-indicesOptions
// tag::close-index-execute
// end::close-index-execute
// tag::close-index-response
// <1>
// end::close-index-response
// tag::close-index-execute-listener
// <1>
// <2>
// end::close-index-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::close-index-execute-async
// <1>
// end::close-index-execute-async
// tag::exists-alias-request
// end::exists-alias-request
// tag::exists-alias-request-alias
// <1>
// end::exists-alias-request-alias
// tag::exists-alias-request-indices
// <1>
// end::exists-alias-request-indices
// tag::exists-alias-request-indicesOptions
// <1>
// end::exists-alias-request-indicesOptions
// tag::exists-alias-request-local
// <1>
// end::exists-alias-request-local
// tag::exists-alias-execute
// end::exists-alias-execute
// tag::exists-alias-execute-listener
// <1>
// <2>
// end::exists-alias-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::exists-alias-execute-async
// <1>
// end::exists-alias-execute-async
// tag::update-aliases-request
// <1>
// <2>
// <3>
// end::update-aliases-request
// tag::update-aliases-request2
// <1>
// <2>
// <3>
// <4>
// end::update-aliases-request2
// tag::update-aliases-request-timeout
// <1>
// <2>
// end::update-aliases-request-timeout
// tag::update-aliases-request-masterTimeout
// <1>
// <2>
// end::update-aliases-request-masterTimeout
// tag::update-aliases-execute
// end::update-aliases-execute
// tag::update-aliases-response
// <1>
// end::update-aliases-response
// tag::update-aliases-execute-listener
// <1>
// <2>
// end::update-aliases-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::update-aliases-execute-async
// <1>
// end::update-aliases-execute-async
// tag::shrink-index-request
// <1>
// end::shrink-index-request
// tag::shrink-index-request-timeout
// <1>
// <2>
// end::shrink-index-request-timeout
// tag::shrink-index-request-masterTimeout
// <1>
// <2>
// end::shrink-index-request-masterTimeout
// tag::shrink-index-request-waitForActiveShards
// <1>
// <2>
// end::shrink-index-request-waitForActiveShards
// tag::shrink-index-request-settings
// <1>
// <2>
// end::shrink-index-request-settings
// tag::shrink-index-request-aliases
// <1>
// end::shrink-index-request-aliases
// tag::shrink-index-execute
// end::shrink-index-execute
// tag::shrink-index-response
// <1>
// <2>
// end::shrink-index-response
// tag::shrink-index-execute-listener
// <1>
// <2>
// end::shrink-index-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::shrink-index-execute-async
// <1>
// end::shrink-index-execute-async
// tag::split-index-request
// <1>
// <2>
// end::split-index-request
// tag::split-index-request-timeout
// <1>
// <2>
// end::split-index-request-timeout
// tag::split-index-request-masterTimeout
// <1>
// <2>
// end::split-index-request-masterTimeout
// tag::split-index-request-waitForActiveShards
// <1>
// <2>
// end::split-index-request-waitForActiveShards
// tag::split-index-request-settings
// <1>
// end::split-index-request-settings
// tag::split-index-request-aliases
// <1>
// end::split-index-request-aliases
// tag::split-index-execute
// end::split-index-execute
// tag::split-index-response
// <1>
// <2>
// end::split-index-response
// tag::split-index-execute-listener
// <1>
// <2>
// end::split-index-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::split-index-execute-async
// <1>
// end::split-index-execute-async
// tag::clone-index-request
// <1>
// <2>
// end::clone-index-request
// tag::clone-index-request-timeout
// <1>
// <2>
// end::clone-index-request-timeout
// tag::clone-index-request-masterTimeout
// <1>
// <2>
// end::clone-index-request-masterTimeout
// tag::clone-index-request-waitForActiveShards
// <1>
// <2>
// end::clone-index-request-waitForActiveShards
// tag::clone-index-request-settings
// <1>
// end::clone-index-request-settings
// tag::clone-index-request-aliases
// <1>
// end::clone-index-request-aliases
// tag::clone-index-execute
// end::clone-index-execute
// tag::clone-index-response
// <1>
// <2>
// end::clone-index-response
// tag::clone-index-execute-listener
// <1>
// <2>
// end::clone-index-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::clone-index-execute-async
// <1>
// end::clone-index-execute-async
// tag::rollover-index-request
// <1>
// <2>
// <3>
// <4>
// end::rollover-index-request
// tag::rollover-index-request-timeout
// <1>
// end::rollover-index-request-timeout
// tag::rollover-index-request-masterTimeout
// <1>
// end::rollover-index-request-masterTimeout
// tag::rollover-index-request-dryRun
// <1>
// end::rollover-index-request-dryRun
// tag::rollover-index-request-waitForActiveShards
// <1>
// <2>
// end::rollover-index-request-waitForActiveShards
// tag::rollover-index-request-settings
// <1>
// end::rollover-index-request-settings
// tag::rollover-index-request-mapping
// <1>
// end::rollover-index-request-mapping
// tag::rollover-index-request-alias
// <1>
// end::rollover-index-request-alias
// tag::rollover-index-execute
// end::rollover-index-execute
// tag::rollover-index-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// end::rollover-index-response
// tag::rollover-index-execute-listener
// <1>
// <2>
// end::rollover-index-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::rollover-index-execute-async
// <1>
// end::rollover-index-execute-async
// tag::get-alias-request
// end::get-alias-request
// tag::get-alias-request-alias
// <1>
// end::get-alias-request-alias
// tag::get-alias-request-indices
// <1>
// end::get-alias-request-indices
// tag::get-alias-request-indicesOptions
// <1>
// end::get-alias-request-indicesOptions
// tag::get-alias-request-local
// <1>
// end::get-alias-request-local
// tag::get-alias-execute
// end::get-alias-execute
// tag::get-alias-response
// <1>
// end::get-alias-response
// tag::get-alias-execute-listener
// <1>
// <2>
// end::get-alias-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-alias-execute-async
// <1>
// end::get-alias-execute-async
// tag::indices-put-settings-request
// <1>
// <2>
// <3>
// end::indices-put-settings-request
// tag::indices-put-settings-create-settings
// <1>
// end::indices-put-settings-create-settings
// tag::indices-put-settings-request-index-settings
// end::indices-put-settings-request-index-settings
// tag::indices-put-settings-settings-builder
// <1>
// end::indices-put-settings-settings-builder
// tag::indices-put-settings-settings-map
// <1>
// end::indices-put-settings-settings-map
// tag::indices-put-settings-settings-source
// <1>
// end::indices-put-settings-settings-source
// tag::indices-put-settings-request-preserveExisting
// <1>
// end::indices-put-settings-request-preserveExisting
// tag::indices-put-settings-request-timeout
// <1>
// <2>
// end::indices-put-settings-request-timeout
// tag::indices-put-settings-request-masterTimeout
// <1>
// <2>
// end::indices-put-settings-request-masterTimeout
// tag::indices-put-settings-request-indicesOptions
// <1>
// end::indices-put-settings-request-indicesOptions
// tag::indices-put-settings-execute
// end::indices-put-settings-execute
// tag::indices-put-settings-response
// <1>
// end::indices-put-settings-response
// tag::indices-put-settings-execute-listener
// <1>
// <2>
// end::indices-put-settings-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::indices-put-settings-execute-async
// <1>
// end::indices-put-settings-execute-async
// tag::put-template-request
// <1>
// <2>
// end::put-template-request
// tag::put-template-request-settings
// <1>
// end::put-template-request-settings
// tag::put-template-request-mappings-json
// <1>
// end::put-template-request-mappings-json
//tag::put-template-request-mappings-map
// <1>
//end::put-template-request-mappings-map
//tag::put-template-request-mappings-xcontent
// <1>
//end::put-template-request-mappings-xcontent
// tag::put-template-request-aliases
// <1>
// <2>
// end::put-template-request-aliases
// tag::put-template-request-order
// <1>
// end::put-template-request-order
// tag::put-template-request-version
// <1>
// end::put-template-request-version
// tag::put-template-whole-source
// <1>
// end::put-template-whole-source
// tag::put-template-request-create
// <1>
// end::put-template-request-create
// tag::put-template-request-masterTimeout
// <1>
// <2>
// end::put-template-request-masterTimeout
// make test happy
// tag::put-template-execute
// end::put-template-execute
// tag::put-template-response
// <1>
// end::put-template-response
// tag::put-template-execute-listener
// <1>
// <2>
// end::put-template-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-template-execute-async
// <1>
// end::put-template-execute-async
// tag::get-templates-request
// <1>
// <2>
// <3>
// end::get-templates-request
// tag::get-templates-request-masterTimeout
// <1>
// <2>
// end::get-templates-request-masterTimeout
// tag::get-templates-execute
// end::get-templates-execute
// tag::get-templates-response
// <1>
// end::get-templates-response
// tag::get-templates-execute-listener
// <1>
// <2>
// end::get-templates-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-templates-execute-async
// <1>
// end::get-templates-execute-async
// tag::templates-exist-request
// <1>
// <2>
// <3>
// end::templates-exist-request
// tag::templates-exist-request-optionals
// <1>
// <2>
// <3>
// end::templates-exist-request-optionals
// tag::templates-exist-execute
// end::templates-exist-execute
// tag::templates-exist-execute-listener
// <1>
// <2>
// end::templates-exist-execute-listener
// tag::templates-exist-execute-async
// <1>
// end::templates-exist-execute-async
// tag::indices-validate-query-request
// <1>
// end::indices-validate-query-request
// tag::indices-validate-query-request-query
// <1>
// <2>
// end::indices-validate-query-request-query
// tag::indices-validate-query-request-explain
// <1>
// end::indices-validate-query-request-explain
// tag::indices-validate-query-request-allShards
// <1>
// end::indices-validate-query-request-allShards
// tag::indices-validate-query-request-rewrite
// <1>
// end::indices-validate-query-request-rewrite
// tag::indices-validate-query-execute
// <1>
// end::indices-validate-query-execute
// tag::indices-validate-query-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// <8>
// <9>
// <10>
// <11>
// <12>
// end::indices-validate-query-response
// tag::indices-validate-query-execute-listener
// <1>
// <2>
// end::indices-validate-query-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::indices-validate-query-execute-async
// <1>
// end::indices-validate-query-execute-async
// tag::analyze-builtin-request
// <1>
// <2>
// end::analyze-builtin-request
// tag::analyze-custom-request
// <1>
// <2>
// <3>
// <4>
// <5>
// end::analyze-custom-request
// tag::analyze-custom-normalizer-request
// end::analyze-custom-normalizer-request
// tag::analyze-request-explain
// <1>
// <2>
// end::analyze-request-explain
// tag::analyze-execute
// end::analyze-execute
// tag::analyze-response-tokens
// <1>
// end::analyze-response-tokens
// tag::analyze-response-detail
// <1>
// end::analyze-response-detail
// tag::analyze-index-request
// <1>
// <2>
// end::analyze-index-request
// tag::analyze-execute-listener
// <1>
// <2>
// end::analyze-execute-listener
// use a built-in analyzer in the test
// Use a blocking listener in the test
// tag::analyze-execute-async
// <1>
// end::analyze-execute-async
// tag::analyze-index-normalizer-request
// <1>
// <2>
// end::analyze-index-normalizer-request
// tag::analyze-field-request
// end::analyze-field-request
// tag::freeze-index-request
// <1>
// end::freeze-index-request
// tag::freeze-index-request-timeout
// <1>
// end::freeze-index-request-timeout
// tag::freeze-index-request-masterTimeout
// <1>
// end::freeze-index-request-masterTimeout
// tag::freeze-index-request-waitForActiveShards
// <1>
// end::freeze-index-request-waitForActiveShards
// tag::freeze-index-request-indicesOptions
// <1>
// end::freeze-index-request-indicesOptions
// tag::freeze-index-execute
// end::freeze-index-execute
// tag::freeze-index-response
// <1>
// <2>
// end::freeze-index-response
// tag::freeze-index-execute-listener
// <1>
// <2>
// end::freeze-index-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::freeze-index-execute-async
// <1>
// end::freeze-index-execute-async
// tag::freeze-index-notfound
// <1>
// end::freeze-index-notfound
// tag::unfreeze-index-request
// <1>
// end::unfreeze-index-request
// tag::unfreeze-index-request-timeout
// <1>
// end::unfreeze-index-request-timeout
// tag::unfreeze-index-request-masterTimeout
// <1>
// end::unfreeze-index-request-masterTimeout
// tag::unfreeze-index-request-waitForActiveShards
// <1>
// end::unfreeze-index-request-waitForActiveShards
// tag::unfreeze-index-request-indicesOptions
// <1>
// end::unfreeze-index-request-indicesOptions
// tag::unfreeze-index-execute
// end::unfreeze-index-execute
// tag::unfreeze-index-response
// <1>
// <2>
// end::unfreeze-index-response
// tag::unfreeze-index-execute-listener
// <1>
// <2>
// end::unfreeze-index-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::unfreeze-index-execute-async
// <1>
// end::unfreeze-index-execute-async
// tag::unfreeze-index-notfound
// <1>
// end::unfreeze-index-notfound
// tag::delete-template-request
// <1>
// end::delete-template-request
// tag::delete-template-request-masterTimeout
// <1>
// <2>
// end::delete-template-request-masterTimeout
// tag::delete-template-execute
// end::delete-template-execute
// tag::delete-template-response
// <1>
// end::delete-template-response
// tag::delete-template-execute-listener
// <1>
// <2>
// end::delete-template-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-template-execute-async
// <1>
// end::delete-template-execute-async
// tag::reload-analyzers-request
// <1>
// end::reload-analyzers-request
// tag::reload-analyzers-request-indicesOptions
// <1>
// end::reload-analyzers-request-indicesOptions
// tag::reload-analyzers-execute
// end::reload-analyzers-execute
// tag::reload-analyzers-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// end::reload-analyzers-response
// tag::reload-analyzers-execute-listener
// <1>
// <2>
// end::reload-analyzers-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::reload-analyzers-execute-async
// <1>
// end::reload-analyzers-execute-async
// tag::reload-analyzers-notfound
// <1>
// end::reload-analyzers-notfound
// tag::delete-alias-request
// end::delete-alias-request
// tag::delete-alias-request-timeout
// <1>
// end::delete-alias-request-timeout
// tag::delete-alias-request-masterTimeout
// <1>
// end::delete-alias-request-masterTimeout
// tag::delete-alias-execute
// end::delete-alias-execute
// tag::delete-alias-response
// <1>
// end::delete-alias-response
// <1>
// tag::delete-alias-execute-listener
// <1>
// <2>
// end::delete-alias-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-alias-execute-async
// <1>
// end::delete-alias-execute-async
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// tag::example
// end::example
// tag::put-pipeline-request
// <1>
// <2>
// <3>
// end::put-pipeline-request
// tag::put-pipeline-request-timeout
// <1>
// <2>
// end::put-pipeline-request-timeout
// tag::put-pipeline-request-masterTimeout
// <1>
// <2>
// end::put-pipeline-request-masterTimeout
// tag::put-pipeline-execute
// <1>
// end::put-pipeline-execute
// tag::put-pipeline-response
// <1>
// end::put-pipeline-response
// tag::put-pipeline-execute-listener
// <1>
// <2>
// end::put-pipeline-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-pipeline-execute-async
// <1>
// end::put-pipeline-execute-async
// tag::get-pipeline-request
// <1>
// end::get-pipeline-request
// tag::get-pipeline-request-masterTimeout
// <1>
// <2>
// end::get-pipeline-request-masterTimeout
// tag::get-pipeline-execute
// <1>
// end::get-pipeline-execute
// tag::get-pipeline-response
// <1>
// <2>
// <3>
// end::get-pipeline-response
// tag::get-pipeline-execute-listener
// <1>
// <2>
// end::get-pipeline-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-pipeline-execute-async
// <1>
// end::get-pipeline-execute-async
// tag::delete-pipeline-request
// <1>
// end::delete-pipeline-request
// tag::delete-pipeline-request-timeout
// <1>
// <2>
// end::delete-pipeline-request-timeout
// tag::delete-pipeline-request-masterTimeout
// <1>
// <2>
// end::delete-pipeline-request-masterTimeout
// tag::delete-pipeline-execute
// <1>
// end::delete-pipeline-execute
// tag::delete-pipeline-response
// <1>
// end::delete-pipeline-response
// tag::delete-pipeline-execute-listener
// <1>
// <2>
// end::delete-pipeline-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-pipeline-execute-async
// <1>
// end::delete-pipeline-execute-async
// tag::simulate-pipeline-request
// <1>
// <2>
// end::simulate-pipeline-request
// tag::simulate-pipeline-request-pipeline-id
// <1>
// end::simulate-pipeline-request-pipeline-id
// For testing we set this back to null
// tag::simulate-pipeline-request-verbose
// <1>
// end::simulate-pipeline-request-verbose
// tag::simulate-pipeline-execute
// <1>
// end::simulate-pipeline-execute
// tag::simulate-pipeline-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// <8>
// end::simulate-pipeline-response
// tag::simulate-pipeline-execute-listener
// <1>
// <2>
// end::simulate-pipeline-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::simulate-pipeline-execute-async
// <1>
// end::simulate-pipeline-execute-async
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//tag::put-license-execute
// <1>
// <2>
//end::put-license-execute
//tag::put-license-response
// <1>
// <2>
// <3>
// <4>
// <5>
//end::put-license-response
// Should fail because we are trying to downgrade from platinum trial to gold
// tag::put-license-execute-listener
// <1>
// <2>
// end::put-license-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-license-execute-async
// <1>
// end::put-license-execute-async
// we cannot actually delete the license, otherwise the remaining tests won't work
//tag::delete-license-execute
//end::delete-license-execute
//tag::delete-license-response
// <1>
//end::delete-license-response
// tag::delete-license-execute-listener
// <1>
// <2>
// end::delete-license-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-license-execute-async
// <1>
// end::delete-license-execute-async
//tag::get-license-execute
//end::get-license-execute
//tag::get-license-response
// <1>
//end::get-license-response
// tag::get-license-execute-listener
// <1>
// <2>
// end::get-license-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-license-execute-async
// <1>
// end::get-license-execute-async
// Make sure that it still works in other formats
// tag::start-trial-execute
// <1>
// end::start-trial-execute
// tag::start-trial-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// end::start-trial-response
// tag::start-trial-execute-listener
// <1>
// <2>
// end::start-trial-execute-listener
// tag::start-trial-execute-async
// end::start-trial-execute-async
//tag::start-basic-execute
//end::start-basic-execute
//tag::start-basic-response
// <1>
// <2>
// <3>
// <4>
// <5>
//end::start-basic-response
// tag::start-basic-listener
// <1>
// <2>
// end::start-basic-listener
// Replace the empty listener by a blocking listener in test
// tag::start-basic-execute-async
// <1>
// end::start-basic-execute-async
//tag::get-trial-status-execute
//end::get-trial-status-execute
//tag::get-trial-status-response
// <1>
//end::get-trial-status-response
//tag::get-basic-status-execute
//end::get-basic-status-execute
//tag::get-basic-status-response
// <1>
//end::get-basic-status-response
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// tag::example
// end::example
//tag::get-deprecation-info-request
// <1>
//end::get-deprecation-info-request
// tag::get-deprecation-info-execute
// end::get-deprecation-info-execute
// tag::get-deprecation-info-response
// <1>
// <2>
// <3>
// <4>
// end::get-deprecation-info-response
// tag::get-deprecation-info-execute-listener
// <1>
// <2>
// end::get-deprecation-info-execute-listener
// tag::get-deprecation-info-execute-async
// <1>
// end::get-deprecation-info-execute-async
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// tag::example[]
// end::example[]
//tag::migration-cluster-health
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
//end::migration-cluster-health
//tag::migration-request-ctor
// <1>
//end::migration-request-ctor
//tag::migration-request-ctor-execution
//end::migration-request-ctor-execution
//tag::migration-request-async-execution
// <1>
// <2>
// <3>
// <4>
//end::migration-request-async-execution
//tag::migration-request-sync-execution
// <1>
//end::migration-request-sync-execution
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//tag::main-execute
//end::main-execute
//tag::main-response
//end::main-response
//tag::ping-execute
//end::ping-execute
//tag::x-pack-info-execute
// <1>
// <2>
//end::x-pack-info-execute
//tag::x-pack-info-response
// <1>
// <2>
// <3>
// <4>
//end::x-pack-info-response
// tag::x-pack-info-execute-listener
// <1>
// <2>
// end::x-pack-info-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::x-pack-info-execute-async
// <1>
// end::x-pack-info-execute-async
//tag::x-pack-usage-execute
//end::x-pack-usage-execute
//tag::x-pack-usage-response
//end::x-pack-usage-response
// tag::x-pack-usage-execute-listener
// <1>
// <2>
// end::x-pack-usage-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::x-pack-usage-execute-async
// <1>
// end::x-pack-usage-execute-async
//tag::rest-high-level-client-init
//end::rest-high-level-client-init
//tag::rest-high-level-client-close
//end::rest-high-level-client-close
/*
//www.apache.org/licenses/LICENSE-2.0
// tag::put-job-detector
// <1>
// <2>
// <3>
// end::put-job-detector
// tag::put-job-analysis-config
// <1>
// <2>
// <3>
// end::put-job-analysis-config
// tag::put-job-data-description
// <1>
// end::put-job-data-description
// tag::put-job-config
// <1>
// <2>
// <3>
// <4>
// end::put-job-config
// tag::put-job-request
// <1>
// end::put-job-request
// tag::put-job-execute
// end::put-job-execute
// tag::put-job-response
// <1>
// end::put-job-response
// tag::put-job-execute-listener
// <1>
// <2>
// end::put-job-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-job-execute-async
// <1>
// end::put-job-execute-async
// tag::get-job-request
// <1>
// <2>
// end::get-job-request
// tag::get-job-execute
// end::get-job-execute
// tag::get-job-response
// <1>
// <2>
// end::get-job-response
// tag::get-job-execute-listener
// <1>
// <2>
// end::get-job-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-job-execute-async
// <1>
// end::get-job-execute-async
//tag::delete-job-request
// <1>
//end::delete-job-request
//tag::delete-job-request-force
// <1>
//end::delete-job-request-force
//tag::delete-job-request-wait-for-completion
// <1>
//end::delete-job-request-wait-for-completion
//tag::delete-job-execute
//end::delete-job-execute
//tag::delete-job-response
// <1>
// <2>
//end::delete-job-response
//tag::delete-job-execute-listener
// <1>
// <2>
// end::delete-job-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-job-execute-async
// <1>
// end::delete-job-execute-async
// tag::open-job-request
// <1>
// <2>
// end::open-job-request
// tag::open-job-execute
// end::open-job-execute
// tag::open-job-response
// <1>
// end::open-job-response
// tag::open-job-execute-listener
// <1>
// <2>
// end::open-job-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::open-job-execute-async
// <1>
// end::open-job-execute-async
// tag::close-job-request
// <1>
// <2>
// <3>
// <4>
// end::close-job-request
// tag::close-job-execute
// end::close-job-execute
// tag::close-job-response
// <1>
// end::close-job-response
// tag::close-job-execute-listener
// <1>
// <2>
// end::close-job-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::close-job-execute-async
// <1>
// end::close-job-execute-async
// tag::update-job-detector-options
// <1>
// <2>
// <3>
// end::update-job-detector-options
// tag::update-job-options
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// <8>
// <9>
// <10>
// <11>
// <12>
// end::update-job-options
// tag::update-job-request
// <1>
// end::update-job-request
// tag::update-job-execute
// end::update-job-execute
// tag::update-job-response
// <1>
// end::update-job-response
// tag::update-job-execute-listener
// <1>
// <2>
// end::update-job-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::update-job-execute-async
// <1>
// end::update-job-execute-async
// We need to create a job for the datafeed request to be valid
// tag::put-datafeed-config
// <1>
// <2>
// end::put-datafeed-config
// tag::put-datafeed-config-set-aggregations
// <1>
// end::put-datafeed-config-set-aggregations
// Clearing aggregation to avoid complex validation rules
// tag::put-datafeed-config-set-chunking-config
// <1>
// end::put-datafeed-config-set-chunking-config
// tag::put-datafeed-config-set-frequency
// <1>
// end::put-datafeed-config-set-frequency
// tag::put-datafeed-config-set-query
// <1>
// end::put-datafeed-config-set-query
// tag::put-datafeed-config-set-query-delay
// <1>
// end::put-datafeed-config-set-query-delay
// tag::put-datafeed-config-set-delayed-data-check-config
// <1>
// end::put-datafeed-config-set-delayed-data-check-config
// no need to accidentally trip internal validations due to job bucket size
// tag::put-datafeed-config-set-script-fields
// <1>
// end::put-datafeed-config-set-script-fields
// tag::put-datafeed-config-set-scroll-size
// <1>
// end::put-datafeed-config-set-scroll-size
// tag::put-datafeed-request
// <1>
// end::put-datafeed-request
// tag::put-datafeed-execute
// end::put-datafeed-execute
// tag::put-datafeed-response
// <1>
// end::put-datafeed-response
// We need to create a job for the datafeed request to be valid
// tag::put-datafeed-execute-listener
// <1>
// <2>
// end::put-datafeed-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-datafeed-execute-async
// <1>
// end::put-datafeed-execute-async
// tag::update-datafeed-config
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// <8>
// <9>
// end::update-datafeed-config
// Clearing aggregation to avoid complex validation rules
// tag::update-datafeed-request
// <1>
// end::update-datafeed-request
// tag::update-datafeed-execute
// end::update-datafeed-execute
// tag::update-datafeed-response
// <1>
// end::update-datafeed-response
// tag::update-datafeed-execute-listener
// <1>
// <2>
// end::update-datafeed-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::update-datafeed-execute-async
// <1>
// end::update-datafeed-execute-async
// tag::get-datafeed-request
// <1>
// <2>
// end::get-datafeed-request
// tag::get-datafeed-execute
// end::get-datafeed-execute
// tag::get-datafeed-response
// <1>
// <2>
// end::get-datafeed-response
// tag::get-datafeed-execute-listener
// <1>
// <2>
// end::get-datafeed-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-datafeed-execute-async
// <1>
// end::get-datafeed-execute-async
// tag::delete-datafeed-request
// <1>
// end::delete-datafeed-request
// tag::delete-datafeed-execute
// end::delete-datafeed-execute
// tag::delete-datafeed-response
// <1>
// end::delete-datafeed-response
// Recreate datafeed to allow second deletion
// tag::delete-datafeed-execute-listener
// <1>
// <2>
// end::delete-datafeed-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-datafeed-execute-async
// <1>
// end::delete-datafeed-execute-async
// tag::preview-datafeed-request
// <1>
// end::preview-datafeed-request
// tag::preview-datafeed-execute
// end::preview-datafeed-execute
// tag::preview-datafeed-response
// <1>
// <2>
// end::preview-datafeed-response
// tag::preview-datafeed-execute-listener
// <1>
// <2>
// end::preview-datafeed-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::preview-datafeed-execute-async
// <1>
// end::preview-datafeed-execute-async
// tag::start-datafeed-request
// <1>
// end::start-datafeed-request
// tag::start-datafeed-request-options
// <1>
// <2>
// <3>
// end::start-datafeed-request-options
// tag::start-datafeed-execute
// end::start-datafeed-execute
// tag::start-datafeed-response
// <1>
// end::start-datafeed-response
// tag::start-datafeed-execute-listener
// <1>
// <2>
// end::start-datafeed-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::start-datafeed-execute-async
// <1>
// end::start-datafeed-execute-async
// tag::stop-datafeed-request
// <1>
// end::stop-datafeed-request
// tag::stop-datafeed-request-options
// <1>
// <2>
// <3>
// end::stop-datafeed-request-options
// tag::stop-datafeed-execute
// end::stop-datafeed-execute
// tag::stop-datafeed-response
// <1>
// end::stop-datafeed-response
// tag::stop-datafeed-execute-listener
// <1>
// <2>
// end::stop-datafeed-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::stop-datafeed-execute-async
// <1>
// end::stop-datafeed-execute-async
//tag::get-datafeed-stats-request
// <1>
// <2>
//end::get-datafeed-stats-request
//tag::get-datafeed-stats-execute
//end::get-datafeed-stats-execute
//tag::get-datafeed-stats-response
// <1>
// <2>
//end::get-datafeed-stats-response
// tag::get-datafeed-stats-execute-listener
// <1>
// <2>
// end::get-datafeed-stats-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-datafeed-stats-execute-async
// <1>
// end::get-datafeed-stats-execute-async
// Let us index a bucket
// tag::get-buckets-request
// <1>
// end::get-buckets-request
// tag::get-buckets-timestamp
// <1>
// end::get-buckets-timestamp
// Set timestamp to null as it is incompatible with other args
// tag::get-buckets-anomaly-score
// <1>
// end::get-buckets-anomaly-score
// tag::get-buckets-desc
// <1>
// end::get-buckets-desc
// tag::get-buckets-end
// <1>
// end::get-buckets-end
// tag::get-buckets-exclude-interim
// <1>
// end::get-buckets-exclude-interim
// tag::get-buckets-expand
// <1>
// end::get-buckets-expand
// tag::get-buckets-page
// <1>
// end::get-buckets-page
// Set page params back to null so the response contains the bucket we indexed
// tag::get-buckets-sort
// <1>
// end::get-buckets-sort
// tag::get-buckets-start
// <1>
// end::get-buckets-start
// tag::get-buckets-execute
// end::get-buckets-execute
// tag::get-buckets-response
// <1>
// <2>
// end::get-buckets-response
// tag::get-buckets-execute-listener
// <1>
// <2>
// end::get-buckets-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-buckets-execute-async
// <1>
// end::get-buckets-execute-async
// tag::flush-job-request
// <1>
// end::flush-job-request
// tag::flush-job-request-options
// <1>
// <2>
// <3>
// <4>
// <5>
// end::flush-job-request-options
// tag::flush-job-execute
// end::flush-job-execute
// tag::flush-job-response
// <1>
// <2>
// end::flush-job-response
// tag::flush-job-execute-listener
// <1>
// <2>
// end::flush-job-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::flush-job-execute-async
// <1>
// end::flush-job-execute-async
// tag::delete-forecast-request
// <1>
// end::delete-forecast-request
// tag::delete-forecast-request-options
// <1>
// <2>
// <3>
// end::delete-forecast-request-options
// tag::delete-forecast-execute
// end::delete-forecast-execute
// tag::delete-forecast-response
// <1>
// end::delete-forecast-response
// tag::delete-forecast-execute-listener
// <1>
// <2>
// end::delete-forecast-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-forecast-execute-async
// <1>
// end::delete-forecast-execute-async
// tag::get-job-stats-request
// <1>
// <2>
// end::get-job-stats-request
// tag::get-job-stats-execute
// end::get-job-stats-execute
// tag::get-job-stats-response
// <1>
// <2>
// end::get-job-stats-response
// tag::get-job-stats-execute-listener
// <1>
// <2>
// end::get-job-stats-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-job-stats-execute-async
// <1>
// end::get-job-stats-execute-async
// tag::forecast-job-request
// <1>
// end::forecast-job-request
// tag::forecast-job-request-options
// <1>
// <2>
// end::forecast-job-request-options
// tag::forecast-job-execute
// end::forecast-job-execute
// tag::forecast-job-response
// <1>
// <2>
// end::forecast-job-response
// tag::forecast-job-execute-listener
// <1>
// <2>
// end::forecast-job-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::forecast-job-execute-async
// <1>
// end::forecast-job-execute-async
// Let us index some buckets
// tag::get-overall-buckets-request
// <1>
// end::get-overall-buckets-request
// tag::get-overall-buckets-bucket-span
// <1>
// end::get-overall-buckets-bucket-span
// tag::get-overall-buckets-end
// <1>
// end::get-overall-buckets-end
// tag::get-overall-buckets-exclude-interim
// <1>
// end::get-overall-buckets-exclude-interim
// tag::get-overall-buckets-overall-score
// <1>
// end::get-overall-buckets-overall-score
// tag::get-overall-buckets-start
// <1>
// end::get-overall-buckets-start
// tag::get-overall-buckets-top-n
// <1>
// end::get-overall-buckets-top-n
// tag::get-overall-buckets-execute
// end::get-overall-buckets-execute
// tag::get-overall-buckets-response
// <1>
// <2>
// end::get-overall-buckets-response
// tag::get-overall-buckets-execute-listener
// <1>
// <2>
// end::get-overall-buckets-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-overall-buckets-execute-async
// <1>
// end::get-overall-buckets-execute-async
// Let us index a record
// tag::get-records-request
// <1>
// end::get-records-request
// tag::get-records-desc
// <1>
// end::get-records-desc
// tag::get-records-end
// <1>
// end::get-records-end
// tag::get-records-exclude-interim
// <1>
// end::get-records-exclude-interim
// tag::get-records-page
// <1>
// end::get-records-page
// Set page params back to null so the response contains the record we indexed
// tag::get-records-record-score
// <1>
// end::get-records-record-score
// tag::get-records-sort
// <1>
// end::get-records-sort
// tag::get-records-start
// <1>
// end::get-records-start
// tag::get-records-execute
// end::get-records-execute
// tag::get-records-response
// <1>
// <2>
// end::get-records-response
// tag::get-records-execute-listener
// <1>
// <2>
// end::get-records-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-records-execute-async
// <1>
// end::get-records-execute-async
// tag::post-data-request
// <1>
// <2>
// <3>
// <4>
// end::post-data-request
// tag::post-data-request-options
// <1>
// <2>
// end::post-data-request-options
// tag::post-data-execute
// end::post-data-execute
// tag::post-data-response
// <1>
// end::post-data-response
// tag::post-data-execute-listener
// <1>
// <2>
// end::post-data-execute-listener
// <1>
// Replace the empty listener by a blocking listener in test
// tag::post-data-execute-async
// <1>
// end::post-data-execute-async
// tag::find-file-structure-request
// <1>
// <2>
// end::find-file-structure-request
// tag::find-file-structure-request-options
// <1>
// <2>
// end::find-file-structure-request-options
// tag::find-file-structure-execute
// end::find-file-structure-execute
// tag::find-file-structure-response
// <1>
// end::find-file-structure-response
// tag::find-file-structure-execute-listener
// <1>
// <2>
// end::find-file-structure-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::find-file-structure-execute-async
// <1>
// end::find-file-structure-execute-async
// Let us index a record
// tag::get-influencers-request
// <1>
// end::get-influencers-request
// tag::get-influencers-desc
// <1>
// end::get-influencers-desc
// tag::get-influencers-end
// <1>
// end::get-influencers-end
// tag::get-influencers-exclude-interim
// <1>
// end::get-influencers-exclude-interim
// tag::get-influencers-influencer-score
// <1>
// end::get-influencers-influencer-score
// tag::get-influencers-page
// <1>
// end::get-influencers-page
// Set page params back to null so the response contains the influencer we indexed
// tag::get-influencers-sort
// <1>
// end::get-influencers-sort
// tag::get-influencers-start
// <1>
// end::get-influencers-start
// tag::get-influencers-execute
// end::get-influencers-execute
// tag::get-influencers-response
// <1>
// <2>
// end::get-influencers-response
// tag::get-influencers-execute-listener
// <1>
// <2>
// end::get-influencers-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-influencers-execute-async
// <1>
// end::get-influencers-execute-async
// Let us index a category
// tag::get-categories-request
// <1>
// end::get-categories-request
// tag::get-categories-category-id
// <1>
// end::get-categories-category-id
// tag::get-categories-page
// <1>
// end::get-categories-page
// Set page params back to null so the response contains the category we indexed
// tag::get-categories-execute
// end::get-categories-execute
// tag::get-categories-response
// <1>
// <2>
// end::get-categories-response
// tag::get-categories-execute-listener
// <1>
// <2>
// end::get-categories-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-categories-execute-async
// <1>
// end::get-categories-execute-async
// tag::delete-expired-data-request
// <1>
// end::delete-expired-data-request
// tag::delete-expired-data-execute
// end::delete-expired-data-execute
// tag::delete-expired-data-response
// <1>
// end::delete-expired-data-response
// tag::delete-expired-data-execute-listener
// <1>
// <2>
// end::delete-expired-data-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-expired-data-execute-async
// <1>
// end::delete-expired-data-execute-async
// Let us index a snapshot
// tag::delete-model-snapshot-request
// <1>
// end::delete-model-snapshot-request
// tag::delete-model-snapshot-execute
// end::delete-model-snapshot-execute
// tag::delete-model-snapshot-response
// <1>
// end::delete-model-snapshot-response
// tag::delete-model-snapshot-execute-listener
// <1>
// <2>
// end::delete-model-snapshot-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-model-snapshot-execute-async
// <1>
// end::delete-model-snapshot-execute-async
// Let us index a snapshot
// tag::get-model-snapshots-request
// <1>
// end::get-model-snapshots-request
// tag::get-model-snapshots-snapshot-id
// <1>
// end::get-model-snapshots-snapshot-id
// Set snapshot id to null as it is incompatible with other args
// tag::get-model-snapshots-desc
// <1>
// end::get-model-snapshots-desc
// tag::get-model-snapshots-end
// <1>
// end::get-model-snapshots-end
// tag::get-model-snapshots-page
// <1>
// end::get-model-snapshots-page
// Set page params back to null so the response contains the snapshot we indexed
// tag::get-model-snapshots-sort
// <1>
// end::get-model-snapshots-sort
// tag::get-model-snapshots-start
// <1>
// end::get-model-snapshots-start
// tag::get-model-snapshots-execute
// end::get-model-snapshots-execute
// tag::get-model-snapshots-response
// <1>
// <2>
// end::get-model-snapshots-response
// tag::get-model-snapshots-execute-listener
// <1>
// <2>
// end::get-model-snapshots-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-model-snapshots-execute-async
// <1>
// end::get-model-snapshots-execute-async
// Let us index a snapshot
// tag::revert-model-snapshot-request
// <1>
// end::revert-model-snapshot-request
// tag::revert-model-snapshot-delete-intervening-results
// <1>
// end::revert-model-snapshot-delete-intervening-results
// tag::revert-model-snapshot-execute
// end::revert-model-snapshot-execute
// tag::revert-model-snapshot-response
// <1>
// end::revert-model-snapshot-response
// tag::revert-model-snapshot-execute-listener
// <1>
// <2>
// end::revert-model-snapshot-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::revert-model-snapshot-execute-async
// <1>
// end::revert-model-snapshot-execute-async
// Let us index a snapshot
// tag::update-model-snapshot-request
// <1>
// end::update-model-snapshot-request
// tag::update-model-snapshot-description
// <1>
// end::update-model-snapshot-description
// tag::update-model-snapshot-retain
// <1>
// end::update-model-snapshot-retain
// tag::update-model-snapshot-execute
// end::update-model-snapshot-execute
// tag::update-model-snapshot-response
// <1>
// <2>
// end::update-model-snapshot-response
// tag::update-model-snapshot-execute-listener
// <1>
// <2>
// end::update-model-snapshot-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::update-model-snapshot-execute-async
// <1>
// end::update-model-snapshot-execute-async
// tag::put-calendar-request
// <1>
// end::put-calendar-request
// tag::put-calendar-execute
// end::put-calendar-execute
// tag::put-calendar-response
// <1>
// end::put-calendar-response
// tag::put-calendar-execute-listener
// <1>
// <2>
// end::put-calendar-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-calendar-execute-async
// <1>
// end::put-calendar-execute-async
// tag::put-calendar-job-request
// <1>
// <2>
// end::put-calendar-job-request
// tag::put-calendar-job-execute
// end::put-calendar-job-execute
// tag::put-calendar-job-response
// <1>
// end::put-calendar-job-response
// tag::put-calendar-job-execute-listener
// <1>
// <2>
// end::put-calendar-job-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-calendar-job-execute-async
// <1>
// end::put-calendar-job-execute-async
// tag::delete-calendar-job-request
// <1>
// <2>
// end::delete-calendar-job-request
// tag::delete-calendar-job-execute
// end::delete-calendar-job-execute
// tag::delete-calendar-job-response
// <1>
// end::delete-calendar-job-response
// tag::delete-calendar-job-execute-listener
// <1>
// <2>
// end::delete-calendar-job-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-calendar-job-execute-async
// <1>
// end::delete-calendar-job-execute-async
// tag::get-calendars-request
// <1>
// end::get-calendars-request
// tag::get-calendars-id
// <1>
// end::get-calendars-id
// tag::get-calendars-page
// <1>
// end::get-calendars-page
// reset page params
// tag::get-calendars-execute
// end::get-calendars-execute
// tag::get-calendars-response
// <1>
// <2>
// end::get-calendars-response
// tag::get-calendars-execute-listener
// <1>
// <2>
// end::get-calendars-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-calendars-execute-async
// <1>
// end::get-calendars-execute-async
// tag::delete-calendar-request
// <1>
// end::delete-calendar-request
// tag::delete-calendar-execute
// end::delete-calendar-execute
// tag::delete-calendar-response
// <1>
// end::delete-calendar-response
// tag::delete-calendar-execute-listener
// <1>
// <2>
// end::delete-calendar-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-calendar-execute-async
// <1>
// end::delete-calendar-execute-async
// tag::get-calendar-events-request
// <1>
// end::get-calendar-events-request
// tag::get-calendar-events-page
// <1>
// end::get-calendar-events-page
// tag::get-calendar-events-start
// <1>
// end::get-calendar-events-start
// tag::get-calendar-events-end
// <1>
// end::get-calendar-events-end
// tag::get-calendar-events-jobid
// <1>
// end::get-calendar-events-jobid
// reset params
// tag::get-calendar-events-execute
// end::get-calendar-events-execute
// tag::get-calendar-events-response
// <1>
// <2>
// end::get-calendar-events-response
// tag::get-calendar-events-execute-listener
// <1>
// <2>
// end::get-calendar-events-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-calendar-events-execute-async
// <1>
// end::get-calendar-events-execute-async
// tag::post-calendar-event-request
// <1>
// <2>
// end::post-calendar-event-request
// tag::post-calendar-event-execute
// end::post-calendar-event-execute
// tag::post-calendar-event-response
// <1>
// end::post-calendar-event-response
// <1>
// tag::post-calendar-event-execute-listener
// <1>
// <2>
// end::post-calendar-event-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::post-calendar-event-execute-async
// <1>
// end::post-calendar-event-execute-async
// tag::delete-calendar-event-request
// <1>
// <2>
// end::delete-calendar-event-request
// tag::delete-calendar-event-execute
// end::delete-calendar-event-execute
// tag::delete-calendar-event-response
// <1>
// end::delete-calendar-event-response
// tag::delete-calendar-event-execute-listener
// <1>
// <2>
// end::delete-calendar-event-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-calendar-event-execute-async
// <1>
// end::delete-calendar-event-execute-async
// tag::get-data-frame-analytics-request
// <1>
// end::get-data-frame-analytics-request
// tag::get-data-frame-analytics-execute
// end::get-data-frame-analytics-execute
// tag::get-data-frame-analytics-response
// end::get-data-frame-analytics-response
// tag::get-data-frame-analytics-execute-listener
// <1>
// <2>
// end::get-data-frame-analytics-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-data-frame-analytics-execute-async
// <1>
// end::get-data-frame-analytics-execute-async
// tag::get-data-frame-analytics-stats-request
// <1>
// end::get-data-frame-analytics-stats-request
// tag::get-data-frame-analytics-stats-execute
// end::get-data-frame-analytics-stats-execute
// tag::get-data-frame-analytics-stats-response
// end::get-data-frame-analytics-stats-response
// tag::get-data-frame-analytics-stats-execute-listener
// <1>
// <2>
// end::get-data-frame-analytics-stats-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-data-frame-analytics-stats-execute-async
// <1>
// end::get-data-frame-analytics-stats-execute-async
// tag::put-data-frame-analytics-query-config
// end::put-data-frame-analytics-query-config
// tag::put-data-frame-analytics-source-config
// <1>
// <2>
// <3>
// <4>
// end::put-data-frame-analytics-source-config
// tag::put-data-frame-analytics-dest-config
// <1>
// <2>
// end::put-data-frame-analytics-dest-config
// tag::put-data-frame-analytics-outlier-detection-default
// <1>
// end::put-data-frame-analytics-outlier-detection-default
// tag::put-data-frame-analytics-outlier-detection-customized
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// end::put-data-frame-analytics-outlier-detection-customized
// tag::put-data-frame-analytics-classification
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// <8>
// <9>
// <10>
// <11>
// end::put-data-frame-analytics-classification
// tag::put-data-frame-analytics-regression
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// <8>
// <9>
// <10>
// end::put-data-frame-analytics-regression
// tag::put-data-frame-analytics-analyzed-fields
// end::put-data-frame-analytics-analyzed-fields
// tag::put-data-frame-analytics-config
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// end::put-data-frame-analytics-config
// tag::put-data-frame-analytics-request
// <1>
// end::put-data-frame-analytics-request
// tag::put-data-frame-analytics-execute
// end::put-data-frame-analytics-execute
// tag::put-data-frame-analytics-response
// end::put-data-frame-analytics-response
// tag::put-data-frame-analytics-execute-listener
// <1>
// <2>
// end::put-data-frame-analytics-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-data-frame-analytics-execute-async
// <1>
// end::put-data-frame-analytics-execute-async
// tag::delete-data-frame-analytics-request
// <1>
// end::delete-data-frame-analytics-request
//tag::delete-data-frame-analytics-request-force
// <1>
//end::delete-data-frame-analytics-request-force
// tag::delete-data-frame-analytics-execute
// end::delete-data-frame-analytics-execute
// tag::delete-data-frame-analytics-response
// end::delete-data-frame-analytics-response
// tag::delete-data-frame-analytics-execute-listener
// <1>
// <2>
// end::delete-data-frame-analytics-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-data-frame-analytics-execute-async
// <1>
// end::delete-data-frame-analytics-execute-async
// tag::start-data-frame-analytics-request
// <1>
// end::start-data-frame-analytics-request
// tag::start-data-frame-analytics-execute
// end::start-data-frame-analytics-execute
// tag::start-data-frame-analytics-response
// end::start-data-frame-analytics-response
// tag::start-data-frame-analytics-execute-listener
// <1>
// <2>
// end::start-data-frame-analytics-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::start-data-frame-analytics-execute-async
// <1>
// end::start-data-frame-analytics-execute-async
// tag::stop-data-frame-analytics-request
// <1>
// <2>
// end::stop-data-frame-analytics-request
// tag::stop-data-frame-analytics-execute
// end::stop-data-frame-analytics-execute
// tag::stop-data-frame-analytics-response
// end::stop-data-frame-analytics-response
// tag::stop-data-frame-analytics-execute-listener
// <1>
// <2>
// end::stop-data-frame-analytics-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::stop-data-frame-analytics-execute-async
// <1>
// end::stop-data-frame-analytics-execute-async
// #0
// #1
// #2
// #3
// #4
// #5
// #6
// #7
// #8
// #9
// tag::evaluate-data-frame-evaluation-softclassification
// <1>
// <2>
// <3>
// Evaluation metrics // <4>
// <5>
// <6>
// <7>
// <8>
// end::evaluate-data-frame-evaluation-softclassification
// tag::evaluate-data-frame-request
// <1>
// <2>
// <3>
// <4>
// end::evaluate-data-frame-request
// tag::evaluate-data-frame-execute
// end::evaluate-data-frame-execute
// tag::evaluate-data-frame-response
// <1>
// end::evaluate-data-frame-response
// tag::evaluate-data-frame-results-softclassification
// <1>
// <2>
// <3>
// <4>
// end::evaluate-data-frame-results-softclassification
// docs #8 and #9
// doc #4
// docs #0, #1, #2 and #3
// docs #5, #6 and #7
// tag::evaluate-data-frame-execute-listener
// <1>
// <2>
// end::evaluate-data-frame-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::evaluate-data-frame-execute-async
// <1>
// end::evaluate-data-frame-execute-async
// #0
// #1
// #2
// #3
// #4
// #5
// #6
// #7
// #8
// #9
// tag::evaluate-data-frame-evaluation-classification
// <1>
// <2>
// <3>
// Evaluation metrics // <4>
// <5>
// <6>
// <7>
// <8>
// end::evaluate-data-frame-evaluation-classification
// tag::evaluate-data-frame-results-classification
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// <8>
// <9>
// end::evaluate-data-frame-results-classification
// #0
// #1
// #2
// #3
// #4
// #5
// #6
// #7
// #8
// #9
// tag::evaluate-data-frame-evaluation-regression
// <1>
// <2>
// <3>
// Evaluation metrics // <4>
// <5>
// <6>
// end::evaluate-data-frame-evaluation-regression
// tag::evaluate-data-frame-results-regression
// <1>
// <2>
// <3>
// <4>
// end::evaluate-data-frame-results-regression
// tag::explain-data-frame-analytics-id-request
// <1>
// end::explain-data-frame-analytics-id-request
// tag::explain-data-frame-analytics-config-request
// <1>
// end::explain-data-frame-analytics-config-request
// tag::explain-data-frame-analytics-execute
// end::explain-data-frame-analytics-execute
// tag::explain-data-frame-analytics-response
// <1>
// <2>
// end::explain-data-frame-analytics-response
// <1>
// <2>
// We are pretty liberal here as this test does not aim at verifying concrete numbers but rather end-to-end user workflow.
// tag::explain-data-frame-analytics-execute-listener
// <1>
// <2>
// end::explain-data-frame-analytics-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::explain-data-frame-analytics-execute-async
// <1>
// end::explain-data-frame-analytics-execute-async
// tag::get-trained-models-request
// <1>
// <2>
// <3>
// <4>
// <5>
// end::get-trained-models-request
// tag::get-trained-models-execute
// end::get-trained-models-execute
// tag::get-trained-models-response
// end::get-trained-models-response
// tag::get-trained-models-execute-listener
// <1>
// <2>
// end::get-trained-models-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-trained-models-execute-async
// <1>
// end::get-trained-models-execute-async
// tag::put-trained-model-config
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// end::put-trained-model-config
// tag::put-trained-model-request
// <1>
// end::put-trained-model-request
// tag::put-trained-model-execute
// end::put-trained-model-execute
// tag::put-trained-model-response
// end::put-trained-model-response
// tag::put-trained-model-execute-listener
// <1>
// <2>
// end::put-trained-model-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-trained-model-execute-async
// <1>
// end::put-trained-model-execute-async
// tag::get-trained-models-stats-request
// <1>
// <2>
// <3>
// end::get-trained-models-stats-request
// tag::get-trained-models-stats-execute
// end::get-trained-models-stats-execute
// tag::get-trained-models-stats-response
// end::get-trained-models-stats-response
// tag::get-trained-models-stats-execute-listener
// <1>
// <2>
// end::get-trained-models-stats-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-trained-models-stats-execute-async
// <1>
// end::get-trained-models-stats-execute-async
// tag::delete-trained-model-request
// <1>
// end::delete-trained-model-request
// tag::delete-trained-model-execute
// end::delete-trained-model-execute
// tag::delete-trained-model-response
// end::delete-trained-model-response
// tag::delete-trained-model-execute-listener
// <1>
// <2>
// end::delete-trained-model-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-trained-model-execute-async
// <1>
// end::delete-trained-model-execute-async
// tag::put-filter-config
// <1>
// <2>
// <3>
// end::put-filter-config
// tag::put-filter-request
// <1>
// end::put-filter-request
// tag::put-filter-execute
// end::put-filter-execute
// tag::put-filter-response
// <1>
// end::put-filter-response
// tag::put-filter-execute-listener
// <1>
// <2>
// end::put-filter-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-filter-execute-async
// <1>
// end::put-filter-execute-async
// tag::get-filters-request
// <1>
// end::get-filters-request
// tag::get-filters-filter-id
// <1>
// end::get-filters-filter-id
// tag::get-filters-page-params
// <1>
// <2>
// end::get-filters-page-params
// tag::get-filters-execute
// end::get-filters-execute
// tag::get-filters-response
// <1>
// <2>
// end::get-filters-response
// tag::get-filters-execute-listener
// <1>
// <2>
// end::get-filters-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-filters-execute-async
// <1>
// end::get-filters-execute-async
// tag::update-filter-request
// <1>
// end::update-filter-request
// tag::update-filter-description
// <1>
// end::update-filter-description
// tag::update-filter-add-items
// <1>
// end::update-filter-add-items
// tag::update-filter-remove-items
// <1>
// end::update-filter-remove-items
// tag::update-filter-execute
// end::update-filter-execute
// tag::update-filter-response
// <1>
// end::update-filter-response
// tag::update-filter-execute-listener
// <1>
// <2>
// end::update-filter-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::update-filter-execute-async
// <1>
// end::update-filter-execute-async
// tag::delete-filter-request
// <1>
// end::delete-filter-request
// tag::delete-filter-execute
// end::delete-filter-execute
// tag::delete-filter-response
// <1>
// end::delete-filter-response
// tag::delete-filter-execute-listener
// <1>
// <2>
// end::delete-filter-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-filter-execute-async
//<1>
// end::delete-filter-execute-async
// tag::get-ml-info-request
// <1>
// end::get-ml-info-request
// tag::get-ml-info-execute
// end::get-ml-info-execute
// tag::get-ml-info-response
// <1>
// end::get-ml-info-response
// tag::get-ml-info-execute-listener
// <1>
// <2>
// end::get-ml-info-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-ml-info-execute-async
// <1>
// end::get-ml-info-execute-async
// tag::set-upgrade-mode-request
// <1>
// <2>
// end::set-upgrade-mode-request
// Set to false so that the cluster setting does not have to be unset at the end of the test.
// tag::set-upgrade-mode-execute
// end::set-upgrade-mode-execute
// tag::set-upgrade-mode-response
// <1>
// end::set-upgrade-mode-response
// tag::set-upgrade-mode-execute-listener
// <1>
// <2>
// end::set-upgrade-mode-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::set-upgrade-mode-execute-async
// <1>
// end::set-upgrade-mode-execute-async
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// tag::bool
// <1>
// <1>
// <2>
// <3>
// <4>
// end::bool
// tag::boosting
// <1>
// <2>
// <3>
// end::boosting
// tag::constant_score
// <1>
// <2>
// end::constant_score
// tag::dis_max
// <1>
// <2>
// <3>
// <4>
// end::dis_max
// tag::exists
// <1>
// end::exists
// tag::function_score
// <1>
// <2>
// <3>
// end::function_score
// tag::fuzzy
// <1>
// <2>
// end::fuzzy
// tag::geo_bounding_box
// <1>
// <2>
// <3>
// end::geo_bounding_box
// tag::geo_distance
// <1>
// <2>
// <3>
// end::geo_distance
// tag::geo_polygon
// <1>
// <2>
// end::geo_polygon
// tag::geo_shape
// <1>
// <2>
// <3>
// end::geo_shape
// tag::indexed_geo_shape
// Using pre-indexed shapes
// <1>
// <2>
// <3>
// <4>
// <5>
// end::indexed_geo_shape
// tag::has_child
// <1>
// <2>
// <3>
// end::has_child
// tag::has_parent
// <1>
// <2>
// <3>
// end::has_parent
// tag::ids
// <1>
// end::ids
// tag::match_all
// end::match_all
// tag::match
// <1>
// <2>
// end::match
// tag::more_like_this
// <1>
// <2>
// <3>
// <4>
// end::more_like_this
// tag::multi_match
// <1>
// <2>
// end::multi_match
// tag::nested
// <1>
// <2>
// <3>
// end::nested
// tag::prefix
// <1>
// <2>
// end::prefix
// tag::query_string
// end::query_string
// tag::range
// <1>
// <2>
// <3>
// <4>
// <5>
// end::range
// tag::range_simplified
// A simplified form using gte, gt, lt or lte
// <1>
// <2>
// <3>
// end::range_simplified
// tag::regexp
// <1>
// <2>
// end::regexp
// tag::script_inline
// <1>
// end::script_inline
// tag::script_file
// <1>
// <2>
// <3>
// <4>
// end::script_file
// tag::simple_query_string
// end::simple_query_string
// tag::span_containing
// <1>
// <2>
// end::span_containing
// tag::span_first
// <1>
// <2>
// end::span_first
// tag::span_multi
// <1>
// end::span_multi
// tag::span_near
// <1>
// <2>
// <1>
// <1>
// <3>
// end::span_near
// tag::span_not
// <1>
// <2>
// end::span_not
// tag::span_or
// <1>
// <1>
// <1>
// end::span_or
// tag::span_term
// <1>
// <2>
// end::span_term
// tag::span_within
// <1>
// <2>
// end::span_within
// tag::term
// <1>
// <2>
// end::term
// tag::terms
// <1>
// <2>
// end::terms
// tag::wildcard
// <1>
// <2>
// end::wildcard
// tag::wrapper
// <1>
// end::wrapper
/*
//www.apache.org/licenses/LICENSE-2.0
//tag::x-pack-rollup-put-rollup-job-group-config
// <1>
// <2>
// <3>
// <4>
//end::x-pack-rollup-put-rollup-job-group-config
//tag::x-pack-rollup-put-rollup-job-metrics-config
// <1>
// <2>
// <3>
//end::x-pack-rollup-put-rollup-job-metrics-config
//tag::x-pack-rollup-put-rollup-job-config
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// <8>
//end::x-pack-rollup-put-rollup-job-config
//tag::x-pack-rollup-put-rollup-job-request
// <1>
//end::x-pack-rollup-put-rollup-job-request
//tag::x-pack-rollup-put-rollup-job-execute
//end::x-pack-rollup-put-rollup-job-execute
//tag::x-pack-rollup-put-rollup-job-response
// <1>
//end::x-pack-rollup-put-rollup-job-response
// tag::x-pack-rollup-put-rollup-job-execute-listener
// <1>
// <2>
// end::x-pack-rollup-put-rollup-job-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::x-pack-rollup-put-rollup-job-execute-async
// <1>
// end::x-pack-rollup-put-rollup-job-execute-async
// tag::x-pack-rollup-get-rollup-job-request
// <1>
// <2>
// end::x-pack-rollup-get-rollup-job-request
// tag::x-pack-rollup-get-rollup-job-execute
// end::x-pack-rollup-get-rollup-job-execute
// tag::x-pack-rollup-get-rollup-job-response
// <1>
// end::x-pack-rollup-get-rollup-job-response
// tag::x-pack-rollup-get-rollup-job-execute-listener
// <1>
// <2>
// end::x-pack-rollup-get-rollup-job-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::x-pack-rollup-get-rollup-job-execute-async
// <1>
// end::x-pack-rollup-get-rollup-job-execute-async
// tag::rollup-start-job-request
// <1>
// end::rollup-start-job-request
// tag::rollup-start-job-execute
// end::rollup-start-job-execute
// tag::rollup-start-job-response
// <1>
// end::rollup-start-job-response
// Swallow any exception, this test does not test actually cancelling.
// stop job to prevent spamming exceptions on next start request
// tag::rollup-start-job-execute-listener
// <1>
// <2>
// end::rollup-start-job-execute-listener
// tag::rollup-start-job-execute-async
// <1>
// end::rollup-start-job-execute-async
// stop job so it can correctly be deleted by the test teardown
// tag::rollup-stop-job-request
// <1>
// <2>
// <3>
// end::rollup-stop-job-request
// tag::rollup-stop-job-execute
// end::rollup-stop-job-execute
// tag::rollup-stop-job-response
// <1>
// end::rollup-stop-job-response
// Swallow any exception, this test does not test actually cancelling.
// tag::rollup-stop-job-execute-listener
// <1>
// <2>
// end::rollup-stop-job-execute-listener
// tag::rollup-stop-job-execute-async
// <1>
// end::rollup-stop-job-execute-async
// Setup a rollup index to query
// tag::search-request
// end::search-request
// tag::search-execute
// end::search-execute
// tag::search-response
// end::search-response
// tag::search-execute-listener
// <1>
// <2>
// end::search-execute-listener
// tag::search-execute-async
// <1>
// end::search-execute-async
// <1>
// <1>
//tag::x-pack-rollup-get-rollup-caps-setup
//end::x-pack-rollup-get-rollup-caps-setup
// Now that the job is created, we should have a rollup index with metadata.
// We can test out the caps API now.
//tag::x-pack-rollup-get-rollup-caps-request
//end::x-pack-rollup-get-rollup-caps-request
//tag::x-pack-rollup-get-rollup-caps-execute
//end::x-pack-rollup-get-rollup-caps-execute
//tag::x-pack-rollup-get-rollup-caps-response
// indexName will be "docs" in this case... the index pattern that we rolled up
// Each index pattern can have multiple jobs that rolled it up, so `getJobCaps()`
// returns a list of jobs that rolled up the pattern
// jobID is the identifier we used when we created the job (e.g. `job1`)
// rollupIndex is the location that the job stored it's rollup docs (e.g. `rollup`)
// indexPattern is the same as the indexName that we retrieved earlier, redundant info
// Finally, fieldCaps are the capabilities of individual fields in the config
// The key is the field name, and the value is a RollupFieldCaps object which
// provides more info.
// If we retrieve the "timestamp" field, it returns a list of maps.  Each list
// item represents a different aggregation that can be run against the "timestamp"
// field, and any additional details specific to that agg (interval, etc)
// In contrast to the timestamp field, the temperature field has multiple aggs configured
//end::x-pack-rollup-get-rollup-caps-response
// tag::x-pack-rollup-get-rollup-caps-execute-listener
// <1>
// <2>
// end::x-pack-rollup-get-rollup-caps-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::x-pack-rollup-get-rollup-caps-execute-async
// <1>
// end::x-pack-rollup-get-rollup-caps-execute-async
// <1>
// <1>
//tag::x-pack-rollup-get-rollup-index-caps-setup
//end::x-pack-rollup-get-rollup-index-caps-setup
// Now that the job is created, we should have a rollup index with metadata.
// We can test out the caps API now.
//tag::x-pack-rollup-get-rollup-index-caps-request
//end::x-pack-rollup-get-rollup-index-caps-request
//tag::x-pack-rollup-get-rollup-index-caps-execute
//end::x-pack-rollup-get-rollup-index-caps-execute
//tag::x-pack-rollup-get-rollup-index-caps-response
// indexName will be "rollup", the target index we requested
// Each index pattern can have multiple jobs that rolled it up, so `getJobCaps()`
// returns a list of jobs that rolled up the pattern
// jobID is the identifier we used when we created the job (e.g. `job1`)
// rollupIndex is the location that the job stored it's rollup docs (e.g. `rollup`)
// Finally, fieldCaps are the capabilities of individual fields in the config
// The key is the field name, and the value is a RollupFieldCaps object which
// provides more info.
// If we retrieve the "timestamp" field, it returns a list of maps.  Each list
// item represents a different aggregation that can be run against the "timestamp"
// field, and any additional details specific to that agg (interval, etc)
// In contrast to the timestamp field, the temperature field has multiple aggs configured
//end::x-pack-rollup-get-rollup-index-caps-response
// tag::x-pack-rollup-get-rollup-index-caps-execute-listener
// <1>
// <2>
// end::x-pack-rollup-get-rollup-index-caps-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::x-pack-rollup-get-rollup-index-caps-execute-async
// <1>
// end::x-pack-rollup-get-rollup-index-caps-execute-async
// tag::rollup-delete-job-request
// <1>
// end::rollup-delete-job-request
// tag::rollup-delete-job-execute
// end::rollup-delete-job-execute
// tag::rollup-delete-job-response
// <1>
// end::rollup-delete-job-response
// Swallow any exception, this test does not test actually cancelling.
// tag::rollup-delete-job-execute-listener
// <1>
// <2>
// end::rollup-delete-job-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::rollup-delete-job-execute-async
// <1>
// end::rollup-delete-job-execute-async
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// tag::search-request-basic
// <1>
// <2>
// <3>
// <4>
// end::search-request-basic
// tag::search-request-indices
// <1>
// end::search-request-indices
// tag::search-request-routing
// <1>
// end::search-request-routing
// tag::search-request-indicesOptions
// <1>
// end::search-request-indicesOptions
// tag::search-request-preference
// <1>
// end::search-request-preference
// tag::search-source-basics
// <1>
// <2>
// <3>
// <4>
// <5>
// end::search-source-basics
// tag::search-source-sorting
// <1>
// <2>
// end::search-source-sorting
// tag::search-source-filtering-off
// end::search-source-filtering-off
// tag::search-source-filtering-includes
// end::search-source-filtering-includes
// tag::search-source-setter
// end::search-source-setter
// tag::search-execute
// end::search-execute
// tag::search-execute-listener
// <1>
// <2>
// end::search-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::search-execute-async
// <1>
// end::search-execute-async
// tag::search-response-1
// end::search-response-1
// tag::search-response-2
// failures should be handled here
// end::search-response-2
// tag::search-hits-get
// end::search-hits-get
// tag::search-hits-info
// the total number of hits, must be interpreted in the context of totalHits.relation
// whether the number of hits is accurate (EQUAL_TO) or a lower bound of the total (GREATER_THAN_OR_EQUAL_TO)
// end::search-hits-info
// tag::search-hits-singleHit
// do something with the SearchHit
// end::search-hits-singleHit
// tag::search-hits-singleHit-properties
// end::search-hits-singleHit-properties
// tag::search-hits-singleHit-source
// end::search-hits-singleHit-source
// tag::search-query-builder-ctor
// <1>
// end::search-query-builder-ctor
// tag::search-query-builder-options
// <1>
// <2>
// <3>
// end::search-query-builder-options
// tag::search-query-builders
// end::search-query-builders
// tag::search-query-setter
// end::search-query-setter
// tag::search-request-aggregations
// end::search-request-aggregations
// tag::search-request-aggregations-get
// <1>
// <2>
// <3>
// end::search-request-aggregations-get
// tag::search-request-aggregations-get-wrongCast
// <1>
// end::search-request-aggregations-get-wrongCast
// tag::search-request-aggregations-asMap
// end::search-request-aggregations-asMap
// tag::search-request-aggregations-asList
// end::search-request-aggregations-asList
// tag::search-request-aggregations-iterator
// end::search-request-aggregations-iterator
// tag::search-request-suggestion
// <1>
// <2>
// end::search-request-suggestion
// tag::search-request-suggestion-get
// <1>
// <2>
// <3>
// <4>
// end::search-request-suggestion-get
// tag::search-request-highlighting
// <1>
// <2>
// <3>
// <4>
// end::search-request-highlighting
// tag::search-request-highlighting-get
// <1>
// <2>
// end::search-request-highlighting-get
// tag::search-request-profiling
// end::search-request-profiling
// tag::search-request-profiling-get
// <1>
// <2>
// <3>
// <4>
// end::search-request-profiling-get
// tag::search-request-profiling-queries
// <1>
// <2>
// end::search-request-profiling-queries
// tag::search-request-profiling-queries-results
// <1>
// <2>
// <3>
// <4>
// end::search-request-profiling-queries-results
// tag::search-request-profiling-queries-collectors
// <1>
// <2>
// <3>
// <4>
// end::search-request-profiling-queries-collectors
// tag::search-request-profiling-aggs
// <1>
// <2>
// <3>
// <4>
// <5>
// end::search-request-profiling-aggs
// tag::search-scroll-init
// <1>
// <2>
// <3>
// <4>
// end::search-scroll-init
// tag::search-scroll2
// <1>
// <2>
// <3>
// end::search-scroll2
// tag::scroll-request-arguments
// <1>
// <2>
// end::scroll-request-arguments
// tag::search-scroll-execute-sync
// end::search-scroll-execute-sync
// tag::search-scroll-execute-listener
// <1>
// <2>
// end::search-scroll-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::search-scroll-execute-async
// <1>
// end::search-scroll-execute-async
// tag::clear-scroll-request
// <1>
// <2>
// end::clear-scroll-request
// tag::clear-scroll-add-scroll-id
// end::clear-scroll-add-scroll-id
// tag::clear-scroll-add-scroll-ids
// end::clear-scroll-add-scroll-ids
// tag::clear-scroll-execute
// end::clear-scroll-execute
// tag::clear-scroll-response
// <1>
// <2>
// end::clear-scroll-response
// tag::clear-scroll-execute-listener
// <1>
// <2>
// end::clear-scroll-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::clear-scroll-execute-async
// <1>
// end::clear-scroll-execute-async
// tag::search-scroll-example
// <1>
// <2>
// <3>
// <4>
// <5>
// end::search-scroll-example
// tag::search-template-request-inline
// <1>
// <2>
// <3>
// end::search-template-request-inline
// tag::search-template-response
// end::search-template-response
// tag::render-search-template-request
// <1>
// end::render-search-template-request
// tag::render-search-template-response
// <1>
// end::render-search-template-response
// tag::search-template-request-stored
// end::search-template-request-stored
// tag::search-template-request-options
// end::search-template-request-options
// tag::search-template-execute
// end::search-template-execute
// tag::search-template-execute-listener
// <1>
// <2>
// end::search-template-execute-listener
// Replace the empty listener by a blocking listener for tests.
// tag::search-template-execute-async
// <1>
// end::search-template-execute-async
// tag::multi-search-template-request-inline
// <1>
// <2>
// <3>
// end::multi-search-template-request-inline
// tag::multi-search-template-request-sync
// end::multi-search-template-request-sync
// tag::multi-search-template-response
// <1>
// <2>
// <3>
// end::multi-search-template-response
// tag::multi-search-template-request-stored
// end::multi-search-template-request-stored
// tag::multi-search-template-execute
// end::multi-search-template-execute
// tag::multi-search-template-execute-listener
// <1>
// <2>
// end::multi-search-template-execute-listener
// Replace the empty listener by a blocking listener for tests.
// tag::multi-search-template-execute-async
// end::multi-search-template-execute-async
// tag::register-script
// end::register-script
// tag::explain-request
// end::explain-request
// tag::explain-request-routing
// <1>
// end::explain-request-routing
// tag::explain-request-preference
// <1>
// end::explain-request-preference
// tag::explain-request-source
// <1>
// end::explain-request-source
// tag::explain-request-stored-field
// <1>
// end::explain-request-stored-field
// tag::explain-execute
// end::explain-execute
// tag::explain-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// end::explain-response
// tag::get-result
// <1>
// <2>
// end::get-result
// tag::explain-execute-listener
// <1>
// <2>
// end::explain-execute-listener
// tag::explain-execute-async
// <1>
// end::explain-execute-async
// tag::field-caps-request
// end::field-caps-request
// tag::field-caps-request-indicesOptions
// <1>
// end::field-caps-request-indicesOptions
// tag::field-caps-execute
// end::field-caps-execute
// tag::field-caps-response
// <1>
// <2>
// <3>
//<4>
// end::field-caps-response
// tag::field-caps-execute-listener
// <1>
// <2>
// end::field-caps-execute-listener
// Replace the empty listener by a blocking listener for tests.
// tag::field-caps-execute-async
// <1>
// end::field-caps-execute-async
// tag::rank-eval-request-basic
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// end::rank-eval-request-basic
// tag::rank-eval-execute
// end::rank-eval-execute
// tag::rank-eval-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// <7>
// end::rank-eval-response
// tag::rank-eval-execute-listener
// <1>
// <2>
// end::rank-eval-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::rank-eval-execute-async
// <1>
// end::rank-eval-execute-async
// tag::multi-search-request-basic
// <1>
// <2>
// <3>
// <4>
// end::multi-search-request-basic
// tag::multi-search-execute
// end::multi-search-execute
// tag::multi-search-response
// <1>
// <2>
// <3>
// <4>
// end::multi-search-response
// tag::multi-search-execute-listener
// <1>
// <2>
// end::multi-search-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::multi-search-execute-async
// <1>
// end::multi-search-execute-async
// tag::count-request-basic
// <1>
// <2>
// <3>
// <4>
// end::count-request-basic
// tag::count-request-args
// <1>
// <2>
// <3>
// <4>
// end::count-request-args
// tag::count-source-basics
// <1>
// <2>
// end::count-source-basics
// tag::count-source-setter
// end::count-source-setter
// tag::count-execute
// end::count-execute
// tag::count-execute-listener
// <1>
// <2>
// end::count-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::count-execute-async
// <1>
// end::count-execute-async
// tag::count-response-1
// end::count-response-1
// tag::count-response-2
// failures should be handled here
// end::count-response-2
/*
//www.apache.org/licenses/LICENSE-2.0
//tag::get-users-request
//end::get-users-request
//tag::get-users-execute
//end::get-users-execute
//tag::get-users-response
//end::get-users-response
//tag::get-users-list-request
//end::get-users-list-request
//tag::get-users-all-request
//end::get-users-all-request
// 9 users are expected to be returned
// test_users (3): user1, user2, user3
// system_users (6): elastic, beats_system, apm_system, logstash_system, kibana, remote_monitoring_user
//tag::get-users-execute-listener
// <1>
// <2>
//end::get-users-execute-listener
// Replace the empty listener by a blocking listener in test
//tag::get-users-execute-async
// <1>
//end::get-users-execute-async
//tag::put-user-password-request
//end::put-user-password-request
//tag::put-user-execute
//end::put-user-execute
//tag::put-user-response
// <1>
//end::put-user-response
// no need for secure random in a test; it could block and would not be reproducible anyway
//tag::put-user-hash-request
//end::put-user-hash-request
// This is expected to fail as the server will not be using PBKDF2, but that's easiest hasher to support
// in a standard JVM without introducing additional libraries.
//tag::put-user-update-request
//end::put-user-update-request
// tag::put-user-execute-listener
// <1>
// <2>
// end::put-user-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-user-execute-async
// <1>
// end::put-user-execute-async
// tag::delete-user-request
// <1>
// end::delete-user-request
// tag::delete-user-execute
// end::delete-user-execute
// tag::delete-user-response
// <1>
// end::delete-user-response
// check if deleting the already deleted user again will give us a different response
//tag::delete-user-execute-listener
// <1>
// <2>
//end::delete-user-execute-listener
// Replace the empty listener by a blocking listener in test
//tag::delete-user-execute-async
// <1>
//end::delete-user-execute-async
// tag::put-role-mapping-execute
// end::put-role-mapping-execute
// tag::put-role-mapping-response
// <1>
// end::put-role-mapping-response
// tag::put-role-mapping-execute-listener
// <1>
// <2>
// end::put-role-mapping-execute-listener
// avoid unused local warning
// Replace the empty listener by a blocking listener in test
// tag::put-role-mapping-execute-async
// <1>
// end::put-role-mapping-execute-async
// tag::get-role-mappings-execute
// end::get-role-mappings-execute
// tag::get-role-mappings-response
// end::get-role-mappings-response
// tag::get-role-mappings-list-execute
// end::get-role-mappings-list-execute
// tag::get-role-mappings-all-execute
// end::get-role-mappings-all-execute
// tag::get-role-mappings-execute-listener
// <1>
// <2>
// end::get-role-mappings-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-role-mappings-execute-async
// <1>
// end::get-role-mappings-execute-async
//tag::enable-user-execute
//end::enable-user-execute
//tag::enable-user-execute-listener
// <1>
// <2>
//end::enable-user-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::enable-user-execute-async
// <1>
// end::enable-user-execute-async
//tag::disable-user-execute
//end::disable-user-execute
//tag::disable-user-execute-listener
// <1>
// <2>
//end::disable-user-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::disable-user-execute-async
// <1>
// end::disable-user-execute-async
//tag::get-roles-request
//end::get-roles-request
//tag::get-roles-execute
//end::get-roles-execute
//tag::get-roles-response
//end::get-roles-response
//tag::get-roles-list-request
//end::get-roles-list-request
//tag::get-roles-all-request
//end::get-roles-all-request
// 29 system roles plus the three we created
//tag::get-roles-execute-listener
// <1>
// <2>
//end::get-roles-execute-listener
// Replace the empty listener by a blocking listener in test
//tag::get-roles-execute-async
// <1>
//end::get-roles-execute-async
//tag::authenticate-execute
//end::authenticate-execute
//tag::authenticate-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
//end::authenticate-response
// tag::authenticate-execute-listener
// <1>
// <2>
// end::authenticate-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::authenticate-execute-async
// <1>
// end::authenticate-execute-async
//tag::has-privileges-request
//end::has-privileges-request
//tag::has-privileges-execute
//end::has-privileges-execute
//tag::has-privileges-response
// <1>
// <2>
// <3>
//end::has-privileges-response
// tag::has-privileges-execute-listener
// <1>
// <2>
// end::has-privileges-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::has-privileges-execute-async
// <1>
// end::has-privileges-execute-async
//tag::get-user-privileges-execute
//end::get-user-privileges-execute
//tag::get-user-privileges-response
//end::get-user-privileges-response
//tag::get-user-privileges-execute-listener
// <1>
// <2>
//end::get-user-privileges-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-user-privileges-execute-async
// <1>
// end::get-user-privileges-execute-async
//tag::clear-realm-cache-request
//end::clear-realm-cache-request
//tag::clear-realm-cache-execute
//end::clear-realm-cache-execute
//tag::clear-realm-cache-response
// <1>
//end::clear-realm-cache-response
//tag::clear-realm-cache-execute-listener
// <1>
// <2>
//end::clear-realm-cache-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::clear-realm-cache-execute-async
// <1>
// end::clear-realm-cache-execute-async
//tag::clear-roles-cache-request
//end::clear-roles-cache-request
//tag::clear-roles-cache-execute
//end::clear-roles-cache-execute
//tag::clear-roles-cache-response
// <1>
//end::clear-roles-cache-response
//tag::clear-roles-cache-execute-listener
// <1>
// <2>
//end::clear-roles-cache-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::clear-roles-cache-execute-async
// <1>
// end::clear-roles-cache-execute-async
//tag::get-certificates-execute
//end::get-certificates-execute
//tag::get-certificates-response
// <1>
//end::get-certificates-response
// tag::get-certificates-execute-listener
// <1>
// <2>
// end::get-certificates-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-certificates-execute-async
// <1>
// end::get-certificates-execute-async
//tag::change-password-execute
//end::change-password-execute
//tag::change-password-execute-listener
// <1>
// <2>
//end::change-password-execute-listener
// Replace the empty listener by a blocking listener in test
//tag::change-password-execute-async
// <1>
//end::change-password-execute-async
// Create role mappings
// tag::delete-role-mapping-execute
// end::delete-role-mapping-execute
// tag::delete-role-mapping-response
// <1>
// end::delete-role-mapping-response
// tag::delete-role-mapping-execute-listener
// <1>
// <2>
// end::delete-role-mapping-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-role-mapping-execute-async
// <1>
// end::delete-role-mapping-execute-async
// tag::delete-role-request
// <1>
// end::delete-role-request
// tag::delete-role-execute
// end::delete-role-execute
// tag::delete-role-response
// <1>
// end::delete-role-response
// check if deleting the already deleted role again will give us a different response
//tag::delete-role-execute-listener
// <1>
// <2>
//end::delete-role-execute-listener
// Replace the empty listener by a blocking listener in test
//tag::delete-role-execute-async
// <1>
//end::delete-role-execute-async
// tag::put-role-request
// end::put-role-request
// tag::put-role-execute
// end::put-role-execute
// tag::put-role-response
// <1>
// end::put-role-response
// tag::put-role-execute-listener
// <1>
// <2>
// end::put-role-execute-listener
// Avoid unused variable warning
// Replace the empty listener by a blocking listener in test
// tag::put-role-execute-async
// <1>
// end::put-role-execute-async
// false because it has already been created by the sync variant
// Setup user
// tag::create-token-password-request
// end::create-token-password-request
// tag::create-token-execute
// end::create-token-execute
// tag::create-token-response
// <1>
// <2>
// end::create-token-response
// tag::create-token-refresh-request
// end::create-token-refresh-request
// tag::create-token-client-credentials-request
// end::create-token-client-credentials-request
//tag::create-token-execute-listener
// <1>
// <2>
//end::create-token-execute-listener
// Avoid unused variable warning
// Replace the empty listener by a blocking listener in test
//tag::create-token-execute-async
// <1>
//end::create-token-execute-async
// "client-credentials" grants aren't refreshable
// Setup users
// Create tokens
// tag::invalidate-access-token-request
// end::invalidate-access-token-request
// tag::invalidate-token-execute
// end::invalidate-token-execute
// tag::invalidate-token-response
// end::invalidate-token-response
// tag::invalidate-refresh-token-request
// end::invalidate-refresh-token-request
// tag::invalidate-user-tokens-request
// end::invalidate-user-tokens-request
// We have one refresh and one access token for that user
// tag::invalidate-user-realm-tokens-request
// end::invalidate-user-realm-tokens-request
// We have one refresh and one access token for that user in this realm
// tag::invalidate-realm-tokens-request
// end::invalidate-realm-tokens-request
//tag::invalidate-token-execute-listener
// <1>
// <2>
//end::invalidate-token-execute-listener
// Avoid unused variable warning
// Replace the empty listener by a blocking listener in test
//tag::invalidate-token-execute-async
// <1>
//end::invalidate-token-execute-async
//We still have 4 tokens ( 2 access_tokens and 2 refresh_tokens ) for the default_native realm
//tag::get-builtin-privileges-execute
//end::get-builtin-privileges-execute
//tag::get-builtin-privileges-response
//end::get-builtin-privileges-response
// tag::get-builtin-privileges-execute-listener
// <1>
// <2>
// end::get-builtin-privileges-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-builtin-privileges-execute-async
// <1>
// end::get-builtin-privileges-execute-async
/*"), null);
/*"), metadata);
/*", "manage:*"), null);
/*"), metadata2);
/*"), null);
/*", "manage:*"), null);
//tag::get-privileges-request
//end::get-privileges-request
//tag::get-privileges-execute
//end::get-privileges-execute
//tag::get-all-application-privileges-request
//end::get-all-application-privileges-request
//tag::get-privileges-response
//end::get-privileges-response
/*"));
/*"));
/*", "manage:*"));
//tag::get-all-privileges-request
//end::get-all-privileges-request
//tag::get-privileges-execute-listener
// <1>
// <2>
//end::get-privileges-execute-listener
// Avoid unused variable warning
// Replace the empty listener by a blocking listener in test
//tag::get-privileges-execute-async
// <1>
//end::get-privileges-execute-async
// tag::put-privileges-request
// end::put-privileges-request
// tag::put-privileges-execute
// end::put-privileges-execute
// tag::put-privileges-response
// <1>
// end::put-privileges-response
// tag::put-privileges-execute-listener
// <1>
// <2>
// end::put-privileges-execute-listener
// Avoid unused variable warning
// Replace the empty listener by a blocking listener in test
//tag::put-privileges-execute-async
// <1>
//end::put-privileges-execute-async
/*")
/*")
/*")
// tag::delete-privileges-request
// <1>
// <2>
// end::delete-privileges-request
// tag::delete-privileges-execute
// end::delete-privileges-execute
// tag::delete-privileges-response
// <1>
// <2>
// end::delete-privileges-response
// check if deleting the already deleted privileges again will give us a different response
//tag::delete-privileges-execute-listener
// <1>
// <2>
//end::delete-privileges-execute-listener
// Replace the empty listener by a blocking listener in test
//tag::delete-privileges-execute-async
// <1>
//end::delete-privileges-execute-async
// tag::create-api-key-request
// end::create-api-key-request
// tag::create-api-key-execute
// end::create-api-key-execute
// tag::create-api-key-response
// <1>
// <2>
// end::create-api-key-response
// tag::create-api-key-execute-listener
// <1>
// <2>
// end::create-api-key-execute-listener
// Avoid unused variable warning
// Replace the empty listener by a blocking listener in test
// tag::create-api-key-execute-async
// <1>
// end::create-api-key-execute-async
// Create API Keys
// tag::get-api-key-id-request
// end::get-api-key-id-request
// tag::get-api-key-execute
// end::get-api-key-execute
// tag::get-api-key-name-request
// end::get-api-key-name-request
// tag::get-realm-api-keys-request
// end::get-realm-api-keys-request
// tag::get-user-api-keys-request
// end::get-user-api-keys-request
// tag::get-api-keys-owned-by-authenticated-user-request
// end::get-api-keys-owned-by-authenticated-user-request
// tag::get-all-api-keys-request
// end::get-all-api-keys-request
// tag::get-user-realm-api-keys-request
// end::get-user-realm-api-keys-request
// tag::get-api-key-response
// end::get-api-key-response
// tag::get-api-key-execute-listener
// <1>
// <2>
// end::get-api-key-execute-listener
// Avoid unused variable warning
// Replace the empty listener by a blocking listener in test
// tag::get-api-key-execute-async
// <1>
// end::get-api-key-execute-async
// Create API Keys
// tag::invalidate-api-key-id-request
// end::invalidate-api-key-id-request
// tag::invalidate-api-key-execute
// end::invalidate-api-key-execute
// tag::invalidate-api-key-name-request
// end::invalidate-api-key-name-request
// tag::invalidate-realm-api-keys-request
// end::invalidate-realm-api-keys-request
// tag::invalidate-user-api-keys-request
// end::invalidate-user-api-keys-request
// tag::invalidate-user-realm-api-keys-request
// end::invalidate-user-realm-api-keys-request
// tag::invalidate-api-key-response
// end::invalidate-api-key-response
// tag::invalidate-api-key-execute-listener
// <1>
// <2>
// end::invalidate-api-key-execute-listener
// Avoid unused variable warning
// Replace the empty listener by a blocking listener in test
// tag::invalidate-api-key-execute-async
// <1>
// end::invalidate-api-key-execute-async
// tag::invalidate-api-keys-owned-by-authenticated-user-request
// end::invalidate-api-keys-owned-by-authenticated-user-request
//tag::delegate-pki-request
//end::delegate-pki-request
//tag::delegate-pki-execute
//end::delegate-pki-execute
//tag::delegate-pki-response
// <1>
//end::delegate-pki-response
//tag::delegate-pki-execute-listener
// <1>
// <2>
//end::delegate-pki-execute-listener
// Replace the empty listener by a blocking listener in test
//tag::delegate-pki-execute-async
// <1>
//end::delegate-pki-execute-async
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// tag::example
// end::example
// tag::create-repository-request
// end::create-repository-request
// tag::create-repository-create-settings
// <1>
// end::create-repository-create-settings
// tag::create-repository-request-repository-settings
// <1>
// end::create-repository-request-repository-settings
// tag::create-repository-settings-builder
// <1>
// end::create-repository-settings-builder
// tag::create-repository-settings-map
// <1>
// end::create-repository-settings-map
// tag::create-repository-settings-source
// <1>
// end::create-repository-settings-source
// tag::create-repository-request-name
// <1>
// end::create-repository-request-name
// tag::create-repository-request-type
// <1>
// end::create-repository-request-type
// tag::create-repository-request-masterTimeout
// <1>
// <2>
// end::create-repository-request-masterTimeout
// tag::create-repository-request-timeout
// <1>
// <2>
// end::create-repository-request-timeout
// tag::create-repository-request-verify
// <1>
// end::create-repository-request-verify
// tag::create-repository-execute
// end::create-repository-execute
// tag::create-repository-response
// <1>
// end::create-repository-response
// tag::create-repository-execute-listener
// <1>
// <2>
// end::create-repository-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::create-repository-execute-async
// <1>
// end::create-repository-execute-async
// tag::get-repository-request
// end::get-repository-request
// tag::get-repository-request-repositories
// <1>
// end::get-repository-request-repositories
// tag::get-repository-request-local
// <1>
// end::get-repository-request-local
// tag::get-repository-request-masterTimeout
// <1>
// <2>
// end::get-repository-request-masterTimeout
// tag::get-repository-execute
// end::get-repository-execute
// tag::get-repository-response
// end::get-repository-response
// tag::get-repository-execute-listener
// <1>
// <2>
// end::get-repository-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-repository-execute-async
// <1>
// end::get-repository-execute-async
// tag::restore-snapshot-request
// end::restore-snapshot-request
// we need to restore as a different index name
// tag::restore-snapshot-request-masterTimeout
// <1>
// <2>
// end::restore-snapshot-request-masterTimeout
// tag::restore-snapshot-request-waitForCompletion
// <1>
// end::restore-snapshot-request-waitForCompletion
// tag::restore-snapshot-request-partial
// <1>
// end::restore-snapshot-request-partial
// tag::restore-snapshot-request-include-global-state
// <1>
// end::restore-snapshot-request-include-global-state
// tag::restore-snapshot-request-include-aliases
// <1>
// end::restore-snapshot-request-include-aliases
// tag::restore-snapshot-request-indices
// <1>
// end::restore-snapshot-request-indices
// tag::restore-snapshot-request-rename
// <1>
// <2>
// end::restore-snapshot-request-rename
// tag::restore-snapshot-request-index-settings
// <1>
// <2>
// <3>
// end::restore-snapshot-request-index-settings
// tag::restore-snapshot-execute
// end::restore-snapshot-execute
// tag::restore-snapshot-response
// <1>
// end::restore-snapshot-response
// tag::restore-snapshot-execute-listener
// <1>
// <2>
// end::restore-snapshot-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::restore-snapshot-execute-async
// <1>
// end::restore-snapshot-execute-async
// tag::delete-repository-request
// end::delete-repository-request
// tag::delete-repository-request-masterTimeout
// <1>
// <2>
// end::delete-repository-request-masterTimeout
// tag::delete-repository-request-timeout
// <1>
// <2>
// end::delete-repository-request-timeout
// tag::delete-repository-execute
// end::delete-repository-execute
// tag::delete-repository-response
// <1>
// end::delete-repository-response
// tag::delete-repository-execute-listener
// <1>
// <2>
// end::delete-repository-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-repository-execute-async
// <1>
// end::delete-repository-execute-async
// tag::verify-repository-request
// end::verify-repository-request
// tag::verify-repository-request-masterTimeout
// <1>
// <2>
// end::verify-repository-request-masterTimeout
// tag::verify-repository-request-timeout
// <1>
// <2>
// end::verify-repository-request-timeout
// tag::verify-repository-execute
// end::verify-repository-execute
// tag::verify-repository-response
// end::verify-repository-response
// tag::verify-repository-execute-listener
// <1>
// <2>
// end::verify-repository-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::verify-repository-execute-async
// <1>
// end::verify-repository-execute-async
// tag::create-snapshot-request
// end::create-snapshot-request
// tag::create-snapshot-request-repositoryName
// <1>
// end::create-snapshot-request-repositoryName
// tag::create-snapshot-request-snapshotName
// <1>
// end::create-snapshot-request-snapshotName
// tag::create-snapshot-request-indices
// <1>
// end::create-snapshot-request-indices
// tag::create-snapshot-request-indicesOptions
// <1>
// end::create-snapshot-request-indicesOptions
// tag::create-snapshot-request-partial
// <1>
// end::create-snapshot-request-partial
// tag::create-snapshot-request-includeGlobalState
// <1>
// end::create-snapshot-request-includeGlobalState
// tag::create-snapshot-request-masterTimeout
// <1>
// <2>
// end::create-snapshot-request-masterTimeout
// tag::create-snapshot-request-waitForCompletion
// <1>
// end::create-snapshot-request-waitForCompletion
// tag::create-snapshot-execute
// end::create-snapshot-execute
// tag::create-snapshot-response
// <1>
// end::create-snapshot-response
// tag::create-snapshot-response-snapshot-info
// <1>
// end::create-snapshot-response-snapshot-info
// tag::create-snapshot-execute-listener
// <1>
// <2>
// end::create-snapshot-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::create-snapshot-execute-async
// <1>
// end::create-snapshot-execute-async
// tag::get-snapshots-request
// end::get-snapshots-request
// tag::get-snapshots-request-repositoryName
// <1>
// end::get-snapshots-request-repositoryName
// tag::get-snapshots-request-snapshots
// <1>
// end::get-snapshots-request-snapshots
// tag::get-snapshots-request-masterTimeout
// <1>
// <2>
// end::get-snapshots-request-masterTimeout
// tag::get-snapshots-request-verbose
// <1>
// end::get-snapshots-request-verbose
// tag::get-snapshots-request-ignore-unavailable
// <1>
// end::get-snapshots-request-ignore-unavailable
// tag::get-snapshots-execute
// end::get-snapshots-execute
// tag::get-snapshots-response
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// end::get-snapshots-response
// tag::get-snapshots-execute-listener
// <1>
// <2>
// end::get-snapshots-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-snapshots-execute-async
// <1>
// end::get-snapshots-execute-async
// tag::snapshots-status-request
// end::snapshots-status-request
// tag::snapshots-status-request-repository
// <1>
// end::snapshots-status-request-repository
// tag::snapshots-status-request-snapshots
// <1>
// end::snapshots-status-request-snapshots
// tag::snapshots-status-request-ignoreUnavailable
// <1>
// end::snapshots-status-request-ignoreUnavailable
// tag::snapshots-status-request-masterTimeout
// <1>
// <2>
// end::snapshots-status-request-masterTimeout
// tag::snapshots-status-execute
// end::snapshots-status-execute
// tag::snapshots-status-response
// <1>
// <2>
// <3>
// end::snapshots-status-response
// tag::snapshots-status-execute-listener
// <1>
// <2>
// end::snapshots-status-execute-listener
// Replace the empty listener with a blocking listener in test
// tag::snapshots-status-execute-async
// <1>
// end::snapshots-status-execute-async
// tag::delete-snapshot-request
// end::delete-snapshot-request
// tag::delete-snapshot-request-masterTimeout
// <1>
// <2>
// end::delete-snapshot-request-masterTimeout
// tag::delete-snapshot-execute
// end::delete-snapshot-execute
// tag::delete-snapshot-response
// <1>
// end::delete-snapshot-response
// tag::delete-snapshot-execute-listener
// <1>
// <2>
// end::delete-snapshot-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-snapshot-execute-async
// <1>
// end::delete-snapshot-execute-async
// check that the request went ok without parsing JSON here. When using the high level client, check acknowledgement instead.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// tag::example
// end::example
// tag::get-stored-script-request
// <1>
// end::get-stored-script-request
// tag::get-stored-script-request-masterTimeout
// <1>
// <2>
// end::get-stored-script-request-masterTimeout
// tag::get-stored-script-execute
// end::get-stored-script-execute
// tag::get-stored-script-response
// <1>
// <2>
// <3>
// <4>
// end::get-stored-script-response
// tag::get-stored-script-execute-listener
// <1>
// <2>
// end::get-stored-script-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-stored-script-execute-async
// <1>
// end::get-stored-script-execute-async
// tag::delete-stored-script-request
// <1>
// end::delete-stored-script-request
// tag::delete-stored-script-request-masterTimeout
// <1>
// <2>
// end::delete-stored-script-request-masterTimeout
// tag::delete-stored-script-request-timeout
// <1>
// <2>
// end::delete-stored-script-request-timeout
// tag::delete-stored-script-execute
// end::delete-stored-script-execute
// tag::delete-stored-script-response
// <1>
// end::delete-stored-script-response
// tag::delete-stored-script-execute-listener
// <1>
// <2>
// end::delete-stored-script-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-stored-script-execute-async
// <1>
// end::delete-stored-script-execute-async
// tag::put-stored-script-request
// <1>
// <2>
// end::put-stored-script-request
// tag::put-stored-script-context
// <1>
// end::put-stored-script-context
// tag::put-stored-script-timeout
// <1>
// <2>
// end::put-stored-script-timeout
// tag::put-stored-script-masterTimeout
// <1>
// <2>
// end::put-stored-script-masterTimeout
// tag::put-stored-script-content-painless
// <1>
// end::put-stored-script-content-painless
// tag::put-stored-script-execute
// end::put-stored-script-execute
// tag::put-stored-script-response
// <1>
// end::put-stored-script-response
// tag::put-stored-script-execute-listener
// <1>
// <2>
// end::put-stored-script-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-stored-script-execute-async
// <1>
// end::put-stored-script-execute-async
// tag::put-stored-script-content-mustache
// <1>
// end::put-stored-script-content-mustache
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// tag::example
// end::example
// tag::list-tasks-request
// end::list-tasks-request
// tag::list-tasks-request-filter
// <1>
// <2>
// <3>
// end::list-tasks-request-filter
// tag::list-tasks-request-detailed
// <1>
// end::list-tasks-request-detailed
// tag::list-tasks-request-wait-completion
// <1>
// <2>
// <3>
// end::list-tasks-request-wait-completion
// tag::list-tasks-execute
// end::list-tasks-execute
// tag::list-tasks-response-tasks
// <1>
// end::list-tasks-response-tasks
// tag::list-tasks-response-calc
// <1>
// <2>
// end::list-tasks-response-calc
// tag::list-tasks-response-failures
// <1>
// <2>
// end::list-tasks-response-failures
// tag::list-tasks-execute-listener
// <1>
// <2>
// end::list-tasks-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::list-tasks-execute-async
// <1>
// end::list-tasks-execute-async
// tag::cancel-tasks-request
// end::cancel-tasks-request
// tag::cancel-tasks-request-filter
// <1>
// <2>
// <3>
// end::cancel-tasks-request-filter
// tag::cancel-tasks-execute
// end::cancel-tasks-execute
// tag::cancel-tasks-response-tasks
// <1>
// end::cancel-tasks-response-tasks
// tag::cancel-tasks-response-calc
// <1>
// <2>
// end::cancel-tasks-response-calc
// tag::cancel-tasks-response-failures
// <1>
// <2>
// end::cancel-tasks-response-failures
// tag::cancel-tasks-execute-listener
// <1>
// <2>
// end::cancel-tasks-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::cancel-tasks-execute-async
// <1>
// end::cancel-tasks-execute-async
/*
//www.apache.org/licenses/LICENSE-2.0
// tag::put-transform-query-config
// end::put-transform-query-config
// tag::put-transform-source-config
// end::put-transform-source-config
// tag::put-transform-dest-config
// end::put-transform-dest-config
// tag::put-transform-group-config
// <1>
// <2>
// end::put-transform-group-config
// tag::put-transform-agg-config
// <1>
// end::put-transform-agg-config
// tag::put-transform-pivot-config
// <1>
// <2>
// <3>
// end::put-transform-pivot-config
// tag::put-transform-config
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// end::put-transform-config
// tag::put-transform-request
// <1>
// <2>
// end::put-transform-request
// tag::put-transform-execute
// end::put-transform-execute
// tag::put-transform-execute-listener
// <1>
// <2>
// end::put-transform-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::put-transform-execute-async
// <1>
// end::put-transform-execute-async
// tag::update-transform-config
// <1>
// <2>
// <3>
// <4>
// <5>
// end::update-transform-config
// tag::update-transform-request
// <1>
// <2>
// <3>
// end::update-transform-request
// tag::update-transform-execute
// end::update-transform-execute
// tag::update-transform-execute-listener
// <1>
// <2>
// end::update-transform-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::update-transform-execute-async
// <1>
// end::update-transform-execute-async
// tag::start-transform-request
// <1>
// end::start-transform-request
// tag::start-transform-request-options
// <1>
// end::start-transform-request-options
// tag::start-transform-execute
// end::start-transform-execute
// tag::stop-transform-request
// <1>
// end::stop-transform-request
// tag::stop-transform-request-options
// <1>
// <2>
// <3>
// end::stop-transform-request-options
// tag::stop-transform-execute
// end::stop-transform-execute
// tag::start-transform-execute-listener
// <1>
// <2>
// end::start-transform-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::start-transform-execute-async
// <1>
// end::start-transform-execute-async
// tag::stop-transform-execute-listener
// <1>
// <2>
// end::stop-transform-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::stop-transform-execute-async
// <1>
// end::stop-transform-execute-async
// tag::delete-transform-request
// <1>
// <2>
// end::delete-transform-request
// tag::delete-transform-execute
// end::delete-transform-execute
// tag::delete-transform-execute-listener
// <1>
// <2>
// end::delete-transform-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::delete-transform-execute-async
// <1>
// end::delete-transform-execute-async
// tag::preview-transform-request
// <1>
// <2>
// <3>
// end::preview-transform-request
// tag::preview-transform-execute
// end::preview-transform-execute
// tag::preview-transform-execute-listener
// <1>
// <2>
// end::preview-transform-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::preview-transform-execute-async
// <1>
// end::preview-transform-execute-async
// tag::get-transform-stats-request
// <1>
// end::get-transform-stats-request
// tag::get-transform-stats-request-options
// <1>
// <2>
// end::get-transform-stats-request-options
// tag::get-transform-stats-execute
// end::get-transform-stats-execute
// tag::get-transform-stats-response
// <1>
// <2>
// <3>
// <4>
// <5>
// end::get-transform-stats-response
// tag::get-transform-stats-execute-listener
// <1>
// <2>
// end::get-transform-stats-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-transform-stats-execute-async
// <1>
// end::get-transform-stats-execute-async
// tag::get-transform-request
// <1>
// end::get-transform-request
// tag::get-transform-request-options
// <1>
// <2>
// end::get-transform-request-options
// tag::get-transform-execute
// end::get-transform-execute
// tag::get-transform-response
// end::get-transform-response
// tag::get-transform-execute-listener
// <1>
// <2>
// end::get-transform-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-transform-execute-async
// <1>
// end::get-transform-execute-async
/*
//www.apache.org/licenses/LICENSE-2.0
//tag::start-watch-service-request
//end::start-watch-service-request
//tag::start-watch-service-execute
//end::start-watch-service-execute
//tag::start-watch-service-response
// <1>
//end::start-watch-service-response
//tag::stop-watch-service-request
//end::stop-watch-service-request
//tag::stop-watch-service-execute
//end::stop-watch-service-execute
//tag::stop-watch-service-response
// <1>
//end::stop-watch-service-response
// tag::start-watch-service-execute-listener
// <1>
// <2>
// end::start-watch-service-execute-listener
// tag::start-watch-service-execute-async
// <1>
// end::start-watch-service-execute-async
// tag::stop-watch-service-execute-listener
// <1>
// <2>
// end::stop-watch-service-execute-listener
// tag::stop-watch-service-execute-async
// <1>
// end::stop-watch-service-execute-async
//tag::x-pack-put-watch-execute
// you can also use the WatchSourceBuilder from org.elasticsearch.plugin:x-pack-core to create a watch programmatically
// <1>
//end::x-pack-put-watch-execute
//tag::x-pack-put-watch-response
// <1>
// <2>
// <3>
//end::x-pack-put-watch-response
// tag::x-pack-put-watch-execute-listener
// <1>
// <2>
// end::x-pack-put-watch-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::x-pack-put-watch-execute-async
// <1>
// end::x-pack-put-watch-execute-async
// tag::x-pack-execute-watch-by-id
// <1>
// <2>
// <3>
// <4>
// <5>
// <6>
// end::x-pack-execute-watch-by-id
// tag::x-pack-execute-watch-by-id-response
// <1>
// <2>
// <3>
// end::x-pack-execute-watch-by-id-response
// tag::x-pack-execute-watch-by-id-execute-listener
// <1>
// <2>
// end::x-pack-execute-watch-by-id-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::x-pack-execute-watch-by-id-execute-async
// <1>
// end::x-pack-execute-watch-by-id-execute-async
//tag::get-watch-request
//end::get-watch-request
//tag::get-watch-execute
//end::get-watch-execute
//tag::get-watch-response
// <1>
// <2>
// <3>
// <4>
// <5>
//end::get-watch-response
// tag::get-watch-execute-listener
// <1>
// <2>
// end::get-watch-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::get-watch-execute-async
// <1>
// end::get-watch-execute-async
//tag::x-pack-delete-watch-execute
//end::x-pack-delete-watch-execute
//tag::x-pack-delete-watch-response
// <1>
// <2>
// <3>
//end::x-pack-delete-watch-response
// tag::x-pack-delete-watch-execute-listener
// <1>
// <2>
// end::x-pack-delete-watch-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::x-pack-delete-watch-execute-async
// <1>
// end::x-pack-delete-watch-execute-async
// tag::x-pack-execute-watch-inline
// <1>
// <2>
// <3>
// <4>
// <5>
// end::x-pack-execute-watch-inline
// tag::x-pack-execute-watch-inline-response
// <1>
// <2>
// <3>
// end::x-pack-execute-watch-inline-response
// tag::x-pack-execute-watch-inline-execute-listener
// <1>
// <2>
// end::x-pack-execute-watch-inline-execute-listener
// Replace the empty listener by a blocking listener in test
// tag::x-pack-execute-watch-inline-execute-async
// <1>
// end::x-pack-execute-watch-inline-execute-async
// TODO: use the high-level REST client here once it supports 'execute watch'.
//tag::ack-watch-request
// <1>
// <2>
//end::ack-watch-request
//tag::ack-watch-execute
//end::ack-watch-execute
//tag::ack-watch-response
// <1>
// <2>
//end::ack-watch-response
// tag::ack-watch-execute-listener
// <1>
// <2>
// end::ack-watch-execute-listener
// For testing, replace the empty listener by a blocking listener.
// tag::ack-watch-execute-async
// <1>
// end::ack-watch-execute-async
//tag::deactivate-watch-execute
//end::deactivate-watch-execute
// tag::deactivate-watch-execute-listener
// <1>
// <2>
// end::deactivate-watch-execute-listener
// For testing, replace the empty listener by a blocking listener.
// tag::deactivate-watch-execute-async
// <1>
// end::deactivate-watch-execute-async
// <1>
//tag::activate-watch-request
//end::activate-watch-request
//tag::activate-watch-response
// <1>
//end::activate-watch-response
//tag::activate-watch-request-listener
// <1>
// <2>
//end::activate-watch-request-listener
//Replace the empty listener by a blocking listener in test
//tag::activate-watch-request-async
// <1>
//end::activate-watch-request-async
//tag::watcher-stats-request
//end::watcher-stats-request
//tag::watcher-stats-execute
//end::watcher-stats-execute
//tag::watcher-stats-response
// <1>
//end::watcher-stats-response
// tag::watcher-stats-execute-listener
// <1>
// <2>
// end::watcher-stats-execute-listener
// tag::watcher-stats-execute-async
// <1>
// end::watcher-stats-execute-async
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//Create random set of vertices
//Wire up half the vertices randomly
// String rep since they are different classes
//Sort the vertices lists before equality test (map insertion sequences can cause order differences)
/*
//www.apache.org/licenses/LICENSE-2.0
// this whole structure expects to be maps of strings, so more complex objects would just mess that up.
// setting it this way allows for new fields at the root
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// phases is a list of Phase parsable entries only
// these are all meant to be maps of strings, so complex objects will confuse the parser
// actions are meant to be a list of LifecycleAction parsable entries only
// field.isEmpty() means do not insert an object at the root of the json. This parser expects
// every root level named object to be parsable as a specific type
/*
//www.apache.org/licenses/LICENSE-2.0
// actions are plucked from the named registry, and it fails if the action is not in the named registry
// This is a bytes reference, so any new fields are tested for equality in this bytes reference.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// phases is a list of Phase parsable entries only
// these are all meant to be maps of strings, so complex objects will confuse the parser
// actions are meant to be a list of LifecycleAction parsable entries only
/*
//www.apache.org/licenses/LICENSE-2.0
// these items all have some specific parsing that does not allow them to have additional objects within them.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// actions are plucked from the named registry, and it fails if the action is not in the named registry
/*
//www.apache.org/licenses/LICENSE-2.0
// actions are plucked from the named registry, and it fails if the action is not in the named registry
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// As Alias#equals only looks at name, we check the equality of the other Alias parameters here.
/*
//www.apache.org/licenses/LICENSE-2.0
// allow random fields at the level of `index` and `index.mappings.field`
// otherwise random field could be evaluated as index name or type name
// if mappings is empty, means that fields are not found
// As the client class GetFieldMappingsResponse doesn't have toXContent method, adding this method here only for the test
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// rarely have no fields
// Not meant to be exhaustive
/*
//www.apache.org/licenses/LICENSE-2.0
// uses parser.map()
// cannot have extra properties
// Check there's no doc types at the root of the mapping
// As the client class GetIndexTemplatesResponse doesn't have toXContent method, adding this method here only for the test
//Create a server-side counterpart for the client-side class and call toXContent on it
/*
//www.apache.org/licenses/LICENSE-2.0
// rarely have no fields
/*
//www.apache.org/licenses/LICENSE-2.0
// some ASCII or Latin-1 control characters, especially newline, can lead to
// problems with yaml parsers, that's why we filter them here (see #30911)
/*
//www.apache.org/licenses/LICENSE-2.0
// if the original `source` is null, the parsed source should be so too
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Create a random server request, and copy its contents into the HLRC request.
// Because client requests only accept typeless mappings, we must swap out the
// mapping definition for one that does not contain types.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// failures are grouped
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// test constructor
// test assignment of conditions
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// the list of index settings cannot be zero, but the other lists can be, so this boolean is used to make the min number
// of elements for this list.
// old school parsing
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// allow unknown fields in root only
/*
//www.apache.org/licenses/LICENSE-2.0
// allow unknown fields in the metrics map (i.e. alongside named metrics like "precision" or "recall")
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Sample is not included in the X-Content representation
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//This is just to create a random object to stand in the place of random data
//Because this is raw a BytesReference, the shuffling done via `AbstractXContentTestCase` is unacceptable and causes equality failures
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// time span is required to be at least 1 millis, so we use a custom method to generate a time value here
/*
//www.apache.org/licenses/LICENSE-2.0
// can only test with a single agg as the xcontent order gets randomized by test base class and then
// the actual xcontent isn't the same and test fail.
// Testing with a single agg is ok as we don't have special list xcontent logic
// Unlike the config version of this test, the metadata parser should tolerate the unknown future field
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// can only test with a single agg as the xcontent order gets randomized by test base class and then
// the actual xcontent isn't the same and test fail.
// Testing with a single agg is ok as we don't have special list xcontent logic
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// allow unknown fields in the root of the object only
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// allow unknown fields in the root of the object only as QueryConfig stores a Map<String, Object>
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// allow unknown fields in the root of the object only
/*
//www.apache.org/licenses/LICENSE-2.0
// allow unknown fields in the root of the object only
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// allow unknown fields in the root of the object only
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// allow unknown fields in the root of the object only
/*
//www.apache.org/licenses/LICENSE-2.0
// allow unknown fields in the root of the object only
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// disallow unknown fields in the root of the object as field names must be parsable as numbers
/*
//www.apache.org/licenses/LICENSE-2.0
// disallow unknown fields in the root of the object as field names must be parsable as numbers
/*
//www.apache.org/licenses/LICENSE-2.0
// disallow unknown fields in the root of the object as field names must be parsable as numbers
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// unknown fields are only guaranteed to be ignored at the top level - below this several data
// structures (e.g. mappings, ingest pipeline, field stats) will preserve arbitrary fields
/*
//www.apache.org/licenses/LICENSE-2.0
// Did we inflate to the same object?
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// at least one of the two should be present
// no need for random condition (it is already tested)
/*
//www.apache.org/licenses/LICENSE-2.0
// no need for random DetectionRule (it is already tested)
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// The parser should tolerate unknown fields
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// base cannot have extra things in it
// the field list expects to be a nested object of a certain type
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// base cannot have extra things in it
// the field list expects to be a nested object of a certain type
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Create RollupFieldCaps for the date histogram
// Create RollupFieldCaps for the histogram
// Create RollupFieldCaps for the term
// Create RollupFieldCaps for the metrics
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// large name so we don't accidentally collide
/*
//www.apache.org/licenses/LICENSE-2.0
//second
//minute
//hour
//day of month
//month
//day of week
//year
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//metadata is a series of kv pairs, so we dont want to add random fields here for test equality
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*\" ]" +
/*\" ]," +
/*\" , \"manage:*\"]" +
/*\" ]," +
/*\" ]" +
/*\" , \"manage:*\"]" +
/*"), null);
/*"), metadata);
/*", "manage:*"), null);
/*"), metadata2);
/*"), null);
/*", "manage:*"), null);
/*"),
/*", "action:login"),
/*", "manage:*"), null));
/*", "manage:*"), null);
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** tests the Response for getting users from the security HLRC */
// This sub object should support unknown fields, but metadata cannot contain complex extra objects or it will fail
// change intelligence
/*
//www.apache.org/licenses/LICENSE-2.0
/*"))
/*\"]" +
/*
//www.apache.org/licenses/LICENSE-2.0
/*", Collections.singletonMap("execute", false))
/*", "execute"), Matchers.is(false));
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Use hash set to force a unique set of resources
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*\",\n"
/*", "action:login"), metadata);
/*", "action:login"), metadata));
/*", "action:login"), metadata));
/*", "action:login")
/*", "action:login"));
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// true really means inserting bogus privileges
// duplicate
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// checking tasks
//checking failures
/**
// it knows the hardcoded address space.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Serialisation of TaskOperationFailure and ElasticsearchException changes
// the object so use a custom compare method rather than Object.equals
// actualException is a wrapped copy of expectedException so the
// error messages won't be the same but actualException should contain
// the error message from expectedException
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Serialisation of TaskOperationFailure and ElasticsearchException changes
// the object so use a custom compare method rather than Object.equals
/*
//www.apache.org/licenses/LICENSE-2.0
// null id and destination is valid
// null source is not valid
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// allow unknown fields in the root of the object only as QueryConfig stores a Map<String, Object>
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// allow unknown fields in the root of the object only
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// ensure that the unlikely does not happen: 2 group_by's share the same name
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// allow unknown fields in the root of the object only
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// = new DateHistogramGroupSource(field);
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// do not mix up the order of nodes, it will cause the tests to fail
// everything in nodes needs to be a particular parseable object
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// need this to prevent some compilation errors, i.e. in 1.8.0_91
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
/*
// ES7 cname/ip:port format
//" + cnameAndURI[1]);
//" + address);
//" + parser.getValueAsString());
//http section is not present if http is not enabled on the node, ignore such nodes
/*
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/*
/**
//sniffOnFailure does nothing until the initial sniffing round has been completed
/*
/*
/*
//tasks are run by a single threaded executor, so swapping is safe with a simple volatile variable
/**
/**
/*
/**
/**
/*
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
//the first call is to schedule the first sniff round from the Sniffer constructor, with delay O
//all of the subsequent times "schedule" is called with delay set to the configured sniff interval
//we submit rather than scheduling to make the test quick and not depend on time
//the executor is closed externally, shutdown is tested separately
//the last future is the only one that may not be completed yet, as the count down happens
//while scheduling the next round which is still part of the execution of the runnable itself.
/**
//run from the same thread so the sniffer gets for sure initialized and the scheduled task gets cancelled on close
/**
//we need to make sure that the sniffer is initialized, so the sniffOnFailure
//call does what it needs to do. Otherwise nothing happens until initialized.
//with tasks executing quickly one after each other, it is very likely that the onFailure round gets skipped
//as another round is already running. We retry till enough runs get through as that's what we want to test.
//the task was either cancelled before starting, in which case it will never start (thanks to Future#cancel),
//or skipped, in which case it will run but do nothing (thanks to Task#skip).
//Here we want to make sure that whenever skip returns true, the task either won't run or it won't do anything,
//otherwise we may end up with parallel sniffing tracks given that each task schedules the following one. We need to
// make sure that onFailure takes scheduling over while at the same time ordinary rounds don't go on.
//if a future is cancelled when its execution has already started, future#get throws CancellationException before
//completion. The execution continues though so we use a latch to try and wait for the task to be completed.
//Here we want to make sure that whenever skip returns false, the task will be completed, otherwise we may be
//missing to schedule the following round, which means no sniffing will ever happen again besides on failure sniffing.
//the future may or may not be cancelled but the task has for sure started and completed
//subsequent cancel calls return false for sure
/**
/**
//check that if communication breaks, sniffer keeps on working
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// tag::example[]
// end::example[]
//tag::sniffer-init
//end::sniffer-init
//tag::sniffer-close
//end::sniffer-close
//tag::sniffer-interval
//end::sniffer-interval
//tag::sniff-on-failure
// <1>
// <2>
// <3>
//end::sniff-on-failure
//tag::sniffer-https
//end::sniffer-https
//tag::sniff-request-timeout
//end::sniff-request-timeout
//tag::custom-nodes-sniffer
// <1>
//end::custom-nodes-sniffer
/*
//www.apache.org/licenses/LICENSE-2.0
// See LUCENE-3995 for rationale.
// 5 sec lingering
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//randomly exercise the code path that supports multiple headers with same key
# Licensed to Elasticsearch under one or more contributor
# license agreements. See the NOTICE file distributed with
# this work for additional information regarding copyright
# ownership. Elasticsearch licenses this file to you under
# the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance  with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on
# an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
# either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
# before 1.4.0, the zip file contains windows scripts, and tar.gz contained *nix scripts
# 1.2.0 was pulled from download.elasticsearch.org because of routing bug:
# for some reason python's tarfile module has trouble with ES tgz?
# Licensed to Elasticsearch under one or more contributor
# license agreements. See the NOTICE file distributed with
# this work for additional information regarding copyright
# ownership. Elasticsearch licenses this file to you under
# the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance  with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on
# an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
# either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
# Prepare a release: Update the documentation and commit
#
# USAGE:
#
# python3 ./dev-tools/prepare_release_update_documentation.py
#
# Note: Ensure the script is run from the root directory
#       This script needs to be run and then pushed,
#       before proceeding with prepare_release_create-release-version.py
#       on your build VM
#
# Make sure no local mods:
# Make sure no untracked files:
# Make sure we have all changes from origin:
# Make sure we no local unpushed changes (this is supposed to be a clean area):
# Reads the given file and applies the
# callback to it. If the callback changed
# a line the given file is replaced with
# the modified input.
#Remove original file
#Move new file
# nothing to do - just remove the tmp file
# Checks the pom.xml for the release version.
# This method fails if the pom file has no SNAPSHOT version set ie.
# if the version is already on a release version we fail.
# Returns the next version string ie. 0.90.7
# Stages the given files for the next git commit
# print("Adding file: %s" % (file))
# Updates documentation feature flags
# Walks the given directory path (defaults to 'docs')
# and replaces all 'coming[$version]' tags with
# 'added[$version]'. This method only accesses asciidoc files.
# expects var args use * to expand
# Licensed to Elasticsearch under one or more contributor
# license agreements. See the NOTICE file distributed with
# this work for additional information regarding copyright
# ownership. Elasticsearch licenses this file to you under
# the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance  with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on
# an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
# either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
# Smoke-tests a release candidate
#
# 1. Downloads the tar.gz, deb, RPM and zip file from the staging URL
# 2. Verifies it's sha1 hashes and GPG signatures against the release key
# 3. Installs all official plugins
# 4. Starts one node for tar.gz and zip packages and checks:
#    -- if it runs with Java 1.8
#    -- if the build hash given is the one that is returned by the status response
#    -- if the build is a release version and not a snapshot version
#    -- if all plugins are loaded
#    -- if the status response returns the correct version
#
# USAGE:
#
# python3 -B ./dev-tools/smoke_test_rc.py --version 2.0.0-beta1 --hash bfa3e47
#
# to also test other plugins try run
#
# python3 -B ./dev-tools/smoke_test_rc.py --version 2.0.0-beta1 --hash bfa3e47 --plugins license,shield,watcher
#
# Note: Ensure the script is run from the elasticsearch top level directory
#
# For testing a release from sonatype try this:
#
# python3 -B dev-tools/smoke_test_rc.py --version 2.0.0-beta1 --hash bfa3e47 --fetch_url https://oss.sonatype.org/content/repositories/releases/
#
# console colors
#that is ok it might not be there yet
# here we create a temp gpg home where we download the release key as the only key into
# when we verify the signature it will fail if the signed key is not in the keystore and that
# way we keep the executing host unmodified since we don't have to import the key into the default keystore
# nothing to do here
# we now get / and /_nodes to fetch basic infos like hashes etc and the installed plugins
# try reading the pid and kill the node
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we have to delete the index otherwise the second indexing request will route to the single shard and not produce a 201
/*
//www.apache.org/licenses/LICENSE-2.0
/** Rest integration test. Runs against a cluster started by {@code gradle integTest} */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// First get the current usage figures
// Do some requests to get some rest usage stats
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Wait for the refresh listener to start waiting
// Close the index. That should flush the listener.
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
//TODO we still have passwords in Strings in headers. Maybe we can look into using a CharSequence?
/*
//www.apache.org/licenses/LICENSE-2.0
// lexicographically compare two lists, treating missing entries as zeros
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// no leniency!
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// jopt simple has issue with multiple non options, so we just get one set of them here
// and convert to File when necessary
// see https://github.com/jopt-simple/jopt-simple/issues/103
/* always use empty passphrase for auto created keystore */);
/* TODO: prompt for password when they are supported */);
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// pkg private so tests can manipulate
/* always use empty passphrase for auto created keystore */);
/* TODO: prompt for password when they are supported */);
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// terminal.readSecret("Enter passphrase (empty for no passphrase): ");
/* TODO: uncomment when entering passwords on startup is supported
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/* TODO: prompt for password when they are supported */);
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/* TODO: prompt for password when they are supported */);
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// explicit no
// explicit no
/*
//www.apache.org/licenses/LICENSE-2.0
// explicit no
// explicit no
// force
// force
// force
/*
//www.apache.org/licenses/LICENSE-2.0
// default is no
// explicit no
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we do our own mocking
// default to posix, but tests may call setupEnv(false) to overwrite
// restored by restoreFileSystem in ESTestCase
/*
//www.apache.org/licenses/LICENSE-2.0
// nothing left
// value hashes accessible even when the keystore is closed
// upgrade does not overwrite seed
// No password
// Indicate that the secret string is longer than it is so readFully() fails
// No password
// Indicate that the encryptedBytes is larger than it is so readFully() fails
// No password
// So that readFully during decryption will not consume the entire stream
// No password
// One entry
// hasPassword = false
// hasPassword = false
// string algo
// file algo
// num settings
// PBE only stores the lower 8 bits, so this narrowing is ok
/*
//www.apache.org/licenses/LICENSE-2.0
// sorted
/*
//www.apache.org/licenses/LICENSE-2.0
// account for keystore.seed too
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
// package private for testing
// package private for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// now append the JVM options from ES_JAVA_OPTS
// add the system JVM options first so that they can be overridden
/**
/**
/**
/**
/**
// lines beginning with "#" are treated as comments
// skip blank lines
// no range present, unconditionally apply the JVM option
// no range is present, apply the JVM option to the specified major version only
// a range of the form \\d+- is present, apply the JVM option to all major versions larger than the specified one
// a range of the form \\d+-\\d+ is present, apply the JVM option to the specified range of major versions
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
// pre-touch JVM emory pages during initialization
// explicitly set the stack size
// set to headless, just in case,
// ensure UTF-8 encoding by default (e.g., filenames)
// use our provided JNA always versus the system one
/*
// flags to configure Netty
// log4j 2
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// exit codes for install
/** A plugin with the same name is already installed. */
/** The plugin zip is not properly structured. */
/** The builtin modules, which are plugins, but cannot be installed or removed. */
/** The official plugins that can be installed simply by name. */
// Bin directory get chmod 755
// Bin files also get chmod 755
// Config directory get chmod 750
// Config files get chmod 660
// Plugin directory get chmod 755
// Plugins files get chmod 644
// pkg private for testing
// swap the entry by plugin id for one with the installed plugin name, it gives a cleaner error message for URL installs
/** Downloads the plugin and returns the file it was downloaded to. */
// now try as maven coordinates, a valid URL would only have a colon and slash
// fall back to plain old URL
// definitely not a valid url, so assume it is a plugin name
// pkg private so tests can override
/** Returns the url for an official elasticsearch plugin. */
//artifacts.elastic.co/downloads/elasticsearch-plugins/%s", pluginId);
//%s.elastic.co/%s-%s/downloads/elasticsearch-plugins/%s", hostname, version, stagingHash, pluginId);
/** Returns the url for an elasticsearch plugin in maven. */
//repo1.maven.org/maven2/%s/%s/%s", groupId, artifactId, version);
/**
// pkg private for tests to manipulate
/** Returns all the official plugin names that look similar to pluginId. **/
/** Downloads a zip from the url, into a temp file under the given temp dir. */
// pkg private for tests
// must overwrite since creating the temp file above actually created the file
/**
/**
// fallback to sha1, until 7.0, but with warning
/*
// read the bytes of the plugin zip in chunks to avoid out of memory errors
// this should never happen as we are using SHA-1 and SHA-512 here
/**
// fin is a file stream over the downloaded plugin zip whose signature to verify
// sin is a URL stream to the signature corresponding to the downloaded plugin zip
// ain is a input stream to the public key in ASCII-Armor format (RFC4880)
// validate the signature has key ID matching our public key ID
// compute the signature of the downloaded plugin zip
// finally we verify the signature of the downloaded plugin zip matches the expected signature
/**
/**
/**
/**
// pkg private for tests
// unzip plugin to a staging temp dir
// Using the entry name as a path can result in an entry outside of the plugin dir,
// either if the name starts with the root of the filesystem, or it is a relative
// entry like ../whatever. This check attempts to identify both cases by first
// normalizing the path (which removes foo/..) and ensuring the normalized entry
// is still rooted with the target plugin directory.
// be on the safe side: do not rely on that directories are always extracted
// before their children (although this makes sense, but is it guaranteed?)
// Jimfs throws an IAE where it should throw an UOE
// remove when google/jimfs#30 is integrated into Jimfs
// and the Jimfs test dependency is upgraded to include
// this pull request
// checking for existing version of the plugin
// don't let user install plugin conflicting with module...
// they might be unavoidably in maven central and are packaged up the same way)
/** Load information about the plugin, and verify it can be installed with no errors. */
// checking for existing version of the plugin
// check for jar hell before any copying
/** check a candidate plugin for jar hell before installing it */
// create list of current jars in classpath
// read existing bundles. this does some checks on the installation too.
// check jarhell of all plugins so we know this plugin and anything depending on it are ok together
// TODO: optimize to skip any bundles not connected to the candidate plugin?
// TODO: no jars should be an error
// TODO: verify the classname exists in one of the jars!
/**
// read optional security policy (extra permissions), if it exists, confirm or warn the user
/** Moves bin and config directories from the plugin if they exist */
// some files may already exist, and we don't remove plugin config files on plugin removal,
// so any installed config files are left on failure too
/** Moves the plugin directory into its final destination. **/
// "MacOS" is an alternative to "bin" on macOS
/** Copies the files from {@code tmpBinDir} into {@code destBinDir}, along with permissions from dest dirs parent. */
// clean up what we just copied
/**
// clean up what we just copied
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// are we done?
// rounding up to 100% would mean we say we are done, before we are...
// this also catches issues, when expectedTotalSize was guessed wrong
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// exit codes for remove
/** A plugin cannot be removed because it is extended by another plugin. */
/**
// first make sure nothing extends this plugin
/*
/*
/*
/*
/*
// add the plugin directory
// finally, add the marker file
/*
//www.apache.org/licenses/LICENSE-2.0
// no jarhell check
/** Creates a test environment with bin, config and plugins directories. */
/** creates a fake jar file with empty class files */
// no package names, just support simple classes
/** creates a plugin .zip and returns the url for testing */
// fake should have been removed when the file not found exception occurred
// has two colons, so it appears similar to maven coordinates
//host:1234", env.v1()));
// has two colons, so it appears similar to maven coordinates
// jar hell test needs a real filesystem
// adds plugin.jar with FakePlugin
// these both share the same FakePlugin class
// make sure at least one execute perm is missing, so we know we forced it during installation
// first find the beginning of our list of official plugins
// now check each line compares greater than the last, until we reach an empty line
// qa is not really a plugin and it shouldn't sneak in
// No progress bar in batch mode
// if batch is enabled, we also want to add a security policy
// calc sha an return file URL to it
// no jarhell check
//artifacts.elastic.co/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-" +
//snapshots.elastic.co/%s-abc123/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-%s.zip",
//snapshots.elastic.co/%s-abc123/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-%s.zip",
// attemping to install a release build of a plugin (no staging ID) on a snapshot build should throw a user exception
//staging.elastic.co/" + Version.CURRENT + "-abc123/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-"
//artifacts.elastic.co/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-" + Platforms.PLATFORM_NAME +
//snapshots.elastic.co/%s-abc123/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-%s-%s.zip",
//staging.elastic.co/" + Version.CURRENT + "-abc123/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-"
//repo1.maven.org/maven2/mygroup/myplugin/1.0.0/myplugin-1.0.0.zip";
//repo1.maven.org/maven2/mygroup/myplugin/1.0.0/myplugin-" + Platforms.PLATFORM_NAME + "-1.0.0.zip";
//repo1.maven.org/maven2/mygroup/myplugin/1.0.0/myplugin-1.0.0.zip";
//artifacts.elastic.co/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-" +
//repo1.maven.org/maven2/mygroup/myplugin/1.0.0/myplugin-1.0.0.zip";
//artifacts.elastic.co/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-" +
//artifacts.elastic.co/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-" +
//artifacts.elastic.co/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-" +
//artifacts.elastic.co/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-" +
//repo1.maven.org/maven2/mygroup/myplugin/1.0.0/myplugin-1.0.0.zip";
//artifacts.elastic.co/downloads/elasticsearch-plugins/analysis-icu/" + icu + "-" +
/*
// the actual key used for signing
// the expected key used for signing
//artifacts.elastic.co/downloads/elasticsearch-plugins/analysis-icu/" + icu + "-" +
/*
// checks the plugin requires a policy confirmation, and does not install when that is rejected by the user
// the plugin is installed after this method completes
// accept warnings we have already tested
// default answer, does not install
// explicitly do not install
// accept warnings we have already tested
// allow installation
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Can happen when bumping majors: 8.0 is only compat back to 7.0, but that's not released yet
// In this case, ignore what's released and just find that latest version before current
// did not remove
/*
//www.apache.org/licenses/LICENSE-2.0
//The default 20 minutes timeout isn't always enough, please do not increase further than 30 before analyzing what makes this suite so slow
/**
// throw out the END_OBJECT to conform with other ExecutableSections
/**
// Don't look up stashed values
// Check the text and produce an error message with the utf8 sequence if they don't match.
// Now check the whole map just in case the text matches but something else differs
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** A description of the command, used in the help output. */
/** The option parser for this command. */
/**
/** Parses options for this command from args and executes it. */
// StringWriter#close declares a checked IOException from the Closeable interface but the Javadocs for StringWriter
// say that an exception here is impossible
// print help to stderr on exceptions
/**
/** Prints a help message for the command to the terminal. */
/** Prints additional help information, specific to the command */
/**
/**
/** Gets the shutdown hook thread if it exists **/
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/* command line usage error */
/* data format error */
/* cannot open input */
/* addressee unknown */
/* host name unknown */
/* service unavailable */
/* internal software error */
/* can't create (user) output file */
/* input/output error */
/* temp failure; user is invited to retry */
/* remote error in protocol */
/* permission denied */
/* configuration error */
/* no instance, just constants */ }
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// .values(...) returns an unmodifiable list
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Writer to standard error - not supplied by the {@link Console} API, so we share with subclasses */
/** The default terminal implementation, which will be a console if available, or stdout/stderr if not. */
/** Defines the available verbosity levels of messages to be printed. */
/* always printed */
/* printed when no options are given to cli */
/* printed only when cli is passed verbose option */
/** The current verbosity for the terminal, defaulting to {@link Verbosity#NORMAL}. */
/** The newline used when calling println. */
/** Sets the verbosity of the terminal. */
/** Reads clear text from the terminal input. See {@link Console#readLine()}. */
/** Reads password text from the terminal input. See {@link Console#readPassword()}}. */
/** Returns a Writer which can be used to write to the terminal directly using standard output. */
/** Returns a Writer which can be used to write to the terminal directly using standard error. */
/** Prints a line to the terminal at {@link Verbosity#NORMAL} verbosity level. */
/** Prints a line to the terminal at {@code verbosity} level. */
/** Prints message to the terminal's standard output at {@code verbosity} level, without a newline. */
/** Prints message to the terminal at {@code verbosity} level, without a newline. */
/** Prints a line to the terminal's standard error at {@link Verbosity#NORMAL} verbosity level, without a newline. */
/** Prints a line to the terminal's standard error at {@link Verbosity#NORMAL} verbosity level. */
/** Prints a line to the terminal's standard error at {@code verbosity} level. */
/** Checks if is enough {@code verbosity} level to be printed */
/**
// prompts should go to standard error to avoid mixing with list output
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The exist status the cli should use when catching this user error. */
/** Constructs a UserException with an exit status and message to show the user. */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** no instantiation */
/** Simple driver class, can be used eg. from builds. Returns non-zero on jar-hell */
/**
/**
/**
// order is already lost, but some filesystems have it
// Technically empty classpath element behaves like CWD.
// So below is the "correct" code, however in practice with ES, this is usually just a misconfiguration,
// from old shell scripts left behind or something:
//   if (element.isEmpty()) {
//      element = System.getProperty("user.dir");
//   }
// Instead we just throw an exception, and keep it clean.
// we should be able to just Paths.get() each element, but unfortunately this is not the
// whole story on how classpath parsing works: if you want to know, start at sun.misc.Launcher,
// be sure to stop before you tear out your eyes. we just handle the "alternative" filename
// specification which java seems to allow, explicitly, right here...
// "correct" the entry to become a normal entry
// change to correct file separators
// if there is a drive letter, nuke the leading separator
// now just parse as ordinary file
// Eclipse adds this to the classpath when running unit tests...
// junit4.childvm.count
// should not happen, as we use the filesystem API
/**
// we don't try to be sneaky and use deprecated/internal/not portable stuff
// like sun.boot.class.path, and with jigsaw we don't yet have a way to get
// a "list" at all. So just exclude any elements underneath the java home
// exclude system resources
// inspect entries
// for jar format, the separator is defined as /
// case for tests: where we have class files in the classpath
// don't try and walk class or resource directories that don't exist
// gradle will add these to the classpath even if they never get created
// normalize with the os separator, remove '.class'
/** inspect manifest for sure incompatibilities */
// give a nice error if jar requires a newer java version
/**
// Ignore jigsaw module descriptions
// https://issues.apache.org/jira/browse/XMLBEANS-499
// throw a better exception in this ridiculous case.
// unfortunately the zip file format allows this buggy possibility
// UweSays: It can, but should be considered as bug :-)
/*
//www.apache.org/licenses/LICENSE-2.0
// for Java 8 there is ambiguity since both 1.8 and 8 are supported,
// so we rewrite the former to the latter
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// only for the null case we do that here!
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// there is no guarantee that the char buffers backing array is the right size
// so we need to make a copy
// clear sensitive data
// if the buffer is not read only we can reset and fill with 0's
// reset
/**
// there is no guarantee that the byte buffers backing array is the right size
// so we need to make a copy
// clear sensitive data
// if the buffer is not read only we can reset and fill with 0's
// reset
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Double wildcard "**" - skipping the first "*"
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** 
// TODO: can we move this to the .env package and make it package-private?
/** no instantiation */
/** the actual JDK default */
/** can be changed by tests */
/** 
/** 
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** How many nano-seconds in one milli-second */
// 106751.9 days is Long.MAX_VALUE nanoseconds, so we cannot store 106752 days
/**
/**
/**
/**
// Limit fraction pieces to a min of 0 and maximum of 10
// Location where the fractional values end
// Determine the value of the fraction, so if it were .000 the
// actual long value is 0, in which case, it can be elided.
// Either the part of the fraction we were asked to report is
// zero, or the user requested 0 fraction pieces, so return
// only the integral value
// Build up an array of fraction characters, without going past
// the end of the string. This keeps track of trailing '0' chars
// that should be truncated from the end to avoid getting a
// string like "1.3000d" (returning "1.3d" instead) when the
// value is 1.30000009
// No more pieces, the fraction has ended
// Generate the fraction string from the char array, truncating any trailing zeros
// parsing minutes should be case-sensitive as 'M' means "months", not "minutes"; this is the only special case.
// Missing units:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
//github.com/yannrichet/jmathplot/blob/f25426e0ab0e68647ad2b75f577c7be050ecac86/src/main/java/org/math/plot/utils/FastMath.java
/**
//--------------------------------------------------------------------------
// RE-USABLE CONSTANTS
//--------------------------------------------------------------------------
//--------------------------------------------------------------------------
// CONSTANTS AND TABLES FOR ATAN
//--------------------------------------------------------------------------
// We use the formula atan(-x) = -atan(x)
// ---> we only have to compute atan(x) on [0,+infinity[.
// For values corresponding to angles not close to +-PI/2, we use look-up tables;
// for values corresponding to angles near +-PI/2, we use code derived from fdlibm.
// Supposed to be >= tan(67.7deg), as fdlibm code is supposed to work with values > 2.4375.
// 1.57079632679489655800e+00 atan(inf)hi
// 6.12323399573676603587e-17 atan(inf)lo
//  3.33333333333329318027e-01
// -1.99999999998764832476e-01
//  1.42857142725034663711e-01
// -1.11111104054623557880e-01
//  9.09088713343650656196e-02
// -7.69187620504482999495e-02
//  6.66107313738753120669e-02
// -5.83357013379057348645e-02
//  4.97687799461593236017e-02
// -3.65315727442169155270e-02
// 1.62858201153657823623e-02
// atan
// x: in [0,ATAN_MAX_VALUE_FOR_TABS].
/**
// sinh(x) = (exp(x)-exp(-x))/2
// Might be more accurate, if value < 1: return h*((t+t)-t*t/(t+1.0)).
/**
// We want "exact" result for 1.0.
// value > ATAN_MAX_VALUE_FOR_TABS, or value is NaN
// This part is derived from fdlibm.
// value >= 2^66, or value is NaN
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** gets the name of this instance */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// use the inst...
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Static utils methods
/**
/**
/**
/**
// since we only assigned an IOException or a RuntimeException to ex above, in this case ex must be a RuntimeException
/**
/**
// noinspection EmptyCatchBlock
/**
/**
// noinspection EmptyCatchBlock
/**
// TODO: remove this leniency
// TODO: replace with constants class if needed (cf. org.apache.lucene.util.Constants)
/**
// opening a directory on Windows fails, directories can not be fsynced there
// yet do not suppress trying to fsync directories that do not exist
// ignore exception if it is a directory
// throw original exception
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// ignore
/*
//www.apache.org/licenses/LICENSE-2.0
// the java api for zip file does not allow creating duplicate entries (good!) so
// this bogus jar had to be with https://github.com/jasontedor/duplicate-classes
// classpath testing is system specific, so we just write separate tests for *nix and windows cases
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we want a different string, so ensure the first character is different, but the same overall length
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Space is allowed before unit:
// Time values of months should throw an exception as months are not
// supported. Note that this is the only unit that is not case sensitive
// as `m` is the only character that is overloaded in terms of which
// time unit is expected between the upper and lower case versions
/*
//www.apache.org/licenses/LICENSE-2.0
// accuracy for atan(x)
// accuracy for sinh(x)
// for small x
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// ensure that at least one always throws
///"));
// we create a tree of files that IOUtils#rm should delete
/**
//", delegate);
// no exception
//", Objects.requireNonNull(delegate));
///"));
// no exception, we early return and do not even try to open the directory
// no exception
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
//generated
//package private for testing
//throw should never happen
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//should never happen
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//append validation - look through all of the keys to see if there are any keys that need to participate in an append operation
// but don't have the '+' defined
//reference validation - ensure that '*' and '&' come in pairs
/**
/**
//ensure leading delimiter matches
//grab the first key/delimiter pair
//start dissection after the first delimiter
//start walking the input string byte by byte, look ahead for matches where needed
//if a match is found jump forward to the end of the match
//potential match between delimiter and input string
//look ahead to see if the entire delimiter matches the input string
//found a full delimiter match
//record the key/value tuple
//jump to the end of the match
//look for consecutive delimiters (e.g. a,,,,d,e)
//found consecutive delimiters
//jump to the end of the match
//progress the keys/delimiter if possible
//the while loop
//add the key with an empty value for the empty delimiter
//the while loop
//progress the keys/delimiter if possible
//the for loop
//i is always one byte after the last found delimiter, aka the start of the next value
//the last key, grab the rest of the input (unless consecutive delimiters already grabbed the last key)
//and there is no trailing delimiter
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//should never happen due to regex
/*
//www.apache.org/licenses/LICENSE-2.0
//allow for a-z values
//allow for a-z values
//allow for a-z values
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/logstash-plugins/logstash-filter-dissect/blob/master/src/test/java/org/logstash/dissect/DissectorTest.java
//Logstash supports implicit ordering based anchored by the key without the '+'
//This implementation will only honor implicit ordering for appending right to left else explicit order (/N) is required.
//The results of this test differ from Logstash.
//Same test as above, but with same result as Logstash using explicit ordering in the pattern
//parallel arrays
//int to ensures values and delimiters don't overlap, else validation can fail
//parallel arrays
// keys should be unique in this test
//int to ensures values and delimiters don't overlap, else validation can fail
//leading
//trailing
//middle
//skipped with empty values
//only whitespace is trimmed in the absence of trailing characters
//consecutive delimiters + right padding can be used to skip over the trailing delimiters
/**
/**
//8rursodiol.enjin.com\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) " +
//8rursodiol.enjin.com", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36" +
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//en.wikipedia.org/wiki/Visitor_pattern">Visitor Pattern</a>
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// not serialized by itself in WKT or WKB
// not part of the actual WKB spec
// not part of the actual WKB spec
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// magic numbers for bit interleaving
// shift values for bit interleaving
/**
//graphics.stanford.edu/~seander/bithacks.html#InterleaveBMN
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** maximum precision for geohash strings */
/** number of bits used for quantizing latitude and longitude values */
/** Bit encoded representation of the latitude of north pole */
// no instance:
/** Returns a {@link Point} instance from a geohash string */
/**
// bottom left is the coordinate
// shift away the level
// deinterleave
// add 1 to lat and lon to get topRight
// We cannot go north of north pole, so just using 90 degrees instead of calculating it using
// add 1 to lon to get lon of topRight, we are going to use 90 for lat
/**
/**
/**
/**
// Decoding the Geohash bit pattern to determine grid coordinates
// first bit of x
// first bit of y
// second bit of x
// second bit of y
// third bit of x
// combine the bitpattern to grid coordinates.
// note that the semantics of x and y are swapping
// on each level
// Root cells at north (namely "bcfguvyz") or at
// south (namely "0145hjnp") do not have neighbors
// in north/south direction
// define grid coordinates for next level
// if the defined neighbor has the same parent a the current cell
// encode the cell directly. Otherwise find the cell next to this
// cell recursively. Since encoding wraps around within a cell
// it can be encoded here.
// xLimit and YLimit must always be respectively 7 and 3
// since x and y semantics are swapping on each level.
/**
// shift to appropriate level
/**
/**
// convert to geohashlong
/**
/** base32 encode at the given grid coordinate */
/**
// We cannot handle more than 12 levels
/**
// We cannot handle more than 12 levels
// encode lat/lon flipping the sign bit so negative ints sort before positive ints
/** encode latitude to integer */
// the maximum possible value cannot be encoded without overflow
/** encode longitude to integer */
// the maximum possible value cannot be encoded without overflow
/** returns the latitude value from the string based geohash */
/** returns the latitude value from the string based geohash */
/** decode longitude value from morton encoded geo point */
/** decode latitude value from morton encoded geo point */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/* The instance of WKT serializer that coerces values and accepts Z component */
// walk through coordinates:
// minX, maxX, maxY, minY
// setup the tokenizer; configured to read words w/o numbers
/**
// Not part of the standard, but we need it for internal serialization
/**
// TODO: Add 3D support
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Auto closing in coerce mode
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//Ensure that for all points at all supported levels of precision
// that the long encoding of a geohash is compatible with its
// String based counterpart
// string encode from geohashlong encoded location
// string encode from full res lat lon
// ensure both strings are the same
// decode from the full-res geohash string
// decode from the geohash encoded long
// check that the length is as expected
// Adding some random geohash characters at the end
// Bounding box with maximum precision touching north pole
// Should be 90 degrees
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// add rest of the path:
// no match found
/**
// TODO(tal): Support definitions
/**
/**
// TODO: I think we should throw an error here?
// Code for loading built-in grok patterns packaged with the jar file:
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// not removing the entry here, this happens in the unregister() method.
/*
//www.apache.org/licenses/LICENSE-2.0
// '60' second is a leap second.
// invalid month
// invalid month
// invalid day
// invalid day
// invalid day
// invalid day
// invalid hour
// invalid hour
// invalid hour
// invalid minute
// invalid minute
// invalid second
// invalid second
// invalid second
// invalid timezone
// invalid timezone
// invalid timezone
// invalid timezone
// invalid timezone
// invalid timezone
// invalid timezone
// invalid timezone
//8rursodiol.enjin.com\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) " +
//8rursodiol.enjin.com\"", matches.get("referrer"));
//semicomplete.com/presentations/logstash-monitorama-2013/\" " +
//semicomplete.com/presentations/logstash-monitorama-2013/\"");
// to avoid a lingering thread when test has completed
/*
//www.apache.org/licenses/LICENSE-2.0
// to avoid a lingering thread when test has completed
// need to call #register() method on a different thread, assertBusy() fails if current thread gets interrupted
// wait here so that the size of the registry can be asserted
// Periodic action is not scheduled because no thread is registered
// Periodic action is scheduled because a thread is registered
// Registering the first thread should have caused the command to get scheduled again
// Periodic action is not scheduled again because no thread is registered
// Registering a second thread does not cause the command to get scheduled twice
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Package private for testing
// public for tests
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// This can happen if getRemoteAddress throws IOException.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// The offset is an int as it is the offset of where the bytes begin in the first buffer
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Only select until the next task needs to be run. Do not select with a value of 0 because
// that blocks without a timeout.
/**
/**
/**
/**
// This should very rarely happen. The only times a channel is exposed outside the event loop,
// but might not registered is through the exception handler and channel accepted callbacks.
// If the channel does not currently have anything that is ready to flush, we should flush after
// the write operation is queued.
// We only attempt the write if the connect process is complete and the context is not
// signalling that it should be closed.
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// This is reference counted as some implementations want to retain the byte pages by calling
// duplicate. With reference counting we can increment the reference count, return a new page,
// and safely close the pages independently. The closeable will not be called until each page is
// released.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Set to true in order to reject new writes before queuing with selector
// Poll for new flush operations to close
// Some protocols might produce messages to flush during a read operation.
/**
// When you read or write to a nio socket in java, the heap memory passed down must be copied to/from
// direct memory. The JVM internally does some buffering of the direct memory, however we can save space
// by reusing a thread-local direct buffer (provided by the NioSelector).
//
// Each network event loop is given a 64kb DirectByteBuffer. When we read we use this buffer and copy the
// data after the read. When we go to write, we copy the data to the direct memory before calling write.
// The choice of 64KB is rather arbitrary. We can explore different sizes in the future. However, any
// data that is copied to the buffer for a write, but not successfully flushed immediately, must be
// copied again on the next call.
// Currently we limit to 64KB. This is a trade-off which means more syscalls, in exchange for less
// copying.
// Set reuse address first as it must be set before a bind call. Some implementations throw
// exceptions on other socket options if the channel is not connected. But setting reuse first,
// we ensure that it is properly set before any bind attempt.
// Ignore if not connect complete. Some implementations fail on setting socket options if the
// socket is not connected. We will try again after connection.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: This should eventually be removed once ExceptionsHelper is moved to a core library jar
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Override this functionality to avoid having to connect the accepted channel
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// ctor starts threads. So we are testing that close() stops the threads. Our thread linger checks
// will throw an exception is stop fails
// ctor starts threads. So we are testing that a failure to construct will stop threads. Our thread
// linger checks will throw an exception is stop fails
/*
//www.apache.org/licenses/LICENSE-2.0
// As this is a timing based test, we must assertBusy in the very small chance that the loop is
// delayed for 50 milliseconds (causing a selectNow())
/*
//www.apache.org/licenses/LICENSE-2.0
// We do not want to call the actual register with selector method as it will throw a NPE
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Loaders of plugins extended by a plugin. */
// continue
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//cs.oswego.edu/pipermail/concurrency-interest/2009-August/006508.html">
//cs.oswego.edu/pipermail/concurrency-interest/2009-August/006508.html</a>
/**
/**
/**
// surefire test runner
// junit4 test runner
// eclipse test runner
// intellij test runner (before IDEA version 2019.3)
// intellij test runner (since IDEA version 2019.3)
// java.security.debug support
// simple check that they are trying to debug
// thread permission logic
// first, check if we can modify threads at all.
// check the threadgroup, if its our thread group or an ancestor, its fine.
// its a dead thread, do nothing.
// first, check if we can modify thread groups at all.
// check the threadgroup, if its our thread group or an ancestor, its fine.
// we are a dead thread, do nothing
// exit permission logic
/**
// this exit point is allowed, we return normally from closure:
// anything else in stack trace is not allowed, break and throw SecurityException below:
// should never happen, only if JVM hides stack trace - replace by generic:
// we passed the stack check, delegate to super, so default policy can still deny permission:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Simple tests for SecureSM */
// install a mock security policy:
// AllPermission to source code
// ThreadPermission not granted anywhere else
// no exception
// no exception
// no exception
// try to bogusly interrupt our sibling
// sibling attempted to but was not able to muck with its other sibling
// but we are the parent and can terminate
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/groovenauts/jmeter_oauth_plugin/blob/master/jmeter/src/
// Constructed Flag
// Tag and data types
// getLength() can return any 32 bit integer, so ensure that a corrupted encoding won't
// force us into allocating a very large array
/**
// A single byte short length
// We can't handle length longer than 4 bytes
//$NON-NLS-1$
/**
/**
/**
//$NON-NLS-1$
/**
//$NON-NLS-1$
// octet string is basically a byte array
//$NON-NLS-1$
//$NON-NLS-1$
//$NON-NLS-1$
//$NON-NLS-1$
//$NON-NLS-1$
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Enforce a single instance
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// This should never happen so callers really shouldn't be forced to deal with it themselves.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Verify that the key starts with the correct header before passing it to parseOpenSslEC
/**
// Verify that the key starts with the correct header before passing it to parseOpenSslDsa
/**
/**
// Parse PEM headers according to https://www.ietf.org/rfc/rfc1421.txt
/**
// Unencrypted
// Parse PEM headers according to https://www.ietf.org/rfc/rfc1421.txt
/**
// Unencrypted
// Parse PEM headers according to https://www.ietf.org/rfc/rfc1421.txt
/**
/**
//We only handle PEM encryption
//malformed pem
/**
/**
//www.openssl.org/docs/man1.1.0/crypto/PEM_write_bio_PrivateKey_traditional.html
// AES IV (salt) is longer but we only need 8 bytes
// MD5 digests are 16 bytes
// use previous round digest as IV
/**
/**
// version
/**
// (version) We don't need it but must read to get to modulus
/**
// (version) We don't need it but must read to get to p
// we don't need x
/**
// version
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// nothing to do here
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
//docs.oracle.com/en/java/javase/11/docs/specs/security/standard-names.html#sslcontext-algorithms">
// ignore since we support JVMs using BCJSSE in FIPS mode which doesn't support TLSv1.3
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// Trust
/**
/**
/**
/**
/**
/**
// Key Management
// -- Keystore
/**
/**
/**
/**
/**
/**
/**
// -- PEM
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TLSv1.3 cipher has PFS, AEAD, hardware support
// PFS, AEAD, hardware support
// PFS, AEAD, hardware support
// PFS, hardware support
// PFS, hardware support
// PFS, hardware support
// PFS, hardware support
// AEAD, hardware support
// hardware support
// hardware support
// TLSv1.3 cipher has PFS, AEAD, hardware support
// TLSv1.3 cipher has PFS, AEAD
// PFS, AEAD, hardware support
// PFS, AEAD, hardware support
// PFS, AEAD
// PFS, hardware support
// PFS, hardware support
// PFS, hardware support
// PFS, hardware support
// AEAD, hardware support
// hardware support
// hardware support
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// skip index-0, that's the peer cert.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// utility class
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// single instances
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// This is a sample of the CAs that we expect on every JRE.
// We can safely change this list if the JRE's issuer list changes, but we want to assert something useful.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// When running on BC-FIPS, an invalid file format *might* just fail to parse, without any errors (just like an empty file)
// or it might behave per the SUN provider, and throw a GSE (depending on exactly what was invalid)
/*
// Tag value indicating an ASN.1 "SEQUENCE". Reference: sun.security.util.DerValue.tag_Sequence = 0x30
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// If this is not set, the loader will guess from the extension
// If this is not set, the loader will guess from the extension
// If this is not set, the loader will guess from the extension
// If this is not set, the loader will guess from the extension
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Some constants for use in mock certificates
// From time to time, CAs issue updated certificates based on the same underlying key-pair.
// For example, they might move to new signature algorithms (dropping SHA-1), or the certificate might be
// expiring and need to be reissued with a new expiry date.
// In this test, we assume that the server provides a certificate that is signed by the new CA cert, and we trust the old CA cert
// Our diagnostic message should make clear that we trust the CA, but using a different cert fingerprint.
// Note: This would normally succeed, so we wouldn't have an exception to diagnose, but it's possible that the hostname is wrong.
// uses "new" CA
/*
//www.apache.org/licenses/LICENSE-2.0
// Because (a) cannot load a JKS as a PKCS12 & (b) the password is wrong.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// if this parse field has not been completely deprecated then try to
// match the preferred name
// Now try to match against one of the deprecated names. Note that if
// the parse field is entirely deprecated (allReplacedWith != null) all
// fields will be in the deprecatedNames array
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Using a method reference here angers some compilers
// Using a method reference here angers some compilers
// Using a method reference here angers some compilers
// Using a method reference here angers some compilers
/**
// single value
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Safe because we never call the method. This is just trickery to make the interface pretty.
/**
// Safe because we never call the method. This is just trickery to make the interface pretty.
/*
/*
/*
// The target has already been built. Call the callback now.
/*
/**
// The target has already been built. Just apply the consumer now.
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
// There were non-optional constructor arguments missing.
/*
// All missing constructor arguments were optional. Just build the target and return it.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** The class that this entry can read. */
/** A name for the entry which is unique within the {@link #categoryClass}. */
/** A parser capability of parser the entry's class. */
/** Creates a new entry which can be stored by the registry. */
/**
// we've seen the last of this category, put it into the big map
// handle the last category
/**
// The "empty" registry will never work so we throw a better exception as a hint.
/* Note that this shouldn't happen because we already looked up the entry using the names but we need to call `match` anyway
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// TODO It'd be lovely if we could the options here but we don't have the right stuff plumbed through. We'll get to it!
/**
/**
/**
/**
/**
/**
/**
/**
/**
// This creates and parses the named object
// This messy exception nesting has the nice side effect of telling the use which field failed to parse
// Fields are just named entries in a single object
// Fields are objects in an array. Each object contains a named field.
// Move to the first field in the object
// Move past the object, should be back to into the array
/**
/**
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// treat strings as already converted
// Load pluggable extensions
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
////////////////////////////////////////////////////////////////////////////
// Structure (object, array, field, null values...)
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// Boolean
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// Byte
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// Double
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// Float
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// Integer
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// Long
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// Short
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// BigInteger
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// BigDecimal
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// String
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// Binary
//////////////////////////////////
/**
////////////////////////////////////////////////////////////////////////////
// Date
//////////////////////////////////
/**
/**
/**
////////////////////////////////////////////////////////////////////////////
// LatLon
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// Path
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// Objects
//
// These methods are used when the type of value is unknown. It tries to fallback
// on typed methods and use Object.toString() as a last resort. Always prefer using
// typed methods over this.
//////////////////////////////////
//Path implements Iterable<Path> and causes endless recursion and a StackOverFlow if treated as an Iterable here
// Write out the Enum toString
////////////////////////////////////////////////////////////////////////////
// ToXContent
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// Maps & Iterable
//////////////////////////////////
/** writes a map without the start object and end object headers */
// checks that the map does not contain references to itself because
// iterating over map entries will cause a stackoverflow error
// pass ensureNoSelfReferences=false as we already performed the check at a higher level
//treat as single value
// checks that the iterable does not contain references to itself because
// iterating over entries will cause a stackoverflow error
// pass ensureNoSelfReferences=false as we already performed the check at a higher level
////////////////////////////////////////////////////////////////////////////
// Human readable fields
//
// These are fields that have a "raw" value and a "human readable" value,
// such as time values or byte sizes. The human readable variant is only
// used if the humanReadable flag has been set
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// Misc.
//////////////////////////////////
////////////////////////////////////////////////////////////////////////////
// Raw fields
//////////////////////////////////
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Should we throw a failure here? Smile idea is to use it in bytes....
// CBOR is not supported
/**
/**
/**
/**
/*
// scan until we find the first non-whitespace character or the end of the stream
// now guess the content type off the next GUESS_HEADER_LENGTH bytes including the current byte
/**
/**
// CBOR logic similar to CBORFactory#hasCBORFormat
// Actually, specific "self-describe tag" is a very good indicator
// for small objects, some encoders just encode as major type object, we can safely
// say its CBOR since it doesn't contradict SMILE or JSON, and its a last resort
// JSON may be preceded by UTF-8 BOM
// a last chance for JSON
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// usually a binary value
/**
/**
/**
/**
/**
/**
// TODO remove context entirely when it isn't needed
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we have reached the end of the wrapped object
// skip if not starting on an object or an array
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*}. This method can be used to parse the {@code Accept} HTTP header or a
/*")) {
/**
// we also support newline delimited JSON: http://specs.okfnlabs.org/ndjson/
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// this trips on many mappings now...
// Do not automatically close unclosed objects/arrays in com.fasterxml.jackson.dataformat.cbor.CBORGenerator#close() method
/*
//www.apache.org/licenses/LICENSE-2.0
// nothing here
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// this trips on many mappings now...
// Do not automatically close unclosed objects/arrays in com.fasterxml.jackson.core.json.UTF8JsonGenerator#close() method
/*
//www.apache.org/licenses/LICENSE-2.0
/** Generator used to write content **/
/**
/**
// In case of combined inclusion and exclusion filters, we have one and only one another delegating level
// Use the low level generator to write the startObject so that the root
// start object is always written even if a filtered generator is used
// Use the low level generator to write the startObject so that the root
// start object is always written even if a filtered generator is used
// as jackson's JsonGenerator doesn't have this method for BigInteger
// we have to implement it ourselves
// needed for the XContentFactory.xContentType call
// EMPTY is safe here because we never call namedObject when writing raw data
// It's okay to pass the throwing deprecation handler
// because we should not be writing raw fields when
// generating JSON
// If we've just started a field we'll need to add the separator
// A basic copy of Java 9's InputStream#transferTo
// When the current generator is filtered (ie filter != null)
// or the content is in a different format than the current generator,
// we need to copy the whole structure so that it will be correctly
// filtered or converted
/** Whether this generator supports writing raw data directly */
// EMPTY is safe here because we never call namedObject
// It's okay to pass the throwing deprecation handler because we
// should not be writing raw fields when generating JSON
// the start of the parser
/**
// Let's handle field-name separately first
// fall-through to copy the associated value
// others are simple:
// Bypass generator to always write the line feed
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// for now, this is an overhead, might make sense for web sockets
// this trips on many mappings now...
// Do not automatically close unclosed objects/arrays in com.fasterxml.jackson.dataformat.smile.SmileGenerator#close() method
/*
//www.apache.org/licenses/LICENSE-2.0
// nothing here
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Currently this is not a setting that can be changed and is a policy
// that relates to how parsing of things like "boost" are done across
// the whole of Elasticsearch (eg if String "1.0" is a valid float).
// The idea behind keeping it as a constant is that we can track
// references to this policy decision throughout the codebase and find
// and change any code that needs to apply an alternative policy.
//Need to throw type IllegalArgumentException as current catch logic in
//NumberFieldMapper.parseCreateField relies on this for "malformed" value detection
// The 3rd party parsers we rely on are known to silently truncate fractions: see
//   http://fasterxml.github.io/jackson-core/javadoc/2.3.0/com/fasterxml/jackson/core/JsonParser.html#getShortValue()
// If this behaviour is flagged as undesirable and any truncation occurs
// then this method is called to trigger the"malformed" handling logic
// Need to throw type IllegalArgumentException as current catch
// logic in NumberFieldMapper.parseCreateField relies on this
// for "malformed" value detection
/* irrelevant */);
// weak bounds on the BigDecimal representation to allow for coercion
/** Return the long that {@code stringValue} stores or throws an exception if the
// we will try again with BigDecimal
// asserting that no ArithmeticException is thrown
// Must point to field name
// And then the value...
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// nothing here
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// The following JSON needs to include newlines, in order to affect the line numbers
// included in the exception
// The following JSON needs to include newlines, in order to affect the line numbers
// included in the exception
/**
// either(yeahMatcher).or(nullValue) is broken by https://github.com/hamcrest/JavaHamcrest/issues/49
// ctor arg first so we can test for the bug we found one time
// and ctor arg second just in case
// and without the constructor arg if we've made it optional
// We're just using 0 as the default because it is easy for testing
/*
// Create our own parser for this test so we can disable support for the "ordered" mode specified by the array above
// Now firing the xml through it fails
// Create our own parser for this test so we can disable support for the "ordered" mode specified by the array above
// Now firing the xml through it fails
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//foobar\", \"port\" : 80}, \"name\" : \"foobarbaz\"}"
// Create our own parser for this test so we can disable support for the "ordered" mode specified by the array above
// Now firing the xml through it fails
// Make sure that we didn't break the null handling in arrays that shouldn't support nulls
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// CBOR parses back a float
// JSON, YAML and SMILE parses back the float value as a double
// This will change for SMILE in Jackson 2.9 where all binary based
// formats will return a float
// Calling XContentParser.list() or listOrderedMap() to read a simple
// value or object should throw an exception
// sometimes read the start array token, sometimes not
// sometimes read the start array token, sometimes not
// sometimes read the start array token, sometimes not
// sometimes read the start array token, sometimes not
// Verify map contents, ignore the iteration order.
// Verify map contents, ignore the iteration order.
// Verify that map's iteration order is the same as the order in which fields appear in JSON.
// first field
// foo
// marked field
// {
// Simulate incomplete parsing
// And sometimes skipping children
// last field
// array field
// [
// Simulate incomplete parsing
// And sometimes skipping children
// first field
// Simulate incomplete parsing
/**
// don't need to go too deep
// don't need to go too deep
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** per shard stats needed to compute stats */
/** final result */
/** per shard ctor */
/**
/** get the number of documents */
/** get the number of samples for the given field. == docCount - numMissing */
/** get the mean for the given field */
/** get the variance for the given field */
/** get the distribution skewness for the given field */
/** get the distribution shape for the given field */
/** get the covariance between the two fields */
/** get the correlation between the two fields */
// name
// count
// mean
// variance
// skewness
// kurtosis
// covariance
// correlation
// merge stats across all shards
// return empty result iff all stats are null
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** return the total document count */
/** return total field count (differs from docCount if there are missing values) */
/** return the field mean */
/** return the field variance */
/** return the skewness of the distribution */
/** return the kurtosis of the distribution */
/** return the covariance between field x and field y */
/** return the correlation coefficient of field x and field y */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Multiple ValuesSource with field names */
/** array of descriptive stats, per shard, needed to compute the correlation */
// get fields
// add document fields to correlation stats
/**
// loop over fields
// TODO: Fix matrix stats to treat neg inf as any other value
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** object holding results - computes results in place */
/** pearson product correlation coefficients */
/** Base ctor */
/** creates and computes result from provided stats */
/** creates a results object from the given stream */
/** Marshalls MatrixStatsResults */
// marshall results
// marshall correlation
/** return document count */
/** return the field counts - not public, used for getProperty() */
/** return the fied count for the requested field */
/** return the means - not public, used for getProperty() */
/** return the mean for the requested field */
/** return the variances - not public, used for getProperty() */
/** return the variance for the requested field */
/** return the skewness - not public, used for getProperty() */
/** return the skewness for the requested field */
/** return the kurtosis */
/** return the kurtosis for the requested field */
/** return the covariances as a map - not public, used for getProperty() */
/** return the covariance between two fields */
/** return the correlations as a map - not public, used for getProperty() */
/** return the correlation coefficient between two fields */
/** return the value for two fields in an upper triangular matrix, regardless of row col location. */
// for the co-value to exist, one of the two (or both) fields has to be a row key
// fieldX exists as a row key
// fieldY exists as a col key to fieldX
// otherwise fieldX is the col key to fieldY
// fieldX did not exist as a row key, it must be a col key
/** Computes final covariance, variance, and correlation */
// compute final skewness and kurtosis
// update skewness
// update kurtosis
// update variances
// compute final covariances and correlation
// update covariance
// update correlation
// if there is no variance in the data then correlation is NaN
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//prod.sandia.gov/techlib/access-control.cgi/2008/086212.pdf
/** count of observations (same number of observations per field) */
/** per field sum of observations */
/** counts */
/** mean values (first moment) */
/** variance values (second moment) */
/** skewness values (third moment) */
/** kurtosis values (fourth moment) */
/** covariance values */
/** Ctor to create an instance of running statistics */
// read doc count
// read fieldSum
// counts
// means
// variances
// skewness
// kurtosis
// read covariances
// Convert Map to HashMap if it isn't
// marshall doc count
// marshall fieldSum
// counts
// mean
// variances
// skewness
// kurtosis
// covariances
/** updates running statistics with a documents field values **/
// update total, mean, and variance
// moments
// update counts
// update running sum
// update running deltas
// update running mean, variance, skewness, kurtosis
// update running means
// update running variances
/** Update covariance matrix */
// deep copy of hash keys (field names)
// update running covariances
/**
//prod.sandia.gov/techlib/access-control.cgi/2008/086212.pdf
// merge count
// across fields
// merge counts of two sets
// merge means of two sets
// merge deltas
// merge totals
// merge variances, skewness, and kurtosis of two sets
// delta mean
// delta mean squared
// delta mean cubed
// delta mean 4th power
// num samples squared
// doc A num samples squared
// doc B num samples squared
// variance
// skeewness
// kurtosis
/** Merges two covariance matrices */
// merge covariances of two sets
// merge covariances
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// the specific value source type is undefined, but for scripts,
// we need to have a specific value source
// type to know how to handle the script values, so we fallback
// on Bytes
// we can't figure it out
/**
// todo add ParseField support to XContentBuilder
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// getFieldCount returns 0 for unknown fields
/*
//www.apache.org/licenses/LICENSE-2.0
// Since `search` doesn't do any reduction, and the InternalMatrixStats object will have a null `MatrixStatsResults`
// object.  That is created during the final reduction, which also does a final round of computations
// So we have to create a MatrixStatsResults object here manually so that the final `compute()` is called
// Unlike testTwoFields, `searchAndReduce` will execute reductions so the `MatrixStatsResults` object
// will be populated and fully computed.  We should use that value directly to test against
/*
//www.apache.org/licenses/LICENSE-2.0
// set count
// compute mean
// fieldA
// compute variance, skewness, and kurtosis
// compute covariance
// compute correlation
// means
// variances
// skewness (multi-pass is more susceptible to round-off error so we need to slightly relax the tolerance)
// kurtosis (multi-pass is more susceptible to round-off error so we need to slightly relax the tolerance)
// covariances
// correlation
// means
// variances
// skewness (multi-pass is more susceptible to round-off error so we need to slightly relax the tolerance)
// kurtosis (multi-pass is more susceptible to round-off error so we need to slightly relax the tolerance)
// covariances
// correlation
// shortcut, handles infinities
// a or b is zero or both are extremely close to it
// relative error is less meaningful here
// use relative error
/*
//www.apache.org/licenses/LICENSE-2.0
/** test running stats */
/** Test merging stats across observation shards */
// slice observations into shards
// create a document with two numeric fields
// running stats computation
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// don't decompound synonym file
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// posInc is always 1 at the beginning of a tokenstream and the convention
// from the _analyze endpoint is that tokenstream positions are 0-based
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// See https://issues.apache.org/jira/browse/LUCENE-7536 for the reasoning
// Normalization should only emit a single token, so always turn off preserveOriginal
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Returns true if, and only if, the provided character matches this character class. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// old index: best effort
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Language analyzers:
// TODO deprecate and remove in API
/*PatternAnalyzer.NON_WORD_PATTERN*/, null), true,
// Language analyzers:
// chinese analyzer: only for old indices, best effort
// TODO this one seems useless
/**
// The stop filter is in lucene-core but the English stop words set is in lucene-analyzers-common
// TODO deprecate and remove in API
// This is already broken with normalization, so backwards compat isn't necessary?
// Temporary shim for aliases. TODO deprecate after they are moved
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// side=BACK is not supported anymore but applying ReverseStringFilter up-front and after the token filter has the same effect
// TODO: Expose preserveOriginal
// side=BACK is not supported anymore but applying ReverseStringFilter up-front and after the token filter has the same effect
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// This condition follows up on the overridden analyze method. In case lenient was set to true and there was an
// exception during super.analyze we return a zero-length CharsRef for that word which caused an exception. When
// the synonym mappings for the words are added using the add method we skip the ones that were left empty by
// analyze i.e., in the case when lenient is set we only add those combinations which are non-zero-length. The
// else would happen only in the case when the input or output is empty and lenient is set, in which case we
// quietly ignore it. For more details on the control-flow see SolrSynonymParser::addInternal.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// This condition follows up on the overridden analyze method. In case lenient was set to true and there was an
// exception during super.analyze we return a zero-length CharsRef for that word which caused an exception. When
// the synonym mappings for the words are added using the add method we skip the ones that were left empty by
// analyze i.e., in the case when lenient is set we only add those combinations which are non-zero-length. The
// else would happen only in the case when the input or output is empty and lenient is set, in which case we
// quietly ignore it. For more details on the control-flow see SolrSynonymParser::addInternal.
/*
//www.apache.org/licenses/LICENSE-2.0
/** OpenRefine Fingerprinting, which uses a Standard tokenizer and lowercase + stop + fingerprint + asciifolding filters */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// for javadoc
// unsupported ancient option
// we don't allow both or none
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// a pattern for matching keywords is specified, as opposed to a
// set of keyword strings to match against
// a set of keywords (or a path to them) is specified
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// ancient unsupported option
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// source => target
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// also merge and transfer token filter analysis modes with analyzer
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: Expose preserveOriginal
/*
//www.apache.org/licenses/LICENSE-2.0
// Populate with unicode categories from java.lang.Character
// just ignore
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Simple regex-based analyzer based on PatternTokenizer + lowercase + stopwords */
/*
//www.apache.org/licenses/LICENSE-2.0
/*PatternAnalyzer.NON_WORD_PATTERN*/);
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// when not set or set to "", use "".
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*PatternAnalyzer.NON_WORD_PATTERN*/);
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Filters {@link StandardTokenizer} with {@link
/** Builds the named analyzer with no stop words. */
/** Builds the named analyzer with the given stop words. */
/** Constructs a {@link StandardTokenizer} filtered by a {@link LowerCaseFilter}, a {@link StopFilter},
// remove the possessive 's for english stemmers
// Use a special lowercase filter for turkish, the stemmer expects it.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//elasticsearch-users.115913.n3.nabble.com/Using-the-Snowball-stemmers-tp2126106p2127111.html
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// check that we have a valid language by trying to create a TokenStream
// Dutch stemmers
// English stemmers
// Finnish stemmers
// leaving this for backward compatibility
// French stemmers
// Galician stemmers
// German stemmers
// Hungarian stemmers
// Irish stemmer
// Italian stemmers
// Norwegian (Bokmål) stemmers
// Norwegian (Nynorsk) stemmers
// Portuguese stemmers
// Russian stemmers
// Spanish stemmers
// Sorani Kurdish stemmer
// Swedish stemmers
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// In order to allow chained synonym filters, we return IDENTITY here to
// ensure that synonyms don't get applied to the synonym map itself,
// which doesn't support stacked input tokens
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// clone the term, and add to the set of seen terms.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Sample Format for the type table:
// $ => DIGIT
// % => DIGIT
// . => DIGIT
// \u002C => DIGIT
// \u200D => ALPHANUM
// If set, causes parts of words to be generated: "PowerShot" => "Power" "Shot"
// If set, causes number subwords to be generated: "500-42" => "500" "42"
// 1, causes maximum runs of word parts to be catenated: "wi-fi" => "wifi"
// If set, causes maximum runs of number parts to be catenated: "500-42" => "50042"
// If set, causes all subword parts to be catenated: "wi-fi-4000" => "wifi4000"
// 1, causes "PowerShot" to be two tokens; ("Power-Shot" remains two parts regards)
// If set, includes original words in subwords: "500-42" => "500" "42" "500-42"
// 1, causes "j2se" to be three tokens; "j" "2" "se"
// If set, causes trailing "'s" to be removed for each subword: "O'Neil's" => "O", "Neil"
// If not null is the set of tokens to protect from being delimited
/*
//www.apache.org/licenses/LICENSE-2.0
// Sample Format for the type table:
// $ => DIGIT
// % => DIGIT
// . => DIGIT
// \u002C => DIGIT
// \u200D => ALPHANUM
// If set, causes parts of words to be generated: "PowerShot" => "Power" "Shot"
// If set, causes number subwords to be generated: "500-42" => "500" "42"
// 1, causes maximum runs of word parts to be catenated: "wi-fi" => "wifi"
// If set, causes maximum runs of number parts to be catenated: "500-42" => "50042"
// If set, causes all subword parts to be catenated: "wi-fi-4000" => "wifi4000"
// 1, causes "PowerShot" to be two tokens; ("Power-Shot" remains two parts regards)
// If set, includes original words in subwords: "500-42" => "500" "42" "500-42"
// 1, causes "j2se" to be three tokens; "j" "2" "se"
// If set, causes trailing "'s" to be removed for each subword: "O'Neil's" => "O", "Neil"
// If not null is the set of tokens to protect from being delimited
// source => type
/**
// ensure the table is always at least as big as DEFAULT_WORD_DELIM_TABLE for performance
/*
//www.apache.org/licenses/LICENSE-2.0
// this variable is always initialized
// read supplementary char aware with CharacterUtils
// so next offset += dataLen won't decrement offset
// use CharacterUtils here to support < 3.1 UTF-16 code unit behavior if the char based methods are gone
// if it's a token char
// start of token
// check if a supplementary could run out of bounds
// make sure a supplementary fits in the buffer
// buffer it, normalized
// buffer overflow! make sure to check for >= surrogate pair could break == test
// at non-Letter w/ chars
// return 'em
// set final offset
// make sure to reset the IO buffer!!
/*
//www.apache.org/licenses/LICENSE-2.0
// NORELEASE we should prevent the usage on indices created after 7.0 in order to be able to remove in 8
/*
//www.apache.org/licenses/LICENSE-2.0
// but the multi-term aware component still emits a single token
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// no exception
/*
//www.apache.org/licenses/LICENSE-2.0
// category Ll
// category Ll
// category Lu
// category Lu
// category Lm
// category Lo
// category Lt
// nbsp
// ARABIC-INDIC DIGIT ONE
// category Sc
// category Sm
// category Sm
// category Sk
// category Sc
// category Ps
// category Pe
// category Pc
// category Po
// category Pd
// category Pi
// category Pf
/*
//www.apache.org/licenses/LICENSE-2.0
// This config outputs different size of ngrams so graph analysis is disabled
// This config uses only bigrams so graph analysis is enabled
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// tokenizers.put("lowercase", XLowerCaseTokenizerFactory.class);
// this filter is not exposed and should only be used internally
// TODO: these charfilters are not yet exposed: useful?
// handling of zwnj for persian
// TODO drop aliases once they are moved to module
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// tests for prebuilt tokenizer
// same batch of tests for custom tokenizer definition in the settings
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// parsed queries for "text_shingle_unigram:(foo bar baz)" with query parsers
// that ignores position length attribute
// parsed query for "text_shingle_unigram:\"foo bar baz\" with query parsers
// that ignores position length attribute
// parsed query for "text_shingle:(foo bar baz)
// parsed query for "text_shingle:"foo bar baz"
/*
//www.apache.org/licenses/LICENSE-2.0
// Before 7.3 we return ngrams of length 1 only
// Check deprecated name as well
// Afterwards, we return ngrams of length 1 and 2, to match the default factory settings
// Check deprecated name as well, needs version before 8.0 because throws IAE after that
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// "wow that's funny" and "what the fudge" are separate side paths, in parallel with "wtf", on input:
// ... but on output, it's flattened to wtf/what/wow that's/the fudge/funny happened:
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.facebook.com http://elasticsearch.org "
//xing.com http://cnn.com http://quora.com http://twitter.com this is "
//www.facebook.com "
//elasticsearch.org http://xing.com http://cnn.com http://quora.com "
//twitter.com this is a test for highlighting feature")
//www.facebook.com "))
//www.facebook.com</em>"));
//www.facebook.com "
//elasticsearch.org http://xing.com http://cnn.com "
//quora.com http://twitter.com this is a test for highlighting "
//www.facebook.com http://elasticsearch.org "
//xing.com http://cnn.com http://quora.com http://twitter.com this "
//www.facebook.com</em> <em>http://elasticsearch.org</em> "
//xing.com</em> <em>http://cnn.com</em> http://quora.com"));
// with synonyms
/*
//www.apache.org/licenses/LICENSE-2.0
// test our none existing setup is picked up
// test our none existing setup is picked up
/*
//www.apache.org/licenses/LICENSE-2.0
// either use default mode or set "include" mode explicitly
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// jogging is not part of the keywords set, so verify that its the only stemmed word
/**
// running should match the pattern, so it should not be stemmed but sleeping should
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// with_rotation is true by default, and hash_set_size is 1, so even though the source doesn't
// have enough tokens to fill all the buckets, we still expect 512 tokens.
// despite the fact that bucket_count is 2 and hash_set_size is 1,
// because with_rotation is false, we only expect 1 token here.
/*
//www.apache.org/licenses/LICENSE-2.0
// Duplicates are removed
/*
//www.apache.org/licenses/LICENSE-2.0
// no exception
// no exception
// Make sure that pretokenization works well and that it can be used even with token chars which are supplementary characters
// Make sure that pretokenization works well and that it can be used even with token chars which are supplementary characters
/*`
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Split on non-letter pattern, do not lowercase, no stopwords
// split on non-letter pattern, lowercase, english stopwords
/**
// Split on whitespace patterns, do not lowercase, no stopwords
// Split on whitespace patterns, lowercase, english stopwords
/**
// Split on comma, do not lowercase, no stopwords
// split on comma, lowercase, english stopwords
/**
// 5000 a's
// a space
// 2000 b's
// Split on whitespace patterns, do not lowercase, no stopwords
/** blast some random strings through the analyzer */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Before 7.3 we don't adjust offsets
// Afger 7.3 we do adjust offsets
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//we've reached end of string, we need to handle last field
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//tools.ietf.org/html/rfc4180">RCF 4180</a> with one exception: whitespaces are
/*
//www.apache.org/licenses/LICENSE-2.0
// 1356138046000
// support the 6.x BWC compatible way of parsing java 8 dates
// if UTC zone is set here, the time zone specified in the format will be ignored, leading to wrong dates
// if there is no year nor year-of-era, we fall back to the current one and
// fill the rest of the date up with the parsed date
/*
//www.apache.org/licenses/LICENSE-2.0
// Date can be specified as a string or long:
// Not use Objects.toString(...) here, because null gets changed to "null" which may confuse some date parsers
//try the next parser and keep track of the exceptions
// use UTC instead of Z is string representation of UTC, so behaviour is the same between 6.x and 7
/*
//www.apache.org/licenses/LICENSE-2.0
// Not use Objects.toString(...) here, because null gets changed to "null" which may confuse some date parsers
//try the next parser and keep track of the exceptions
/*
//www.apache.org/licenses/LICENSE-2.0
//package private members for testing
/*
//www.apache.org/licenses/LICENSE-2.0
// check whether we actually can expand the field in question into an object field.
// part of the path may already exist and if part of it would be a value field (string, integer etc.)
// then we can't override it with an object field and we should fail with a good reason.
// IngestDocument#setFieldValue(...) would fail too, but the error isn't very understandable
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we are on the same thread and we need to fork to another thread to avoid recursive stack overflow on a single thread
// only fork after 10 recursive calls, then fork every 10 to keep the number of threads down
// we are on a different thread (we went asynchronous), it's safe to recurse
// or we have recursed less then 10 times with the same thread, it's safe to recurse
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// shortcut, no need to create a string builder and go through each char
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We fail here if the target field point to an array slot that is out of range.
// If we didn't do this then we would fail if we set the value in the target_field
// and then on failure processors would not see that value we tried to rename as we already
// removed it.
// setting the value back to the original field shouldn't as we just fetched it from that field:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// verify script is able to be compiled before successfully creating processor.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// most results types are Strings
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// here any metadata field value becomes a list, which won't make sense in most of the cases,
// but support for append is streamlined like for set so we test it
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//largest value that allows all results < Long.MAX_VALUE bytes
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//verify that only proper boolean values are supported and we are strict about it
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//all good
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Since testJavaPatternLocale is muted in FIPS mode, test that we can correctly parse dates in english
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// foo already exists and if a leaf field and therefor can't be replaced by a map field:
// so because foo is no branch field but a value field the `foo.bar` field can't be expanded
// into [foo].[bar], so foo should be renamed first into `[foo].[bar]:
//asking to expand a (literal) field that is not present in the source document
//abc.def does not exist in source, so don't mutate document
//hasField returns false since it requires the expanded form, which is not expanded since we did not ask for it to be
//nothing has changed
//abc.def is not found anywhere
//asking to expand a (literal) field that does not exist, but the nested field does exist
//foo.bar, the literal value (as opposed to nested value) does not exist in source, so don't mutate document
//hasField returns true because the nested/expanded form exists in the source document
//nothing changed
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//execute runnable on same thread for simplicity. some tests will override this and actually run async
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Ideally I like this test to live in the server module, but otherwise a large part of the ScriptProcessor
// ends up being copied into this test.
// Prior to making this ScriptService implement ClusterStateApplier instead of ClusterStateListener,
// pipelines with a script processor failed to load causing these pipelines and pipelines that were
// supposed to load after these pipelines to not be available during ingestion, which then causes
// the next index request in this test to fail.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//the set failed, the old field has not been removed
//the set failed, the old field has not been removed
// for fun lets try to restore it (which don't allow today)
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//used to verify that there are no conflicts between subsequent fields going to be added
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// cache the database type so that we do not re-read it on every pipeline execution
/**
// read the last 512 bytes
// find the database_type header
// read the database type
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ISO 3166-2 code for country subdivisions.
// See iso.org/iso-3166-country-codes.html
// Geoip2's AddressNotFoundException is checked and due to the fact that we need run their code
// inside a PrivilegedAction code block, we are forced to catch any checked exception and rethrow
// it with an unchecked exception.
//package private for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// load the default databases
// load any custom databases
// Use iterator instead of forEach otherwise IOException needs to be caught twice...
/**
//package private for testing
//can't use cache.computeIfAbsent due to the elevated permissions for the jackson (run via the cache loader)
//intentionally non-locking for simplicity...it's OK if we re-put the same key/value in the cache during a race condition.
//only useful for testing
/**
//generated
//generated
/*
//www.apache.org/licenses/LICENSE-2.0
// Loading another database reader instances, because otherwise we can't test lazy loading as the
// database readers used at class level are reused between tests. (we want to keep that otherwise running this
// test will take roughly 4 times more time)
// these are lazy loaded until first use so we expect null here
// the first ingest should trigger a database load
// these are lazy loaded until first use so we expect null here
// the first ingest should trigger a database load
// these are lazy loaded until first use so we expect null here
// the first ingest should trigger a database load
// fake the GeoIP2-City database
/*
// these are lazy loaded until first use so we expect null here
// the first ingest should trigger a database load
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/elastic/elasticsearch/issues/37342", Constants.WINDOWS);
// the geo-IP databases should not be loaded on any nodes as they are all non-ingest nodes
// start an ingest node
// the geo-IP database should not be loaded yet as we have no indexed any documents using a pipeline that has a geo-IP processor
// now the geo-IP database should be loaded on the ingest node
// the geo-IP database should still not be loaded on the non-ingest nodes
/*
//www.apache.org/licenses/LICENSE-2.0
/** Don't silently do DNS lookups or anything trappy on bogus data */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//add a key
// evict old key by adding another value
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// EMPTY is safe here because we don't use namedObject
// Only flag present in the current default regexes.yaml
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Parse the user agent in the ECS (Elastic Common Schema) format
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Copy file, leaving out the device parsers at the end
/*
//www.apache.org/licenses/LICENSE-2.0
//www.easou.com/search/spider.html)");
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ValueSource uses a rawtype
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no instance
// supported variables
// supported methods
// date-specific
/*
//www.apache.org/licenses/LICENSE-2.0
/** Extracts a portion of a date field with {@code Calendar.get()} */
// ValueSource uses a rawtype
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no instance
// supported variables
// supported methods
/*
//www.apache.org/licenses/LICENSE-2.0
/** Extracts a portion of a date field with joda time */
// ValueSource uses a rawtype
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ValueSource uses a rawtype
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// _value
// Fake the scorer until setScorer is called.
// _value isn't used in script if specialValue == null
/*
//www.apache.org/licenses/LICENSE-2.0
// Fake the scorer until setScorer is called.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Fake the scorer until setScorer is called.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Fake the scorer until setScorer is called.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// classloader created here
// snapshot our context here, we check on behalf of the expression
// NOTE: validation is delayed to allow runtime vars, and we don't have access to per index stuff here
// NOTE: if we need to do anything complicated with bindings in the future, we can just extend Bindings,
// instead of complicating SimpleBindings (which should stay simple)
// delegate valuesource creation based on field's type
// there are three types of "fields" to expressions, and each one has a different "api" of variables and methods.
// we defer "binding" of variables until here: give context for that variable
// NOTE: if we need to do anything complicated with bindings in the future, we can just extend Bindings,
// instead of complicating SimpleBindings (which should stay simple)
// delegate valuesource creation based on field's type
// there are three types of "fields" to expressions, and each one has a different "api" of variables and methods.
// we defer "binding" of variables until here: give context for that variable
// NOTE: if we need to do anything complicated with bindings in the future, we can just extend Bindings,
// instead of complicating SimpleBindings (which should stay simple)
// noop: _value is special for aggregations, and is handled in ExpressionScriptBindings
// TODO: if some uses it in a scoring expression, they will get a nasty failure when evaluating...need a
// way to know this is for aggregations and so _value is ok to have...
// delegate valuesource creation based on field's type
// there are three types of "fields" to expressions, and each one has a different "api" of variables and methods.
// we defer "binding" of variables until here: give context for that variable
/**
//github.com/elastic/elasticsearch/issues/26429.
// NOTE: if we need to do anything complicated with bindings in the future, we can just extend Bindings,
// instead of complicating SimpleBindings (which should stay simple)
// noop: _value is special for aggregations, and is handled in ExpressionScriptBindings
// TODO: if some uses it in a scoring expression, they will get a nasty failure when evaluating...need a
// way to know this is for aggregations and so _value is ok to have...
// delegate valuesource creation based on field's type
// there are three types of "fields" to expressions, and each one has a different "api" of variables and methods.
// we defer "binding" of variables until here: give context for that variable
/**
// .value is the default for doc['field'], its optional.
// true if the variable is of type doc['field'].date.xxx
// access to the .date "object" within the field
// geo
// date object
// date field itself
// number
// TODO: document and/or error if params contains _score?
// NOTE: by checking for the variable in params first, it allows masking document fields with a global constant,
// but if we were to reverse it, we could provide a way to supply dynamic defaults for documents missing the field?
// NOTE: by checking for the variable in vars first, it allows masking document fields with a global constant,
// but if we were to reverse it, we could provide a way to supply dynamic defaults for documents missing the field?
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Fake the scorer until setScorer is called.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ValueSource uses a rawtype
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ValueSource uses a rawtype
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no instance
// supported variables
// supported methods
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ValueSource uses a rawtype
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ValueSource uses a rawtype
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no instance
// supported variables
// supported methods
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: please convert to unit tests!
// make sure DF is consistent
// make sure DF is consistent
// make sure count() works for missing
// make sure .empty works in the same way
// a = int, b = double, c = long
// i.e. _value for aggregations
// specifically to test a script w/o _value
// i.e. expression script for term aggregations, which is not allowed
// shards that don't have docs with the "text" field will not fail,
// so we may or may not get a total failure
// at least the shards containing the docs should have failed
// test to make sure expressions are not allowed to be used as update scripts
// test to make sure expressions are allowed to be used for reduce in pipeline aggregations
// access .lat
// access .lon
// access .empty
// call haversin
// access .value
// access .empty
// ternary operator
// vip's have a 50% discount
/*
//www.apache.org/licenses/LICENSE-2.0
//TODO: please convert to unit tests!
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Variable name is in plain text and has type WriteCode
/**
// Do not handle as JSON
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// if it's not a number it is as if the key doesn't exist
// if it's not a number it is as if the key doesn't exist
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
//The MultiSearchTemplateResponse is identical to the multi search response so we reuse the parsing logic in multi search response
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/** Factory template. */
/**
// crazy reflection here
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Creates the render template request
/*
//www.apache.org/licenses/LICENSE-2.0
// Creates the search request with all required params
// Creates the search template request
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//convert the template to json which is the only supported XContentType (see CustomMustacheFactory#createEncoder)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Contains the source of the rendered template **/
/** Contains the search response, if any **/
//we can assume the template is always json as we convert it before compiling it
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Search #1
// Search #2 (Simulate is true)
// Search #3
// Search #4 (Fail because of unknown index)
// Search #5 (Simulate is true)
/*
//www.apache.org/licenses/LICENSE-2.0
// Create a random request.
// scroll is not supported in the current msearch or msearchtemplate api, so unset it:
// batched reduce size is currently not set-able on a per-request basis as it is a query string parameter only
//Serialize the request
//Deserialize the request
// For object equality purposes need to set the search requests' source to non-null
// Compare the deserialized request object with the original request object
// Finally, serialize the deserialized request to compare JSON equivalence (in case Object.equals() fails to reveal a discrepancy)
/*
//www.apache.org/licenses/LICENSE-2.0
// Creating a minimal response is OK, because SearchResponse self
// is tested elsewhere.
// Creating a minimal response is OK, because SearchResponse is tested elsewhere.
/**
//with random fields insertion in the inner exceptions, some random stuff may be parsed back as metadata,
//but that does not bother our assertions, as we only want to test that we don't break.
//exceptions are not of the same type whenever parsed back
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Sets can come out in any order
// HashSet iteration order isn't fixed
//www.elastic.co",
//localhost:9200/{{#join}}indices{{/join}}/_stats{{/url}}", params,
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Relates to #6318
/**
/**
/**
// Relates to #10397
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Unclosed template id
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The {@link ClassLoader} used to look up the whitelisted Java classes, constructors, methods, and fields. */
/** The {@link List} of all the whitelisted Painless classes. */
/** The {@link List} of all the whitelisted static Painless methods. */
/** The {@link List} of all the whitelisted Painless class bindings. */
/** The {@link List} of all the whitelisted Painless instance bindings. */
/** Standard constructor. All values must be not {@code null}. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Information about where this class was white-listed from. */
/** The Java class name this class represents. */
/** The {@link List} of whitelisted ({@link WhitelistConstructor}s) available to this class. */
/** The {@link List} of whitelisted ({@link WhitelistMethod}s) available to this class. */
/** The {@link List} of whitelisted ({@link WhitelistField}s) available to this class. */
/** The {@link Map} of annotations for this class. */
/** Standard constructor. All values must be not {@code null}. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Information about where this constructor was whitelisted from. */
/** The Java class name this class binding targets. */
/** The method name for this class binding. */
/** The canonical type name for the return type. */
/**
/** The {@link Map} of annotations for this class binding. */
/** Standard constructor. All values must be not {@code null}. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Information about where this constructor was whitelisted from. */
/**
/** The {@link Map} of annotations for this constructor. */
/** Standard constructor. All values must be not {@code null}. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Information about where this method was whitelisted from. */
/** The field name used to look up the field reflection object. */
/** The canonical type name for the field which can be used to look up the Java field through reflection. */
/** The {@link Map} of annotations for this field. */
/** Standard constructor.  All values must be not {@code null}. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Information about where this constructor was whitelisted from. */
/** The Java instance this instance binding targets. */
/** The method name for this class binding. */
/** The canonical type name for the return type. */
/**
/** The {@link Map} of annotations for this instance binding. */
/** Standard constructor. All values must be not {@code null}. */
/*
//www.apache.org/licenses/LICENSE-2.0
/** Loads and creates a {@link Whitelist} from one to many text files. */
/**
/**
// Execute a single pass through the whitelist text files.  This will gather all the
// constructors, methods, augmented methods, and fields for each whitelisted class.
// Skip any lines that are either blank or comments.
// Handle a new class by resetting all the variables necessary to construct a new WhitelistClass for the whitelist.
// Expects the following format: 'class' ID annotations? '{' '\n'
// Ensure the final token of the line is '{'.
// Parse the Java class name and annotations if they exist.
// Reset all the constructors, methods, and fields to support a new class.
// Ensure the final token of the line is '{'.
// Handle the end of a definition and reset all previously gathered values.
// Expects the following format: '}' '\n'
// Create a new WhitelistClass with all the previously gathered constructors, methods,
// augmented methods, and fields, and add it to the list of whitelisted classes.
// Reset the parseType.
// Handle static import definition types.
// Expects the following format: ID ID '(' ( ID ( ',' ID )* )? ')' ( 'from_class' | 'bound_to' ) ID annotations? '\n'
// Mark the origin of this parsable object.
// Parse the tokens prior to the method parameters.
// Based on the number of tokens, look up the Java method name.
// Parse the method parameters.
// Handle the case for a method with no parameters.
// Parse the annotations if they exist.
// Parse the static import type and class.
// Based on the number of tokens, look up the type and class.
// Add a static import method or binding depending on the static import type.
// Handle class definition types.
// Mark the origin of this parsable object.
// Handle the case for a constructor definition.
// Expects the following format: '(' ( ID ( ',' ID )* )? ')' annotations? '\n'
// Parse the constructor parameters.
// Handle the case for a constructor with no parameters.
// Parse the annotations if they exist.
// Handle the case for a method or augmented method definition.
// Expects the following format: ID ID? ID '(' ( ID ( ',' ID )* )? ')' annotations? '\n'
// Parse the tokens prior to the method parameters.
// Based on the number of tokens, look up the Java method name and if provided the Java augmented class.
// Parse the method parameters.
// Handle the case for a method with no parameters.
// Parse the annotations if they exist.
// Handle the case for a field definition.
// Expects the following format: ID ID annotations? '\n'
// Parse the annotations if they exist.
// Parse the field tokens.
// Ensure the correct number of tokens.
// Ensure all classes end with a '}' token before the end of the file.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Information about where this method was whitelisted from. */
/**
/** The method name used to look up the method reflection object. */
/**
/**
/** The {@link Map} of annotations for this method. */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//" + System.getProperty("cluster.uri") + "/_scripts/painless/_context").openConnection();
//" + System.getProperty("cluster.uri") + "/_scripts/painless/_context?context=" + contextName).openConnection();
// This file is auto-generated. Do not edit.");
// temporary fix to not print org.elasticsearch.script.ScoreScript parameter until
// class instance bindings are created and the information is appropriately added to the context info classes
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: remove this when the transition from Joda to Java datetimes is completed
// TODO: remove this when the transition from Joda to Java datetimes is completed
// TODO: In the rare case we still haven't reached a correct promotion we need
// TODO: to calculate the highest upper bound for the two types and return that.
// TODO: However, for now we just return objectType that may require an extra cast.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Setup the code privileges.
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Catch everything to let the user know this is something caused internally.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: Once Java has a factory for those in java.lang.invoke.MethodHandles, use it:
/** Helper class for isolating MethodHandles and methods to get the length of arrays
//bugs.openjdk.java.net/browse/JDK-8156915
// getArrayLength() methods are are actually used, javac just does not know :)
/** pointer to Map.get(Object) */
/** pointer to Map.put(Object,Object) */
/** pointer to List.get(int) */
/** pointer to List.set(int,Object) */
/** pointer to Iterable.iterator() */
/** pointer to {@link Def#mapIndexNormalize}. */
/** pointer to {@link Def#listIndexNormalize}. */
/** factory for arraylength MethodHandle (intrinsic) from Java 9 (pkg-private for tests) */
// lookup up the factory for arraylength MethodHandle (intrinsic) from Java 9:
// https://bugs.openjdk.java.net/browse/JDK-8156915
/** Hack to rethrow unknown Exceptions from {@link MethodHandle#invokeExact}: */
/** Returns an array length getter MethodHandle for the given array type */
/**
// simple case: no lambdas
// convert recipe string to a bitset for convenience (the code below should be refactored...)
// otherwise: first we have to compute the "real" arity. This is because we have extra arguments:
// e.g. f(a, g(x), b, h(y), i()) looks like f(a, g, x, b, h, y, i).
// lookup the method with the proper arity, then we know everything (e.g. interface types of parameters).
// based on these we can finally link any remaining lambdas that were deferred.
// its a functional reference, replace the argument with an impl
// decode signature of form 'type.call,2'
// the implementation is strongly typed, now that we know the interface type,
// we have everything.
// the interface type is now known, but we need to get the implementation.
// this is dynamically based on the receiver type (and cached separately, underneath
// this cache). It won't blow up since we never nest here (just references)
// the filter now ignores the signature (placeholder) on the stack
/**
/** Returns a method handle to an implementation of clazz, given method reference signature. */
/**
// first try whitelist
// special case: arrays, maps, and lists
// arrays expose .length as a read-only getter
// maps allow access like mymap.key
// wire 'key' as a parameter, its a constant in painless
// lists allow access like mylist.0
// wire '0' (index) as a parameter, its a constant. this also avoids
// parsing the same integer millions of times!
/**
// first try whitelist
// special case: maps, and lists
// maps allow access like mymap.key
// wire 'key' as a parameter, its a constant in painless
// lists allow access like mylist.0
// wire '0' (index) as a parameter, its a constant. this also avoids
// parsing the same integer millions of times!
/**
// noop so that mymap[key] doesn't do funny things with negative keys
/**
// maps allow access like mymap[key]
/**
// maps allow access like mymap[key]
/** Helper class for isolating MethodHandles and methods to get iterators over arrays
// iterator() methods are are actually used, javac just does not know :)
/**
// Conversion methods for def to primitive types.
// Conversion methods for def to boxed types.
// TODO: remove this when the transition from Joda to Java datetimes is completed
/**
/**
/**
// normalizeIndex() methods are are actually used, javac just does not know :)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//code.google.com/archive/p/jsr292-cookbook/, BSD license)
// NOTE: this class must be public, because generated painless classes are in a different classloader,
// and it needs to be accessible by that code.
// no instance!
// NOTE: these must be primitive types, see https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-6.html#jvms-6.5.invokedynamic
/** static bootstrap parameter indicating a dynamic method call, e.g. foo.bar(...) */
/** static bootstrap parameter indicating a dynamic load (getter), e.g. baz = foo.bar */
/** static bootstrap parameter indicating a dynamic store (setter), e.g. foo.bar = baz */
/** static bootstrap parameter indicating a dynamic array load, e.g. baz = foo[bar] */
/** static bootstrap parameter indicating a dynamic array store, e.g. foo[bar] = baz */
/** static bootstrap parameter indicating a dynamic iteration, e.g. for (x : y) */
/** static bootstrap parameter indicating a dynamic method reference, e.g. foo::bar */
/** static bootstrap parameter indicating a unary math operator, e.g. ~foo */
/** static bootstrap parameter indicating a binary math operator, e.g. foo / bar */
/** static bootstrap parameter indicating a shift operator, e.g. foo &gt;&gt; bar */
/** static bootstrap parameter indicating a request to normalize an index for array-like-access */
// constants for the flags parameter of operators
/**
/**
/**
/**
/** maximum number of types before we go megamorphic */
// pkg-protected for testing
/**
/**
/**
// it's too stupid that we cannot throw checked exceptions... (use rethrow puzzler):
/**
// we revert the whole cache and build a new megamorphic one
/**
/**
// shifts are treated as unary, as java allows long arguments without a cast (but bits are ignored)
// can handle nulls, casts if supported
// static cast to the return type
// dynamic cast to the receiver's type
/**
// caching defeated
// for math operators: WrongMethodType can be confusing. convert into a ClassCastException if they screw up.
// some binary operators support nulls, we handle them separate
// case 1: only the receiver is unknown, just check that
// case 2: only the argument is unknown, just check that
// case 3: check both receiver and argument
// unary operator
// very special cases, where even the receiver can be null (see JLS rules for string concat)
// we wrap + with an NPE catcher, and use our generic method in that case.
/**
/**
/**
/**
//docs.oracle.com/javase/specs/jvms/se7/html/jvms-6.html#jvms-6.5.invokedynamic
// validate arguments
// "function-call" like things get a polymorphic cache
// operators get monomorphic cache, with a generic impl for a fallback
// we just don't need it anywhere else.
// we just don't need it anywhere else.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Unary not: only applicable to integral types
// unary negation and plus: applicable to all numeric types
// multiplication/division/remainder/subtraction: applicable to all integer types
// addition: applicable to all numeric types.
// additionally, if either type is a string, the other type can be any arbitrary type (including null)
// eq: applicable to any arbitrary type, including nulls for both arguments!!!
// comparison operators: applicable for any numeric type
// helper methods to convert an integral according to numeric promotion
// this is used by the generic code for bitwise and shift operators
// bitwise operators: valid only for integral types
// shift operators, valid for any integral types, but does not promote.
// we implement all shifts as long shifts, because the extra bits are ignored anyway.
/**
/** Unary promotion. All Objects are promoted to Object. */
// if either is a non-primitive type -> Object.
// always promoted to integer
/** Binary promotion. */
// if either is a non-primitive type -> Object.
// boolean -> boolean
// ordinary numeric promotion
/** Returns an appropriate method handle for a unary or shift operator, based only on the receiver (LHS) */
/** Returns an appropriate method handle for a binary operator, based on promotion of the LHS and RHS arguments */
/** Returns a generic method handle for any operator, that can handle all valid signatures, nulls, corner cases */
/**
/**
/** Slowly returns a Number for o. Just for supporting dynamicCast */
/** Looks up generic method, with a dynamic cast to the receiver's type. (compound assignment) */
// adapt dynamic receiver cast to the generic method
// drop the RHS parameter
// combine: f(x,y) -> g(f(x,y), x, y);
/** Looks up generic method, with a dynamic cast to the specified type. (explicit assignment) */
// adapt dynamic cast to the generic method
// bind to the boxed type
/** Forces a cast to class A for target (only if types differ) */
// don't do a conversion if types are the same. explicitCastArguments has this opto,
// but we do it explicitly, to make the boolean check simpler
// we don't allow the to/from boolean conversions of explicitCastArguments
// null return values are not possible for our arguments.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** functional interface method name */
/** functional interface method signature */
/** class of the delegate method to be called */
/** whether a call is made on a delegate interface */
/** the invocation type of the delegate method */
/** the name of the delegate method */
/** delegate method signature */
/** factory (CallSite) method signature */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Create a new Globals from the set of statement boundaries */
/** Adds a new constant initializer to be written */
/** Returns the current initializers */
/** Returns the set of statement boundaries */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//cr.openjdk.java.net/~briangoetz/lambda/lambda-translation.html
/**
/**
/**
/**
/**
// Handles the special case where a method reference refers to a ctor (we need a static wrapper method):
// replace the delegate with our static wrapper:
/**
/**
/**
/**
// Add a factory method, if lambda takes captures.
// @uschindler says: I talked with Rémi Forax about this. Technically, a plain ctor
// and a MethodHandle to the ctor would be enough - BUT: Hotspot is unable to
// do escape analysis through a MethodHandles.findConstructor generated handle.
// Because of this we create a factory method. With this factory method, the
// escape analysis can figure out that everything is final and we don't need
// an instance, so it can omit object creation on heap!
/**
/**
// Loads any captured variables onto the stack.
// Loads any passed in arguments onto the stack.
// Handles the case for a lambda function or a static reference method.
// interfaceMethodType and delegateMethodType both have the captured types
// inserted into their type signatures.  This later allows the delegate
// method to be invoked dynamically and have the interface method types
// appropriately converted to the delegate method types.
// Example: Integer::parseInt
// Example: something.each(x -> x + 1)
// Handles the case for a virtual or interface reference method with no captures.
// delegateMethodType drops the 'this' parameter because it will be re-inserted
// when the method handle for the dynamically invoked delegate method is created.
// Example: Object::toString
// Handles the case for a virtual or interface reference method with 'this'
// captured. interfaceMethodType inserts the 'this' type into its
// method signature. This later allows the delegate
// method to be invoked dynamically and have the interface method types
// appropriately converted to the delegate method types.
// Example: something::toString
/**
/**
// DEBUG:
// new ClassReader(classBytes).accept(new TraceClassVisitor(new PrintWriter(System.out)), ClassReader.SKIP_DEBUG);
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Reserved word: loop counter */
/** Reserved word: unused */
/** Set of reserved keywords. */
/** Creates a new local variable scope (e.g. loop) inside the current scope */
/**
// TODO: allow non-captures to be r/w:
// boolean isCapture = i < captureCount;
// currently, this cannot be allowed, as we swap in real types,
// but that can prevent a store of a different type...
// Loop counter to catch infinite loops.  Internal use only.
/** Creates a new function scope inside the current scope */
// Loop counter to catch infinite loops.  Internal use only.
/** Creates a new main method scope */
// This reference. Internal use only.
// Method arguments
// Loop counter to catch infinite loops.  Internal use only.
/** Creates a new program scope as the root of all scopes */
/** Checks if a variable exists or not, in this scope or any parents. */
/** Accesses a variable. This will throw IAE if the variable does not exist */
/** Creates a new variable. Throws IAE if the variable has already been defined (even in a parent) or reserved. */
/** Return type of this scope (e.g. int, if inside a function that returns int) */
/** Returns the top-level program scope. */
///// private impl
// parent scope
// return type of this scope
// keywords for this scope
// next slot number to assign
// variable name -> variable
/**
/**
/** Returns the parent scope */
/** Looks up a variable at this scope only. Returns null if the variable does not exist. */
/** Defines a variable at this scope internally. */
// TODO: check result
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// This maximum length is theoretically 65535 bytes, but as it's CESU-8 encoded we don't know how large it is in bytes, so be safe
/** Computes the file name (mostly important for stacktraces) */
// its an anonymous script, include at least a portion of the source to help identify which one it is
// but don't create stacktraces with filenames that contain newlines or huge names.
// truncate to the first newline
// truncate to our limit
// if we truncated, make it obvious
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// ensure we don't have duplicate stuff going in here. can catch bugs
// (e.g. nodes get assigned wrong offsets by antlr walker)
/**
// TODO: maybe track these in bitsets too? this is trickier...
// TODO: remove this when the transition from Joda to Java datetimes is completed
// TODO: remove this when the transition from Joda to Java datetimes is completed
// TODO: remove this when the transition from Joda to Java datetimes is completed
/**
/** Starts a new string concat.
// Java 9+: we just push our argument collector onto deque
// nothing added to stack
// Java 8: create a StringBuilder in bytecode
// StringBuilder on stack
// Java 9+: record type information
// prevent too many concat args.
// If there are too many, do the actual concat:
// add the return value type as new first param for next concat:
// Java 8: push a StringBuilder append
// Java 9+: use type information and push invokeDynamic
// Java 8: call toString() on StringBuilder
/** Writes a dynamic binary instruction: returnType, lhs, and rhs can be different */
// if either side is primitive, then the + operator should always throw NPE on null,
// so we don't need a special NPE guard.
// otherwise, we need to allow nulls for possible string concatenation.
/** Writes a static binary instruction */
/**
// invokeStatic assumes that the owner class is not an interface, so this is a
// special case for interfaces where the interface method boolean needs to be set to
// true to reference the appropriate class constant when calling a static interface
// method since java 8 did not check, but java 9 and 10 do
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
// Moving Function Pipeline Agg
// Functions used for scoring docs
// we might have a context that only uses the base whitelists, so would not have been filled in by reloadSPI
// this is a hack to bind the painless script engine in guice (all components are added to guice), so that
// the painless context api. this is a temporary measure until transport actions do no require guice
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// create a script stack: this is just the script portion
// found the script portion
// offset is 1 based, line numbers must be!
// should never happen unless we hit exc in ctor prologue...
// TODO: if this is still too long, truncate and use ellipses
// but filter our own internal stacks (e.g. indy bootstrap)
/** returns true for methods that are part of the runtime */
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
/**
/**
/**
// Check we ourselves are not being called by unprivileged code.
// Create our loader (which loads compiled code with no permissions).
/**
/**
// Catch everything to let the user know this is something caused internally.
// Drop all permissions to actually compile the code itself.
// Note that it is safe to catch any of the following errors since Painless is stateless.
// Use the default settings.
// Use custom settings specified by params.
// Except regexes enabled - this is a node level setting and can't be changed in the request.
// create a script stack: this is just the script portion
// found the script portion
// offset is 1 based, line numbers must be!
// very simple heuristic: +/- 25 chars. can be improved later.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Find the main method and the uses$argName methods
// Look up the argument
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** 
// TODO: we should really try to get this fixed in ASM!
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: remove this when the transition from Joda to Java datetimes is completed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// All of these types are caught by the main method and rethrown as ScriptException
// TODO: remove this when the transition from Joda to Java datetimes is completed
/**
// TODO: remove this when the transition from Joda to Java datetimes is completed
/** invokedynamic bootstrap for lambda expression/method references */
/** dynamic invokedynamic bootstrap for indy string concats (Java 9+) */
// ensure it is there:
// not Java 9 - we set it null, so MethodWriter uses StringBuilder:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// For testing only:
/** Return the parameters for this script. */
// Forking a thread here, because only light weight operations should happen on network thread and
// Creating a in-memory index is not light weight
// TODO: is MANAGEMENT TP the right TP? Right now this is an admin api (see action name).
// Consume the first (and only) match.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/* Use a simple heuristic to guess if the unrecognized characters were trying to be a string but has a broken escape sequence.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ANTLR GENERATED CODE: DO NOT EDIT
/**
/**
//en.wikipedia.org/wiki/The_lexer_hack">The lexer hack</a>.
/**
//\6\2FFHHffhh\4\2$$^^\4\2))^^\3\2\f"+
// ANTLR GENERATED CODE: DO NOT EDIT
/**
// ANTLR GENERATED CODE: DO NOT EDIT
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// ANTLR GENERATED CODE: DO NOT EDIT
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Diagnostic listener invokes syntaxError on other listeners for ambiguity issues,
// a second listener to fail the test when the above happens.
// Enable exact ambiguity detection (costly). we enable exact since its the default for
// DiagnosticErrorListener, life is too short to think about what 'inexact ambiguity' might mean.
// Strip the leading and trailing quotes and replace the escape sequences with their literal equivalents
// single expression
/*
//www.apache.org/licenses/LICENSE-2.0
/** Additional methods added to classes. These must be static methods with receiver as first argument */
// static methods only!
/** Exposes List.size() as getLength(), so that .length shortcut works on lists */
/** Exposes Matcher.group(String) as namedGroup(String), so it doesn't conflict with group(int) */
// some groovy methods on iterable
// see http://docs.groovy-lang.org/latest/html/groovy-jdk/java/lang/Iterable.html
/** Iterates over the contents of an iterable, and checks whether a predicate is valid for at least one element. */
/** Converts this Iterable to a Collection. Returns the original Iterable if it is already a Collection. */
/** Converts this Iterable to a List. Returns the original Iterable if it is already a List. */
/** Counts the number of occurrences which satisfy the given predicate from inside this Iterable. */ 
// instead of covariant overrides for every possibility, we just return receiver as 'def' for now
// that way if someone chains the calls, everything works.
/** Iterates through an Iterable, passing each item to the given consumer. */
/** 
/**
/**
/**
/**
/**
/**
// some groovy methods on collection
// see http://docs.groovy-lang.org/latest/html/groovy-jdk/java/util/Collection.html
/**
/**
/**
/**
/**
/**
/**
// some groovy methods on map
// see http://docs.groovy-lang.org/latest/html/groovy-jdk/java/util/Map.html
/**
/**
/** Counts the number of occurrences which satisfy the given predicate from inside this Map */ 
/** Iterates through a Map, passing each item to the given consumer. */
/**
/**
/**
// try to preserve some properties of the receiver (see the groovy javadocs)
/**
/**
/**
/**
// try to preserve some properties of the receiver (see the groovy javadocs)
// CharSequence augmentation
/**
// CharSequqence's toString is *supposed* to always return the characters in the sequence as a String
/**
// CharSequqence's toString is *supposed* to always return the characters in the sequence as a String
/**
/**
/**
/**
/**
// Check if it's even possible to perform a split
// List of string segments we have found
// Keep track of where we are in the string
// indexOf(tok, startPos) is faster than creating a new search context ever loop with substring(start, end)
// Loop until we hit the limit or forever if we are passed in less than one (signifying no limit)
// If Integer.MIN_VALUE is passed in, it will still continue to loop down to 1 from MAX_VALUE
// This edge case should be fine as we are limited by receiver length (Integer.MAX_VALUE) even if we split at every char
// Find the next occurrence of token after current pos
// Reached the end of the string without another match
// Add the found segment to the result list
// Move our search position to the next possible location
// Add the remaining string to the result list
// O(N) or faster depending on implementation
/**
/**
/**
/**
// Dispatches to getByPathMap, getByPathList or returns obj if done. See handleMissing for dealing with missing
// elements.
// lookup existing key in map, call back to dispatch.
// lookup existing index in list, call back to dispatch.  Throws IllegalArgumentException with NumberFormatException
// if index can't be parsed as an int.
// Split path on '.', throws IllegalArgumentException for empty paths and paths ending in '.'
// A supplier that throws IllegalArgumentException
// Use defaultSupplier if at last path element, otherwise throw IllegalArgumentException
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Marker class for def type to be used during type analysis. */
/*
//www.apache.org/licenses/LICENSE-2.0
/** Create a standard cast with no boxing/unboxing. */
/** Create a cast where the original type will be unboxed, and then the cast will be performed. */
/** Create a cast where the target type will be unboxed, and then the cast will be performed. */
/** Create a cast where the original type will be boxed, and then the cast will be performed. */
/** Create a cast where the target type will be boxed, and then the cast will be performed. */
/** Create a cast where the original type is unboxed, cast to a target type, and the target type is boxed. */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// javaClassNamesToClasses is all the classes that need to be available to the custom classloader
// including classes used as part of imported methods and class bindings but not necessarily whitelisted
// individually. The values of javaClassNamesToClasses are a superset of the values of
// canonicalClassNamesToClasses.
// canonicalClassNamesToClasses is all the whitelisted classes available in a Painless script including
// classes with imported canonical names but does not include classes from imported methods or class
// bindings unless also whitelisted separately. The values of canonicalClassNamesToClasses are a subset
// of the values of javaClassNamesToClasses.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// For the case where a cast is not required and a constant is not set
// or the node is already an EConstant no changes are required to the tree.
// For the case where a cast is not required but a
// constant is set, an EConstant replaces this node
// with the constant copied from this node.  Note that
// for constants output data does not need to be copied
// from this node because the output data for the EConstant
// will already be the same.
// For the case where a cast is required and a constant is not set.
// Modify the tree to add an ECast between this node and its parent.
// The output data from this node is copied to the ECast for
// further reads done by the parent.
// For the case where a cast is required, a constant is set,
// and the constant can be immediately cast to the expected type.
// An EConstant replaces this node with the constant cast appropriately
// from the constant value defined by this node.  Note that
// for constants output data does not need to be copied
// from this node because the output data for the EConstant
// will already be the same.
// For the case where a cast is required, a constant is set,
// the constant cannot be immediately cast to the expected type,
// and this node is already an EConstant.  Modify the tree to add
// an ECast between this node and its parent.  Note that
// for constants output data does not need to be copied
// from this node because the output data for the EConstant
// will already be the same.
// For the case where a cast is required, a constant is set,
// the constant cannot be immediately cast to the expected type,
// and this node is not an EConstant.  Replace this node with
// an Econstant node copying the constant from this node.
// Modify the tree to add an ECast between the EConstant node
// and its parent.  Note that for constants output data does not
// need to be copied from this node because the output data for
// the EConstant will already be the same.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
// Below are utilities for building the toString
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Everywhere when it says 'array' below that could also be a list
// The stack after each instruction:       array, unnormalized_index
// array, unnormalized_index, unnormalized_index
// array, unnormalized_index
// negative_index, array
// array, negative_index, array
// array, negative_index, length
// array, noralized_index
// array, noralized_index
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// for shifts, the RHS is promoted independently
// shifts are promoted independently, but for the def type, we need object.
// If the lhs node is a def optimized node we update the actual type to remove the need for a cast.
// Otherwise, we must adapt the rhs type to the lhs type with a cast.
/**
// For the case where the assignment represents a String concatenation
// we must, depending on the Java version, write a StringBuilder or
// track types going onto the stack.  This must be done before the
// lhs is read because we need the StringBuilder to be placed on the
// stack ahead of any potential concatenation arguments.
// Cast the lhs to a storeable to perform the necessary operations to store the rhs.
// call the setup method on the lhs to prepare for a load/store operation
// Handle the case where we are doing a compound assignment
// representing a String concatenation.
// dup the top element and insert it
// before concat helper on stack
// read the current lhs's value
// append the lhs's value using the StringBuilder
// write the bytecode for the rhs
// check to see if the rhs has already done a concatenation
// append the rhs's value since it's hasn't already
// put the value for string concat onto the stack
// if necessary, cast the String to the lhs actual type
// if this lhs is also read
// from dup the value onto the stack
// store the lhs's value from the stack in its respective variable/field/array
// Handle the case where we are doing a compound assignment that
// does not represent a String concatenation.
// if necessary, dup the previous lhs's value
// to be both loaded from and stored to
// load the current lhs's value
// dup the value if the
// lhs is also
// read from and is a post
// increment
// if necessary cast the current lhs's value
// to the promotion type between the lhs and rhs types
// write the bytecode for the rhs
// XXX: fix these types, but first we need def compound assignment tests.
// its tricky here as there are possibly explicit casts, too.
// write the operation instruction for compound assignment
// if necessary cast the promotion type value back to the lhs's type
// dup the value if the lhs
// is also
// read from and is not a post
// increment
// store the lhs's value from the stack in its respective variable/field/array
// Handle the case for a simple write.
// write the bytecode for the rhs rhs
// dup the value if the lhs
// is also read from
// store the lhs's value from the stack in its respective variable/field/array
// Make sure "=" is in the symbol so this is easy to read at a glance
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// promoted type
// for shifts, the rhs is promoted independently
// record whether there was originally an explicit cast
// def calls adopt the wanted return value. if there was a narrowing cast,
// we need to flag that so that its done at runtime.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// user cannot call internal functions, reset to null if an internal function is found
// check to see if this class binding requires an implicit this reference
// This extra check looks for a possible match where the class binding requires an implicit this
// reference.  This is a temporary solution to allow the class binding access to data from the
// base script class without need for a user to add additional arguments.  A long term solution
// will likely involve adding a class instance binding where any instance can have a class binding
// as part of its API.  However, the situation at run-time is difficult and will modifications that
// are a substantial change if even possible to do.
// if the class binding is using an implicit this reference then the arguments counted must
// be incremented by 1 as the this reference will not be part of the arguments passed into
// the class binding call
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// dynamic implementation
// typed implementation
// static case
// dynamic interface: push captured parameter on stack
// TODO: don't do this: its just to cutover :)
// typed interface, dynamic implementation
// typed interface, typed implementation
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// do nothing
// TODO: don't do this: its just to cutover :)
// no captures
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ensure the specified type is part of the definition
// map to wrapped type for primitive types
// analyze and cast the expression
// record if the expression returns a primitive
// map to wrapped type for primitive types
// primitive types
// run the expression anyway (who knows what it does)
// discard its result
// push our result: its a primitive so it cannot be null.
// ordinary instanceof
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//cr.openjdk.java.net/~briangoetz/lambda/lambda-translation.html">
// extracted variables required to determine captures
// desugared synthetic method (lambda body)
// captured variables
// static parent, static lambda
// dynamic parent, deferred until link time
// inspect the target first, set interface method if we know it.
// we don't know anything: treat as def
// don't infer any types, replace any null types with def
// we know the method statically, infer return type and any unknown/def types
// check arity before we manipulate parameters
// for method invocation, its allowed to ignore the return value
// replace any null types with the actual type
// any of those variables defined in our scope need to be captured
// prepend capture list to lambda's arguments
// desugar lambda body into a synthetic method
// setup method reference to synthetic method
// load captures
// placeholder
// load captures
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// do nothing
// push a null instruction as a placeholder for future lambda instructions
// no captures
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing.
// Check if we can parse as a long. If so then hint that the user might prefer that.
// Ignored
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing.
// Do nothing.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// record whether there was originally an explicit cast
// Def calls adopt the wanted return value. If there was a narrowing cast,
// we need to flag that so that it's done at runtime.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing.
/*
//www.apache.org/licenses/LICENSE-2.0
/** 
/** Returns reference to resolve at link-time */
/** Returns the types of captured parameters. Can be empty */
/** Returns the number of captured parameters */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// set the known expected type
// analyze the child node to set the child's actual type
// add an implicit cast node if the child node's
// actual type is not the expected type and set the
// expression's child to the implicit cast node
// analyze the child node to set the child's actual type
// get the promotion type for the child based on
// the current operation and child's actual type
// set the expected type to the promotion type
// add an implicit cast node if the child node's
// actual type is not the expected type and set the
// expression's child to the implicit cast node
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: remove ZonedDateTime exception when JodaCompatibleDateTime is removed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// encode this parameter as a deferred reference
// TODO: remove ZonedDateTime exception when JodaCompatibleDateTime is removed
// first parameter is the receiver, we never know its type: always Object
// append each argument
// create method type from return value and arguments
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: remove ZonedDateTime exception when JodaCompatibleDateTime is removed
// Do nothing.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Note that we do not need to check after the last statement because
// there is no statement that can be unreachable after the last.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// copy protection is required because synthetic functions are
// added for lambdas/method references and analysis here is
// only for user-defined functions
// Note that we do not need to check after the last statement because
// there is no statement that can be unreachable after the last.
// Create the ClassWriter.
// Write the a method to bootstrap def calls
// Write static variables for name, source and statements used for writing exception messages
// Write the static variables used by the method to bootstrap def calls
// Write the constructor:
// Write a method to get static variable source
// Write a method to get static variable source
// Write a method to get static variable statements
// Write the method defined in the interface:
// Write all functions:
// Write all fields:
// Write the constants
// Initialize the constants in a static initializer
// Write any needsVarName methods for used variables
// End writing the class and store the generated bytes.
// We wrap the whole method in a few try/catches to handle and/or convert other exceptions to ScriptException
// if there is infinite loop protection, we do this once:
// int #loop = settings.getMaxLoopCounter()
// This looks like:
// } catch (PainlessExplainError e) {
//   throw this.convertToScriptException(e, e.getHeaders($DEFINITION))
// }
// This looks like:
// } catch (PainlessError | BootstrapMethodError | OutOfMemoryError | StackOverflowError | Exception e) {
//   throw this.convertToScriptException(e, e.getHeaders())
// }
// We *think* it is ok to catch OutOfMemoryError and StackOverflowError because Painless is stateless
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we reset the list for function scope
// note this is not stored for this node
// but still required for lambdas
/** Writes the function to given ClassVisitor. */
// if there is infinite loop protection, we do this once:
// int #loop = settings.getMaxLoopCounter()
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We must store the array and index as variables for securing slots on the stack, and
// also add the location offset to make the names unique in case of nested for each loops.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We must store the iterator as a variable for securing a slot on the stack, and
// also add the location offset to make the name unique in case of nested for each loops.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for addition operator across all types */
//TODO: NaN/Inf/overflow/...
// byte
// short
// char
// int
// long
// float
// double
// byte
// short
// char
// int
// long
// float
// double
// byte
// short
// char
// int
// long
// float
// double
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for and operator across all types */
// boolean
// byte
// short
// char
// int
// long
// boolean
// byte
// short
// char
// int
// long
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/* If this fails you *might* be missing -XX:-OmitStackTraceInFastThrow in the test jvm
// Mark the exception we are testing as suppressed so we get its stack trace.
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for working with arrays. */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// While we're here we can verify that painless correctly finds used variables
// The important thing is that this contains the opcode for returning void
// We shouldn't contain any weird "default to null" logic
// The important thing here is that we have the bytecode for returning an integer instead of an object. booleans are integers.
// The important thing here is that we have the bytecode for returning an integer instead of an object
// The important thing here is that we have the bytecode for returning a float instead of an object
// The important thing here is that we have the bytecode for returning a double instead of an object
/*
//www.apache.org/licenses/LICENSE-2.0
/** Test loads and stores with a map */
/** Test loads and stores with update script equivalent */
/** Test loads and stores with a list */
/** Test shortcut for getters with isXXXX */
/** Test list method invocation */
// TODO: remove this when the transition from Joda to Java datetimes is completed
/*
//www.apache.org/licenses/LICENSE-2.0
/** simple tests returning a constant value */
// The readability of this test suffers from having to escape `\` and `"` in java strings. Please be careful. Sorry!
// `\\` is a `\`
// `\"` is a `"` if surrounded by `"`s
// `\'` is a `'` if surrounded by `'`s
// We don't break native escapes like new line
// And we're ok with strings with multiple escape sequences
// `'` inside of a string delimited with `"` should be ok
// `"` inside of a string delimited with `'` should be ok
/** declaring variables for primitive types */
/**
// return
// assignment
// comparison
// Objects in general
//   Call
//   Call with primitive result
//   Read shortcut
// Maps
//   Call
//   Call with primitive result
//   Read shortcut
// Read shortcut
// Read shortcut
// Read shortcut
// Read shortcut
// Array
// Since you can't invoke methods on arrays we skip the toString and hashCode tests
// Results from maps (should just work but let's test anyway)
// Chains
// Assignments
// Writes, all unsupported at this point
//        assertEquals(null, exec("org.elasticsearch.painless.FeatureTestObject a = null; return a?.x"));            // Read field
//        assertEquals(null, exec("org.elasticsearch.painless.FeatureTestObject a = null; a?.x = 7; return a?.x"));  // Write field
//        assertEquals(null, exec("Map a = null; a?.other = 'wow'; return a?.other")); // Write shortcut
//        assertEquals(null, exec("def a = null; a?.other = 'cat'; return a?.other")); // Write shortcut
//        assertEquals(null, exec("Map a = ['thing': 'bar']; a.other?.cat = 'no'; return a.other?.cat"));
//        assertEquals(null, exec("def a = ['thing': 'bar']; a.other?.cat = 'no'; return a.other?.cat"));
//        assertEquals(null, exec("Map a = ['thing': 'bar']; a.other?.cat?.dog = 'wombat'; return a.other?.cat?.dog"));
//        assertEquals(null, exec("def a = ['thing': 'bar']; a.other?.cat?.dog = 'wombat'; return a.other?.cat?.dog"));
// test to ensure static interface methods are called correctly
/*
//www.apache.org/licenses/LICENSE-2.0
// One statement in the block in case that is a special case
// Two statements in the block, in case that is the general case
// tests both single break and multiple breaks used in a script
// single break test
// multiple breaks test
// multiple breaks test, ignore inner break
// multiple breaks test, ignore outer break
// tests both single break and multiple breaks used in a script
// single break test
// multiple breaks test
// multiple breaks test, ignore outer break
// multiple breaks test, ignore inner break
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// **** Docs Generator Code ****
/*
// **** Initial Mappings ****
/*
// Create Ingest to Modify Dates:
/*
// Post Generated Data:
/*
// Use script_fields API to add two extra fields to the hits
/*
// Testing only params, as I am not sure how to test Script Doc Values in painless
// Use script query request to filter documents
/*
// Use script_fields API to add two extra fields to the hits
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** quick and dirty tools for debugging */
/** compiles source to bytecode, and returns debugging output */
/** compiles to bytecode, and returns debugging output */
/*
//www.apache.org/licenses/LICENSE-2.0
// Debug.explain can explain an object
// Null should be ok
// You can't catch the explain exception
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** calls toString() on integers, twice */
// invoke with integer, needs lookup
// invoked with integer again: should be cached
// both these should be cached
// if this changes, test must be rewritten
/** test that we revert to the megamorphic classvalue cache and that it works as expected */
// mark megamorphic
// test operators with null guards
// make sure these operators work without null guards too
// for example, nulls are only legal for + if the other parameter is a String,
// and can be disabled in some circumstances.
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: remove this when the transition from Joda to Java datetimes is completed
/*
//www.apache.org/licenses/LICENSE-2.0
// horrible, sorry
// needs null guard
// still needs null guard, NPE is the wrong thing!
// a primitive argument is present: no null guard needed
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for division operator across all types */
//TODO: NaN/Inf/overflow/...
// TODO: byte,short,char
// byte
// short
// char
// int
// long
// float
// double
// byte
// short
// char
// int
// long
// float
// double
// byte
// short
// char
// int
// long
// def
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Basics
// Assigning to a primitive
// Assigning to an object
// Explicit casting
// Now some chains
// Precedence
// Yes, this is silly, but it should be valid
// Weird casts
// Combining
// TODO This could be expanded to allow primitives where neither of the two operations allow them alone
/**
/* Sadly this is a super finicky about the output of the disassembly but I think it is worth having because it makes sure that
/*
//www.apache.org/licenses/LICENSE-2.0
/* This test needs an Integer that isn't cached by Integer.valueOf so we draw one randomly. We can't use any fixed integer because
/* Now check that we use valueOf with the boxing used for comparing primitives to def. For this we need an
/* This test needs an Integer that isn't cached by Integer.valueOf so we draw one randomly. We can't use any fixed integer because
/* Now check that we use valueOf with the boxing used for comparing primitives to def. For this we need an
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Currently just a dummy class for testing a few features not yet exposed by whitelist! */
/** static method that returns true */
/** static method that returns what you ask it */
/** static method only whitelisted as a static */
/** static method with a type parameter Number */
/** empty ctor */
/** ctor with params */
/** getter for x */
/** setter for x */
/** getter for y */
/** setter for y */
/** getter for i */
/** setter for y */
/** method taking two functions! */
/** method to take in a list */
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests floating point overflow cases */
// float
// double
// float
// double
// float
// double
// float
// double
// float division, constant division, and assignment
// double division, constant division, and assignment
// float division, constant division, and assignment
// double division, constant division, and assignment
/*
//www.apache.org/licenses/LICENSE-2.0
/* Because the type isn't known and we use the lexer hack this fails to parse. I find this error message confusing but it is the one
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for explicit casts */
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for increment/decrement operators across all data types */
/** incrementing byte values */
/** incrementing char values */
/** incrementing short values */
/** incrementing integer values */
/** incrementing long values */
/** incrementing float values */
/** incrementing double values */
/** incrementing def values */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests integer overflow cases */
// byte
// short
// char
// int
// long
// byte
// short
// char
// int
// long
// byte
// char
// int
// long
// byte
// short
// cannot happen for char: unsigned
// int
// long
// byte
// short
// char
// int
// long
/*
//www.apache.org/licenses/LICENSE-2.0
/** interface ignores return value */
/** interface ignores return value */
/** Lambda parameters shouldn't be able to mask a variable already in scope */
// If we can catch the error at compile time we do
// Otherwise we convert the void into a null
/*
//www.apache.org/licenses/LICENSE-2.0
/** Runs yaml rest tests */
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for working with lists. */
// Double is implicit for decimal constants
// This exception is locale dependent so we attempt to reproduce it
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for working with maps. */
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for multiplication operator across all types */
//TODO: NaN/Inf/overflow/...
// TODO: short,byte,char
// byte
// short
// char
// int
// long
// float
// double
// byte
// short
// char
// int
// long
// float
// double
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: can we test this better? this is a port of the ExpressionsTests method.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for or operator across all types */
// boolean
// byte
// short
// char
// int
// long
// boolean
// byte
// short
// char
// int
// long
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests method overloading */
//assertEquals(2, exec("return 'abc123abc'.indexOf('c');"));
//assertEquals(8, exec("return 'abc123abc'.indexOf('c', 3);"));
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// byte/byte
// byte/char
// byte/short
// byte/int
// byte/long
// byte/float
// byte/double
// char/byte
// char/char
// char/short
// char/int
// char/long
// char/float
// char/double
// short/byte
// short/char
// short/short
// short/int
// short/long
// short/float
// short/double
// int/byte
// int/char
// int/short
// int/int
// int/long
// int/float
// int/double
// long/byte
// long/char
// long/short
// long/int
// long/long
// long/float
// long/double
// float/byte
// float/char
// float/short
// float/int
// float/long
// float/float
// float/double
// double/byte
// double/char
// double/short
// double/int
// double/long
// double/float
// double/double
// byte/byte
// byte/char
// byte/short
// byte/int
// byte/long
// byte/float
// byte/double
// char/byte
// char/char
// char/short
// char/int
// char/long
// char/float
// char/double
// short/byte
// short/char
// short/short
// short/int
// short/long
// short/float
// short/double
// int/byte
// int/char
// int/short
// int/int
// int/long
// int/float
// int/double
// long/byte
// long/char
// long/short
// long/int
// long/long
// long/float
// long/double
// float/byte
// float/char
// float/short
// float/int
// float/long
// float/float
// float/double
// double/byte
// double/char
// double/short
// double/int
// double/long
// double/float
// double/double
/*
//www.apache.org/licenses/LICENSE-2.0
// Enable regexes just for this test. They are disabled by default.
//' ==~ /\\/\\//"));
// Both of these are single backslashes but java escaping + Painless escaping....
// Make sure some methods on Pattern are whitelisted
// Make sure the flags are set
// Invalid unicode
// And make sure the location of the error points to the offset inside the pattern
// Not picky so we get a non-assertion error
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for division operator across all types */
//TODO: NaN/Inf/overflow/...
// TODO: byte,short,char
// byte
// short
// char
// int
// long
// float
// double
// byte
// short
// char
// int
// long
// float
// double
/*
//www.apache.org/licenses/LICENSE-2.0
// we don't need a real index, just need to construct a LeafReaderContext which cannot be mocked
/*
//www.apache.org/licenses/LICENSE-2.0
// We know its Map<String, Object> because we put them there in the test
// We know its Map<String, Object> because we put them there ourselves
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/** Compiles and returns the result of {@code script} */
/** Compiles and returns the result of {@code script} with access to {@code picky} */
/** Compiles and returns the result of {@code script} with access to {@code vars} */
/** Compiles and returns the result of {@code script} with access to {@code vars} and compile-time parameters */
// test for ambiguity errors before running the actual script if picky is true
// test actual script execution
/**
/**
/** Checks a specific exception class is thrown (boxed inside ScriptException) and returns it. */
/** Checks a specific exception class is thrown (boxed inside ScriptException) and returns it. */
/* If this fails you *might* be missing -XX:-OmitStackTraceInFastThrow in the test jvm
/**
// This particular incantation of assertions makes the error messages more useful
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for shift operator across all types */
// byte
// short
// char
// int
// long
// long shift distance
// byte
// short
// char
// int
// long
// long shift distance
// byte
// short
// char
// int
// long
// long shift distance
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// boolean
// byte
// short
// char
// int
// long
// float
// double
// String
// boolean
// byte
// short
// char
// int
// long
// float
// double
// String
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for subtraction operator across all types */
//TODO: NaN/Inf/overflow/...
// byte
// short
// char
// int
// long
// float
// double
// byte
// short
// char
// int
// long
// float
// double
/*
//www.apache.org/licenses/LICENSE-2.0
/** tests for throw/try/catch in painless */
/** throws an exception */
/** catches the exact exception */
/** catches superclass of the exception */
/** tries to catch a different type of exception */
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for unary operators across different types */
/** basic tests */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// trigger NPE at line 1 of the script
// null deref at x.isEmpty(), the '.' is offset 30
// trigger NPE at line 2 of the script
// null deref at x.isEmpty(), the '.' is offset 25
// trigger NPE at line 3 of the script
// null deref at y.isEmpty(), the '.' is offset 39
// trigger NPE at line 4 in script (inside conditional)
// null deref at x.isEmpty(), the '.' is offset 53
// right below limit: ok
/**
// We don't want PICKY here so we get the normal error message
// If it isn't a valid long we don't give any suggestions
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for xor operator across all types */
// boolean
// byte
// short
// char
// int
// long
// boolean
// byte
// short
// char
// int
// long
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Testing XContent serialization manually here, because the xContentType field in ContextSetup determines
// how the request needs to parse and the xcontent serialization framework randomizes that. The XContentType
// is not known and accessable when the test request instance is created in the xcontent serialization framework.
// Changing that is a big change. Writing a custom xcontent test here is the best option for now, because as far
// as I know this request class is the only case where this is a problem.
/*
//www.apache.org/licenses/LICENSE-2.0
// START-OBJECT
// FIELD-NAME
// result value
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Compound
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// skip
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ignore feature, this is consistent with numeric fields
/*
//www.apache.org/licenses/LICENSE-2.0
/** A {@link FieldMapper} for scaled floats. Values are internally multiplied
// use the same default as numbers
/**
// since we encode to a long, we have no way to carry NaNs and infinities
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO should we allow to configure the prefix field
// set up the prefix field
// wrap the root field's index analyzer with shingles and edge ngrams
// wrap the root field's search analyzer with only shingles
// don't wrap the root field's search quote analyzer as prefix field doesn't support phrase queries
// set up the shingle fields
// wrap the root field's index, search, and search quote analyzers with shingles
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// prevent extensions by users
/**
/**
/** Constructor with a default pivot, computed as the geometric average of
/**
/**
// unmapped field
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Runs yaml rest tests */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// after 5.x
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// make sure the accuracy loss of scaled floats only occurs at index time
// this test checks that searching scaled floats yields the same results as
// searching doubles that are rounded to the closest half float
// single-valued
// multi-valued
/*
//www.apache.org/licenses/LICENSE-2.0
//"quick red fox lazy brown",
/*
//www.apache.org/licenses/LICENSE-2.0
// default setting
// can disable them on shingle fields
// todo are these queries generated for the prefix field right?
// the use of new ArrayList<>() here is to avoid the varargs form of arrayContainingInAnyOrder
/*
//www.apache.org/licenses/LICENSE-2.0
// this term should be a length that can be rewriteable to a term query on the prefix field
// our defaults don't allow a situation where a term can be too small
// this term should be too long to be rewriteable to a term query on the prefix field
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// previous mapper has not been modified
// but the new one has the change
/**
/**
// Token without an increment
// Normal token with one increment
// Funny token with more than one increment
// Final token increment
// TODO: we have no CannedAnalyzer?
// Empty name not allowed in index created after 5.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// these two filters are cached in the parser
// Set the scorer, since we now replay only the child docIds
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// The meta field that ensures that there is no other parent-join in the mapping
/**
/**
/**
// it is forbidden to add a parent to an existing child
// it is forbidden to add a parent to an existing child
// Index the document as a child
// Index the document as a parent
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// TODO: Find a way to remove this query and let doToQuery(...) just return the query from JoinUtil.createJoinQuery(...)
// asserting reader passes down a MultiReader during rewrite which makes this
// blow up since for this query to work we have to have a DirectoryReader otherwise
// we can't load global ordinals - for this to work we simply check if the reader has no leaves
// and rewrite to match nothing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Only include child documents that have the current hit as parent:
// and only include child documents of a single relation:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Need to take child type into account, otherwise a child doc of different type with the same id could match
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: this array is always of length 1, and testChildrenAggs fails if this is changed
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// The 'towns' and 'parent_names' aggs operate on parent docs and if child docs are in different segments we need
// to ensure those segments which child docs are also evaluated to in the post collect phase.
// Before we only evaluated segments that yielded matches in 'towns' and 'parent_names' aggs, which caused
// us to miss to evaluate child docs in segments we didn't have parent matches for.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// intentionally not writing any docs
// TODO set "maybeWrap" to true for IndexSearcher once #23338 is resolved
// verify with all documents
// verify for each children
// sort larger values first
// on equal value, sort by key
// TODO set "maybeWrap" to true for IndexSearcher once #23338 is resolved
// verify a terms-aggregation inside the parent-aggregation
// TODO set "maybeWrap" to true for IndexSearcher once #23338 is resolved
// verify a terms-aggregation inside the parent-aggregation which itself is inside a
// terms-aggregation on the child-documents
/*long parentDocId =*/ iw.addDocument(parentDocument);
//System.out.println("Parent: " + parent + ": " + randomValue + ", id: " + parentDocId);
/*long childDocId =*/ iw.addDocument(childDocument);
//System.out.println("Child: " + "child" + c + "_" + parent + ": " + randomSubValue + ", id: " + childDocId);
// run a terms aggregation on the number in child-documents, then a parent aggregation and then terms on the parent-number
/*
//www.apache.org/licenses/LICENSE-2.0
// Nothing extra to assert
/*
//www.apache.org/licenses/LICENSE-2.0
// Nothing extra to assert
/*
//www.apache.org/licenses/LICENSE-2.0
// no children for this category -> no entry in the child to parent-aggregation
// count all articles in this category which have at least one comment
// only articles with this category
// only articles which have comments
// find all articles for the comments for the current commenter
// find all articles for the comments for the current commenter
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// intentionally not writing any docs
// TODO set "maybeWrap" to true for IndexSearcher once #23338 is resolved
/*
//www.apache.org/licenses/LICENSE-2.0
// Doc without join
// Doc parent
// Doc child
// Unknown join name
// Doc without join
// Doc parent
// Doc child
// Doc child missing parent
// Doc child missing routing
// Doc grand_child
// Unknown join name
/*
//www.apache.org/licenses/LICENSE-2.0
// see #2744
// index simple data
// index simple data
// TEST FETCHING _parent from child
// TEST matching on parent
// HAS CHILD
// HAS PARENT
// Issue #3290
// index simple data
// randomly break out and dont' have deletes / updates
// Childless parent
// index simple data with flushes, so we have many segments
// HAS CHILD QUERY
// HAS CHILD FILTER
// index simple data
// index simple data
// update p1 and see what that we get updated values...
// index simple data
// Parent 1 and its children
// Parent 2 and its children
// why "p"????
// Parent 3 and its children
// Issue #2536
// query filter in case for p/c shouldn't execute per segment, but rather
// index simple data
// Issue #3144
// index simple data
// re-index
// Issue #3203
// index simple data
// Score needs to be 3 or above!
// index simple data
// Issue #3818
// index simple data
// Parent 1 and its children
// Parent 2 and its children
// Parent 3 and its children
// Parent 4 and its children
// Score mode = NONE
// Score mode = SUM
// Score mode = MAX
// Score mode = AVG
//make sure that when we explicitly set a type, the inner query is executed in the context of the child type instead
//make sure that when we explicitly set a type, the inner query is executed in the context of the parent type instead
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// WTF is this why do we have two?
// have to rewrite again because the provided queryBuilder hasn't been rewritten (directly returned from
// doCreateTestQueryBuilder)
/**
// all good
//check the inner ids query, we have to call rewrite to get to check the type it's executed against
//check the type filter
// WrapperQueryBuilder makes sure we always rewrite
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// have to rewrite again because the provided queryBuilder hasn't been rewritten (directly returned from
// doCreateTestQueryBuilder)
/**
// WrapperQueryBuilder makes sure we always rewrite
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// aggressive filter caching so that we can assert on the filter cache size
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// cost of matching the query against the document, arbitrary as it would be really complex to estimate
// We use the verifiedDocsBits to skip the expensive MemoryIndex verification.
// If docId also appears in the verifiedDocsBits then that means during indexing
// we were able to extract all query terms and for this candidate match
// and we determined based on the nature of the query that it is safe to skip
// the MemoryIndex verification.
// This query uses a significant amount of memory, let's never
// cache it or compound queries that wrap it.
// Comparing identity here to avoid being cached
// Note that in theory if the same instance gets used multiple times it could still get cached,
// however since we create a new query instance each time we this query this shouldn't happen and thus
// this risk neglectable.
// Computing hashcode based on identity to avoid caching.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// In 7x, typeless percolate queries are represented by null documentType values
// In 7x, typeless percolate queries are represented by null indexedDocumentType values
// not executed yet
// Call nowInMillis() so that this query becomes un-cacheable since we
// can't be sure that it doesn't use now or scripts
// Need to this custom impl because FieldNameAnalyzer is strict and the percolator sometimes isn't when
// 'index.percolator.map_unmapped_fields_as_string' is enabled:
//pkg-private for testing
// Indexing in order here, so that the user provided order matches with the docid sequencing:
// Query builder's content is stored via BinaryFieldMapper, which has a custom encoding
// to encode multiple binary values into a single binary doc values field.
// This is the reason we need to first need to read the number of values and
// then the length of the field value in bytes.
/*
//www.apache.org/licenses/LICENSE-2.0
// nul code point
// Range field is of type ip, because that matches closest with BinaryRange field. Otherwise we would
// have to introduce a new field type...
// For now no doc values, because in processQuery(...) only the Lucene range fields get added:
// We can only skip the MemoryIndex verification when percolating a single non nested document. We cannot
// skip MemoryIndex verification when percolating multiple documents, because when terms and
// ranges are extracted from IndexReader backed by a RamDirectory holding multiple documents we do
// not know to which document the terms belong too and for certain queries we incorrectly emit candidate
// matches as actual match.
// `1 + ` is needed to take into account the EXTRACTION_FAILED should clause
// include extractionResultField:failed, because docs with this term have no extractedTermsField
// and otherwise we would fail to return these docs. Docs that failed query term extraction
// always need to be verified by MemoryIndex:
// This was extracted the method above, because otherwise it is difficult to test what terms are included in
// the query in case a CoveringQuery is used (it does not have a getter to retrieve the clauses)
// not != 0 because range fields are not supported
// If a percolator query has been defined in an array object then multiple percolator queries
// could be provided. In order to prevent this we fail if we try to parse more than one query
// for the current document.
// Fetching of terms, shapes and indexed scripts happen during this rewrite:
// This means that fields in the query need to exist in the mapping prior to registering this query
// The reason that this is required, is that if a field doesn't exist then the query assumes defaults, which may be undesired.
//
// Even worse when fields mentioned in percolator queries do go added to map after the queries have been registered
// then the percolator queries don't work as expected any more.
//
// Query parsing can't introduce new fields in mappings (which happens when registering a percolator query),
// because field type can't be inferred from queries (like document do) so the best option here is to disallow
// the usage of unmapped fields in percolator queries to avoid unexpected behaviour
//
// if index.percolator.map_unmapped_fields_as_string is set to true, query can contain unmapped fields which will be mapped
// as an analyzed string.
/**
// First compute hash for field name and write the full hash into the byte array
// Secondly, overwrite the min and max encoded values in the byte array
// This way we are able to reuse as much as possible from the hash for any range type.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// for testing
// shouldn't happen as we checked for the existence of a percolator query in hitsExecutionNeeded(...)
// It possible that a hit did not match with a particular percolate query,
// so then continue highlighting with the next hit.
// Enforce highlighting by source, because MemoryIndex doesn't support stored fields.
// In case multiple documents are being percolated we need to identify to which document
// a highlight belongs to.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// This is not a document with a percolator field.
// This hit didn't match with a percolate query,
// likely to happen when percolating multiple documents
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Need to check whether upper is not smaller than lower, otherwise NumericUtils.subtract(...) fails IAE
// If upper is really smaller than lower then we deal with like MatchNoDocsQuery. (verified and no extractions)
// all conjunctions are unknown, so just return the first one
// In case of range queries each extraction does not simply increment the
// minimum_should_match for that percolator query like for a term based extraction,
// so that can lead to more false positives for percolator queries with range queries
// than term based queries.
// This is because the way number fields are extracted from the document to be
// percolated.  Per field a single range is extracted and if a percolator query has two or
// more range queries on the same field, then the minimum should match can be higher than clauses
// in the CoveringQuery. Therefore right now the minimum should match is only incremented once per
// number field when processing the percolator query at index time.
// For multiple ranges within a single extraction (ie from an existing conjunction or disjunction)
// then this will already have been taken care of, so we only check against fieldnames from
// previously processed extractions, and don't add to the seenRangeFields list until all
// extractions from this result are processed
// In case that there are duplicate term query extractions we need to be careful with
// incrementing msm, because that could lead to valid matches not becoming candidate matches:
// query: (field:val1 AND field:val2) AND (field:val2 AND field:val3)
// doc: field: val1 val2 val3
// So lets be protective and decrease the msm:
// add range fields from this Result to the seenRangeFields set so that minimumShouldMatch is correctly
// calculated for subsequent Results
// If some inner extractions are optional, the result can't be verified
// Keep track of the msm for each clause:
// In case that there are duplicate extracted terms / ranges then the msm should always be equal to the clause
// with lowest msm, because the at percolate time there is no way to know the number of repetitions per
// extracted term and field value from a percolator document may have more 'weight' than others.
// Example percolator query: value1 OR value2 OR value2 OR value3 OR value3 OR value3 OR value4 OR value5 (msm set to 3)
// In the above example query the extracted msm would be 3
// Example document1: value1 value2 value3
// With the msm and extracted terms this would match and is expected behaviour
// Example document2: value3
// This document should match too (value3 appears in 3 clauses), but with msm set to 3 and the fact
// that fact that only distinct values are indexed in extracted terms field this document would
// never match.
// one of the sub queries requires more than one term to match, we can't
// verify it with a single top-level min_should_match
// One of the inner clauses has multiple extractions, we won't be able to
// verify it with a single top-level min_should_match
// Having ranges would mean we need to juggle with the msm and that complicates this logic a lot,
// so for now lets not do it.
// Figure out what the combined msm is for this disjunction:
// (sum the lowest required clauses, otherwise we're too strict and queries may not match)
// When there are duplicated query extractions, percolator can no longer reliably determine msm across this disjunction
// pick lowest msm:
/**
// using BytesRef here just to make use of its compareTo method.
/*
//www.apache.org/licenses/LICENSE-2.0
// many iterations with boolean queries, which are the most complex queries to deal with when nested
// Disable query cache, because ControlQuery cannot be cached...
// use low numbers of clauses more often
// Disable query cache, because ControlQuery cannot be cached...
// Disable query cache, because ControlQuery cannot be cached...
// Empty percolator doc:
// Disable query cache, because ControlQuery cannot be cached...
// Disable query cache, because ControlQuery cannot be cached...
// IW#addDocuments(...) ensures we end up with a single segment
// This will trigger using the TermsQuery instead of individual term query clauses in the CoveringQuery:
// Recreates a similar scenario that made testDuel() fail randomly:
// https://github.com/elastic/elasticsearch/issues/29393
// Additional stored information that is useful when debugging:
// Some queries do not have a msm field. (e.g. unsupported queries)
// Add to string representation of the query to make debugging easier:
// doesn't matter
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//document contains arbitrary content, no error expected when an object is added to it
// If we create two source that have the same field, but these fields have different kind of values (str vs. lng) then
// when these source get indexed, indexing can fail. To solve this test issue, we should generate source that
// always have unique fields:
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// no scoring, wrapping it in a constant score query:
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Now test conjunction:
// Range queries on different fields:
// Now push it over the edge, so that it falls back using TermInSetQuery
// add an query for which we don't extract terms from
// note: it important that range queries never rewrite, otherwise it will cause results to be wrong.
// (it can't use shard data for rewriting purposes, because percolator queries run on MemoryIndex)
// multiple percolator fields are allowed in the mapping, but only one field can be used at index time.
// also includes all other meta fields
// percolator field can be nested under an object field, but only one query can be specified per document
// also includes all other meta fields
// also includes all other meta fields
// Query builder's content is stored via BinaryFieldMapper, which has a custom encoding
// to encode multiple binary values into a single binary doc values field.
// This is the reason we need to first need to read the number of values and
// then the length of the field value in bytes.
// Query builder's content is stored via BinaryFieldMapper, which has a custom encoding
// to encode multiple binary values into a single binary doc values field.
// This is the reason we need to first need to read the number of values and
// then the length of the field value in bytes.
// Just so that we store scripts in percolator queries, but not really execute these scripts.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Need a one doc index:
// A match:
// No match:
// No query:
/*
//www.apache.org/licenses/LICENSE-2.0
// Test long range:
// Test double range:
// Test IP range:
// Test date range:
// doesn't match
// Acceptable:
// Unacceptable:
// this query should never match as it doesn't use nested query:
// non existing doc, so error element
/*
//www.apache.org/licenses/LICENSE-2.0
// to avoid normal document from being cached by BitsetFilterCache
// size 0, because other wise load bitsets for normal document in FetchPhase#findRootDocumentIfNested(...)
// We can't check via api... because BitsetCacheListener requires that it can extract shardId from index reader
// and for percolator it can't do that, but that means we don't keep track of
// memory for BitsetCache in case of percolator
// + 3 hours
/*
//www.apache.org/licenses/LICENSE-2.0
// because of the dup term
// ideally it would return no extractions, but the fact
// that it doesn't consider them verified is probably good enough
// the following span queries aren't exposed in the query dsl and are therefor not supported:
// 1) SpanPositionRangeQuery
// 2) PayloadScoreQuery
// 3) SpanBoostQuery
// The following span queries can't be supported because of how these queries work:
// 1) SpanMultiTermQueryWrapper, not supported, because there is no support for MTQ typed queries yet.
// 2) SpanContainingQuery, is kind of range of spans and we don't know what is between the little and big terms
// 3) SpanWithinQuery, same reason as SpanContainingQuery
// 4) FieldMaskingSpanQuery is a tricky query so we shouldn't optimize this
// int ranges get converted to long ranges:
// Half float ranges get converted to double ranges:
// Float ranges get converted to double ranges:
// For now no extraction support for geo queries:
// multiple range queries on different fields
// multiple disjoint range queries on the same field
// multiple conjunction range queries on the same field
// multiple conjunction range queries on different fields
// mixed term and range conjunctions
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//en.wikipedia.org/wiki/Discounted_cumulative_gain#Discounted_Cumulative_Gain">Discounted Cumulative Gain</a><br>
/** If set to true, the dcg will be normalized (ndcg) */
/** the default search window size */
/** the search window size */
/**
/**
//en.wikipedia.org/wiki/Discounted_cumulative_gain
/**
// unknownDocRating might be null, in which case unrated docs will be ignored in the dcg calculation.
// we still need to add them as a placeholder so the rank of the subsequent ratings is correct
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// only used for parsing internally
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//doi.org/10.1145/1645953.1646033
/** the default search window size */
/** the search window size */
/**
/**
/**
// we can pre-calculate the constant used in metric calculation
/**
// unknownDocRating might be null, in which case unrated docs will be ignored in the dcg calculation.
// we still need to add them as a placeholder so the rank of the subsequent ratings is correct
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** the search window size */
/** ratings equal or above this value will be considered relevant */
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//en.wikipedia.org/wiki/Information_retrieval#Precision_at_K).<br>
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The overall evaluation result. */
/** details about individual ranking evaluation queries, keyed by their id */
/** exceptions for specific ranking evaluation queries, keyed by their id */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** List of search request to use for the evaluation */
/** Definition of the quality metric, e.g. precision at N */
/** Maximum number of requests to execute in parallel. */
/** Default max number of requests. */
/** optional: Templates to base test requests on */
/** Returns the metric to use for quality evaluation.*/
/** Returns a list of intent to query translation specifications to evaluate. */
/** Returns the template to base test requests on. */
/** Returns the max concurrent searches allowed. */
/** Set the max concurrent searches allowed. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// check that not two documents with same _index/id are specified
// ensure that testRequest, if set, does not contain aggregation, suggest or highlighting section
/** return the user supplied request id */
/** return the list of rated documents to evaluate. */
/** return the parameters if this request uses a template, otherwise this will be empty. */
/** return the parameters if this request uses a template, otherwise this will be {@code null}. */
/** returns a list of fields that should be included in the document summary for matched documents */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// check for parts that should not be part of a ranking evaluation request
// if we fail parsing, put the exception into the errors map and continue
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// only create four hits
/**
/**
// also check normalized
/*
//www.apache.org/licenses/LICENSE-2.0
// we need to associate each hit with an index name otherwise rendering will not work
// skip inserting random fields for:
// - the root object, since we expect a particular queryId there in this test
// - the `metric_details` section, which can potentially contain different namedXContent names
// - everything under `hits` (we test lenient SearchHit parsing elsewhere)
// we cannot check equality of object here because some information (e.g. SearchHit#shard) cannot fully be
// parsed back after going through the rest layer. That's why we only check that the original and the parsed item
// have the same xContent representation
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// take 4th rank into window
/**
// if we supply e.g. 2 as unknown docs rating, it should be the same as in the other test above
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// check that if we have fewer search hits than relevant doc position,
// we don't find any result and get 0.0 score
// mark one of the ten docs relevant
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// the following search hits contain only the last three documents
// add an unlabeled search hit
// also try with setting `ignore_unlabeled`
// also try with setting `ignore_unlabeled`
/*
//www.apache.org/licenses/LICENSE-2.0
// add another index for testing closed indices etc...
// set up an alias that can also be used in tests
/**
// the expected Prec@ for the first query is 4/6 and the expected Prec@ for the
// second is 1/6, divided by 2 to get the average
// test that a different window size k affects the result
// if we look only at top 3 documente, the expected P@3 for the first query is
// 2/3 and the expected Prec@ for the second is 1/3, divided by 2 to get the average
/**
// test that a different window size k affects the result
// the expected reciprocal rank for the amsterdam_query is 1/5
// the expected reciprocal rank for the berlin_query is 1/1
// dividing by 2 to get the average
// test that a different window size k affects the result
// limiting to top 3 results, the amsterdam_query has no relevant document in it
// the reciprocal rank for the berlin_query is 1/1
// dividing by 2 to get the average
/**
/**
// test that ignore_unavailable=true works but returns one result less
// test that ignore_unavailable=false or default settings throw an IndexClosedException
// test expand_wildcards
// test allow_no_indices
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// skip inserting random fields for:
// - the `details` section, which can contain arbitrary queryIds
// - everything under `failures` (exceptions parsing is quiet lenient)
// - everything under `hits` (we test lenient SearchHit parsing elsewhere)
// We cannot check equality of object here because some information (e.g.
// SearchHit#shard)  cannot fully be parsed back.
// Also exceptions that are parsed back will be different since they are re-wrapped during parsing.
// However, we can check that there is the expected number
/*
//www.apache.org/licenses/LICENSE-2.0
// this shouldn't happen in tests, re-throw just not to swallow it
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
/**
// The default script applier executes a no-op
/**
/**
/**
/**
/*
/**
/**
// lastBatchStartTime is essentially unused (see WorkerBulkByScrollTaskState.throttleWaitTime. Leaving it for now, since it seems
// like a bug?
/**
// If any of the shards failed that should abort the request.
// Timeouts aren't shard failures but we still need to pass them back to the user.
/*
/**
// Truncate the hits if we have more than the request max docs
/*
/**
/**
// Track the indexes we've seen so we can refresh them if requested
// We've processed all the requested docs.
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// Build the internal request
// Executes the request and waits for completion
/*
/**
/**
/**
// We validate here and in the setters because the setters use "Float.POSITIVE_INFINITY" instead of -1
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Let the requester set search timeout. It is probably only going to be useful for testing but who knows.
/**
// body is optional
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Delete-by-query does not require the source to delete a document
// and the default implementation checks for it
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// TODO move the request to the correct node. maybe here or somehow do it as part of startup for reindex in general....
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Enable basic auth if it is configured
// Stick the task id in the thread name so we can track down tasks from stack traces
// Limit ourselves to one reactor thread because for now the search process is single threaded.
/**
/**
/*
// A little extra paranoia so we log something if we leave any threads running
// Copy the index from the request so we always write where it asked to write
/*
// id and source always come from the found doc. Scripts can change them but they operate on the index request.
// the source xcontent type and destination could be different
// we need to convert
/*
// OpType is synthesized from version so it is handled when we copy version above.
/**
/*
/*
// Check that we didn't round when we fetched the value.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// use sequence number powered optimistic concurrency control
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// It is nasty to build paths with StringBuilder but we'll be careful....
// V_5_0_0
/* Versions of Elasticsearch before 5.0 couldn't parse nanos or micros
// Detect if we should use search_type=scan rather than a sort
// Versions before 2.0.0 need prompting to return interesting fields. Note that timestamp isn't available at all....
// Versions before 1.0.0 don't support `"_source": true` so we have to ask for the _source in a funny way.
// V_5_0_0
// allow_partial_results introduced in 6.3, running remote reindex against earlier versions still silently discards RED shards.
// EMPTY is safe here because we're not calling namedObject
/* We're intentionally a bit paranoid here - copying the query
// Versions before 1.0 don't support `"_source": true` so we have to ask for the source as a stored field.
// V_5_0_0
/* Versions of Elasticsearch before 5.0 couldn't parse nanos or micros
// Versions before 2.0.0 extract the plain scroll_id from the body
// Versions before 2.0.0 extract the plain scroll_id from the body
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
// a hack but this lets us get the right xcontent type to go with the source
// Pre-2.0.0 routing come back in "fields"
// ignore ttls since they have been removed
// ignore parents since they have been removed
/**
// For BWC with nodes pre 7.0
/**
/**
// Pull apart the hits element if we got it
/**
// So we can give a nice error for parsing exceptions
// Make some effort to use the right exceptions
// But it isn't worth trying to get it perfect....
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/* This is called on the RestClient's thread pool and attempting to close the client on its
// Preserve the thread context so headers survive after the call
// Restore the thread context to get the precious headers
// eliminates compiler warning
// EMPTY is safe here because we don't call namedObject
/* Because we're streaming the response we can't get a copy of it here. The best we can do is hint that it
// eliminates compiler warning
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Semaphore used to allow & block indexing operations during the test
// tag::reindex1
// <1>
// end::reindex1
// tag::update-by-query
// end::update-by-query
// tag::update-by-query-filter
// end::update-by-query-filter
// validate order of string params to Script constructor
// tag::update-by-query-size
// end::update-by-query-size
// tag::update-by-query-sort
// end::update-by-query-sort
// tag::update-by-query-script
// end::update-by-query-script
// validate order of string params to Script constructor
// tag::update-by-query-multi-index
// end::update-by-query-multi-index
// tag::update-by-query-routing
// end::update-by-query-routing
// tag::update-by-query-pipeline
// end::update-by-query-pipeline
// tag::update-by-query-list-tasks
// do stuff
// end::update-by-query-list-tasks
// tag::update-by-query-get-task
// end::update-by-query-get-task
// tag::update-by-query-cancel-task
// Cancel all update-by-query requests
// Cancel a specific update-by-query request
// end::update-by-query-cancel-task
// tag::update-by-query-rethrottle
// end::update-by-query-rethrottle
// unblocking the blocked update
// tag::delete-by-query-sync
// <1>
// <2>
// <3>
// <4>
// end::delete-by-query-sync
// tag::delete-by-query-async
// <1>
// <2>
// <3>
// <4>
// Handle the exception
// end::delete-by-query-async
/**
// Checks that the all documents have been indexed and correctly counted
// Scroll by 1 so that cancellation is easier to control
// chose to modify some of docs - rest is still blocked
// Now execute the reindex action...
// 10 seconds is usually fine but on heavily loaded machines this can take a while
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Fill the context with something random so we can make sure we inherited it appropriately.
/**
// Empty strings get special behavior we don't want
// this test primarily tests ClientScrollableHitSource but left it to test integration to status
// use fail() onResponse handler because mocked search never fires on listener.
// this test primarily tests ClientScrollableHitSource but left it to test integration to status
// Default is 0, meaning unstarted
/**
// Use assert busy because the update happens on another thread
/**
// While we're here we can check that the sleep made it through
// When the task is rejected we don't increment the throttled timer
/**
/**
/**
/**
/**
/**
/*
// Set the base for the scroll to wait - this is added to the figure we calculate below
// Set throttle to 1 request per second to make the math simpler
// create a simulated response.
// So the next request is going to have to wait an extra 100 seconds or so (base was 10 seconds, so 110ish)
// Now we can simulate a response and check the delay that we used for the task
// Let's rethrottle between the starting the scroll and getting the response
// The delay uses the new throttle
// Running the command ought to increment the delay counter on the task.
/**
/**
/*
// Refresh or not doesn't matter - we don't try to refresh.
/**
/*
/*
// Send the scroll response which will trigger the custom thread pool above, canceling the request before running the response
// Use a long delay here so the test will time out if the cancellation doesn't reschedule the throttled task
// Now that we've got our cancel we'll just verify that it all came through all right
// Canceled tasks always start to clear the scroll before they die.
// Canceled tasks always start to clear the scroll before they die.
/**
/**
// Force a backoff time of 0 to prevent sleeping
// no-op
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Clear the slice builder if there is one set. We can't call sliceIntoSubRequests if it is.
// If you clear the slice then the slice should be the same request as the parent request
// Except that adding the slice might have added an empty builder
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Round up
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// One of the merged responses gets the expected value for took, the others get a smaller value
// The actual status doesn't matter too much - we test merging those elsewhere
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Semaphore used to allow & block indexing operations during the test
/**
// Total number of documents created for this test (~10 per primary shard per slice)
// Checks that the all documents have been indexed and correctly counted
// Scroll by 1 so that cancellation is easier to control
/* Allow a random number of the documents less the number of workers
// Now execute the reindex action...
/* ... and wait for the indexing operation listeners to block. It
// 10 seconds is usually fine but on heavily loaded machines this can take a while
// Status should show the task running
// Description shouldn't be empty
// Cancel the request while the action is blocked by the indexing operation listeners.
// This will prevent further requests from being sent.
/* The status should now show canceled. The request will still be in the
// Checks that no more operations are executed
/* We can only be sure that we've drained all the permits if we only use a single worker. Otherwise some worker may have
// And check the status of the response
// If we have more than one worker we might not have made all the modifications
// Skip tasks with a parent because those are children of the task we want to cancel
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// ensure we test the happy path on every build.
// Set the base for the scroll to wait - this is added to the figure we calculate below
// create a simulated response.
/*
//www.apache.org/licenses/LICENSE-2.0
// Deletes two docs that matches "foo:a"
// Deletes the two first docs with limit by size
// Deletes but match no docs
// Deletes all remaining docs
// total number of expected deletions
// number of documents to be deleted with the upcoming delete-by-query
// (this number differs for each index)
// Deletes all the documents with candidate=true
// Ok
// Deletes the two docs that matches "foo:a"
// Delete remaining docs
/*
//www.apache.org/licenses/LICENSE-2.0
// Some deletions might fail due to version conflict, but
// what matters here is the total of successful deletions
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//"+ http.get("publish_address");
// Test with external version_type
/*
//www.apache.org/licenses/LICENSE-2.0
// Copy all the docs
// Now none of them
// Now half of them
// Limit with maxDocs
// Copy all the docs
// Use a small batch size so we have to use more than one batch
// Copy some of the docs
// Use a small batch size so we have to use more than one batch
// Copy all the docs
// Use a small batch size so we have to use more than one batch
// Copy some of the docs
// Use a small batch size so we have to use more than one batch
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/* Its a string in the source! */);
/*
// Just put something in the way of the copy.
// CREATE will cause the conflict to prevent the write.
/**
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// enable http
// Whitelist reindexing from the http host we're going to use
/**
/**
/**
/**
//tools.ietf.org/html/rfc1945#section-11.1">HTTP/1.0's RFC</a>.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Always respond with 200
//  * If the reindex sees the 200, it means the SSL connection was established correctly.
//  * We can check client certs in the handler.
//github.com/elastic/elasticsearch/issues/49094", inFipsJvm());
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Copy a subset of the docs sorted
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// writing directly into the index of which this is the alias works though
// The index doesn't have to exist
// And it doesn't matter if they are the same index. They are considered to be different because the remote one is, well, remote.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/* Add ten documents per slice so most slices will have many documents to process, having to go to multiple batches.
// Start a request that will never finish unless we rethrottle it
// Throttle "forever"
// Make sure we use multiple batches
// There should be a sane number of child tasks running
// Wait for all of the sub tasks to start (or finish, some might finish early, all that matters is that not all do)
// Now rethrottle it so it'll finish
// No throttle or "very fast"
// Now check the resulting requests per second.
// If there is a single slice it should match perfectly
/* Check that at least one slice was rethrottled. We won't always rethrottle all of them because they might have completed.
/* The slice can be null here because it was completed but hadn't reported its success back to the task when the
// This slice reports as not having completed so it should have been processed.
/* Now assert that the parent request has the total requests per second. This is a much weaker assertion than that the parent
// Now the response should come back quickly because we've rethrottled the request
// It'd be bad if the entire require completed in a single batch. The test wouldn't be testing anything.
// the task isn't ready to be rethrottled until it has figured out how many slices it will use. if we rethrottle when the task is
// in this state, the request will fail. so we try a few times
// We want to retry in this case so we throw an assertion error
// The parent task hasn't started yet
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// enable HTTP so we can test retries on reindex from remote; in this case the "remote" cluster is just this cluster
// whitelist reindexing from the HTTP host we're going to use
/*
/*
// use pools of size 1 so we can block them
// use queues of size 1 because size 0 is broken and because search requests need the queue to function
// Create the source index on the node with small thread pools so we can block them.
// Not all test cases use the dest index but those that do require that it be on the node will small thread pools
// Build the test data. Don't use indexRandom because that won't work consistently with such small thread pools.
// Make sure we use more than one batch so we have to scroll
// Keep a copy of the current number of search rejections so we can assert that we get more when we block the scroll
// Fetch the response just in case we blew up half way through. This will make sure the failure is thrown up to the top level.
/**
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Try regular slices with a version that doesn't support slices=auto, which should succeed
// Try regular slices with a version that doesn't support slices=auto, which should succeed
// Try regular slices with a version that doesn't support slices=auto, which should succeed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Capture the sub request and the listener so we can verify they are sane
// Magical generics incantation.....
// The whole thing succeeded so we should have got the success
/* There are no async tasks to simulate because the listener is called for us. */},
// Rethrow any failures just so we get a nice exception if there were any. We don't expect any though.
/*
//www.apache.org/licenses/LICENSE-2.0
// Reindex all the docs
// Now none of them
// Now half of them
// Limit with size
// Only the first three documents are updated because of sort
// Reindex all the docs
// Now none of them
// Now half of them
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Reindex using the external version_type
//127.0.0.1:" + oldEsPort + "\"\n"
// Reindex using the default internal version_type
//127.0.0.1:" + oldEsPort + "\"\n"
// 11 requests per second should give us a nice "funny" number on the scroll timeout
// 11 requests per second should give us a nice "funny" number on the scroll timeout
// 11 requests per second should give us a nice "funny" number on the scroll timeout
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// test a specific date math + all characters that need escaping.
// re-escape already escaped (no special handling).
// Test sort:_doc for versions that support it.
// Test search_type scan for versions that don't support sort:_doc.
// Test sorting by some field. Version doesn't matter.
// Test request without any fields
// Test stored_fields for versions that support it
// V_5_0_0_alpha4 => current
// Test fields for versions that support it
// V_2_0_0 => V_5_0_0_alpha3
// Test extra fields for versions that need it
// But only versions before 1.0 force _source to be in the list
// V_5_0_0
// Versions of Elasticsearch prior to 5.0 can't parse nanos or micros in TimeValue.
// Source filtering is included if set up
// Invalid XContent fails
// Test with version < 2.0.0
// Test with version < 2.0.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// assert for V_5_0_0 (no qualifier) since we no longer consider qualifier in Version since 7
// V_5_0_0 since we no longer consider qualifier in Version
/**
/**
/**
// The rejection comes through in the handler because the mocked http response isn't marked as an error
// Handling a scroll rejection is the same as handling a search rejection so we reuse the verification code
// The rejection comes through in the handler because the mocked http response isn't marked as an error
// Handling a scroll rejection is the same as handling a search rejection so we reuse the verification code
// Successfully get the status without a body
// Successfully get the status without a body
// Successfully get the status with a broken body
// Fail to get the status without a body
// Fail to get the status without a body
// Fail to get the status with a broken body
// Unwrap the some artifacts from the test
// This next exception is what the user sees
// And that exception is reported as being caused by the underlying exception returned by the client
// Use the response from a main action instead of a proper start response to generate a parse error
/**
// Throw away the current thread context to simulate running async httpclient's thread pool
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// nothing to do here...
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// only use for testing
// only use for testing
/**
// URL matches white list - no additional processing is needed
// We didn't match white list - try to resolve against path.repo
/*
//www.apache.org/licenses/LICENSE-2.0
//localhost:6001/";
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// prefix instead of http://.
// Create a FS repository using the path.repo location
// Create a URL repository using the file://{path.repo} URL
// Create a URL repository using the http://{fixture} URL
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// eliminate thread name check as we create repo manually on test/main threads
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// our build is configured to only include this module in the package distributions
/*
// treat failure to notify systemd of readiness as a startup failure
// do not treat failure to notify systemd of stopping as a failure
/*
//www.apache.org/licenses/LICENSE-2.0
// noinspection OptionalGetWithoutIsPresent
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
// clamp value to the allowed range
// Netty's CompositeByteBuf implementation does not allow less than two components.
// NettyAllocator will return the channel type designed to work with the configuredAllocator
// Set the allocators for both the server channel and the child channels created
// Netty logs a warning if it can't set the option, so try this only on supported platforms
// otherwise we leak threads since we never moved to started
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//");
/**
// strip protocol from origin
/**
// Not a CORS request so we cannot validate it. It may be a non CORS request.
// if the origin is the same as the host of the request, then allow
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// All written so clear OP_WRITE
// Directly return here so incompleteWrite(...) is not called.
// Ensure the pending writes are made of ByteBufs only.
// We have something else beside ByteBuffers to write so fallback to normal writes.
// Zero length buffers are not added to nioBuffers by ChannelOutboundBuffer, so there is no need
// to check if the total size of all the buffers is non-zero.
// Protected so that tests can verify behavior and simulate partial writes
// Protected so that tests can verify behavior
// By default we track the SO_SNDBUF when ever it is explicitly set. However some OSes may dynamically change
// SO_SNDBUF (and other characteristics that determine how much data can be written at once) so we should try
// make a best effort to adjust as OS behavior changes.
// Multiply by 2 to give some extra space in case the OS can process write data faster than we can provide.
/*
//www.apache.org/licenses/LICENSE-2.0
// here we set the netty4 transport and http transport as the default. This is a set once setting
// ie. if another plugin does that as well the server will fail - only one default network can exist!
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// NOTE: It is unsafe to share a reference of the internal structure, so we
// use the default implementation which will copy the bytes. It is unsafe because
// a netty ByteBuf might be pooled which requires a manual release to prevent
// memory leaks.
// NOTE: It is unsafe to share a reference of the internal structure, so we
// use the default implementation which will copy the bytes. It is unsafe because
// a netty ByteBuf might be pooled which requires a manual release to prevent
// memory leaks.
// nothing to do here
/*
//www.apache.org/licenses/LICENSE-2.0
// We do not want to log read complete events because we log inbound messages in the TcpTransport.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// If the message length is greater than the network bytes available, we have not read a complete frame.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// See AdaptiveReceiveBufferSizePredictor#DEFAULT_XXX for default values in netty..., we can use higher ones for us, even fixed one
// NettyAllocator will return the channel type designed to work with the configured allocator
// Netty logs a warning if it can't set the option, so try this only on supported platforms
// NettyAllocator will return the channel type designed to work with the configuredAllocator
// Set the allocators for both the server channel and the child channels created
// Netty logs a warning if it can't set the option, so try this only on supported platforms
// using a dot as a prefix means this cannot come from any settings parsed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we set this to false in tests to avoid tests that randomly set processors from stepping on each other
/*
/*
/**
// usually we have one, two, or three components from the header, the message, and a buffer
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// randomize netty settings
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//TODO: This is a *temporary* workaround to ensure a timeout does not mask other problems
/*
//www.apache.org/licenses/LICENSE-2.0
// Set up an HTTP transport with only the CORS enabled setting
// inspect response and validate
// create an HTTP transport with CORS enabled and allow origin configured
// inspect response and validate
// create an HTTP transport with CORS enabled
// inspect response and validate
//" + originValue;
// inspect response and validate
// inspect response and validate
// construct request and send it over the transport layer
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Safe not because it doesn't do anything with the type parameters but because it won't leak them into other methods.
// Safe not because it doesn't do anything with the type parameters but because it won't leak them into other methods.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// random order execution
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// enable http
// we use the limit size as a (very) rough indication on how many requests we should sent to hit the limit
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Test pre-flight request
// Test short-circuited request
/*
//www.apache.org/licenses/LICENSE-2.0
// enable http
// check if opaque ids are monotonically increasing
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// These tests are here today so they have access to a proper REST client. They cannot be in :server:integTest since the REST client needs a
// proper transport implementation, and they cannot be REST tests today since they need to restart nodes. When #35599 and friends land we
// should be able to move these tests to run against a proper cluster instead. TODO do this.
// enable http
// assign shards
// causes rebalancing
/*
//www.apache.org/licenses/LICENSE-2.0
// If greater than a KB, possibly invoke a partial write.
/*
//www.apache.org/licenses/LICENSE-2.0
// this advances the index of the channel buffer
// this advances the index of the channel buffer
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// end of stream
/*
//www.apache.org/licenses/LICENSE-2.0
// bound addresses
// publish address
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// should bind 127.0.0.1:XYZ
// should bind [::1]:XYZ and 127.0.0.1:XYZ+1
// fails if port of publish address does not match corresponding bound address
// IPv4 address is preferred publish address for _local_
/*
//www.apache.org/licenses/LICENSE-2.0
// we gather the buffers into a channel buffer
// we know bytes stream output always creates a paged bytes reference, we use it to create randomized content
/*
//www.apache.org/licenses/LICENSE-2.0
// will not actually bind to this
// will not actually bind to this
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//site.icu-project.org/charts/collation-icu4j-sun"
//site.icu-project.org/charts/collation-icu4j-sun</a> for key
/**
// clone the collator: see http://userguide.icu-project.org/collation/architecture
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.icu-project.org/userguide/Collate_Customization.html">
// set the strength flag, otherwise it will be the default.
// set the decomposition flag, otherwise it will be the default.
// expert options: concrete subclasses are always a RuleBasedCollator
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//icu-project.org/apiref/icu4j/com/ibm/icu/text/UnicodeSet.html)
/** Store here the same Normalizer used by the lucene ICUFoldingFilter */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// cjkAsWords nor myanmarAsWords are not configurable yet.
//parse a single RBBi rule file
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// CodingCase(int initialShift, int finalShift)
// CodingCase(int initialShift, int middleShift, int finalShift)
// Export only static methods
/**
// Use long for intermediaries to protect against overflow
/**
// Use long for intermediaries to protect against overflow
/**
// numBytes is 3
// Produce final char (if any) and trailing count chars.
// codingCase.numBytes must be 3
// Add trailing char containing the number of full bytes in final char
// Add trailing char containing the number of full bytes in final char
// No left over bits - last char is completely filled.
// Add trailing char containing the number of full bytes in final char
/**
// numBytes is 3
// Handle final char
// numBytes is 3
/*
//www.apache.org/licenses/LICENSE-2.0
// set the strength flag, otherwise it will be the default.
// set the decomposition flag, otherwise it will be the default.
// expert options: concrete subclasses are always a RuleBasedCollator
// freeze so thread-safe
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// test the tokenizer with single rule file
// test the tokenizer with two rule files
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Min == 1
// Min == 1
// Min == 1
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Tests borrowed from Solr's Icu collation key filter factory test.
/*
/*
/*
/*
/*
/*
// now assert that punctuation still matters: foo-bar < foo bar
/*
/*
// now assert that case still matters: resume < Resume
/*
/*
//bugs.sun.com/bugdatabase/view_bug.do?bug_id=4423383
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// current impl ignores args and shourd always return INTERSECTS
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// both values should collate to same value
// searching for either of the terms should return both results since they collate to the same value
// secondary sort should kick in because both will collate to same value
// everything should be indexed fine, no exceptions
// using sort mode = max, values B and C will be used for the sort
// if mode max we use c and b as sort values, if max we use "a" for both
// will be ignored
// same thing, using different sort mode that will use a for both docs
// if mode max we use c and b as sort values, if max we use "a" for both
// will NOT be ignored and will determine order
/*
// searching for either of the terms should return both results since they collate to the same value
// secondary sort should kick in because both will collate to same value
/*
// secondary sort should kick in because both will collate to same value
/*
// secondary sort should kick in because both will collate to same value
/*
// secondary sort should kick in on docs 1 and 3 because same value collate value
/*
/*
/*
/*
//bugs.sun.com/bugdatabase/view_bug.do?bug_id=4423383
// secondary sort should kick in because both will collate to same value
/*
//www.apache.org/licenses/LICENSE-2.0
// should collate to same value
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// ignore comments
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// パーティー should be stemmed by default
// (min len) コピー should not be stemmed
// パーティー should not be stemmed since min len == 6
// コピー should not be stemmed
// test only kanji
// test only kana
// test default
// fix #59
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// weird, encoder is null at last step in SimplePhoneticAnalysisTests, so we set it to metaphone as default
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.uni-koeln.de/phil-fak/phonetik/Lehre/MA-Arbeiten/magister_wilz.pdf
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//en.wikipedia.org/wiki/NYSIIS">NYSIIS on Wikipedia</a>
//www.dropby.com/NYSIIS.html">NYSIIS on dropby.com</a>
/**
/**
// 1. EV -> AF
// A, E, I, O, U -> A
// 2. Q -> G, Z -> S, M -> N
// 3. KN -> NN else K -> C
// 4. SCH -> SSS
// PH -> FF
// 5. H -> If previous or next is a non vowel, previous.
// 6. W -> If previous is vowel, previous.
/**
/**
/**
/**
/**
/**
/**
// Use the same clean rules as Soundex
// Translate first characters of name:
// MAC -> MCC, KN -> NN, K -> C, PH | PF -> FF, SCH -> SSS
// Translate last characters of name:
// EE -> Y, IE -> Y, DT | RT | RD | NT | ND -> D
// First character of key = first character of name.
// Transcode remaining characters, incrementing by one character each time
// only append the current char to the key if it is different from the last one
// If last character is S, remove it.
// If last characters are AY, replace with Y.
// If last character is A, remove it.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: deprecate and remove, this is a noop token filter; it's here for backwards compat before we had "smartcn_tokenizer"
// TODO: deprecate and remove, this is an alias to "smartcn_tokenizer"; it's here for backwards compat
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: is this the right boilerplate?  I forked this out of TransportAnalyzeAction.java:
// for _na_
// TODO: move to AnalysisFactoryTestCase so we can more easily test thread safety for all factories
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Keystore settings
// so that it can overridden for tests
//management.core.windows.net/", s -> {
/*
//www.apache.org/licenses/LICENSE-2.0
// Azure SDK configuration uses DefaultBuilder which uses java.util.ServiceLoader to load the
// various Azure services. By default, this will use the current thread's context classloader
// to load services. Since the current thread refers to the main application classloader it
// won't find any Azure service implementation.
// Here we basically create a new DefaultBuilder that uses the current class classloader to load services.
// And create a new blank configuration based on the previous DefaultBuilder
/*
//www.apache.org/licenses/LICENSE-2.0
// Deployment name could be set with discovery.azure.deployment.name
// Default to cloud.azure.management.cloud.service.name
// Reading deployment_slot
/**
// We got a remote exception
// We can't find the publish host address... Hmmm. Too bad :-(
// We check the deployment slot
// If provided, we check the deployment name
// We check current deployment status
// In other case, it should be the right deployment so we can add it to the list of instances
// We have a bad parameter here or not enough information from azure
/*
//www.apache.org/licenses/LICENSE-2.0
// overrideable for tests
// Used for testing
/*
//www.apache.org/licenses/LICENSE-2.0
// We add a fake subscription_id to start mock compute service
/**
/**
// Set the private IP address
// Set the public IP address
/**
// Format the InetSocketAddress to a format that contains the port number
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO this should be a IT but currently all ITs in this project run against a real cluster
//" + InetAddress.getLoopbackAddress().getHostAddress() +
/**
//schemas.microsoft.com/windowsazure";
/**
// shut them all down otherwise we get spammed with connection refused exceptions
// only wait for the cluster to form
// add one more node and wait for it to join
/*
//www.apache.org/licenses/LICENSE-2.0
// We expect having 1 node as part of the cluster, let's test that
// We expect having 1 node as part of the cluster, let's test that
/*
//www.apache.org/licenses/LICENSE-2.0
// We expect having 2 nodes as part of the cluster, let's test that
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Simulate an EC2 DescribeInstancesResponse
/**
//ec2.amazonaws.com/doc/2013-02-01/";
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Query EC2 API based on AZ, instance state, and tag.
// NOTE: we don't filter by security group during the describe instances request for two reasons:
// 1. differences in VPCs require different parameters during query (ID vs Name)
// 2. We want to use two different strategies: (all security groups vs. any security groups)
// lets see if we can filter based on groups
// We check if we can find at least one group name or one group id in groups.
// continue to the next instance
// We need tp match all group names or group ids, otherwise we ignore this instance
// continue to the next instance
// Reading the node host from its metadata
// for a given tag key, OR relationship for multiple different values
// OR relationship amongst multiple values of the availability-zone filter
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// proxy for testing
// pkg private for tests
// the response metadata cache is only there for diagnostics purposes,
// but can force objects from every response to the old generation.
// TODO: remove this leniency, these settings should exist together and be validated
// Increase the number of retries in case of 5xx API responses
// pkg private for tests
/**
// shutdown IdleConnectionReaper background thread
// it will be restarted on new client usage
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The access key (ie login id) for connecting to ec2. */
/** The secret key (ie password) for connecting to ec2. */
/** The session token for connecting to ec2. */
/** The host name of a proxy to connect to ec2 through. */
/** The port of a proxy to connect to ec2 through. */
/** An override for the ec2 endpoint to connect to. */
/** The protocol to use to connect to to ec2. */
/** The username of a proxy to connect to s3 through. */
/** The password of a proxy to connect to s3 through. */
/** The socket timeout for connecting to s3. */
/** Credentials to authenticate with ec2. */
/**
/** The protocol to use to talk to ec2. Defaults to https. */
/** An optional proxy host that requests to ec2 should be made through. */
/** The port number the proxy host should be connected on. */
// these should be "secure" yet the api for the ec2 client only takes String, so
// storing them
// as SecureString here won't really help with anything
/** An optional username for the proxy host, for basic authentication. */
/** An optional password for the proxy host, for basic authentication. */
/** The read timeout for the ec2 client. */
// pkg private for tests
/** Parse settings for a single client. */
/*
//www.apache.org/licenses/LICENSE-2.0
// Initializing Jackson requires RuntimePermission accessDeclaredMembers
// The ClientConfiguration class requires RuntimePermission getClassLoader
// kick jackson to do some static caching of declared members info
// ClientConfiguration clinit has some classloader problems
// TODO: fix that
// protected for testing
// eagerly load client settings when secure settings are accessible
// Register EC2 discovery settings: discovery.ec2
// Register cloud node settings: cloud.node
// Adds a node attribute for the ec2 availability zone
// pkg private for testing
// should not happen, we know the url is not malformed, and openConnection does not actually hit network
// this is lenient so the plugin does not fail when installed outside of ec2
// secure settings should be readable
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// some less verbose defaults
/**
// only one address: because we explicitly ask for only one via the Ec2HostnameType
// using this, one has to explicitly specify _ec2_ in network setting
//        return resolve(Ec2HostnameType.DEFAULT, false);
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if explicit, just load it and don't load from env
/*
//www.apache.org/licenses/LICENSE-2.0
// If we have the same tag name and one of the values, we add the instance
// if we have more than one value for the same key, then the key is appended with .x
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// reload secure settings2
// client is not released, it is still using the old settings
/*
//www.apache.org/licenses/LICENSE-2.0
// we just need to ensure we don't resolve DNS here
// We check that we are using here expected address
// We check that we are using here expected address
// We check that we are using here expected address
// We check that we are using here expected address
// wait for cache to expire
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We try to update a setting now
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// redirect EC2 metadata service to httpServer
//" + httpServer.getAddress().getHostName() + ":" + httpServer.getAddress().getPort()));
/**
/**
// redirect EC2 metadata service to unknown location
//127.0.0.1/"));
//127.0.0.1//latest/meta-data/local-ipv4]"));
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// retry the same request 5 times at most
// Simulate an EC2 DescribeInstancesResponse
//" + InetAddresses.toUriString(address.getAddress()) + ":" + address.getPort();
/**
//ec2.amazonaws.com/doc/2013-02-01/";
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Builds the default request handlers **/
// https://cloud.google.com/compute/docs/storing-retrieving-metadata
// https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances
// https://cloud.google.com/compute/docs/reference/rest/v1/instances
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// cloud.gce settings
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// all settings just used for testing - not registered by default
//www.googleapis.com", Function.identity(), Property.NodeScope);
// hack around code messiness in GCE code
// TODO: get this fixed
// assist type inference
// assist type inference
/** Global instance of the HTTP transport. */
/** Global instance of the JSON factory. */
// this code is based on a private GCE method: {@link com.google.cloud.ServiceOptions#getAppEngineProjectIdFromMetadataServer()}
// com.google.cloud.ServiceOptions#headerContainsMetadataFlavor(HttpResponse)}
// this is only used for testing - alternative we could use the defaul keystore but this requires special configs too..
// Forcing Google Token API URL as set in GCE SDK to
//      http://metadata/computeMetadata/v1/instance/service-accounts/default/token
// See https://developers.google.com/compute/docs/metadata#metadataserver
// hack around code messiness in GCE code
// TODO: get this fixed
/*
//www.apache.org/licenses/LICENSE-2.0
// Forcing Google Token API URL as set in GCE SDK to
//      http://metadata/computeMetadata/v1/instance/service-accounts/default/token
// See https://developers.google.com/compute/docs/metadata#metadataserver
// all settings just used for testing - not registered by default
//metadata.google.internal", Function.identity(), Setting.Property.NodeScope);
/** Global instance of the HTTP transport. */
// Forcing Google Token API URL as set in GCE SDK to
//      http://metadata/computeMetadata/v1/instance/service-accounts/default/token
// See https://developers.google.com/compute/docs/metadata#metadataserver
// hack around code messiness in GCE code
// TODO: get this fixed
// This is needed to query meta data: https://cloud.google.com/compute/docs/metadata
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// We replace network placeholder with default network interface value: 0
// We extract the network interface from gce:privateIp:XX
// We replace network placeholder with network interface value
// only one address: because we explicitly ask for only one via the GceHostnameType
// using this, one has to explicitly specify _gce_ in network setting
// We only try to resolve network.host setting when it starts with _gce
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// We check that needed properties have been set
// We can't find the publish host address... Hmmm. Too bad :-(
// We won't simply filter it
// We don't want to connect to TERMINATED status instances
// See https://github.com/elastic/elasticsearch-cloud-gce/issues/3
// see if we need to filter by tag
// If this instance have no tag, we filter it
// check that all tags listed are there on the instance
// Trying to get Public IP Address (For future use)
// If we have both public and private, we can stop here
// We found the current node.
// We can ignore it in the list of DiscoveryNode
// Test if we have es_port metadata defined here
// Ignoring other values
// ip_private is a single IP Address. We need to build a TransportAddress from it
// If user has set `es_port` metadata, we don't need to ping all ports
/*
//www.apache.org/licenses/LICENSE-2.0
// Intercepts the request for filling in the "Authorization"
// header field, as well as recovering from certain unsuccessful
// error codes wherein the Credential must refresh its token for a
// retry.
// A sleeper; you can replace it with a mock in your test.
// Use only for testing.
// Use only for testing
// TODO: figure out why GCE is so bad like this
// If credential decides it can handle it,
// the return code or message indicated
// something specific to authentication,
// and no backoff is desired.
// Otherwise, we defer to the judgement of
// our internal backoff handler.
/*
//www.apache.org/licenses/LICENSE-2.0
/** Determines whether settings those reroutes GCE call should be allowed (for testing purposes only). */
// stashed when created in order to properly close
/*
// overrideable for tests
// Register GCE settings
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Holds a list of the current discovery nodes started in tests **/
// start master node
// start another node
// wait for the cluster to form
// add one more node and wait for it to join
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Slice off the "test" part of the method names so the project names
/**
// to prevent being resolved using default GCE host
//internal")
// to prevent being resolved using default GCE host
//internal")
// to prevent being resolved using default GCE host
//internal")
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//metadata.google.internal/computeMetadata/v1/";
//metadata.google.internal/");
//www.googleapis.com/");
// We extract from the url the mock file path we want to use
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/elastic/elasticsearch/issues/13605
/**
/**
/**
/**
/**
/**
/**
/**
// We were expecting something and not an exception
// We check that we get the expected exception
/*
//www.apache.org/licenses/LICENSE-2.0
// Return failure on the first call
// Return success on the second
//elasticsearch.com"), null);
// important number, use this to get count
//elasticsearch.com"), null);
// should only retry once.
//elasticsearch.com"), null);
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// Elasticsearch config directory
// Resolve the plugin's custom settings file
// Load the settings from the plugin's custom settings file
// Loads the secured setting from the keystore
/*
//www.apache.org/licenses/LICENSE-2.0
// asserts that the setting has been correctly loaded from the custom setting file
/**
// Exposes SIMPLE_SETTING and LIST_SETTING as a node settings
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// The test executes all the test candidates by default
// see ESClientYamlSuiteTestCase.REST_TESTS_SUITE
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Nothing to read
// Nothing to write
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/* Because Heuristics are XContent "fragments" we need to throw away
/*
//www.apache.org/licenses/LICENSE-2.0
// This is a pretty dumb implementation which returns the original text + fieldName + custom config option + 12 or 123
// create two suggestions with 12 and 123 appended
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/**
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// now we should have field name, check and copy fields over to the suggestion builder we return
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// electricity
// example augmentation method
// example method to attach annotations in whitelist
// some logic here
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** An extension of painless which adds a whitelist. */
/*
//www.apache.org/licenses/LICENSE-2.0
// we don't actually need anything here, since whitelists are extended through SPI
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
// Safe because doc ids >= 0
// Sort by score descending, then docID ascending, just like lucene's QueryRescorer
// Safe because doc ids >= 0
// Note that this is inaccurate because it ignores factor field
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Always use a factor > 1 so rescored fields are sorted in front of the unrescored fields.
// Skipping factorField because it is much harder to mock. We'll catch it in an integration test.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//" + externalAddress);
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// The test executes all the test candidates by default
// see ESClientYamlSuiteTestCase.REST_TESTS_SUITE
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** An example {@link ScriptEngine} that uses Lucene segment details to
// tag::expert_engine
// we use the script "source" as the script identifier
// optionally close resources
// PureDfLeafFactory only uses deterministic APIs, this
// implies the results are cacheable.
// Return true if the script needs the score
/*
/*
/*
// end::expert_engine
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// unauthorized
// authorized
// authorized
// unauthorized
// authorized
// unauthorized
/*
//www.apache.org/licenses/LICENSE-2.0
// If the user provided the number of characters to be extracted as part of the document, we use it
// If the field does not exist we fall back to the global limit
// tika 1.17 throws an exception when the InputStream has 0 bytes.
// previously, it did not mind. This is here to preserve that behavior.
// somehow tika seems to append a newline at the end automatically, lets remove that again
// TODO: stop using LanguageIdentifier...
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Exclude some formats */
/** subset of parsers for types we support */
// documents
/** autodetector based on this subset */
/** singleton tika instance */
/**
// check that its not unprivileged code like a script
// checked exception from tika: unbox it
// apply additional containment for parsers, this is intersected with the current permissions
// its hairy, but worth it so we don't have some XML flaw reading random crap from the FS
// compute some minimal permissions for parsers. they only get r/w access to the java temp directory,
// the ability to load some resources from JARs, and read sysprops
// property/env access needed for parsing
// add permissions for resource access:
// classpath
// plugin jars
// jvm's java.io.tmpdir (needs read/write)
// current hacks needed for POI/PDFbox issues:
// xmlbeans, use by POI, needs to get the context classloader
// ZipFile needs accessDeclaredMembers on JDK 10; cf. https://bugs.openjdk.java.net/browse/JDK-8187485
// TODO remove this and from plugin-security.policy when JDK 11 is the only one we support
// this is needed pre 11, but it's fixed in 11 : https://bugs.openjdk.java.net/browse/JDK-8187485
// add resources to (what is typically) a jar, but might not be (e.g. in tests/IDE)
/*
//www.apache.org/licenses/LICENSE-2.0
// ensure allowed fields are mentioned
/*
//www.apache.org/licenses/LICENSE-2.0
//date is not present in the html doc
// lt seems some standard for not detected
// no real detection, just rudimentary
// See (https://issues.apache.org/jira/browse/COMPRESS-432) for information
// about the issue that causes a zip file to hang in Tika versions prior to 1.18.
// behave like CBOR from time to time
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// don't try to parse extraN
/** some test files from tika test suite, zipped up */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** A {@link FieldMapper} for full-text fields with annotation markup e.g.
//Using the analyzer's default BUT need to do the same thing AnalysisRegistry.processAnalyzerFactory
// does to splice in new default of posIncGap=100 by wrapping the analyzer
/**
// Format is markdown-like syntax for URLs eg:
//   "New mayor is [John Smith](type=person&value=John%20Smith) "
//Check "=" sign wasn't in the pair string
//untyped value
// A utility class for use with highlighters where the content being highlighted
// needs plain text format for highlighting but marked-up format for token discovery.
// The class takes markedup format field values and returns plain text versions.
// When asked to tokenize plain-text versions by the highlighter it tokenizes the
// original markup form in order to inject annotations.
// already wrapped
// Abstracts if we are pulling from some pre-cached buffer of
// text tokens or directly from the wrapped TokenStream
// If we are at the right point to inject an annotation....
// Capture the text token's state for later replay - but
// with a zero pos increment so is same as annotation
// that is injected before it
// Buffer up all the other tokens spanned by this annotation to determine length.
//Default annotation type - in future AnnotationTokens may contain custom type info
// Set the annotation's attributes
// We may have multiple annotations at this location - stack them up
//Put at the head of the queue of tokens to be emitted
//Put after the head of the queue of tokens to be emitted
// Flag the inject annotation as null to prevent re-injection.
// Now pop the first of many potential buffered tokens:
// Already wrapped the Analyzer with an AnnotationAnalyzer
// Wrap the analyzer with an AnnotationAnalyzer that will inject required annotations
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Fast exit.
// Check to see if this new markup overlaps with any prior
// existing markup wins - we throw away the new markup that would span this position
// markup list is in start offset order so we can insert at this position then shift others right 
// metadata is key1=value&key2=value&.... syntax used for urls 
// Merge original annotations and search hits into a single set of markups for each passage
// Add search hits first - they take precedence over any other markup
// Now add original text's annotations - ignoring any that might conflict with the search hits markup.
// We should always have UTF-8 support
// its possible to have overlapping terms
// its possible a "term" from the analyzer could span a sentence boundary.
//we remove the paragraph separator if present at the end of the snippet (we used it as separator between values)
//and we trim the snippets too
//This is called from a highlighter where all of the field values are concatenated
// so each annotation offset will need to be adjusted so that it takes into account 
// the previous values AND the MULTIVAL delimiter
//add 1 for the fieldvalue separator character
/*
//www.apache.org/licenses/LICENSE-2.0
// Convert the marked-up values held on-disk to plain-text versions for highlighting
// Store the annotations in the hitContext
// Retrieve the annotations from the hitContext
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Use example of typed and untyped annotations
// Bad markup means value is treated as plain text and fed through tokenisation
//Check we have both text and annotation tokens
// ===== Code below copied from TextFieldMapperTests ========
// special case: default index analyzer
// special case: default search analyzer
// special case: default index/search analyzer
// Empty name not allowed in index created after 5.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Annotated fields wrap the usual analyzer with one that injects extra tokens
// Check that a structured token eg a URL can be highlighted in a query
// on marked-up
// content using an "annotated_text" type field.
//en.wikipedia.org/wiki/Key_Word_in_Context";
/*
//www.apache.org/licenses/LICENSE-2.0
// tweaking these settings is no longer allowed, the entire purpose of murmur3 fields is to store a hash
// this only exists so a check can be done to match the field type to using murmur3 hashing...
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// even setting to the default is not allowed, the setting is invalid
// even setting to the default is not allowed, the setting is invalid
/*
//www.apache.org/licenses/LICENSE-2.0
// we post parse it so we get the size stored, possibly compressed (source will be preParse)
// nothing to do here, we call the parent in postParse
// all are defaults, no need to write it at all
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// issue 5053
// check mapping again
// update some field in the mapping
// make sure size field is still in mapping
// check mapping again
// update some field in the mapping
// make sure size field is still in mapping
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Give more time to repository-azure to complete the snapshot operations
/*
//www.apache.org/licenses/LICENSE-2.0
// On Azure, if the location path is a secondary location, and the blob does not
// exist, instead of returning immediately from the getInputStream call below
// with a 404 StorageException, Azure keeps trying and trying for a long timeout
// before throwing a storage exception.  This can cause long delays in retrieving
// snapshots, so we first check if the blob exists before trying to open an input
// stream to it.
// Executing deletes in parallel since Azure SDK 8 is using blocking IO while Azure does not provide a bulk delete API endpoint
// TODO: Upgrade to newer non-blocking Azure SDK 11 and execute delete requests in parallel that way.
/*
//www.apache.org/licenses/LICENSE-2.0
// locationMode is set per repository, not per client
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Replaced by client
// If the user explicitly did not define a readonly value, we set it by ourselves depending on the location mode setting.
// For secondary_only setting, the repository should be read only
// Remove starting / if any
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// protected for testing
// eagerly load client settings so that secure settings are read
// non-static, package private for testing
// secure settings should be readable
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// 'package' for testing
// eagerly load client settings so that secure settings are read
/**
// Set timeout option if the user sets cloud.azure.storage.timeout or
// cloud.azure.storage.xxx.timeout (it's negative by default)
// We define a default exponential retry policy
// non-static, package private for testing
/**
// clients are built lazily by {@link client(String)}
/**
// We remove the container name from the path
// The 3 magic number cames from the fact if path is /container/path/to/myfile
// First occurrence is empty "/"
// Second occurrence is "container
// Last part contains "path/to/myfile" which is what we want to get
// We return the remaining end of the string
// Container name must be lower case.
// Container name must be lower case.
// uri.getPath is of the form /container/keyPath.* and we want to strip off the /container/
// this requires 1 + container.length() + 1, with each 1 corresponding to one of the /
// NOTE: this should be here: if (prefix == null) prefix = "";
// however, this is really inefficient since deleteBlobsByPrefix enumerates everything and
// then does a prefix match on the result; it should just call listBlobsByPrefix with the prefix!
// uri.getPath is of the form /container/keyPath.* and we want to strip off the /container/
// this requires 1 + container.length() + 1, with each 1 corresponding to one of the /
// uri.getPath is of the form /container/keyPath.* and we want to strip off the /container/
// this requires 1 + container.length() + 1, with each 1 corresponding to one of the /.
// Lastly, we add the length of keyPath to the offset to strip this container's path.
// package private for testing
/*
//www.apache.org/licenses/LICENSE-2.0
// prefix for azure client settings
/** Azure account name */
/** Azure key */
/** Azure SAS token */
/** max_retries: Number of retries in case of Azure errors. Defaults to 3 (RetryPolicy.DEFAULT_CLIENT_RETRY_COUNT). */
/**
/** The type of the proxy to connect to azure through. Can be direct (no proxy, default), http or socks */
/** The host name of a proxy to connect to azure through. */
/** The port of a proxy to connect to azure through. */
// copy-constructor
// Register the proxy if we have any
// Validate proxy settings
/**
// Get the list of existing named configurations
// in case no setting named "default" has been set, let's define our "default"
// as the first named config we get
// pkg private for tests
/** Parse settings for a single client. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//"
// we want all requests to fail at least once
// rarely up to 1mb
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// default chunk size
// chunk size in settings
// zero bytes is not allowed
// negative bytes not allowed
// greater than max chunk size not allowed
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//myaccount1.blob.my_endpoint_suffix"));
//myaccount2.blob.core.windows.net"));
//myaccount11.blob.core.windows.net"));
//myaccount12.blob.core.windows.net"));
// client 3 is missing
// update client settings
// old client 1 not changed
//myaccount11.blob.core.windows.net"));
// new client 1 is changed
//myaccount21.blob.core.windows.net"));
// old client 2 not changed
//myaccount12.blob.core.windows.net"));
// new client2 is gone
// client 3 emerged
//myaccount23.blob.core.windows.net"));
//myaccount1.blob.core.windows.net"));
// reinit with empty settings
// existing client untouched
//myaccount1.blob.core.windows.net"));
// new client also untouched
//myaccount1.blob.core.windows.net"));
// missing key
//myaccount1.blob.core.windows.net"));
// existing client untouched
//myaccount1.blob.core.windows.net"));
//myservice.azure.net/container/path/to/myfile"));
//myservice.azure.net/container/path/to/myfile"));
//127.0.0.1/container/path/to/myfile"));
//127.0.0.1/container/path/to/myfile"));
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// The recommended maximum size of a blob that should be uploaded in a single
// request. Larger files should be uploaded over multiple requests (this is
// called "resumable upload")
// https://cloud.google.com/storage/docs/json_api/v1/how-tos/resumable-upload
/**
/**
/**
// Strip path prefix and trailing slash
/**
/**
// non-static, package private for testing
/**
//cloud.google.com/storage/docs/json_api/v1/how-tos/resumable-upload
// We retry 410 GONE errors to cover the unlikely but possible scenario where a resumable upload session becomes broken and
// needs to be restarted from scratch. Given how unlikely a 410 error should be according to SLAs we retry only twice.
/**
//cloud.google.com/storage/docs/json_api/v1/how-tos/multipart-upload
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** A json Service Account file loaded from secure settings. */
/** An override for the Storage endpoint to connect to. */
/** An override for the Google Project ID. */
/** An override for the Token Server URI in the oauth flow. */
/**
/**
/** Name used by the client when it uses the Google Cloud JSON API. */
/** The credentials used by the client to connect to the Storage endpoint. */
/** The Storage endpoint URL the client should talk to. Null value sets the default. */
/** The Google project ID overriding the default way to infer it. Null value sets the default. */
/** The timeout to establish a connection */
/** The timeout to read data from an established connection */
/** The Storage client application name */
/** The token server URI. This leases access tokens in the oauth flow. */
// this won't find any settings under the default client,
// but it will pull all the fallback static settings
/**
// explicitly returning null here so that the default credential
// can be loaded later when creating the Storage client
/*
//www.apache.org/licenses/LICENSE-2.0
// package-private for tests
// eagerly load client settings so that secure settings are readable (not closed)
// overridable for tests
// Secure settings should be readable inside this method. Duplicate client
// settings in a format (`GoogleCloudStorageClientSettings`) that does not
// require for the `SecureSettings` to be open. Pass that around (the
// `GoogleCloudStorageClientSettings` instance) instead of the `Settings`
// instance.
/*
//www.apache.org/licenses/LICENSE-2.0
// package private for testing
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// build the new lazy clients
// make the new clients available
// release old clients
/**
/**
// requires java.lang.RuntimePermission "setFactory"
// Pin the TLS trust certificates.
// override token server URI
// Rebuild the service account credentials in order to use a custom Token url.
// This is mostly used for testing purpose.
/**
// Null or zero in settings means the default timeout
// negative value means using the default value
// -1 means infinite timeout
// 0 is the infinite timeout expected by Google Cloud SDK
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//" + InetAddresses.toUriString(address.getAddress()) + ":" + address.getPort();
// Auth
// Does bucket exists?
// HTTP server does not send a response
// HTTP server sends a partial response
// HTTP server does not send a response
// See {@link BaseWriteChannel#DEFAULT_CHUNK_SIZE}
// we want all requests to fail at least once
// we must reset the counters because the whole object upload will be retried
/* Resume Incomplete */, -1);
// read all the request body, otherwise the SDK client throws a non-retryable StorageException
// rarely up to 1mb
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// default chunk size
// chunk size in settings
// zero bytes is not allowed
// negative bytes not allowed
// greater than max chunk size not allowed
// override file, to check if we get latest contents
/**
// token content is unique per node (not per request)
// Batch requests are not retried so we don't want to fail them
// The batched request are supposed to be retried (not tested here)
/*
//www.apache.org/licenses/LICENSE-2.0
/** Generates a given number of GoogleCloudStorageClientSettings along with the Settings to build them from **/
/** Generates a random GoogleCloudStorageClientSettings along with the Settings to build it **/
//www.elastic.co", "http://metadata.google.com:88/oauth", "https://www.googleapis.com",
//www.elastic.co:443", "http://localhost:8443", "https://www.googleapis.com/oauth/token");
/** Generates a random GoogleCredential along with its corresponding Service Account file provided as a byte array **/
/*
//www.apache.org/licenses/LICENSE-2.0
//", "https://")
// client 3 is missing
// update client settings
// old client 1 not changed
// new client 1 is changed
// old client 2 not changed
// new client2 is gone
// client 3 emerged
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: See if we can get precise result reporting.
// This exception is ignored
// FSDataInputStream does buffering internally
// FSDataInputStream can open connections on read() or skip() so we wrap in
// HDFSPrivilegedInputSteam which will ensure that underlying methods will
// be called with the proper privileges.
// we pass CREATE, which means it fails if a blob already exists.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Only restrict permissions if not running with HA
// behaves like Files.createDirectories
// behaves like Files.createDirectories
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// initialize some problematic classes with elevated privileges
// hack: on Windows, Shell's clinit has a similar problem that on unix,
// but here we can workaround it for now by setting hadoop home
// on unix: we still want to set this to something we control, because
// if the user happens to have HADOOP_HOME in their environment -> checkHadoopHome goes boom
// TODO: remove THIS when hadoop is fixed
// try to clean up the hack
// try to clean up our temp dir too if we can
/*
// Make sure that the correct class loader was installed.
/*
//www.apache.org/licenses/LICENSE-2.0
// buffer size passed to HDFS read/write methods
// TODO: why 100KB?
// get configuration
// Disable FS cache
// Create a hadoop user
// Sense if HA is enabled
// HA requires elevated permissions during regular usage in the event that a failover operation
// occurs and a new connection is required.
// Create the filecontext with our user information
// This will correctly configure the filecontext to have our UGI as its internal user.
// Validate the authentication method:
// Check if the user added a principal to use, and that there is a keytab file provided
// Check to see if the authentication method is compatible
// Now we can initialize the UGI with the configuration.
// Debugging
// UserGroupInformation (UGI) instance is just a Hadoop specific wrapper around a Java Subject
// Convert principals of the format 'service/_HOST@REALM' by subbing in the local address for '_HOST'.
// Don't worry about host name resolution if they don't have the _HOST pattern in the name.
/*
// initialize our blobstore using elevated privileges.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We can do FS ops with only a few elevated permissions:
// 1) hadoop dynamic proxy is messy with access rules
// 2) allow hadoop to add credentials to our Subject
// 3) RPC Engine requires this for re-establishing pooled connections over the lifetime of the client
// If Security is enabled, we need all the following elevated permissions:
// 1) hadoop dynamic proxy is messy with access rules
// 2) allow hadoop to add credentials to our Subject
// 3) allow hadoop to act as the logged in Subject
// 4) Listen and resolve permissions for kerberos server principals
// We add the following since hadoop requires the client to re-login when the kerberos ticket expires:
// 5) All the permissions needed for UGI to do its weird JAAS hack
// 6) Additional permissions for the login modules
// Included later:
// 7) allow code to initiate kerberos connections as the logged in user
// Still far and away fewer permissions than the original full plugin policy
/**
// KERBEROS
// Leave room to append one extra permission based on the logged in user's info.
// Append a kerberos.ServicePermission to only allow initiating kerberos connections
// as the logged in user.
// SIMPLE
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ensure that keytab exists
// set principal names
// Create repository
//ha-hdfs/\",\n" +
// Get repository
// Failover the namenode to the second.
// Get repository again
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
///")));
// mirrors HdfsRepository.java behaviour
// disable file system cache
// set file system to TestingFs to avoid a bunch of security
// checks, similar to what is done in HdfsTests.java
// create the FileContext with our user
// Constructor will not create dir if read only
// blobContainer() will not create path if read only
// if not read only, directory will be created
/*
//www.apache.org/licenses/LICENSE-2.0
// Ony using a single node here since the TestingFs only supports the single-node case
//github.com/elastic/elasticsearch/issues/31498", HdfsRepositoryTests.isJava11());
///")
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//issues.apache.org/jira/browse/HADOOP-12829">https://issues.apache.org/jira/browse/HADOOP-12829</a>
/*
//www.apache.org/licenses/LICENSE-2.0
//github.com/elastic/elasticsearch/issues/31498", isJava11());
///")
// HDFS repository doesn't have precise cleanup stats so we only check whether or not any blobs were removed
/*
//www.apache.org/licenses/LICENSE-2.0
//github.com/elastic/elasticsearch/issues/31498", JavaVersion.current().equals(JavaVersion.parse("11")));
///")
// Test restore after index deletion
///").build()).get();
///]"));
///some/path").build()).get();
///").build()).get();
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// wrap hadoop rawlocalfilesystem to behave less crazy
// no execution, thank you very much!
// pretend we don't support symlinks (which causes hadoop to want to do crazy things),
// returning the boolean does not seem to really help, link-related operations are still called.
// unfortunately, specific exceptions are not guaranteed. don't wrap hadoop over a zip filesystem or something.
// we set similar values to raw local filesystem, except we are never a symlink
///"), wrap(LuceneTestCase.createTempDir()), configuration, "file", false);
// we do evil stuff, we admit it.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html">S3 Documentation</a>.
/**
// package private for testing
// S3 API only allows 1k blobs per delete so we split up the given blobs into requests of max. 1k deletes
// We are sending quiet mode requests so we can't use the deleted keys entry on the exception and instead
// first remove all keys that were sent in the request and then add back those that ran into an exception.
// The AWS client threw any unexpected exception and did not execute the request at all so we do not
// remove any keys from the outstanding deletes set.
// Stripping the trailing slash off of the common prefix
/**
// Extra safety checks
/**
// non-static, package private for testing
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// prefix for s3 client settings
/** Placeholder client name for normalizing client settings in the repository settings. */
/** The access key (ie login id) for connecting to s3. */
/** The secret key (ie password) for connecting to s3. */
/** The secret key (ie password) for connecting to s3. */
/** An override for the s3 endpoint to connect to. */
/** The protocol to use to connect to s3. */
/** The host name of a proxy to connect to s3 through. */
/** The port of a proxy to connect to s3 through. */
/** The username of a proxy to connect to s3 through. */
/** The password of a proxy to connect to s3 through. */
/** The socket timeout for connecting to s3. */
/** The number of retries to use when an s3 request fails. */
/** Whether retries should be throttled (ie use backoff). */
/** Whether the s3 client should use path style access. */
/** Whether chunked encoding should be disabled or not (Default is false). */
/** Credentials to authenticate with s3. */
/** The s3 endpoint the client should talk to, or empty string to use the default. */
/** The protocol to use to talk to s3. Defaults to https. */
/** An optional proxy host that requests to s3 should be made through. */
/** The port number the proxy host should be connected on. */
// these should be "secure" yet the api for the s3 client only takes String, so storing them
// as SecureString here won't really help with anything
/** An optional username for the proxy host, for basic authentication. */
/** An optional password for the proxy host, for basic authentication. */
/** The read timeout for the s3 client. */
/** The number of retries to use for the s3 client. */
/** Whether the s3 client should use an exponential backoff retry policy. */
/** Whether the s3 client should use path style access. */
/** Whether chunked encoding should be disabled or not. */
/**
// Normalize settings to placeholder client settings prefix so that we can use the affix settings directly
/**
// this won't find any settings under the default client,
// but it will pull all the fallback static settings
// pkg private for tests
/** Parse settings for a single client. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// minimum value
/**
/**
/**
//docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html)
/**
//docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html)
/**
/**
/**
/**
/**
/**
/**
// Parse and validate the user's S3 Storage Class setting
// We make sure that chunkSize is bigger or equal than/to bufferSize
// only use for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// kick jackson to do some static caching of declared members info
// ClientConfiguration clinit has some classloader problems
// TODO: fix that
// eagerly load client settings so that secure settings are read
// proxy method for testing
// named s3 client configuration settings
// secure settings should be readable
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/aws/aws-sdk-java/issues/856 for the related SDK issue
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// shutdown all unused clients
// others will shutdown on their respective release
// clients are built lazily by {@link client}
/**
/**
// proxy for testing
// If the endpoint configuration isn't set on the builder then the default behaviour is to try
// and work out what region we are in and use an appropriate endpoint - see AwsClientBuilder#setRegion.
// In contrast, directly-constructed clients use s3.amazonaws.com unless otherwise instructed. We currently
// use a directly-constructed client, and need to keep the existing behaviour to avoid a breaking change,
// so to move to using the builder we must set it explicitly to keep the existing behaviour.
//
// We do this because directly constructing the client is deprecated (was already deprecated in 1.1.223 too)
// so this change removes that usage of a deprecated API.
// pkg private for tests
// the response metadata cache is only there for diagnostics purposes,
// but can force objects from every response to the old generation.
// TODO: remove this leniency, these settings should exist together and be validated
// pkg private for tests
// the clients will shutdown when they will not be used anymore
// clear previously cached clients, they will be build lazily
// shutdown IdleConnectionReaper background thread
// it will be restarted on new client usage
// InstanceProfileCredentialsProvider as last item of chain
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// no less, no more
// including default
// test default exists and is an Instance provider
// test default exists and is an Instance provider
/*
//www.apache.org/licenses/LICENSE-2.0
// new settings
// reload S3 plugin settings
// check the not-yet-closed client reference still has the same credentials
// check credentials have been updated
/**
// eliminate thread name check as we create repo manually on test/main threads
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//" + InetAddresses.toUriString(address.getAddress()) + ":" + address.getPort();
// HTTP server does not send a response
// HTTP server sends a partial response
// HTTP server closes connection immediately
// HTTP server sends a partial response
// HTTP server does not send a response
// we want all requests to fail at least once
// initiate multipart upload request
// upload part request
// complete multipart upload request
// sends an error back or let the request time out
// rarely up to 1mb
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Fail the initialization request
// Fail the upload part request
// Fail the completion request
// Fits in 1 empty part
// Fits in 1 part exactly
// Fits in N parts exactly
// Fits in N parts plus a bit more
//empty acl
// it should init cannedACL correctly
// it should accept all aws cannedACLs
// it should default to `standard`
// it should accept [standard, standard_ia, onezone_ia, reduced_redundancy, intelligent_tiering]
/*
//www.apache.org/licenses/LICENSE-2.0
// Disable chunked encoding as it simplifies a lot the request parsing on the httpServer side
// Disable request throttling because some random values in tests might generate too many failures for the S3 client
/**
/**
// Amazon SDK client provides a unique ID per request
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO check is closed
// chunk < buffer should fail
// chunk > buffer should pass
// chunk = buffer should pass
// buffer < 5mb should fail
// eliminate thread name check as we create repo manually on test/main threads
/*
//www.apache.org/licenses/LICENSE-2.0
// only test different storage classes when running against the default endpoint, i.e. a genuine S3 service
// S3 is only eventually consistent for the list operations used by this assertions so we retry for 10 minutes assuming that
// listing operations will become consistent within these 10 minutes.
// S3 is only eventually consistent for the list operations used by this assertions so we retry for 10 minutes assuming that
// listing operations will become consistent within these 10 minutes.
// AWS S3 is eventually consistent so we retry for 10 minutes assuming a list operation will never take longer than that
// to become consistent.
// AWS S3 is eventually consistent so we retry for 10 minutes assuming a list operation will never take longer than that
// to become consistent.
// AWS S3 is eventually consistent so we retry for 10 minutes assuming a list operation will never take longer than that
// to become consistent.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// This implementation ensures, that we never write more than CHUNK_SIZE bytes:
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Create an index and index some documents
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// usually we have one, two, or three components from the header, the message, and a buffer
// NOTE: It is unsafe to share a reference of the internal structure, so we
// use the default implementation which will copy the bytes. It is unsafe because
// a netty ByteBuf might be pooled which requires a manual release to prevent
// memory leaks.
// NOTE: It is unsafe to share a reference of the internal structure, so we
// use the default implementation which will copy the bytes. It is unsafe because
// a netty ByteBuf might be pooled which requires a manual release to prevent
// memory leaks.
// nothing to do here
/*
//www.apache.org/licenses/LICENSE-2.0
// As we have copied the buffer, we can release the request
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// This is a little tricky. The embedded channel will complete the promise once it writes the message
// to its outbound buffer. We do not want to complete the promise until the message is sent. So we
// intercept the promise and pass a different promise back to the rest of the pipeline.
// This should be safe as we are not a real network channel
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// NioHttpRequest works from copied unpooled bytes no need to release anything
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// otherwise we leak threads since we never moved to started
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//");
/**
// strip protocol from origin
/**
// Not a CORS request so we cannot validate it. It may be a non CORS request.
// if the origin is the same as the host of the request, then allow
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// loop through all profiles and start them up, special handling for default one
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// randomize nio settings
/*
//www.apache.org/licenses/LICENSE-2.0
// Since we have keep-alive set to false, we should close the channel after the response has been
// flushed
// Set up an HTTP transport with only the CORS enabled setting
// inspect response and validate
// create an HTTP transport with CORS enabled and allow origin configured
// inspect response and validate
// create an HTTP transport with CORS enabled
// inspect response and validate
//" + originValue;
// inspect response and validate
// inspect response and validate
// There was a read. Do not close.
// There was a read. Do not close.
// There has not been a read, however there is still an inflight request. Do not close.
// No reads and no inflight requests, close
//localhost:9090/" + randomAlphaOfLength(8);
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// After closing the pipeline, we must poll to see if any new messages are available. This
// is because HTTP supports a channel being closed as an end of content marker.
/*
//www.apache.org/licenses/LICENSE-2.0
// random order execution
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Test pre-flight request
// Test short-circuited request
/*
//www.apache.org/licenses/LICENSE-2.0
// enable http
// check if opaque ids are monotonically increasing
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//check that skip_unavailable alone cannot be set
//check that seeds cannot be reset alone if skip_unavailable is set
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// the Elasticsearch process should die and disappear from the output of jps
// parse the logs and ensure that Elasticsearch died with the expected cause
// as the cluster is dead its state can not be wiped successfully so we have to bypass wiping the cluster
// increase the timeout here to 90 seconds to handle long waits for a green
// cluster health. the waits for green need to be longer than a minute to
// account for delayed shards
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// create a policy with AllPermission
// restrict ourselves to NoPermission
/**
//bugs.openjdk.java.net/browse/JDK-8129972
/*
//www.apache.org/licenses/LICENSE-2.0
// nothing should happen
/*
//www.apache.org/licenses/LICENSE-2.0
// added by env initialization
// added by env initialization
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** test generated permissions */
// make a fake ES home and ensure we only grant permissions to that.
// the fake es home
// its parent
// some other sibling
// double check we overwrote java.io.tmpdir correctly for the test
/** test generated permissions for all configured paths */
// needs to check settings for deprecated path
// make a fake ES home and ensure we only grant permissions to that.
// the fake es home
// its parent
// some other sibling
// double check we overwrote java.io.tmpdir correctly for the test
// check that all directories got permissions:
// bin file: ro
// lib file: ro
// modules file: ro
// config file: ro
// plugins: ro
// data paths: r/w
// logs: r/w
// temp dir: r/w
// PID file: delete only (for the shutdown hook)
//github.com/elastic/elasticsearch/issues/44558", Constants.WINDOWS);
// symlink
// broken symlink
/** When a configured dir is a symlink, test that permissions work on link target */
// see https://github.com/elastic/elasticsearch/issues/12170
// symlink
/**
// see javadocs
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Simple tests system call filter is working. */
/** command to try to run in tests */
// otherwise security manager will block the execution, no fun
// otherwise, since we don't have TSYNC support, rules are not applied to the test thread
// (randomizedrunner class initialization happens in its own thread, after the test thread is created)
// instead we just forcefully run it for the test thread here.
// we can't guarantee how its converted, currently its an IOException, like this:
/*
// make sure thread inherits this too (its documented that way)
// ok
/*
//www.apache.org/licenses/LICENSE-2.0
// successful removal here asserts that the runtime hook was installed in Command#main
// ensure that we dump the exception
// ensure that we dump the stack trace too
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// tests that custom settings are not overwritten by settings in the config file
// args should overwrite whatever is in the config
/*
//www.apache.org/licenses/LICENSE-2.0
// the first message is a warning for unsupported configuration files
/*
// synchronize the start of all threads
// wait for all threads to complete their iterations
// we appended an integer to each log message, use that for sorting
// to ensure enough markers that the GC should collect some when we force a GC below
// this has the side effect of caching a marker with this prefix
// this will free the weakly referenced keys in the marker cache
// the first message is a warning for unsupported configuration files
// need to use custom config path so we can use a custom log4j2.properties file for the test
/*
//www.apache.org/licenses/LICENSE-2.0
// don't rely on umask: ensure the keystore has minimal permissions
// so the test framework can cleanup
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
// force segments to exist on disk
// trigger a background merge that will be managed by the concurrent merge scheduler
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests plugin manager security check */
/** Test that we can parse the set of permissions correctly for a simple policy */
/** Test that we can parse the set of permissions correctly for a complex policy */
/** Test that we can format some simple permissions properly */
/** Test that we can format an unresolved permission properly */
/*
//www.apache.org/licenses/LICENSE-2.0
// bias towards timeout
// race whether timeout or success (but typically biased towards success)
// bias towards no timeout.
// here, it's ok for the exception not to bubble up. Accessing the future will yield the exception
// fixed_auto_queue_size wraps stuff into TimedRunnable, which is an AbstractRunnable
// bias towards timeout
// race whether timeout or success (but typically biased towards success)
// bias towards no timeout.
// while submit does return a Future, we choose to log exceptions anyway,
// since this is the semi-internal SafeScheduledThreadPoolExecutor that is being used,
// which also logs exceptions for schedule calls.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// be sure to create a "proper" boolean (True, False) for the first document so that automapping is correct
// verifying if we can still read some properties from cluster state api:
// Check some global properties:
// Check some index properties:
// wait for source index to be available on both nodes before starting shrink
// the default number of shards is now one so we have to set the number of shards to be more than one explicitly
// wait for source index to be available on both nodes before starting shrink
/**
// Make sure there are payloads and they are taken into account for the score
// the 'string' field has a boost of 4 in the mappings so it should get a payload boost
// Make sure the query can run on the whole index
// histogram on a long
// terms on a boolean
/**
/**
// if the node with the replica is the first to be restarted, while a replica is still recovering
// then delayed allocation will kick in. When the node comes back, the master will search for a copy
// but the recovering copy will be seen as invalid and the cluster health won't return to GREEN
// before timing out
// fail faster
/**
/* We've had bugs in the past where we couldn't restore
// make sure all recoveries are done
// Force flush so we're sure that all translog are committed
// We had a bug before where we failed to perform peer recovery with sync_id from 5.x to 6.x.
// We added this synced flush so we can exercise different paths of recovery code.
// synced flush is optional here
// Update a few documents so we are sure to have a translog
// flushing here would invalidate the whole thing
// Count the documents in the index to make sure we have as many as we put there
// Find the primaries
/* Mark if we see a primary that looked like it restored from the translog.
/**
// Create the index
// Refresh the index so the count doesn't fail
// Count the documents in the index to make sure we have as many as we put there
// Stick a routing attribute into to cluster settings so we can see it after the restore
// Stick a template into the cluster so we can see it after the restore
// Don't confuse other tests by applying the template
// Create the repo
/**
/**
// Check the snapshot metadata, especially the version
// Remove the routing setting and template so we can test restoring them.
// Restore
// Make sure search finds all documents
// Add some extra documents to the index to be sure we can still write to it after restoring it
// And count to make sure the add worked
// Make sure search finds all documents
// Clean up the index for the next iteration
// Check settings added by the restore process
// Check that the template was restored successfully
// TODO tests for upgrades after shrink. We've had trouble with shrink in the past.
// Only create the first version so we know how many documents are created when the index is first created
/**
/**
// less than 10% of the committed docs (see IndexSetting#FILE_BASED_RECOVERY_THRESHOLD_SETTING).
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//message field is removed as is expected to be provided by a field from a message
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//message field is meant to be overriden (see custom.test config), but is not provided.
//Expected is that it will be emptied
// if a field is defined to be overriden, it has to always be overriden in that appender.
//message field will have a single line with json escaped
//stacktrace field will have each json line will in a separate array element
// For the same key and X-Opaque-ID deprecation should be once
// For the same key and different X-Opaque-ID should be multiple times per key/x-opaque-id
//continuing with message1-ID1 in logs already, adding a new deprecation log line with message2-ID2
// need to use custom config path so we can use a custom log4j2.properties file for the test
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
// Create the repository before taking the snapshot.
// Allocating shards on the BWC nodes to makes sure that taking snapshot happens on those nodes.
// Allocating shards on all nodes, taking snapshots should happen on all nodes.
/*
//www.apache.org/licenses/LICENSE-2.0
// some of the windows test VMs are slow as hell
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// to account for slow as hell VMs
//we index docs with private randomness otherwise the two clusters end up with exactly the same documents
//given that this test class is run twice with same seed.
//this index with a single document is used to test partial failures
//verify that the order in which documents are returned when they all have the same score is the same
//github.com/elastic/elasticsearch/issues/40005")
//github.com/elastic/elasticsearch/issues/40005")
/*
//www.apache.org/licenses/LICENSE-2.0
// to account for slow as hell VMs
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// This plugin will NOT have a controller daemon
/**
/*
// this plugin will have a controller daemon
// this plugin will not have a controller daemon
// as there should only be a reference in the list for the module that had the controller daemon, we expect one here
// fail if the process does not die within one second; usually it will be even quicker but it depends on OS scheduling
// this plugin will have a controller daemon
// if the spawner were not skipping the Desktop Services Store files on macOS this would explode
// we do not ignore these files on non-macOS systems
/*
//www.apache.org/licenses/LICENSE-2.0
// ask for elasticsearch version to quickly exit if java is actually found (ie test failure)
// ask for elasticsearch version to quickly exit if java is actually found (ie test failure)
// this is a hack around the fact that we can't run a command in the same session as the same user but not as administrator.
// the keystore ends up being owned by the Administrators group, so we manually set it to be owned by the vagrant user here.
// from the server's perspective the permissions aren't really different, this is just to reflect what we'd expect in the tests.
// when we run these commands as a role user we won't have to do this
// cleanup from previous test
// once windows 2012 is no longer supported and powershell 5.0 is always available we can change this command
//verify ES can start, stop and run plugin list
//clean up sym link
// Create temporary directory with a space and link to real java home
//verify ES can start, stop and run plugin list
// we have to disable Log4j from using JMX lest it will hit a security
// manager exception before we have configured logging; this will fail
// startup since we detect usages of logging before it is configured
//localhost:9200/_nodes"));
//localhost:9200/_nodes"));
// Ensure that the exit code from the java command is passed back up through the shell script
// TODO: this should be checked on all distributions
// TODO: this should be checked on all distributions
// Run the cli tools from the tmp dir
// TODO: this should be checked on all distributions
/*
//www.apache.org/licenses/LICENSE-2.0
// windows 2012 r2 has powershell 4.0, which lacks Expand-Archive
// windows 2012 r2 has powershell 4.0, which lacks Expand-Archive
//127.0.0.1:9200"), null, null, installation.config("certs/ca/ca.crt"));
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// some config files were not removed
// keystore was removed
// doc files were removed
// sysvinit service file was not removed
// defaults file was not removed
/*
//www.apache.org/licenses/LICENSE-2.0
// runContainer also calls this, so we don't need this method to be annotated as `@After`
/**
/**
//localhost:9200/_xpack").execute().returnResponse().getStatusLine().getStatusCode();
/**
/**
// Move the auto-created one out of the way, or else the CLI prompts asks us to confirm
/**
/**
/**
/**
/**
// we have to disable Log4j from using JMX lest it will hit a security
// manager exception before we have configured logging; this will fail
// startup since we detect usages of logging before it is configured
// Make the temp directory and contents accessible when bind-mounted
// Restart the container
/**
// Make the local directory and contents accessible when bind-mounted
// Restart the container
/**
// ES_JAVA_OPTS_FILE
// File permissions need to be secured in order for the ES wrapper to accept
// them for populating env var values
// Restart the container
//localhost:9200/_nodes"));
/**
// Test relies on configuring security
// ELASTIC_PASSWORD_FILE
// Enable security so that we can test that the password has been used
// File permissions need to be secured in order for the ES wrapper to accept
// them for populating env var values
// Restart the container
// If we configured security correctly, then this call will only work if we specify the correct credentials.
// Also check that an unauthenticated call fails
//localhost:9200/_nodes").execute().returnResponse().getStatusLine().getStatusCode();
/**
// ES_JAVA_OPTS_FILE
// File permissions need to be secured in order for the ES wrapper to accept
// them for populating env var values
/**
// ES_JAVA_OPTS_FILE
// Set invalid file permissions
// Restart the container
/**
// This test relies on a CLI tool attempting to connect to Elasticsearch, and the
// tool in question is only in the default distribution.
// This will fail if the env var above is passed as a -E argument
/**
// Ensure that the exit code from the java command is passed back up through the shell script
/**
/**
/**
/**
/**
//label-schema.org/">Label Schema website</a>
//www.elastic.co/products/elasticsearch");
//www.elastic.co/guide/en/elasticsearch/reference/index.html");
//github.com/elastic/elasticsearch");
// TODO: we should check the actual version value
/**
//github.com/opencontainers/image-spec/blob/master/annotations.md">Open Containers Annotations</a>
//www.elastic.co/products/elasticsearch");
//www.elastic.co/guide/en/elasticsearch/reference/index.html");
//github.com/elastic/elasticsearch");
// TODO: we should check the actual version value
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we always run with java home when no bundled jdk is included, so this test would be repetitive
// we don't require java be installed but some images have it
// check startup script didn't change permissions
// add fake bin directory as if a plugin was installed
// removing must stop the service
// Before version 231 systemctl returned exit code 3 for both services that were stopped, and nonexistent
// services [1]. In version 231 and later it returns exit code 4 for non-existent services.
//
// The exception is Centos 7 and oel 7 where it returns exit code 4 for non-existent services from a systemd reporting a version
// earlier than 231. Centos 6 does not have an /etc/os-release, but that's fine because it also doesn't use systemd.
//
// [1] https://github.com/systemd/systemd/pull/3385
// it can be gc.log or gc.log.0.current
// TEST CASES FOR SYSTEMD ONLY
/**
//github.com/elastic/elasticsearch/issues/11594
//localhost:9200/_nodes"));
// Limits are changed on systemd platforms only
// Create a startup problem by adding an invalid YAML line to the config
// Make sure we don't pick up the journal entries for previous ES instances.
// The custom config directory is not under /tmp or /var/tmp because
// systemd's private temp directory functionally means different
// processes can have different views of what's in these directories
// we have to disable Log4j from using JMX lest it will hit a security
// manager exception before we have configured logging; this will fail
// startup since we detect usages of logging before it is configured
// backup
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// 20 min
// the distribution being tested
// the java installation already installed on the system
// the current installation of the distribution being tested
// a shell to run system commands with
// skip rest of tests once one fails
/** The {@link Distribution} that should be tested in this case */
/**
/**
// nothing, "installing" docker image is running it
// nothing, "installing" docker image is running it
/**
// If log file exists, then we have bootstrapped our logging and the
// error should be in the logs
// For systemd, retrieve the error from journalctl
// In Windows, we have written our stdout and stderr to files in order to run
// in the background
// Otherwise, error should be on shell stderr
/*
//www.apache.org/licenses/LICENSE-2.0
//localhost:9200"), userpass.getKey(), userpass.getValue(), null);
// delete each dir under data, not data itself
// HACK: windows asynchronously releases file locks after processes exit. Unfortunately there is no clear way to wait on
// those locks being released. We might be able to use `openfiles /query`, but that requires modifying global settings
// in our windows images with `openfiles /local on` (which requires a restart, thus needs to be baked into the images).
// The following sleep allows time for windows to release the data file locks from Elasticsearch which was stopped in the
// previous test.
//localhost:9200/_cluster/health?wait_for_status=green&timeout=180s"),
//localhost:9200"), userpass.getKey(), userpass.getValue(), null);
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: rewrite this test to not use a real second distro to try and install
/*
//www.apache.org/licenses/LICENSE-2.0
// config was removed
// sysvinit service file was removed
// defaults file was removed
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// runs the service command, dumping all log files on failure
// NOTE: service description is not attainable through any powershell api, so checking it is not possible...
// the process is stopped async, and can become a zombie process, so we poll for the process actually being gone
// stop is ok when not started
// check failure too
// TODO:
// custom SERVICE_USERNAME/SERVICE_PASSWORD
// custom SERVICE_LOG_DIR
// custom LOG_OPTS (looks like it currently conflicts with setting custom log dir)
// install and run with java opts
// install and run java opts Xmx/s (each data size type)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// in the future we'll run as a role user on Windows
/** This is an arbitrarily chosen value that gives Elasticsearch time to log Bootstrap
// at this time we only install the current version of archive distributions, but if that changes we'll need to pass
// the version through here
// If jayatana is installed then we try to use it. Elasticsearch should ignore it even when we try.
// If it doesn't ignore it then Elasticsearch will fail to start because of security errors.
// This line is attempting to emulate the on login behavior of /usr/share/upstart/sessions/jayatana.conf
// We need to give Elasticsearch enough time to print failures to stderr before exiting
// the tests will run as Administrator in vagrant.
// we don't want to run the server as Administrator, so we provide the current user's
// username and password to the process which has the effect of starting it not as Administrator.
// this starts the server in the background. the -d flag is unsupported on windows
// set up some asynchronous output handlers
// Clear the asynchronous event handlers
/*
//www.apache.org/licenses/LICENSE-2.0
// todo
// kill elasticsearch processes
// the view of processes returned by Get-Process doesn't expose command line arguments, so we use WMI here
// remove elasticsearch users
// when we run es as a role user on windows, add the equivalent here
// delete files that may still exist
// windows needs leniency due to asinine releasing of file locking async from a process exiting
// disable elasticsearch service
// todo add this for windows when adding tests for service intallation
// Doing rpm erase on both packages in one command will remove neither since both cannot be installed
// this may leave behind config files in /etc/elasticsearch, but a later step in this cleanup will get them
/*
//www.apache.org/licenses/LICENSE-2.0
/** The extension of this distribution's file */
/** Whether the distribution is intended for use on the platform the current JVM is running on */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// Run the container in the background
// The container won't run without configuring discovery
// Map ports in the container to the host, so that we can send requests
// Bind-mount any volumes
/**
// Give the container a chance to crash out
/**
// Give the container a chance to exit out
/**
// Remove the container, forcibly killing it if necessary
// I'm not sure why we're already removing this container, but that's OK.
// Null out the containerId under all circumstances, so that even if the remove command fails
// for some reason, the other tests will still proceed. Otherwise they can get stuck, continually
// trying to remove a non-existent container ID.
/**
/**
/**
/**
/**
// Don't leave orphaned containers
// Mount localPath to a known location inside the container, so that we can execute shell commands on it later
// Use a lightweight musl libc based small image
// And run inline commands via the POSIX shell
/**
/**
/**
// The final substring() is because we don't check the directory bit, and we
// also don't want any SELinux security context indicator.
/**
/**
// These are installed to help users who are working with certificates.
// We could run `yum list installed $pkg` but that causes yum to call out to the network.
// rpm does the job just as well.
// at this time we only install the current version of archive distributions, but if that changes we'll need to pass
// the version through here
/**
//localhost:9200/" + path));
// The format below extracts the .Config.Labels value, and prints it as json. Without the json
// modifier, a stringified Go map is printed instead, which isn't helpful.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// gc logs are verbose and not useful in this context
/**
/**
/**
/**
// vagrant creates /tmp for us in windows so we use that to avoid long paths
// TODO: just load this once
/**
// replace single backslash with forward slash, to avoid unintended escapes in scripts
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// in the future we'll run as a role user on Windows
// this isn't a first-class installation feature but we include it for convenience
// same
/**
// windows is always administrator, since there is no sudo
/*
//www.apache.org/licenses/LICENSE-2.0
// we shell out here because java's posix file permission view doesn't support special modes
// at this time we only install the current version of archive distributions, but if that changes we'll need to pass
// the version through here
/**
/**
/*");
// Sometimes the restart fails on Debian 10 with:
//    Job for systemd-journald.service failed because the control process exited with error code.
//    See "systemctl status systemd-journald.service" and "journalctl -xe" for details.]
//
// ...so run these commands in an attempt to figure out what's going on.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// generous timeout  as nested virtualization can be quite slow ...
// TODO: need a way to check if docker has security enabled, the yml config is not bind mounted so can't look from here
// this is fragile, but currently doesn't deviate from a single line enablement and not worth the parsing effort
// with security enabled, we may or may not have setup a user/pass, so we use a more generic port being available check.
// this isn't as good as a health check, but long term all this waiting should go away when node startup does not
// make the http port available until the system is really ready to serve requests
/**
// polls every second for Elasticsearch to be running on 9200
// ignore, only want to establish a connection
// we loop here rather than letting httpclient handle retries so we can measure the entire waiting time
// no cert, so don't use ssl
//localhost:9200/_cluster/health")
//localhost:9200/_cluster/health?wait_for_status=" + status + "&timeout=60s&pretty";
//localhost:9200/_cluster/health/" + index + "?wait_for_status=" + status + "&timeout=60s&pretty";
//localhost:9200/library/_doc/1?refresh=true&pretty")
//localhost:9200/library/_doc/2?refresh=true&pretty")
//localhost:9200/_count?pretty"));
//localhost:9200/_all"));
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Create a snapshot and delete it right away again to test the impact of each version's cleanup functionality that is run
// as part of the snapshot delete
// Every step creates one snapshot and we have to add one more for the temporary snapshot
// only restore from read-only repo in steps 3 and 4
// only create some snapshots in the first two steps
// Every step creates one snapshot
// Bwc lookup since the format of the snapshots response changed between versions
/*
//www.apache.org/licenses/LICENSE-2.0
// increase the timeout here to 90 seconds to handle long waits for a green
// cluster health. the waits for green need to be longer than a minute to
// account for delayed shards
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// wait for long enough that we give delayed unassigned shards to stop being delayed
// Ask for recovery to be quick
// if request goes to 7.5+ node
// if request goes to < 7.5 node
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if the node with the replica is the first to be restarted, while a replica is still recovering
// then delayed allocation will kick in. When the node comes back, the master will search for a copy
// but the recovering copy will be seen as invalid and the cluster health won't return to GREEN
// before timing out
// if the node with the replica is the first to be restarted, while a replica is still recovering
// then delayed allocation will kick in. When the node comes back, the master will search for a copy
// but the recovering copy will be seen as invalid and the cluster health won't return to GREEN
// before timing out
// fail faster
// make sure that we can index while the replicas are recovering
// make sure that we can index while the replicas are recovering
// if the node with the replica is the first to be restarted, while a replica is still recovering
// then delayed allocation will kick in. When the node comes back, the master will search for a copy
// but the recovering copy will be seen as invalid and the cluster health won't return to GREEN
// before timing out
// fail faster
// make sure that no shards are allocated, so we can make sure the primary stays on the old node (when one
// node stops, we lose the master too, so a replica will not be promoted)
// remove the replica and guaranteed the primary is placed on the old node
// wait for the primary to be assigned
// wait for all other shard activity to finish
// ensure the relocation from old node to new node has occurred; otherwise ensureGreen can
// return true even though shards haven't moved to the new node yet (allocation was throttled).
// if the node with the replica is the first to be restarted, while a replica is still recovering
// then delayed allocation will kick in. When the node comes back, the master will search for a copy
// but the recovering copy will be seen as invalid and the cluster health won't return to GREEN
// before timing out
// fail faster
// update
// triggers nontrivial promotion
// fail faster
// fail faster
// trigger a primary relocation by excluding the last old node with a shard filter
/**
// if the node with the replica is the first to be restarted, while a replica is still recovering
// then delayed allocation will kick in. When the node comes back, the master will search for a copy
// but the recovering copy will be seen as invalid and the cluster health won't return to GREEN
// before timing out
// fail faster
// index was created on a version that supports the replication of closed indices,
// so we expect the index to be closed and replicated
/**
// index is created on a version that supports the replication of closed indices,
// so we expect the index to be closed and replicated
/**
// allocate replica to node-2
// index was created on a version that supports the replication of closed indices,
// so we expect the index to be closed and replicated
/**
/**
// We have to spin synced-flush requests here because we fire the global checkpoint sync for the last write operation.
// A synced-flush request considers the global checkpoint sync as an going operation because it acquires a shard permit.
// cause assert busy to retry
// ensure the global checkpoint is synced; otherwise we might trim the commit with syncId
/** Ensure that we can always execute update requests regardless of the version of cluster */
// once detailed recoveries works, remove this if.
/**
// uncommitted docs must be less than 10% of committed docs (see IndexSetting#FILE_BASED_RECOVERY_THRESHOLD_SETTING).
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// to account for slow as hell VMs
// increase the timeout here to 90 seconds to handle long waits for a green
// cluster health. the waits for green need to be longer than a minute to
// account for delayed shards
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//localhost:9200";
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//localhost:9200";
//localhost:9201";
//evil-host:9200");
// a rejected origin gets a FORBIDDEN - 403
//localhost:9200";
//evil-host:9200";
// a rejected origin gets a FORBIDDEN - 403
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// enable http
// change values of deprecated settings so that accessing them is logged
// non-deprecated setting to ensure not everything is logged
/**
//github.com/elastic/elasticsearch/issues/19222")
// add at least one document for each index
// create indices with a single shard to reduce noise; the query only deprecates uniquely by index anyway
// trigger all index deprecations
/**
// deprecated settings should also trigger a deprecation warning
// trigger all deprecations
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Build our cluster settings
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// An ActionRequestValidationException isn't an ElasticsearchException, so when the code tries
// to work out the root cause, all it actually achieves is wrapping the actual exception in
// an ElasticsearchException. At least this proves that the root cause logic is executing.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// enable http
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// enable http
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/elastic/elasticsearch/issues/15335">Unsupported
/**
//tools.ietf.org/html/rfc2616#section-9.2">HTTP/1.1 - 9.2
/**
//tools.ietf.org/html/rfc2616#section-10.4.6">HTTP/1.1 -
/**
//github.com/elastic/elasticsearch/issues/17853">Issue
/*
//www.apache.org/licenses/LICENSE-2.0
// Make sure we have a few segments
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// nothing to do
/**
// nothing to do
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// List<String> casts
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// some of the windows test VMs are slow as hell
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// These match the values defined in org.elasticsearch.gradle.testclusters.ElasticsearchNode
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//github.com/elastic/elasticsearch/issues/45625")
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// TODO consider changing this implementation to emit tokens as-we-go 
// rather than buffering all. However this array is perhaps not the 
// bulk of memory usage (in practice the dupSequenceSpotter requires 
// ~5x the original content size in its internal tree ).
/*
// Revise prior captured State objects if the latest
// token is marked as a duplicate
// Reposition cursor to next free slot
// wrap around the buffer
// clean out the end of the tail that we may overwrite if the
// next iteration adds a new head
// tokenPos is now positioned on tail - emit any valid
// tokens we may about to overwrite in the next iteration
// end loop reading all tokens from stream
// Flush the buffered tokens
// We need to patch in the max sequence length we recorded at
// this position into the token state
// record the patched state
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Default implementation of {@link DisableGraphAttribute}. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// The maximum number of repetitions that are counted
// ==Performance info
// ==== RAM usage estimation settings ====
// Root node object plus inner-class reference to containing "this"
// (profiler suggested this was a cost)
// A TreeNode specialization with an array ref (dynamically allocated and
// fixed-size)
// A KeyedTreeNode specialization with an array ref (dynamically allocated
// and grown)
// A KeyedTreeNode specialization with a short-based hit count and a
// sequence of bytes encoded as an int
/**
/**
// Add latest byte to circular buffer
// replay updated sequence of bytes represented in the circular
// buffer starting from the tail
// The first tier of nodes are addressed using individual bytes from the
// sequence
// The final 3 bytes in the sequence are represented in an int
// where the 4th byte will contain a hit count.
/**
/**
// Node implementation for use at the root of the tree that sacrifices space
// for speed.
// A null-or-256 sized array that can be indexed into using a byte for
// fast access.
// Being near the root of the tree it is expected that this is a
// non-sparse array.
// Depths 0 and 1 use RootTreeNode impl and create
// RootTreeNodeImpl children
// Deeper-level nodes are less visited but more numerous
// so use a more space-friendly data structure
// Node implementation for use by the depth 3 branches of the tree that
// sacrifices speed for space.
// An array dynamically resized but frequently only sized 1 as most 
// sequences leading to end leaves are one-off paths.
// It is scanned for matches sequentially and benchmarks showed
// that sorting contents on insertion didn't improve performance.
// Create array adding new child with the byte sequence combined with hitcount of 1.
// Most nodes at this level we expect to have only 1 child so we start with the  
// smallest possible child array.
// Find existing child and if discovered increment count
// Grow array adding new child
// Combine the byte sequence with a hit count of 1 into an int.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The number of bytes per dimension, use {@link InetAddressPoint#BYTES} as max, because that is maximum we need to support */
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// record that this segment was merged with interleaved segments
// Return a new list that sort segments of the original one by name (older first)
// and then interleave them to colocate oldest and most recent segments together.
// smaller segment first
/*
//www.apache.org/licenses/LICENSE-2.0
// at most 4 comparisons
/*
/*
/*
// does not disjoint AND not within:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we use the max here since it's the only "true" estimation we can make here
// at least max(df) documents have that term. Sum or Averages don't seem
// to have a significant meaning here.
// TODO: Maybe it could also make sense to assume independent distributions of documents and eg. have:
//   df = df1 + df2 - (df1 * df2 / maxDoc)?
// we need to find out the minimum sumTTF to adjust the statistics
// otherwise the statistics don't match
// we are done that term doesn't exist at all
// here we try to add a little bias towards
// the more popular (more frequent) fields
// that acts as a tie breaker
// Use a value of ttf that is consistent with the doc freq (ie. gte)
// sort the terms to make sure equals and hashCode are consistent
// this should be a very small cost and equivalent to a HashSet but less object creation
/*
//www.apache.org/licenses/LICENSE-2.0
/** A {@link Query} that only matches documents that are greater than or equal
// Matching documents depend on the sequence of segments that the index reader
// wraps. Yet matches must be cacheable per-segment, so we need to incorporate
// the reader id in the identity of the query so that a cache entry may only
// be reused if this query is run against the same index reader.
/** Sole constructor. */
// Let's not cache this query, the cached iterator would use more memory
// and be slower anyway.
// Also, matches in a given segment depend on the other segments, which
// makes it a bad candidate for per-segment caching.
// skip directly to minDoc
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// If the sort order includes _doc, then the matches in a segment
// may depend on other segments, which makes this query a bad
// candidate for caching
// DVs use forward iterators so we recreate the iterator for each sort field
// every time we need to compare a document with the <code>after<code> doc.
// We could reuse the iterators when the comparison goes forward but
// this should only be called a few time per segment (binary search).
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// the match cost for two-phase iterators, 0 otherwise
// the current doc, used for comparison
// reference to a next element, see #topList
// An approximation of the iterator, or the iterator itself if it does not
// support two-phase iteration
/** Get the list of scorers which are on the current doc. */
// prepend w1 (iterator) to w2 (list)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The field used for collapsing **/
/** The collapse value for each top doc */
// Refers to one hit:
// Which shard (index into shardHits[]):
// True if we should use the incoming ScoreDoc.shardIndex for sort order
// Which hit within the shard:
// NOTE: we don't assert that shardIndex is -1 here, because caller could in fact have set it but asked us to ignore it now
/**
// Tie break: earlier shard wins
// Tie break in same shard: resolve however the
// shard had resolved it:
// These are really FieldDoc instances:
// Fail gracefully if API is misused:
// Returns true if first is < second
/**
// totalHits can be non-zero even if no hits were
// collected, when searchAfter was used:
// If any hit count is a lower bound then the merged
// total hit count is a lower bound as well
// Not done with this these TopDocs yet:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// expand to next break until we reach maxLen
// the current split is too big,
// so starting from the current term we try to find boundaries on the left first
// and then we try to expand the passage to the right with the remaining size
/**
/**
// Returns the last offset of the current split
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Finds the next word boundary **after** noMatchSize.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// append content before this start
// Look ahead to expand 'end' past all overlapping:
// in case match straddles past passage
// its possible a "term" from the analyzer could span a sentence boundary.
//we remove the paragraph separator if present at the end of the snippet (we used it as separator between values)
//and we trim the snippets too
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
//one single document at a time
// we only highlight one field, one document at a time
/**
// sum position increments beyond 1
// positions are in increasing order.   max(0,...) is just a safeguard.
//if original slop is 0 then require inOrder
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// LUCENE MONITOR
// TODO: remove me!
//flatten query with query boost
// SynonymQuery should be handled by the parent class directly.
// This statement should be removed when https://issues.apache.org/jira/browse/LUCENE-7484 is merged.
// if we have more than 16 terms
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
// noinspection ConstantConditions,AssertWithSideEffects
// noinspection ConstantConditions
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// these are parsed at startup, and we require that we are able to recognize the values passed in by the startup scripts
// not running from the official elasticsearch jar file (unit tests, IDE, uber client jar, shadiness)
// we are not in tests but build.snapshot is set, bail hard
/**
// be lenient when reading on the wire, the enumeration values from other versions might be different than what we know
// be lenient when reading on the wire, the enumeration values from other versions might be different than what we know
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
//we need to enforce this otherwise bw comp doesn't work properly, as "es." was the previous criteria to split headers in two sets
/**
/**
/**
//we need to enforce this otherwise bw comp doesn't work properly, as "es." was the previous criteria to split headers in two sets
/**
/**
/**
/**
/**
/**
/**
// was SearchContextException
/**
// for testing
/**
/**
/**
// Any additional metadata object added by the metadataToXContent method is ignored
// and skipped, so that the parser does not fail on unknown fields. The parser only
// support metadata key-pairs and metadata arrays of values.
// Parse the array and add each item to the corresponding list of metadata.
// Arrays of objects are not supported yet and just ignored and skipped.
//subclasses can print out additional metadata through the metadataToXContent method. Simple key-value pairs will be
//parsed back and become part of this metadata set, while objects and arrays are not supported when parsing back.
//Those key-value pairs become part of the metadata set and inherit the "es." prefix as that is currently required
//by addMetadata. The prefix will get stripped out when printing metadata out so it will be effectively invisible.
//TODO move subclasses that print out simple metadata to using addMetadata directly and support also numbers and booleans.
//TODO rename metadataToXContent and have only SearchPhaseExecutionException use it, which prints out complex objects
// Adds root causes as suppressed exception. This way they are not lost
// after parsing and can be retrieved using getSuppressed() method.
/**
/**
// No exception to render as an error
// Render the exception with a simple message
// Render the exception with all details
/**
// Root causes are parsed in the innerFromXContent() and are added as suppressed exceptions.
/**
/**
// ElasticsearchException knows how to guess its own root cause
/*
/**
// TODO: do we really need to make the exception name in underscore casing?
/**
/**
/**
//      22 was CreateFailedEngineException
// 26 was BatchOperationException
// 28 was DeleteFailedEngineException, deprecated in 6.0, removed in 7.0
// 47 used to be for IndexTemplateAlreadyExistsException which was deprecated in 5.1 removed in 6.0
// 51 used to be for IndexShardAlreadyExistsException which was deprecated in 5.1 removed in 6.0
// 54 was DocumentAlreadyExistsException, which is superseded by VersionConflictEngineException
// 59 used to be EsRejectedExecutionException
// 60 used to be for EarlyTerminationException
// 61 used to be for RoutingValidationException
// 64 was DeleteByQueryFailedEngineException, which was removed in 5.0
// 80 was IndexFailedEngineException, deprecated in 6.0, removed in 7.0
// 85 used to be for AlreadyExpiredException
// 87 used to be for MergeMappingException
// 93 used to be for IndexWarmerMissingException
// 110 used to be FlushNotAllowedEngineException
// 123 used to be IndexAlreadyExistsException and was renamed
// 124 used to be Script.ScriptParseException
// 127 used to be org.elasticsearch.search.SearchContextException
// 129 was EngineClosedException
// We need the exceptionClass because you can't dig it out of the constructor reliably.
/**
/**
// lower cases and adds underscores to transitions in a name
// copy it over here
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// dear god, if we got more than 10 levels down, WTF? just bail
/**
/**
/**
/**
/**
/**
/**
/*
// try to log the current stack trace
/**
//the index name from the failure contains the cluster alias when using CCS. Ideally failures should be grouped by
//index name and cluster alias. That's why the failure index name has the precedence over the one coming from the cause,
//which does not include the cluster alias.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// throw away all information about caller and run with our own privs
// check caller first, to see if they should be allowed to do this
// throw away all information about caller and run with our own privs
/**
// TODO: if we really need we can break out name (e.g. "hack" or "scriptEngineService" or whatever).
// but let's just keep it simple if we can.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// We need at least the major of the Lucene version to be correct.
// Our best guess is to use the same Lucene version as the previous
// version in the list, assuming that it didn't change. This is at
// least correct for patch versions of known minors since we never
// update the Lucene dependency for patch versions.
// this version is older than any supported version, so we
// assume it is the previous major to the oldest Lucene version
// that we know about
/**
/**
/**
/**
// this is some BWC for 2.x and before indices
// we don't support snapshot as part of the version here anymore
// we don't support qualifier as part of the version anymore
//we reverse the version id calculation based on some assumption as we can't reliably reverse the modulo
// TODO: 99 is leftover from alpha/beta/rc, it should be removed
/*
/**
// force the minimum compatibility for version 6 to 5.6 since we don't reference version 5 anymore
// force the minimum compatibility for version 7 to 6.8 since we don't reference version 6 anymore
// all major versions from 8 onwards are compatible with last minor series of the previous major
/**
// we jumped from 2 to 5
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Subclass NamedRegistry for easy registration
//Indexed scripts
// Persistent tasks:
// retention leases
// internal actions
// Scripts API
// Tasks API
// Ingest API
// CAT API
// Fully qualified to prevent interference with rest.action.count.RestCountAction
// Fully qualified to prevent interference with rest.action.indices.RestRecoveryAction
// register ActionType -> transportAction Map used by NodeClient
// bind the action as eager singleton, so the map binder one will reuse it
/*
//www.apache.org/licenses/LICENSE-2.0
// this does not set the listenerThreaded API, if needed, its up to the caller to set it
// since most times, we actually want it to not be threaded...
// this.listenerThreaded = request.listenerThreaded();
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/** Updates a document */
/** Deletes a document */
/** read a document write (index/delete/update) request */
/** write a document write (index/delete/update) request*/
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// needed for deserialization
/**
/**
/**
/**
/**
/**
/**
/**
/** returns the rest status for this response (based on {@link ShardInfo#status()} */
/**
//tools.ietf.org/html/rfc7231#section-7.1.2).
// encode the path components separately otherwise the path separators will be encoded
/**
// index uuid and shard id are unknown and can't be parsed back for now.
// skip potential inner objects for forward compatibility
// skip potential inner arrays for forward compatibility
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//constant to use when original indices are not applicable and will not be serialized across the wire
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// this future is done already - use a non-blocking method.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The index name to use when finding the shard to explain */
/** The shard number to use when finding the shard to explain */
/** Whether the primary or replica should be explained */
/** Whether to include "YES" decider decisions in the response instead of only "NO" decisions */
/** Whether to include information about the gathered disk information of nodes in the cluster */
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/** \
// end "cluster_info"
// end wrapping object
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// public for testing
// public for testing
// If we can use any shard, just pick the first unassigned one (if there are any)
// If we're looking for the primary shard, there's only one copy, so pick it directly
// the primary is assigned to a node other than the node specified in the request
// If looking for a replica, go through all the replica shards
// the request is to explain a replica shard already assigned on a particular node,
// so find that shard copy
// Pick the first replica at the very least
// In case there are multiple replicas where some are assigned and some aren't,
// try to find one that is unassigned at least
// prefer started shards to initializing or relocating shards because started shards
// can be explained
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// throws IAE if no nodes matched or maximum exceeded
/*
//www.apache.org/licenses/LICENSE-2.0
// NB checking for the existence of any node with this persistent ID, because persistent IDs are how votes are counted.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// the default for cluster health request is 0, not 1
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// the default for cluster health is 0, not 1
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// ClusterStateHealth fields
// ClusterHealthResponse fields
// ClusterStateHealth fields
// Can be absent if LEVEL == 'cluster'
// ClusterHealthResponse fields
/** needed for plugins BWC */
/**
//package private for testing
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// this should be executing quickly no need to fork off
// we want users to be able to call this even when there are global blocks, just to check the health (are there blocks?)
// we must use the state from the applier service, because if the state-not-recovered block is in place then the
// applier service has a different view of the cluster state from the one supplied here
// TransportMasterNodeAction implements the retry logic, which is triggered by passing a NotMasterException
// check that they actually exists in the meta data
// we check for a timeout here since this method might be called from the wait_for_events
// response handler which might have timed out already.
// if the state is sufficient for what we where waiting for we don't need to mark this as timedOut.
// We spend too much time in waiting for events such that we might already reached a valid state.
// this should not mark the request as timed out
// if we are waiting for all shards to be active, then the num of unassigned and num of initializing shards must be 0
// there are enough active shards to meet the requirements of the request
// no indices, make sure its RED
// missing indices, wait a bit more...
// one of the specified indices is not there - treat it as RED.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// for serialization
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// TODO: not ideal, make a better api for this (e.g. with jar metadata, and so on)
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// reread keystore from config file
// add the keystore to the original node settings object
// broadcast the new settings object (with the open embedded keystore) to all reloadable plugins
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we are only checking one task, we can optimize it
// The task exists, but doesn't support cancellation
// /In case the task has some child tasks, we need to wait for until ban is set on all nodes
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Node is no longer part of the cluster! Try and look the task up from the results index.
/**
// Task isn't running, go look in the task index
// Shift to the generic thread pool and let it wait for the task to complete so we don't block any important threads.
/**
/*
/**
// We haven't yet created the index for the task results so it can't be found.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// First populate all tasks
// Now go through all task group builders and add children to their parents
// we found parent in the list of tasks - add it to the parent list
// we got zombie or the parent was filtered out - add it to the top task list
// top level task - add it to the top task list
/**
/**
// If the node is no longer part of the cluster, oh well, we'll just skip it's useful information.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// It doesn't make sense to wait for List Tasks and it can cause an infinite loop of the task waiting
// for itself or one of its child tasks
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We add a state applier that will remove any dangling repository cleanup actions on master failover.
// This is safe to do since cleanups will increment the repository state id before executing any operations to prevent concurrent
// operations from corrupting the repository. This is the same safety mechanism used by snapshot deletes.
// Cluster is not affected but we look up repositories in metadata
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// to keep insertion order
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Override equals and hashCode for testing
// Override equals and hashCode for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we go async right away
// The index in the stale primary allocation request was green and hence filtered out by the store status
// request. We ignore it here since the relevant exception will be thrown by the reroute action later on.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
// both transient and persistent settings must be consistent by itself we can't allow dependencies to be
// in either of them otherwise a full cluster restart will break the settings validation
/*
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// allow for dedicated changes to the metadata blocks, so we don't block those to allow to "re-enable" it
// only one setting
// one of the settings above as the only setting in the request means - resetting the block!
// We're about to send a second update task, so we need to check if we're still the elected master
// For example the minimum_master_node could have been breached and we're no longer elected master,
// so we should *not* execute the reroute.
// The reason the reroute needs to be send as separate update task, is that all the *cluster* settings are encapsulate
// in the components (e.g. FilterAllocationDecider), so the changes made by the first call aren't visible
// to the components until the ClusterStateListener instances have been invoked, but are visible after
// the first update task has been completed.
//we wait for the reroute ack only if the update settings was acknowledged
// we return when the cluster reroute is acked or it times out but the acknowledged flag depends on whether the
// update settings was acknowledged
//if the reroute fails we only log
// now, reroute in case things that require it changed (e.g. number of replicas)
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we want consistent ordering here and these values might be generated from a set / map
// might be null if we include non-filtering aliases
/*
//www.apache.org/licenses/LICENSE-2.0
// all in memory work here...
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// chosen arbitrarily
/**
// This should not be possible as we are just rendering the xcontent in memory
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Using the generic instead of the snapshot threadpool here as the snapshot threadpool might be blocked on long running tasks
// which would block the request from getting an error response because of the ongoing task
// We only check metadata block, as we want to snapshot closed indices (which have a read block)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Cluster is not affected but we look up repositories in metadata
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// switch to GENERIC thread pool because it might be long running operation
// short-circuit if there are no repos, because we can not create GroupedActionListener of size 0
// run concurrently for all repos on GENERIC thread pool
// want non-current snapshots as well, which are found in the repository data
// only want current snapshots
/*
//www.apache.org/licenses/LICENSE-2.0
// When there is a master failure after a restore has been started, this listener might not be registered
// on the current master and as such it might miss some intermediary cluster states due to batching.
// Clean up listener in that case and acknowledge completion of restore operation to client.
// restore not completed yet, wait for next cluster state update
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Using the generic instead of the snapshot threadpool here as the snapshot threadpool might be blocked on long running tasks
// which would block the request from getting an error response because of the ongoing task
// Restoring a snapshot might change the global state and create/change an index,
// so we need to check for METADATA_WRITE and WRITE blocks
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Combine the index name in the context with the shard name passed in for the named object parser
// into a ShardId to pass as context for the inner parser.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
// timings stats
// Parse this old school style instead of using the ObjectParser since there's an impedance mismatch between how the
// object has historically been written as JSON versus how it is structured in Java.
// Unknown sub field, skip
// Unknown sub field, skip
// Unknown sub field, skip
// Unknown field, skip
/**
// First time here
// The time the last snapshot ends
// The time the first snapshot starts
// Update duration
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// BWC: only update timestamps when we did not get a start time from an old node
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Store node id for the snapshots that are currently running.
// This operation is never executed remotely
// This operation is never executed remotely
/*
//www.apache.org/licenses/LICENSE-2.0
// There are still some snapshots running - check their progress
// We don't have any in-progress shards, just return current stats
// First process snapshot that are currently processed
// We should have information about this shard from the shard:
// We have full information about this shard
// Now add snapshots on disk that are not currently running
// we've already found this snapshot in the current snapshot entries, so skip over
// neither in the current snapshot entries nor found in the repository
// ignoring unavailable snapshots, so skip over
// Translating both PARTIAL and SUCCESS to SUCCESS for now
// TODO: add the differentiation on the metadata level in the next major release
// Use current time to calculate overall runtime for in-progress snapshots that have endTime == 0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Best effort. Only compare cluster state version and master node id,
// because cluster state doesn't implement equals()
// Best effort. Only use cluster state version and master node id,
// because cluster state doesn't implement  hashcode()
/*
//www.apache.org/licenses/LICENSE-2.0
// very lightweight operation in memory, no need to fork to a thread
// cluster state calls are done also on a fully blocked cluster to figure out what is going
// on in the cluster. For example, which nodes have joined yet the recovery has not yet kicked
// in, we need to make sure we allow those calls
// return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA);
// filter out metadata that shouldn't be returned by the API
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// min/max
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// first index, uninitialized.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// now do the stats that should be deduped by hardware (implemented by ip deduping)
// TODO: do we need to report zeros?
/**
/**
// fd can be -1 if not supported on platform
// we still do min max calc on -1, so we'll have an indication
// of it not being supported on one of the nodes.
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// it may be that the master switched on us while doing the operation. In this case the status may be null.
// only the master node populates the status
// built from nodes rather than from the stream directly
// nodeStats and indicesStats are rebuilt from nodes
/*
//www.apache.org/licenses/LICENSE-2.0
// only report on fully started shards
// shard is closed - no stats is fine
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// TransportAction constructor
// Parser constructor
// CONTEXTS
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ignore
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// very lightweight operation in memory, no need to fork to a thread
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// indices options that require every specified index to exist, expand wildcards only to open
// indices, don't allow that no indices are resolved from wildcard expressions and resolve the
// expressions only against indices
/**
/**
/**
/**
// Since we need to support numbers AND strings here we have to use ValueType.INT.
/**
// Take the first action and complain if there are more than one actions
/**
/**
/**
/**
/**
/**
/**
//remove operations support wildcards among aliases, add operations don't
// equals, and hashCode implemented for easy testing of round trip
/**
// noinspection StatementWithEmptyBody
// nothing to do here, here for symmetry with IndicesAliasesRequest#readFrom
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we go async right away...
//Expand the indices names
// Resolve all the AliasActions into AliasAction instances and gather all the aliases
//for DELETE we expand the aliases
//for ADD and REMOVE_INDEX we just return the current aliases
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// very lightweight operation all in memory no need to fork to a thread pool
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// just execute locally....
// First, we check to see if the request requires a custom analyzer.  If so, then we
// need to build it and then close it after use.
// Otherwise we use a built-in analyzer, which should not be closed
// Get normalizer from indexAnalyzers
// Note that we always pass "" as the field to the various Analyzer methods, because
// the analyzers we use here are all field-specific and so ignore this parameter
// maybe unwrap analyzer from NamedAnalyzer
// note: this is not field-name dependent in our cases so we can leave out the argument
// divide charfilter, tokenizer tokenfilters
// analyzing only tokenizer
// analyzing each tokenfilter
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no need to use a thread pool, we go async right away
/*
//www.apache.org/licenses/LICENSE-2.0
// in order to advance the global checkpoint to the maximum sequence number, the (persisted) local checkpoint needs to be
// advanced first, which, when using async translog syncing, does not automatically hold at the time where we have acquired
// all operation permits. Instead, this requires and explicit sync, which communicates the updated (persisted) local checkpoint
// to the primary (we call this phase1), and phase2 can then use the fact that the global checkpoint has moved to the maximum
// sequence number to pass the verifyShardBeforeIndexClosing check and create a safe commit where the maximum sequence number
// is equal to the global checkpoint.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// wrap it in a type map if its not
// if it has a different type name, then unwrap and rewrap with _doc
/**
/**
/**
/**
/**
// EMPTY is safe here because we never call namedObject
//move to the first alias
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we go async right away
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Delete index should work by default on both open and closed indices.
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// don't wait for any active shards before proceeding, by default
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// shardsResultPerIndex is never modified after it is passed to this
// constructor so this is safe even though shardsResultPerIndex is a
// ConcurrentHashMap
/**
/**
/**
// treat all shard copies as failed
// some shards may have failed during the sync phase
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// to have deterministic order
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// very lightweight operation, no need to fork
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// former types array
// former probablySingleField boolean
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** @param fields a list of fields to retrieve the mapping for */
/** Indicates whether default mapping settings should be returned */
/*
//www.apache.org/licenses/LICENSE-2.0
/** A helper class to build {@link GetFieldMappingsRequest} objects */
/** Sets the fields to retrieve. */
/** Indicates whether default mapping settings should be returned */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// type
/** returns the retrieved field mapping. The return map keys are index, field (as specified in the request). */
/**
/** Returns the mappings as a map. Note that the returned map has a single key which is always the field's {@link Mapper#name}. */
//pkg-private for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//internal action, index already resolved
// Will balance requests between shards
// not a pattern
/*
//www.apache.org/licenses/LICENSE-2.0
// very lightweight operation, no need to fork
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
// reserve "null" for bwc.
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we go async right away
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we go async right away...
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Provides extra details in the response
// Only reports on active recoveries
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//TODO here we should just use TimeValue#writeTo and same for de-serialization in the constructor, we lose information this way
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//TODO here we should just use ByteSizeValue#writeTo and same for de-serialization in the constructor
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// a type is not included, add a dummy _doc type
//the index name "_na_" is never read back, what matters are settings, mappings and aliases
/**
/**
/**
/**
/**
/**
/**
// param isTypeIncluded decides how mappings should be parsed from XContent
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Needs to be duplicated, because shardsAcknowledged gets (de)serailized as last field whereas
// in other subclasses of ShardsAcknowledgedResponse this field (de)serailized as first field.
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we go async right away
// will fail if the index already exists
// conditions not met
/**
// not waiting for shards here, will wait on the alias switch operation
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
//we must assume this is an index entry
//should not be possible here
// no settings, jump over it to shorten the response data
/*
//www.apache.org/licenses/LICENSE-2.0
// Very lightweight operation
/*
//www.apache.org/licenses/LICENSE-2.0
// we go async right away....
// allow for dedicated changes to the metadata blocks, so we don't block those to allow to "re-enable" it
// we have to allow resetting these settings otherwise users can't unblock an index
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// clean up in case the body is wrapped with "settings" : { ... }
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// StoreStatus fields
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// collect relevant shard ids of the requested indices for fetching store infos
// async fetch store infos from all the nodes
// NOTE: instead of fetching shard store info one by one from every node (nShards * nNodes requests)
// we could fetch all shard store info from every node once (nNodes requests)
// we have to implement a TransportNodesAction instead of using TransportNodesListGatewayStartedShards
// for fetching shard stores info, that operates on a list of shards instead of a single shard
// explicitely type lister, some IDEs (Eclipse) are not able to correctly infer the function type
/**
// no-op
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we go async right away
// there is no need to fetch docs stats for split but we keep it simple and do it anyway for simplicity of the code
// static for unittesting this method
// we just execute this to ensure we get the right exceptions if the number of shards is wrong or less then etc.
// we just execute this to ensure we get the right exceptions if the number of shards is wrong etc.
// if we have a source index with 1 shards it's legal to set this
// mappings are updated on the node when creating in the shards, this prevents race-conditions since all mapping must be
// applied once we took the snapshot and if somebody messes things up and switches the index read/write and adds docs we
// miss the mappings for everything is corrupted and hard to debug
/*
//www.apache.org/licenses/LICENSE-2.0
// shard is closed - no stats is fine
/**
// note, requires a wrapping object
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// 14 was previously used for Suggest
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if we don't have the routing entry yet, we need it stats wise, we treat it as if the shard is not ready yet
// shard is closed - no stats is fine
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we go async right away
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// If we did not ask for a specific name, then we return all templates
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// wrap it in a type map if its not
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// EMPTY is safe here because we never call namedObject
//move to the first alias
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we go async right away...
// templates must be consistent with regards to dependencies
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: this comparison is bogus! it would cause us to upgrade even with the same format
// instead, we should check if the codec has changed
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We already have versions for this index - let's see if we need to update them based on the current shard
// For the metadata we are interested in the _latest_ Elasticsearch version that was processing the metadata
// Since we rewrite the mapping during upgrade the metadata is always rewritten by the latest version
// For the lucene version we are interested in the _oldest_ lucene version since it determines the
// oldest version that we need to support
// We are using the current version of Elasticsearch as upgrade version since we update mapping to match the current version
/**
// If some primary shards are not available the request should fail.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we go async right away....
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no types to filter
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// totalShards is documented as "the total shards this request ran against",
// which is 0 since the failure is happening on the coordinating node.
// Random routing to limit request to a single shard
// simply ignore non active shards
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
// no types to filter
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// NOTE: public for testing only
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/** For write failures after operation was assigned a sequence number. */
/**
// can't make an assertion about type names here because too many tests still set their own
// types bypassing various checks
/**
/**
/**
/**
/**
/**
/**
/**
// make 3 instead of 2, because 2 is already in use for 'no responses'
/**
/**
/**
/**
/**
/**
/**
/**
/**
// make 3 instead of 2, because 2 is already in use for 'no responses'
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Item execution is ready to start, no operations have been performed yet */
/**
/**
/**
/** The request has been executed on the primary shard (successfully or not) */
/**
/** move to the next item to execute */
/** gets the current, untranslated item request */
/** returns the result of the request that has been executed on the shard */
/** returns the number of times the current operation has been retried */
/** returns true if the current request has been executed on the primary */
/** returns true if the request needs to wait for a mapping update to arrive from the master */
/** returns true if the current request should be retried without waiting for an external event */
/**
/**
/**
/** returns the name of the index the current request used */
/** returns a translog location that is needed to be synced in order to persist all operations executed so far */
// we always get to the end of the list by using advance, which in turn sets the state to INITIAL
/** returns the primary shard */
/**
/** returns the request that should be executed on the shard. */
/** indicates that the current operation can not be completed and needs to wait for a new mapping from the master */
/** resets the current item state, prepare for a new execution */
/** completes the operation without doing anything on the primary */
/** indicates that the operation needs to be failed as the required mapping didn't arrive in time */
// Make sure to use getCurrentItem().index() here, if you use docWriteRequest.index() it will use the
// concrete index instead of an alias if used!
/** the current operation has been executed on the primary with the specified result */
// set a blank ShardInfo so we can safely send it to the replicas. We won't use it in the real response though.
// Make sure to use request.index() here, if you
// use docWriteRequest.index() it will use the
// concrete index instead of an alias if used!
/** finishes the execution of the current request, with the response that should be returned to the user */
/** builds the bulk shard response to return to the user */
// requestToExecute can be null if the update ended up as NOOP
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Start period flushing task after everything is setup
/**
/**
/**
/**
/**
/**
//bulkRequest and instance swapping is not threadsafe, so execute the mutations under a lock.
//once the bulk request is ready to be shipped swap the instance reference unlock and send the local reference to the handler.
//execute sending the local reference outside the lock to allow handler to control the concurrency via it's configuration.
/**
// needs to be executed under a lock
// may be executed without a lock
// needs to be executed under a lock
// needs to be executed under a lock
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// lack of source is validated in validate() method
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// We first check if refresh has been set
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if we fail on client.bulk() release the semaphore
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: Remove this parameter once the BulkMonitoring endpoint has been removed
/**
/**
/**
// now parse the action
// EMPTY is safe here because we never call namedObject
// move pointers
// Move to START_OBJECT
// Move to FIELD_NAME, that's the action
// at this stage, next token can either be END_OBJECT (and use default index and type, with auto generated id)
// or START_OBJECT which will have another set of parameters
// we use internalAdd so we don't fork here, this allows us not to copy over the big byte array to small chunks
// of index request.
// EMPTY is safe here because we never call namedObject
// move pointers
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// A bulk shard request encapsulates items targeted at a specific shard of an index.
// However, items could be targeting aliases of the index, so the bulk request although
// targeting a single concrete index shard might do so using several alias names.
// These alias names have to be exposed by this method because authorization works with
// aliases too, specifically, the item's target alias can be authorized but the concrete
// index might not be.
// This is included in error messages so we'll try to make it somewhat user friendly.
// all replication requests need to be notified here as well to ie. make sure that internal optimizations are
// disabled see IndexRequest#canHaveDuplicates()
/*
//www.apache.org/licenses/LICENSE-2.0
// NOTE: public for testing only
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Access only when holding a client-side lock, see also #addResponses()
// needed to construct the next bulk request based on the response to the previous one
// volatile as we're called from a scheduled thread
// in contrast to System.currentTimeMillis(), nanoTime() uses a monotonic clock under the hood
// we're done here, include all responses
// Use client-side lock here to avoid visibility issues. This method may be called multiple times
// (based on how many retries we have to issue) and relying that the response handling code will be
// scheduled on the same thread is fragile.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Each index request needs to be evaluated, because this method also modifies the IndexRequest
// this method (doExecute) will be called again, but with the bulk requests updated from the ingest node processing but
// also with IngestService.NOOP_PIPELINE_NAME on each request. This ensures that this on the second time through this method,
// this path is never taken.
// Attempt to create all the indices that we're going to need during the bulk before we start.
// Step 1: collect all the indices in the request
// delete requests should not attempt to create the index (if the index does not
// exists), unless an external versioning is used
/* Step 2: filter that to indices that don't exist and we can create. At the same time build a map of indices we can't create
// Step 3: create all the indices that are missing, if there are any missing. start the bulk after all the creates come back.
// fail all requests involving this index, if create didn't work
// start to look for default or final pipelines via settings found in the index meta data
// check the alias for the index request (this is how normal index requests are modeled)
// check the alias for the action request (this is how upserts are modeled)
// find the default pipeline if one is defined from an existing index setting
// find the final pipeline if one is defined from an existing index setting
// the index does not exist yet (and this is a valid request), so match index templates to look for pipelines
// order of templates are highest order first
// we can not break in case a lower-order template has a final pipeline that we need to collect
// we can not break in case a lower-order template has a default pipeline that we need to collect
// we can break if we have already collected a default and final pipeline
/*
// return whether this index request has a pipeline
/**
//the request can only be null because we set it to null in the previous step, so it gets ignored
// check if routing is required, if so, throw error if routing wasn't specified
// make sure the request gets never processed again
// first, go over all the requests and create a ShardId -> Operations mapping
// we may have no response if item failed
// create failures for all relevant requests
// we running as a last attempt after a timeout has happened. don't retry
// Try one more time...
// make sure the request gets never processed again
// at this stage, the transport bulk action can't deal with a bulk request with no requests,
// so we stop and send an empty response back to the client.
// (this will happen if pre-processing all items in the bulk failed)
// If a processor went async and returned a response on a different thread then
// before we continue the bulk request we should fork back on a write thread:
// If we fork back to a write thread we **not** should fail, because tp queue is full.
// (Otherwise the work done during ingest will be lost)
// It is okay to force execution here. Throttling of write requests happens prior to
// ingest when a node receives a bulk request.
// oversize, but that's ok
// We hit a error during preprocessing a request, so we:
// 1) Remember the request item slot from the bulk, so that we're done processing all requests we know what failed
// 2) Add a bulk item failure for this request
// 3) Continue with the next request in the bulk.
/*
//www.apache.org/licenses/LICENSE-2.0
/** Performs shard-level bulk (index, delete or update) operations */
// We are waiting for a mapping update on another thread, that will invoke this action again once its done
// so we just break out here.
// either completed and moved to next or reset
// We're done, there's no more operations to execute so we resolve the wrapped listener
// Fail all operations after a bulk rejection hit an action that waited for a mapping update and finish the request
/**
// we may fail translating a update to index or delete operation
// we use index result to communicate failure while translating update request
// execute translated update request
// also checks that we're in TRANSLATED state
// Requesting mapping update failed, so we don't have to wait for a cluster state update
/**
// ignore replication as we didn't generate a sequence number for this request.
// primary is on older version, just take the current primary term
// ignore replication as it's a noop
// Even though the primary waits on all nodes to ack the mapping changes to the master
// (see MappingUpdatedAction.updateMappingOnMaster) we still need to protect against missing mappings
// and wait for them. The reason is concurrent requests. Request r1 which has new field f triggers a
// mapping update. Assume that that update is first applied on the primary, and only later on the replica
// (it’s happening concurrently). Request r2, which now arrives on the primary and which also has the new
// field f might see the updated mapping (on the primary), and will therefore proceed to be replicated
// to the replica. When it arrives on the replica, there’s no guarantee that the replica has already
// applied the new mapping, so there is no other option than to wait.
/*
//www.apache.org/licenses/LICENSE-2.0
/** use transport bulk action directly */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: AggregatedDfs. Currently the idf can be different then when executing a normal search with explain.
// Fail fast on the node that received the request.
// No need to check the type, IndexShard#get does it for us
// Advantage is that we're not opening a second searcher to retrieve the _source. Also
// because we are working in the same searcher in engineGetResult we can be sure that a
// doc isn't deleted between the initial get and this call.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// provide predictable order
// provide predictable order
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/* Eclipse can't deal with o -> o.name, maybe because of
// Iff this field is searchable in some indices AND non-searchable in others
// we record the list of non-searchable indices
// Iff this field is aggregatable in some indices AND non-searchable in others
// we keep the list of non-aggregatable indices
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// For serialization
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// pkg private API mainly for cross cluster search to signal that we do multiple reductions ie. the results should not be merged
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// in the case we have one or more remote indices but no local we don't expand to all local indices and just do remote indices
// TODO we should somehow inform the user that we failed
// this is the cross cluster part of this API - we force the other cluster to not merge the results but instead
// send us back all individual index results.
// we need to merge on this node
/*
//www.apache.org/licenses/LICENSE-2.0
//internal action, index already resolved
// Will balance requests between shards
// Resolve patterns and deduplicate
// add nested and object fields
// we added this path on another field already
// checks if the parent field contains sub-fields
// no field type, it must be an object field
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// At this stage we ensure that we parsed enough information to return
// a valid GetResponse instance. If it's not the case, we throw an
// exception so that callers know it and can handle it correctly.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// If unknown tokens are encounter then these should be ignored, because
// this is parsing logic on the client side.
// If unknown tokens are encounter then these should be ignored, because
// this is parsing logic on the client side.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// update the routing (request#index here is possibly an alias)
// Fail fast on the node that received the request.
// we are not tied to a refresh cycle here anyway
/*
//www.apache.org/licenses/LICENSE-2.0
// only failures..
// create failures for all relevant requests
/*
//www.apache.org/licenses/LICENSE-2.0
// we are not tied to a refresh cycle here anyway
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// might as well check for routing here
// generate id if not already provided
// extra paranoia
/* resolve the routing if needed */
// ignore
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// We need a map here because order does not matter for equality
// We only take the sum here to ensure that the order does not matter.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// else it is a value skip it
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// ignore
/*
//www.apache.org/licenses/LICENSE-2.0
/* we should never reject resync because of thread pool capacity on primary */);
// resync should never be blocked because it's an internal action
// resync should never be blocked because it's an internal action
/*
// skip reroute phase
// noinspection ForLoopReplaceableByForEach
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// we need to add 1 for non active partition, since we count it in the total. This means for each shard in the iterator we sum up
// it's number of active shards but use 1 as the default if no replica of a shard is active at this point.
// on a per shards level we use shardIt.remaining() to increment the totalOps pointer but add 1 for the current shard result
// we process hence we add one for the non active partition here.
// in the case were we have less shards than maxConcurrentRequestsPerNode we don't need to throttle
/**
/**
//no search shards to search on, bail with empty response
//(it happens with search across _all with no indices around and consistent with broadcast operations)
// total hits is null in the response if the tracking of total hits is disabled
// Fail-fast verification of all shards being available
//Status red - shard is missing all copies and would produce partial results for an index search
/*
/*
/**
// we can not allow a stuffed queue to reject execution here
/* This is the main search phase transition where we move to the next phase. At this point we check if there is
// we have 0 successful results that means we shortcut stuff and return a failure
// check if there are actual failures in the atomic array since
// successful retries can reset the failures to null
// we always add the shard failure for a specific shard instance
// we do make sure to clean it on a successful response from a shard
// trace log this exception
// no more shards active, add a failure
// do not double log this exception
/**
/**
// we don't aggregate shard failures on non active shards (but do keep the header counts right)
// lazily create shard failures, so we can early build the empty shard failure list in most cases (no failures)
// this is double checked locking but it's fine since SetOnce uses a volatile read internally
// read again otherwise somebody else has created it?
// still null so we are the first and create a new instance
// the failure is already present, try and not override it with an exception that is less meaningless
// for example, getting illegal shard state
// if this shard was successful before (initial phase) we have to adjust the counter
/**
// clean a previous error on this shard group (note, this code will be serialized on the same shardIndex value level
// so its ok concurrency wise to miss potentially the shard failures being created because of another failure
// in the #addShardFailure, because by definition, it will happen on *another* shardIndex
// we need to increment successful ops first before we compare the exit condition otherwise if we
// are fast we could concurrently update totalOps but then preempt one of the threads which can
// cause the successor to read a wrong value from successfulOps if second phase is very fast ie. count etc.
// increment all the "future" shards to update the total ops since we some may work and some may not...
// and when that happens, we break on total ops, so we must maintain them
/**
/**
// as a tribute to @kimchy aka. finishHim()
/**
// only poll if we don't have anything to execute
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//We set max concurrent shard requests to the number of shards so no throttling happens for can_match requests
// this is a special case where we have no hit but we need to get at least one search response in order
// to produce a valid search result with all the aggs etc.
// unneeded
// we have to carry over shard failures in order to account for them in the response.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO we can potentially also consume the actual per shard results from the initial phase here in the aggregateDfs
// to free up memory early
// the query might not have been executed at all (for example because thread pool rejected
// execution) and the search context that was created in dfs phase might not be released.
// release it again to be in the safe side
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we do the heavy lifting in this inner run method where we reduce aggs etc. that's why we fork this phase
// off immediately instead of forking when we send back the response to the user since there we only need
// to merge together the fetched results which is a linear operation.
// query AND fetch optimization
// no docs to fetch -- sidestep everything and return
// we have to release contexts here to free up resources
// we count down every shard in the result no matter if we got any results or not
// no results for this shard ID
// if we got some hits from this shard we have to release the context there
// we do this as we go since it will free up resources and passing on the request on the
// transport layer is cheap.
// in any case we count down this result since we don't talk to this shard anymore
// the search context might not be cleared on the node where the fetch was executed for example
// because the action was rejected by the thread pool. in this case we need to send a dedicated
// request to clear the search context.
/**
// we only release search context that we did not fetch from if we are not scrolling
// and if it has at lease one hit that didn't make it to the global topDocs
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Could be TOP_SCORES but it is always used in a MultiCollector anyway, so this saves some wrapping.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// now parse the action
// move pointers
// now for the body
// move pointers
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// This parsing logic is a bit tricky here, because the multi search response itself is tricky:
// 1) The json objects inside the responses array are either a search response or a serialized exception
// 2) Each response json object gets a status field injected that ElasticsearchException.failureFromXContent(...) does not parse,
//    but SearchResponse.innerFromXContent(...) parses and then ignores. The status field is not needed to parse
//    the response item. However in both cases this method does need to parse the 'status' field otherwise the parsing of
//    the response item in the next json array element will fail due to parsing errors.
// Ignore the status value
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// some impls need to override this
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// EWMA/queue size may be -1 if the query node doesn't support capturing it
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO maybe we can make this concrete later - for now we just implement this in the base class for all initial phases
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// TODO we can move this loop into the reduce call to only loop over this once
/* We loop over all results once, group together the completion suggestions if there are any and collect relevant
// already consumed?
// make sure we set the shard index before we add it - the consumer didn't do that yet
// no relevant docs
// only one shard and no pagination we can just return the topDocs as we got them.
// from is always zero as when we use scroll, we ignore from
// with collapsing we can have more hits than sorted docs
/**
/**
// this can happen if we are hitting a shard failure during the fetch phase
// in this case we referenced the shard result via the ScoreDoc but never got a
// result from fetch.
// TODO it would be nice to assert this in the future
// clean the fetch counter
// with collapsing we can have more fetch hits than sorted docs
// merge hits
// this can happen if we are hitting a shard failure during the fetch phase
// in this case we referenced the shard result via the ScoreDoc but never got a
// result from fetch.
// TODO it would be nice to assert this in the future
/**
/**
/**
// increment for this phase
// early terminate we have nothing to reduce
// we already have results from intermediate reduces and just need to perform the final reduce
// the number of shards was less than the buffer size so we reduce agg results directly
// no aggregations
// count the total (we use the query result provider here, since we might not get any hits (we scrolled past them))
// the sum of all hits across all reduces shards
// the number of returned hits (doc IDs) across all reduces shards
// the max score across all reduces hits or {@link Float#NaN} if no hits returned
// <code>true</code> if at least one reduced result timed out
// non null and true if at least one reduced result was terminated early
// the reduced suggest results
// the reduced internal aggregations
// the reduced profile results
// the number of reduces phases
//encloses info about the merged top docs, the sort fields used to sort the score docs etc.
// the size of the top hits to return
// <code>true</code> iff the query phase had no results. Otherwise <code>false</code>
// the offset into the merged top hits
// sort value formats used to sort / format the result
/**
/**
/**
// no need to buffer anything if we have less expected results. in this case we don't consume any results ahead of time.
// we have to merge here in the same way we collect on a shard
// can't be null
/**
// no matter what the value of track_total_hits is
/**
// no incremental reduce if scroll is used - we only hit a single shard or sometimes more...
// only use this if there are aggs and if there are more shards than we should reduce at once
/*
// the searches merged top docs
// <code>true</code> iff the result score docs is sorted by a field (not score), this implies that <code>sortField</code> is set.
// the top docs sort fields used to sort the score docs, <code>null</code> if the results are not sorted
/*
//www.apache.org/licenses/LICENSE-2.0
// if the cause of this exception is also the cause of one of the shard failures we don't add it
// to prevent duplication in stack traces rendered to the REST layer
// if no successful shards, the failure can be due to EsRejectedExecutionException during fetch phase
// on coordinator node. so get the status from cause instead of returning SERVICE_UNAVAILABLE blindly
// fall back to guessed root cause
// notify that it's grouped
// We don't have a cause when all shards failed, but we do have shards failures so we can "guess" a cause
// (see {@link #getCause()}). Here, we use super.getCause() because we don't want the guessed exception to
// be rendered twice (one in the "cause" field, one in "failed_shards")
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// types no longer relevant so ignore
// types not supported so send an empty array to previous versions
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// generating description in a lazy way since source can be quite big
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// we don't return totalShards - successfulShards, we don't count "no shards available" as a failed shard, just don't
// count it in the successful counter
/**
/**
/**
/**
// 0 for BWC
// we don't need it but need to consume it
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//TODO it may make sense to integrate the remote clusters responses as a shard response in the initial search phase and ignore hits coming
//from the remote clusters in the fetch phase. This would be identical to the removed QueryAndFetch strategy except that only the remote
//cluster response would have the fetch results.
/**
/**
//if the search is only across remote clusters, none of them are available, and all of them have skip_unavailable set to true,
//we end up calling merge without anything to merge, we just return an empty search response
//the current reduce phase counts as one
//in case we didn't track total hits, we get null from each cluster, but we need to set 0 eq to the TopDocs
//there is no point in adding empty search hits and merging them with the others. Also, empty search hits always come
//without sort fields and collapse info, despite sort by field and/or field collapsing was requested, which causes
//issues reconstructing the proper TopDocs instance and breaks mergeTopDocs which expects the same type for each result.
//after going through all the hits and collecting all their distinct shards, we assign shardIndex and set it to the ScoreDocs
//make failures ordering consistent between ordinary search and CCS by looking at the shard they come from
//we could assume that the same shard id cannot come back from multiple clusters as even with same index name and shard index,
//the index uuid does not match. But the same cluster can be registered multiple times with different aliases, in which case
//we may get failures from the same index, yet with a different cluster alias in their shard target.
//go through all the scoreDocs from each cluster and set their corresponding shardIndex
//assign a different shardIndex to each shard, based on their shardId natural ordering and their cluster alias
//merged TopDocs is null whenever all clusters have returned empty hits
//to simplify things, we use a FieldDoc all the time, even when only a ScoreDoc is needed, in which case fields are null.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/**
// protect ourselves against time going backwards
// negative values don't make sense and we want to be able to serialize that thing as a vLong
/**
// no remote clusters
// we can't create a SearchShardTarget here since we don't know the index and shard ID we are talking to
// we only know the node and the search context ID. Yet, the response will contain the SearchShardTarget
// from the target node instead...that's why we pass null here
// don't do this - it's part of the response...
// re-create the search target and add the cluster alias if there is any,
// we need this down the road for subseq. phases
// we need to fail the entire request here - the entire phase just blew up
// don't call onShardFailure or onFailure here since otherwise we'd countDown the counter
// again which would result in an exception
// pkg private for testing
// we do our best to return the shard failures, but its ok if its not fully concurrently safe
// we simply try and return as much as possible
// the scroll ID never changes we always return the same ID. This ID contains all the shards and their context ids
// such that we can talk to them abgain in the next roundtrip.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// the counter is set to the total size of docIdsToLoad
// which can have null values so we have to count them down too
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no need to respond if it was freed or not
// we optimize this and expect a QueryFetchSearchResult if we only have a single shard in the search request
// this used to be the QUERY_AND_FETCH which doesn't exist anymore.
/**
/**
// this is cheap, it does not fetch during the rewrite phase, so we can let it quickly execute on a networking thread
/**
// Increment the number of connections for this node by one
// Decrement the number of connections or remove it entirely if there are no more connections
// We need to remove the entry here so we don't leak when nodes go away forever
// Decrement the number of connections or remove it entirely if there are no more connections
// We need to remove the entry here so we don't leak when nodes go away forever
// Always return true, there is additional asserting here, the boolean is just so this
// can be skipped when assertions are not enabled
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// 2 used to be DFS_QUERY_AND_FETCH
// 3 used to be QUERY_AND_FETCH
/**
/**
/**
/**
// TODO this bwc layer can be removed once this is back-ported to 5.3 QUERY_AND_FETCH is removed now
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// availableProcessors will never be larger than 32, so max defaultMaxConcurrentSearches will never be larger than 49,
// but we don't know about about other search requests that are being executed so lets cap at 10 per node
/**
/*
/*
// we are on the same thread, we need to fork to another thread to avoid recursive stack overflow on a single thread
// we are on a different thread (we went asynchronous), it's safe to recurse
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** The maximum number of shards for a single search request. */
/**
/**
// only set it if it changed - we don't allow null values to be set but it might be already null. this way we catch
// situations when source is rewritten to null due to a bug
//if we are searching against a single remote cluster, we simply forward the original search request to such cluster
//and we directly perform final reduction in the remote cluster
//here we modify the original source so we can re-use it by setting it to each outgoing search request
//add the cluster name to the remote index names for indices disambiguation
//this ends up in the hits returned with the search response
// here we have to map the filters to the UUID since from now on we use the uuid for the lookup
//don't search on any local index (happens when only remote indices were specified)
// TODO: I think startTime() should become part of ActionRequest and that should be used both for index name
// date math expressions and $now in scripts. This way all apis will deal with now in the same way instead
// of just for the _search api
// optimize search type for cases where there is only one shard group to search on
// if we only have one group, then we always want Q_T_F, no need for DFS, and no need to do THEN since we hit one shard
// No user preference defined in search request - apply cluster service default
// disable request cache if we have only suggest
// convert to Q_T_F if we have only suggest
// we can't do this for DFS it needs to fan out to all shards all the time
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO can we get rid of this?
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// not waiting for any active shards
// its possible the index was deleted while waiting for active shard copies,
// in this case, we'll just consider it that we have enough active shard copies
// and we can stop waiting
// its possible the index was closed while waiting for active shard copies,
// in this case, we'll just consider it that we have enough active shard copies
// and we can stop waiting
// all primary shards aren't active yet
// not enough active shard copies yet
/**
// adding 1 for the primary in addition to the total number of replicas,
// which gives us the total number of shard copies
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// wait for the configured number of active shards to be allocated before executing the result consumer
// not waiting, so just run whatever we were to run when the waiting is
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// One volatile read, so that all checks are done against the same instance:
// matches not set, default value of "true"
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We already got the task created on the network layer - no need to create it again on the transport layer
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// true is default here, for bw comp we keep the first 16 values
// in the array same as before + the default value for the new flag
/**
/**
/**
// note that allowAliasesToMultipleIndices is not exposed, always true (only for internal use)
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// we use a timeout of 0 to by pass assertion forbidding to call actionGet() (blocking) on a network thread.
// here we know we will never block
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
// Empty string is IMMEDIATE because that makes "POST /test/test/1?refresh" perform
// a refresh which reads well and is what folks are used to.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// update to concrete indices
// no shards
// count the local operations, and perform the non local ones
// really, no shards active in this group
// no more active shards... (we should not really get here, just safety)
// no node connected, act as failure
// we set the shard failure always, even if its the first in the replication group, and the next one
// will work (it will just override it...)
// we don't aggregate shard failures on non active shards (but do keep the header counts right)
// just override it and return
// we should never really get here...
// the failure is already present, try and not override it with an exception that is less meaningless
// for example, getting illegal shard state
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
// send a request to the shard only if it is assigned to a node that is in the local node's cluster state
// a scenario in which a shard can be assigned but to a node that is not in the local node's cluster state
// is when the shard is assigned to the master node, the local node has detected the master as failed
// and a new master has not yet been elected; in this situation the local node will have removed the
// master node from the local cluster state, but the shards assigned to the master will still be in the
// routing table as such
// this is defensive to protect against the possibility of double invocation
// the current implementation of TransportService#sendRequest guards against this
// but concurrency is hard, safety is important, and the small performance loss here does not matter
// this is defensive to protect against the possibility of double invocation
// the current implementation of TransportService#sendRequest guards against this
// but concurrency is hard, safety is important, and the small performance loss here does not matter
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// if it's not acknowledged, then shards acked should be false too
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// check for block, if blocked, retry, else, execute locally
// accept state as block will be rechecked by doStart() and listener.onFailure() then called
// we want to retry here a bit to see if a new master is elected
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// read operation, lightweight...
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: this class can be removed in master once 7.x is bumped to 7.4.0
// previously nodeId
// previously nodeId
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// nothing to notify
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// exposed for tests
// increase by 1 until we finish all primary coordination
// we have to get the replication group after successfully indexing into the primary in order to honour recovery semantics.
// we have to make sure that every operation indexed into the primary after recovery start will also be replicated
// to the recovery target. If we used an old replication group, we may miss a recovery that has started since then.
// we also have to make sure to get the global checkpoint before the replication group, to ensure that the global checkpoint
// is valid for this replication group. If we would sample in the reverse, the global checkpoint might be based on a subset
// of the sampled replication group, and advanced further than what the given replication group would allow it to.
// This would entail that some shards could learn about a global checkpoint that would be higher than its local checkpoint.
// we have to capture the max_seq_no_of_updates after this request was completed on the primary to make sure the value of
// max_seq_no_of_updates on replica when this request is executed is at least the value on the primary when it was executed
// on.
// TODO: fail shard? This will otherwise have the local / global checkpoint info lagging, or possibly have replicas
// go out of sync with the primary
// if inSyncAllocationIds contains allocation ids of shards that don't exist in RoutingTable, mark copies as stale
// for total stats, add number of unassigned shards and
// number of initializing shards that are not ready yet to receive operations (recovery has not opened engine yet on the target)
// Only report "critical" exceptions - TODO: Reach out to the master node to get the latest shard state then report.
// the index was deleted or this shard was never activated after a relocation; fall through and finish normally
// fail the primary but fall through and let the rest of operation processing complete
// We prefer not to fail the primary to avoid unnecessary warning log
// when the node with the primary shard is gracefully shutting down.
// we are no longer the primary, fail ourselves and start over
/**
// not waiting for any shards
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// force a proper to string to ease debugging
/**
// nothing by default
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// skip potential inner arrays for forward compatibility
// skip potential inner arrays for forward compatibility
/**
/**
// skip potential inner objects for forward compatibility
// skip potential inner arrays for forward compatibility
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Implements equals and hashcode for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// non active shard, ignore
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// package private for testing
// we must never reject on because of thread pool capacity on replicas
/**
// if the wait for active shard count has not been set in the request,
// resolve it from the index settings
/**
/**
/**
/**
// we may end up here if the cluster state used to route the primary is so stale that the underlying
// index shard was replaced with a replica. For example - in a two node cluster, if the primary fails
// the replica will take over and a replica will be assigned to the first node.
// release shard operation lock as soon as possible
// delegate primary phase to relocation target
// it is safe to execute primary phase on relocation target as there are no more in-flight operations where primary
// phase is executed on local shard and all subsequent operations are executed on relocation target as primary phase.
// only log non-closed exceptions
// intentionally swallow, a missed global checkpoint sync should not fail this operation
// release shard operation lock before responding to caller
// release shard operation lock before responding to caller
// allows subclasses to adapt the response
/**
/**
// important: we pass null as a timeout as failing a replica is
// something we want to avoid at all costs
// release shard operation lock before responding to caller
// release shard operation lock before responding to caller
// release shard operation lock before responding to caller
// Forking a thread on local node via transport service so that custom transport service have an
// opportunity to execute custom logic before the replica operation begins
/**
// ensure that the cluster state on the node is at least as high as the node that decided that the index was there
// if the wait for active shard count has not been set in the request,
// resolve it from the index settings
// chasing the node with the active primary for a second hop requires that we are at least up-to-date with the current
// cluster state version this prevents redirect loops between two nodes when a primary was relocated and the relocation
// target is not aware that it is the active primary shard already.
// if we got disconnected from the node, or the node / shard is not in the right state (being closed)
// we running as a last attempt after a timeout has happened. don't retry
// Try one more time...
/**
/**
/*
/**
// This does not need to fail the shard. The idea is that this
// is a non-write operation (something like a refresh or a global
// checkpoint sync) and therefore the replica should still be
// "alive" if it were to fail.
// This does not need to make the shard stale. The idea is that this
// is a non-write operation (something like a refresh or a global
// checkpoint sync) and therefore the replica should still be
// "alive" if it were to be marked as stale.
/** a wrapper class to encapsulate a request when being sent to a specific allocation id **/
/** {@link AllocationId#getId()} of the shard this request is sent to **/
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Syncs operation result to the translog or throws a shard not available failure */
// check if any transient write operation failures should be bubbled up
/* here we are moving forward in the translog with each operation. Under the hood this might
/**
/**
/**
/*
/**
/**
/**
/**
/**
/** calls the response listener if all pending operations have returned otherwise it just decrements the pending opts counter.*/
/*
// decrement pending by one, if there is nothing else to do we just respond with success
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// null means its not set, allows to explicitly direct a request to a specific shard
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// no shardIt, might be in the case between index gateway recovery and shardIt initialization
// this transport only make sense with an iterator that returns a single shard routing (like primary)
// if we got disconnected from the node, or the node / shard is not in the right state (being closed)
// we running as a last attempt after a timeout has happened. don't retry
// just to be on the safe side, see if we can start it now?
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no need to pass threading over the network, they are always false when coming throw a thread pool
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// just execute it on the local node
// if we have a local operation, execute it on a thread since we don't spawn
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// NOTE: This constructor is only needed, because the setters in this class,
// otherwise it can be removed and above fields can be made final.
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we are only checking one task, we can optimize it
/**
// nothing to do
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// types no longer relevant so ignore
/**
/**
/**
// types not supported so send an empty array to previous versions
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// here we read the header to fill the field offset map
// reference to the term vector data
// first, find where in the termVectors bytes the actual term vector for
// this field is stored
// we don't have it.
// read how many terms....
// ...if positions etc. were stored....
// read the field statistics
// reset before asking for an iterator
// convert bytes ref for the terms to actual data
// term string. first the size...
// ...then the value.
// grow the arrays to read the values. this is just
// for performance reasons. Re-use memory instead of
// realloc.
// finally, read the values into the arrays
// currentPosition etc. so that we can just iterate
// later
// read the score if available
// call nextPosition once before calling this one
// because else counter is not advanced
// can return -1 if posistions were not requested or
// stored but offsets were stored and requested
// this is kind of cheating but if you don't need positions
// we safe lots fo space on the wire
// read a vInt. this is used if the integer might be negative. In this case,
// the writer writes a 0 for -1 or value +1 and accordingly we have to
// subtract 1 again
// adds one to mock not existing term freq
// read a vLong. this is used if the integer might be negative. In this
// case, the writer writes a 0 for -1 or value +1 and accordingly we have to
// subtract 1 again
// adds one to mock not existing term freq
/*
//www.apache.org/licenses/LICENSE-2.0
// if no terms found, take the retrieved term vector fields for stats
// one queue per field name
// select terms with highest tf-idf
// remove noise words
// now call on docFreq
// filter based on score
// retain the best terms for quick lookups
// filter out words based on length
// filter out words that don't occur enough times in the source
// filter out words that occur too many times in the source
// filter out words that don't occur in enough docs
// filter out words that occur in too many docs
// index update problem?
// there is still space in the queue
// otherwise update the smallest in the queue in place and update the queue
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: change to String[]
// types no longer relevant so ignore
/**
/**
/**
/**
/**
/**
/**
/**
// assign a random id to this artificial document, for routing
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// types not supported so send an empty array to previous versions
// Do not change the order of these flags we use
// the ordinal for encoding! Only append to the end!
/**
// the following is important for multi request parsing.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// term statistics strings
// field statistics strings
// types no longer relevant so ignore
// types not supported so send an empty array to previous versions
// make the bytes safe
// write field statistics
// start term, optimized writing
// finally write the term vectors
// write term statistics. At this point we do not naturally have a
// boolean that says if these values actually were requested.
// However, we can assume that they were not if the statistic values are
// <= 0.
// init memory for performance reasons
// this should only be -1 if the field
// statistics were not requested at all. In
// this case all 3 values should be -1
/*
//www.apache.org/licenses/LICENSE-2.0
// package only - this is an internal class!
// can we somehow
// predict the
// size here?
// if no terms found, take the retrieved term vector fields for stats
// iterate all terms of the current field
// with filtering we only keep the best terms
// get the doc frequency
// given we have pos or offsets
// if we do not have the positions stored, we need to
// get the frequency from a PostingsEnum.
// now, write the information about offset of the terms in the
// termVectors field
// for each term (iterator next) in this field (field)
// iterate over the docs (should only be one)
// add information on if positions etc. are written
// term freq etc. can be negative if not present... we transport that
// further...
// term freq etc. can be negative if not present... we transport that
// further...
/** Implements an empty {@link Terms}. */
/*
//www.apache.org/licenses/LICENSE-2.0
// only failures..
// create failures for all relevant requests
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// artificial document without routing specified, ignore its "id" and use either random shard or according to preference
// update the routing (request#index here is possibly an alias or a parent)
// Fail fast on the node that received the request.
// it's a realtime request which is not subject to refresh cycles
/*
//www.apache.org/licenses/LICENSE-2.0
// Fail fast on the node that received the request, rather than failing when translating on the index or delete request.
// if we don't have a master, we don't have metadata, that's fine, let it find a master using create index API
// we have the index, do it
// we fetch it from the index request so we don't generate the bytes twice, its already done in the index request
// we fetch it from the index request so we don't generate the bytes twice, its already done in the index request
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// If the document didn't exist, execute the update request as an upsert
// no source, we can't do anything, throw a failure...
// The request has no script, it is a new doc that should be merged with the old document
// The request has a script (or empty script), execute the script and prepare a new index request
/**
// Tell the script that this is a create and not an update
// Only valid options for an upsert script are "create" (the default) or "none", meaning abort upsert
/**
// Run the script to perform the create logic
// It's fine to throw an exception here, the leniency is handled/logged by `executeScriptedUpsert`
// it has to be a "create!"
// in all but the internal versioning mode, we want to create the new document using the given version.
/**
/**
// We can only actually turn the update into a noop if detectNoop is true to preserve backwards compatibility and to handle cases
// where users repopulating multi-fields or adding synonyms, etc.
/**
// The default operation is "index"
// If it was neither an INDEX or DELETE operation, treat it as a noop
/**
// TODO when using delete/none, we can still return the source as bytes by generating it (using the sourceContentType)
/**
// TODO: can we remove this leniency yet??
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// make sure the basics are set
// make sure the basics are set
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** creates a new instance */
// bail out
// keep this thread alive (non daemon thread) until we shutdown
/** initialize native resources */
// check if the user is running as root, and bail
// enable system call filter
// mlockall if requested
// listener for windows close event
// force remainder of JNA to be loaded (if available).
// we've already logged this.
// init lucene random seed. it will use /dev/urandom where available:
// Force probes to be loaded
/**
//github.com/elastic/elasticsearch/issues/50512
// minor time-bomb here to ensure that we reevaluate if final 14 version does not include fix.
// initialize probes before the security manager is installed
// look for jar hell
// Log ifconfig output before SecurityManager is installed
// install SM after natives, shutdown hooks, etc.
/* TODO: read password from stdin */);
// HOSTNAME is set by elasticsearch-env and elasticsearch-env.bat so it is always available
/**
// force the class initializer for BootstrapInfo to run before
// the security manager is installed
// fail if somebody replaced the lucene jars
// install the default uncaught exception handler; must be done before security is
// initialized as we do not want to grant the runtime permission
// setDefaultUncaughtExceptionHandler
// any secure settings must be read during node construction
// We don't close stderr if `--quiet` is passed, because that
// hides fatal startup errors. For example, if Elasticsearch is
// running via systemd, the init script only specifies
// `--quiet`, not `-d`, so we want users to be able to see
// startup errors via journalctl.
// disable console logging, so user does not see the exception twice (jvm will show it already)
// HACK, it sucks to do this, but we will run users out of disk space otherwise
// guice: log the shortened exc to the log file
// full exception
// re-enable it if appropriate, so they can see any logging during the shutdown process
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// the list of checks to execute
// visible for testing
// visible for testing
// see constant OPEN_MAX defined in
// /usr/include/sys/syslimits.h on OS X and its use in JVM
// initialization in int os:init_2(void) defined in the JVM
// code for BSD (contains OS X)
// visible for testing
// visible for testing
// this should be plenty for machines up to 256 cores
// visible for testing
// visible for testing
// visible for testing
/**
// we only enforce the check if a store is allowed to use mmap at all
// visible for testing
// visible for testing
// visible for testing
// visible for testing
// visible for testing
// visible for testing
/**
// visible for testing
/**
// visible for testing
// visible for testing
// visible for testing
// visible for testing
// visible for testing
/**
/**
// HotSpot versions on Java 8 match this regular expression; note that this changes with Java 9 after JEP-223
// HotSpot versions for Java 8 have major version 25, the bad versions are all versions prior to update 40
// visible for testing
// visible for testing
// visible for testing
// visible for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** no instantiation */
/**
/**
/**
/**
// create a view of sysprops map that does not allow modifications
// this must be done this way (e.g. versus an actual typed map), because
// some test methods still change properties, so whitelisted changes must
// be reflected in this view.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: remove this hack when insecure defaults are removed from java
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// visible for testing
// we configure logging later so we override the base class from configuring logging
/**
/*
// grant all permissions so that we can later set the security manager to the one that we want
// It's possible to fail before logging has been configured, in which case there's no point
// suggesting that the user look in the log file.
// round-trip the property to an integer and back to a string to ensure that it parses properly
// a misconfigured java.io.tmpdir can cause hard-to-diagnose problems later, so reject it immediately
// format exceptions to the console in a special way
// to avoid 2MB stacktraces from guice, etc.
/**
//commons.apache.org/proper/commons-daemon/procrun.html
/*
//www.apache.org/licenses/LICENSE-2.0
// we use specific error codes in case the above notification failed, at least we
// will have some indication of the error bringing us down
// Without a final flush, the stacktrace may not be shown before ES exits
// Without a final flush, the stacktrace may not be shown if ES goes on to exit
// we halt to prevent shutdown hooks from running
/*
//www.apache.org/licenses/LICENSE-2.0
/** custom policy for union of static and dynamic permissions */
/** template policy file, the one used in tests */
/** limited policy for scripts */
// codesource can be null when reducing privileges via doPrivileged()
// location can be null... ??? nobody knows
// https://bugs.openjdk.java.net/browse/JDK-8129972
// run scripts with limited permissions
// check for an additional plugin permission: plugin policy is
// only consulted for its codesources.
// Special handling for broken Hadoop code: "let me execute or my classes will not load"
// yeah right, REMOVE THIS when hadoop is fixed
// we found the horrible method: the hack begins!
// force the hadoop code to back down, by throwing an exception that it catches.
// otherwise defer to template + dynamic file permissions
/**
/**
// code should not rely on this method, or at least use it correctly:
// https://bugs.openjdk.java.net/browse/JDK-8014008
// return them a new empty permissions object so jvisualvm etc work
// return UNSUPPORTED_EMPTY_COLLECTION since it is safe.
// TODO: remove this hack when insecure defaults are removed from java
/**
/**
// default policy file states:
// "It is strongly recommended that you either remove this permission
//  from this policy file or further restrict it to code sources
//  that you specify, because Thread.stop() is potentially unsafe."
// not even sure this method still works...
// default policy file states:
// "allows anyone to listen on dynamic ports"
// specified exactly because that is what we want, and fastest since it won't imply any
// expensive checks for the implicit "resolve"
// we apply this pre-implies test because some SocketPermission#implies calls do expensive reverse-DNS resolves
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** no instantiation */
/**
/*
//github.com/elastic/elasticsearch/issues/21534.
/**
// paths may not exist yet, this also checks accessibility
// add each path twice: once for itself, again for files underneath it
/*
//github.com/elastic/elasticsearch/issues/21534.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** corresponds to struct rlimit */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Callbacks must be kept around in order to be able to be called later,
// when the Windows ConsoleCtrlHandler sends an event.
// Native library instance must be kept around for the same reason.
/**
/**
/**
//msdn.microsoft.com/en-us/library/windows/desktop/ms683242%28v=vs.85%29.aspx
/**
//msdn.microsoft.com/en-us/library/windows/desktop/aa366786%28v=vs.85%29.aspx
/**
//msdn.microsoft.com/en-us/library/windows/desktop/aa366775%28v=vs.85%29.aspx
// JNA requires this no-arg constructor to be public,
// otherwise it fails to register kernel32 library
/**
//msdn.microsoft.com/en-us/library/windows/desktop/aa366895%28v=vs.85%29.aspx
/**
//msdn.microsoft.com/en-us/library/windows/desktop/aa366907%28v=vs.85%29.aspx
/**
//msdn.microsoft.com/en-us/library/windows/desktop/ms686234%28v=vs.85%29.aspx
/**
//msdn.microsoft.com/en-us/library/windows/desktop/ms683179%28v=vs.85%29.aspx
/**
//msdn.microsoft.com/en-us/library/windows/desktop/ms724211%28v=vs.85%29.aspx
/**
//msdn.microsoft.com/en-us/library/windows/desktop/aa364989.aspx">{@code GetShortPathName}</a>.
/**
//msdn.microsoft.com/en-us/library/windows/desktop/ms682409%28v=vs.85%29.aspx
/**
//msdn.microsoft.com/en-us/library/windows/desktop/ms681949%28v=vs.85%29.aspx
/**
//msdn.microsoft.com/en-us/library/windows/desktop/ms684147%28v=vs.85%29.aspx
/**
/**
/**
//msdn.microsoft.com/en-us/library/windows/desktop/ms684925%28v=vs.85%29.aspx
/**
//msdn.microsoft.com/en-us/library/windows/desktop/ms686216%28v=vs.85%29.aspx
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** no instantiation */
// Set to true, in case native mlockall call was successful
// Set to true, in case native system call filter install was successful
// Set to true, in case policy can be applied to all threads of the process (even existing ones)
// otherwise they are only inherited for new threads (ES app threads)
// set to the maximum number of threads that can be created for
// the user ID that owns the running Elasticsearch process
// we only know RLIMIT_MEMLOCK for these two at the moment.
// this will have already been logged by CLibrary, no need to repeat it
// mlockall failed for some reason
// give specific instructions for the linux case to make it easy
// this is only valid on Linux and the value *is* different on OS X
// see /usr/include/sys/resource.h on OS X
// on Linux the resource RLIMIT_NPROC means *the number of threads*
// this is in opposition to BSD-derived OSes
/** Returns true if user is root, false if not, or if we don't know */
// don't know
// this will have already been logged by Kernel32Library, no need to repeat it
// By default, Windows limits the number of pages that can be locked.
// Thus, we need to first increase the working set size of the JVM by
// the amount of memory we wish to lock, plus a small overhead (1MB).
// Move to the next region
// this will have already been logged by Kernel32Library, no need to repeat it
/**
// first we get the length of the buffer needed
// knowing the length of the buffer, now we get the short name
// The console Ctrl handler is necessary on Windows platforms only.
// this will have already been logged by Kernel32Library, no need to repeat it
// this is likely to happen unless the kernel is newish, its a best effort at the moment
// so we log stacktrace at debug for now...
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** no instantiation */
// marker to determine if the JNA class files are available to the JVM
// load one of the main JNA classes to see if the classes are available. this does not ensure that all native
// libraries are available, only the ones necessary by JNA to function
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//docs.oracle.com/javase/7/docs/technotes/guides/security/troubleshooting-security.html">
/** no instantiation */
/**
// enable security policy: union of template and environment-based paths, and possibly plugin permissions
// enable security manager
// SecureSM matches class names as regular expressions so we escape the $ that arises from the nested class name
// do some basic tests
/**
// maintain order
// tests :(
/**
// collect up set of plugins and modules by listing directories.
// now process each one
// first get a list of URLs for the plugins' jars:
// we resolve symlinks so map is keyed on the normalize codebase name
// order is already lost, but some filesystems have it
// parse the plugin's policy file into a set of permissions
// consult this policy for each of the plugin's jars:
// just be paranoid ok?
/**
// set codebase properties
// We attempt to use a versionless identifier for each codebase. This assumes a specific version
// format in the jar filename. While we cannot ensure all jars in all plugins use this format, nonconformity
// only means policy grants would need to include the entire jar filename as they always have before.
// clear codebase properties
/** returns dynamic Permissions to configured paths and bind ports */
/** Adds access to classpath jars/classes for jar hell scan, etc */
// add permissions to everything in classpath
// really it should be covered by lib/, but there could be e.g. agents or similar configured)
// resource itself
/**
// read-only dirs
// read-write dirs
/*
// we just need permission to remove the file if its elsewhere.
/**
/**
// http is simple
/**
// transport is way over-engineered
// loop through all profiles and add permissions for each one
// profiles fall back to the transport.port if it's not explicit but we want to only add one permission per range
/**
// listen is always called with 'localhost' but use wildcard to be sure, no name service is consulted.
// see SocketPermission implies() code
/**
// this isn't atomic, but neither is createDirectories.
// verify access, following links (throws exception if something is wrong)
// we only check READ as a sanity test
// doesn't exist, or not a directory
// convert optional specific exception so the context is clear
/** Simple checks that everything is ok */
// check we can manipulate temporary files
// potentially virus scanner
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/**
/*
/**
/*
//msdn.microsoft.com/en-us/library/windows/desktop/ms682425.aspx). Since
//hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/windows/native/java/lang/ProcessImpl_md.c#l319), this
// the only environment variable passes on the path to the temporary directory
// the output stream of the process object corresponds to the daemon's stdin
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//TODO: remove this when guice is removed, and exceptions are cleaned up
//this is horrible, but its what we must do
/** maximum length of a stacktrace, before we truncate it */
/** all lines from this package are RLE-compressed */
/**
/*
// walk to the root cause
// print the root cause message, only if it differs!
// print stacktrace of cause
// skip past contiguous runs of this garbage:
// if its a guice exception, the whole thing really will not be in the log, its megabytes.
// refer to the hack in bootstrap, where we don't log it
// It's possible to fail before logging has been configured, in which case there's no point
// suggested that the user look in the log file.
/**
// we tried
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.kernel.org/doc/Documentation/prctl/seccomp_filter.txt">
//www.kernel.org/doc/Documentation/prctl/seccomp_filter.txt</a>
//reverse.put.as/wp-content/uploads/2011/06/The-Apple-Sandbox-BHDC2011-Paper.pdf">
//reverse.put.as/wp-content/uploads/2011/06/The-Apple-Sandbox-BHDC2011-Paper.pdf</a>
//docs.oracle.com/cd/E23824_01/html/821-1456/prbac-2.html">
//docs.oracle.com/cd/E23824_01/html/821-1456/prbac-2.html</a>
// not an example of how to write code!!!
// Linux implementation, based on seccomp(2) or prctl(2) with bpf filtering
/** Access to non-standard Linux libc methods */
/**
/**
// null if unavailable or something goes wrong.
/** the preferred method is seccomp(2), since we can apply to all threads of the process */
// since Linux 3.17
// since Linux 3.17
/** otherwise, we can use prctl(2), which will at least protect ES application threads */
// since Linux 3.5
// since Linux 3.5
// since Linux 2.6.23
// since Linux 2.6.23
// since Linux Linux 3.5
/** corresponds to struct sock_filter */
// insn
// number of insn to jump (skip) if true
// number of insn to jump (skip) if false
// additional data
/** corresponds to struct sock_fprog */
// number of filters
// filters
// serialize struct sock_filter * explicitly, its less confusing than the JNA magic we would need
// little endian
// BPF "macros" and constants
// some errno constants for error checking/handling
// offsets that our BPF checks
// check with offsetof() when adding a new arch, move to Arch if different.
/** AUDIT_ARCH_XXX constant from linux/audit.h */
/** syscall limit (necessary for blacklisting on amd64, to ban 32-bit syscalls) */
/** __NR_fork */
/** __NR_vfork */
/** __NR_execve */
/**  __NR_execveat */
/** __NR_seccomp */
/** supported architectures map keyed by os.arch */
/** invokes prctl() from linux libc library */
/** invokes syscall() from linux libc library */
/** try to install our BPF filters via seccomp() or prctl() to block execution */
// first be defensive: we can give nice errors this way, at the very least.
// also, some of these security features get backported to old versions, checking kernel version here is a big no-no!
// we couldn't link methods, could be some really ancient kernel (e.g. < 2.1.57) or some bug
// try to check system calls really are who they claim
// you never know (e.g. https://chromium.googlesource.com/chromium/src.git/+/master/sandbox/linux/seccomp-bpf/sandbox_bpf.cc#57)
// test seccomp(BOGUS)
// ok
// ok
// test seccomp(VALID, BOGUS)
// ok
// ok
// test prctl(BOGUS)
// ok
// ok
// now just normal defensive checks
// check for GET_NO_NEW_PRIVS
// not yet set
// already set by caller
// friendly error, this will be the typical case for an old kernel
// check for SECCOMP
// not yet set
// already in filter mode by caller
// check for SECCOMP_MODE_FILTER
// available
// ok, now set PR_SET_NO_NEW_PRIVS, needed to be able to set a seccomp filter as ordinary user
// check it worked
// BPF installed to check arch, limit, then syscall.
// See https://www.kernel.org/doc/Documentation/prctl/seccomp_filter.txt for details.
//
/* 1  */ BPF_STMT(BPF_LD  + BPF_W   + BPF_ABS, SECCOMP_DATA_ARCH_OFFSET),             //
// if (arch != audit) goto fail;
/* 2  */ BPF_JUMP(BPF_JMP + BPF_JEQ + BPF_K,   arch.audit,     0, 7),                 // if (arch != audit) goto fail;
//
/* 3  */ BPF_STMT(BPF_LD  + BPF_W   + BPF_ABS, SECCOMP_DATA_NR_OFFSET),               //
// if (syscall > LIMIT) goto fail;
/* 4  */ BPF_JUMP(BPF_JMP + BPF_JGT + BPF_K,   arch.limit,     5, 0),                 // if (syscall > LIMIT) goto fail;
// if (syscall == FORK) goto fail;
/* 5  */ BPF_JUMP(BPF_JMP + BPF_JEQ + BPF_K,   arch.fork,      4, 0),                 // if (syscall == FORK) goto fail;
// if (syscall == VFORK) goto fail;
/* 6  */ BPF_JUMP(BPF_JMP + BPF_JEQ + BPF_K,   arch.vfork,     3, 0),                 // if (syscall == VFORK) goto fail;
// if (syscall == EXECVE) goto fail;
/* 7  */ BPF_JUMP(BPF_JMP + BPF_JEQ + BPF_K,   arch.execve,    2, 0),                 // if (syscall == EXECVE) goto fail;
// if (syscall == EXECVEAT) goto fail;
/* 8  */ BPF_JUMP(BPF_JMP + BPF_JEQ + BPF_K,   arch.execveat,  1, 0),                 // if (syscall == EXECVEAT) goto fail;
// pass: return OK;
/* 9  */ BPF_STMT(BPF_RET + BPF_K, SECCOMP_RET_ALLOW),                                // pass: return OK;
// fail: return EACCES;
/* 10 */ BPF_STMT(BPF_RET + BPF_K, SECCOMP_RET_ERRNO | (EACCES & SECCOMP_RET_DATA)),  // fail: return EACCES;
// seccomp takes a long, so we pass it one explicitly to keep the JNA simple
// install filter, if this works, after this there is no going back!
// first try it with seccomp(SECCOMP_SET_MODE_FILTER), falling back to prctl()
// now check that the filter was really installed, we should be in filter mode.
// OS X implementation via sandbox(7)
/** Access to non-standard OS X libc methods */
/**
/**
// null if unavailable, or something goes wrong.
/** The only supported flag... */
/** Allow everything except process fork and execution */
/** try to install our custom rule profile into sandbox_init() to block execution */
// first be defensive: we can give nice errors this way, at the very least.
// we couldn't link methods, could be some really ancient OS X (< Leopard) or some bug
// write rules to a temporary file, which will be passed to sandbox_init()
// if sandbox_init() fails, add the message from the OS (e.g. syntax error) and free the buffer
// Solaris implementation via priv_set(3C)
/** Access to non-standard Solaris libc methods */
/**
// null if unavailable, or something goes wrong.
// constants for priv_set(2)
// see privileges(5) for complete list of these
// first be defensive: we can give nice errors this way, at the very least.
// we couldn't link methods, could be some really ancient Solaris or some bug
// drop a null-terminated list of privileges
// BSD implementation via setrlimit(2)
// TODO: add OpenBSD to Lucene Constants
// TODO: JNA doesn't have netbsd support, but this mechanism should work there too.
// not a standard limit, means something different on linux, etc!
// windows impl via job ActiveProcessLimit
// create a new Job
// retrieve the current basic limits of the job
// modify the number of active processes to be 1 (exactly the one process we will add to the job).
// assign ourselves to the job
/**
// try to enable both mechanisms if possible
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// initialize default for es.logger.level because we will not read the log4j2.properties
/*
//www.apache.org/licenses/LICENSE-2.0
/** A cli command which requires an {@link org.elasticsearch.env.Environment} to use current paths and settings. */
/**
/**
/** Create an {@link Environment} for the command to use. Overrideable for tests. */
/** Create an {@link Environment} for the command to use. Overrideable for tests. */
// HOSTNAME is set by elasticsearch-env and elasticsearch-env.bat so it is always available
/** Ensure the given setting exists, reading it from system properties if not already set. */
/** Execute the command with the initialized {@link Environment}. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// nothing really to do
// Discard the task because the Client interface doesn't use it.
/**
/**
/**
// we cannot track the progress if remote cluster requests are splitted.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// We just read this diff, so it's not going to be written
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
// working off of a non-initialized previous state, so use the tombstones for index deletions
// examine the diffs in index metadata between the previous and new cluster states to get the deleted indices
/**
/**
// new custom md added or existing custom md changed
// existing custom md deleted
/**
// no need to check on version, since disco modules will make sure to use the
// same instance if its a version match
/**
/**
/**
/**
/**
/**
/**
// Get the deleted indices by comparing the index metadatas in the previous and new cluster states.
// If an index exists in the previous cluster state, but not in the new cluster state, it must have been deleted.
// If the new cluster state has a new cluster UUID, the likely scenario is that a node was elected
// master that has had its data directory wiped out, in which case we don't want to delete the indices and lose data;
// rather we want to import them as dangling indices instead.  So we check here if the cluster UUID differs from the previous
// cluster UUID, in which case, we don't want to delete indices that the master erroneously believes shouldn't exist.
// See test DiscoveryWithServiceDisruptionsIT.testIndicesDeleted()
// See discussion on https://github.com/elastic/elasticsearch/pull/9952 and
// https://github.com/elastic/elasticsearch/issues/11665
// We look at the full tombstones list to see which indices need to be deleted.  In the case of
// a valid previous cluster state, indicesDeletedFromClusterState() will be used to get the deleted
// list, so a diff doesn't make sense here.  When a node (re)joins the cluster, its possible for it
// to re-process the same deletes or process deletes about indices it never knew about.  This is not
// an issue because there are safeguards in place in the delete store operation in case the index
// folder doesn't exist on the file system.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// node
// end "least_available"
// end "most_available"
// end $nodename
// end "nodes"
// end "shard_sizes"
// end "shard_paths"
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// default
// pkg private for tests
// Cluster State
// Metadata
// Task Status (not Diffable)
// Metadata
// TODO: this is public so allocation benchmark can access the default deciders...can we do that in another way?
/** Return a new {@link AllocationDecider} instance with builtin deciders as well as those from plugins. */
// collect deciders by class so that we can detect duplicates
/** Add the given allocation decider to the given deciders collection, erroring if the class name is already used. */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// built on demand
/**
/**
/**
// always provide the cluster_uuid as part of the top-level response (also part of the metadata response)
// nodes
// meta data
// the type name is the root value, reduce it
// the type name is the root value, reduce it
// index metadata
// routing table
// routing nodes
/**
// used to be minimumMasterNodesOnPublishingMaster, which was used in 7.x for BWC with 6.x
// filter out custom states not supported by the other node
// used to be minimumMasterNodesOnPublishingMaster, which was used in 7.x for BWC with 6.x
// used to be minimumMasterNodesOnPublishingMaster, which was used in 7.x for BWC with 6.x
// used to be minimumMasterNodesOnPublishingMaster, which was used in 7.x for BWC with 6.x
// no need to read the rest - cluster state didn't change
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// observingContext is not null when waiting on cluster state changes
/**
/**
/** sets the last observed state to the currently applied cluster state and returns it */
/** indicates whether this observer has timed out */
/**
// things have timeout while we were busy -> notify
// update to latest, in case people want to retry
// sample a new state. This state maybe *older* than the supplied state if we are called from an applier,
// which wants to wait for something else to happen
// good enough, let's go.
// No need to remove listener as it is the responsibility of the thread that set observingContext to null
// No need to remove listener as it is the responsibility of the thread that set observingContext to null
// double check we're still listening
// update to latest, in case people want to retry
/**
/**
/** called when a new state is observed */
/** called when the cluster service is closed */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// one of task, source is enough
/**
/**
// final, empty implementation here as this method should only be defined in combination
// with a batching executor as it will always be executed within the system context.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// incremental updates
// additions or full updates
/**
/**
/**
// filter out custom states not supported by the other node
// filter out custom states not supported by the other node
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// We return 100.0% in order to fail "open", in that if we have invalid
// numbers for the total bytes, it's as if we don't know disk usage.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// never updated, so we can discard the listener
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Add InternalClusterInfoService to listen for Master changes
// Add to listen for state changes (when nodes are added)
// Submit a job that will reschedule itself after running
// Submit an info update job to be run immediately
// Check whether it was a data node that was added
/**
//schedule again after we refreshed
/**
/**
// Short-circuit if not enabled
// allow tests to adjust the node stats on receipt
/**
// we empty the usages list, to be safe - we don't know what's going on.
// we empty the usages list, to be safe - we don't know what's going on.
// restore interrupt status
// restore interrupt status
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// one of task, source is enough
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Protects changes to targetsByNode and its values (i.e. ConnectionTarget#activityType and ConnectionTarget#listener).
// Crucially there are no blocking calls under this mutex: it is not held while connecting or disconnecting.
// contains an entry for every node in the latest cluster state, as well as for nodes from which we are in the process of
// disconnecting
/**
// new node, set up target and listener
// existing node, but maybe we're disconnecting from it, in which case it was recently removed from the cluster
// state and has now been re-added so we should wait for the re-connection
// known node, try and ensure it's connected but do not wait
/**
// Called by tests after some disruption has concluded. It is possible that one or more targets are currently CONNECTING and have
// been since the disruption was active, and that the connection attempt was thwarted by a concurrent disruption to the connection.
// If so, we cannot simply add our listener to the queue because it will be notified when this CONNECTING activity completes even
// though it was disrupted. We must therefore wait for all the current activity to finish and then go through and reconnect to
// any missing nodes.
/**
// for disruption tests, re-establish any disrupted connections
/**
// indicates what any listeners are awaiting
// only warn every 6th failure
// we may not have disconnected, but will not retry, so this connection might have leaked
// target is disconnected, and we are currently idle, so start a connection process.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: Should we allow parallelism across repositories here maybe?
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// the list of snapshot deletion request entries
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// see #useShardGenerations
/**
/**
// If the state is failed we have to have a reason for this failure
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// used by tests
// can be overridden by tests
// add permits
// remove permits
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// a list of shards that failed during replication
// we keep track of these shards in order to avoid sending duplicate failed shard requests for a single failing shard.
/**
/**
/**
// visible for testing
// we wait indefinitely for a new master
// tasks that correspond to non-existent indices are marked as successful
// The primary term is 0 if the shard failed itself. It is > 0 if a write was done on a primary but was failed to be
// replicated to the shard copy with the provided allocation id. In case where the shard failed itself, it's ok to just
// remove the corresponding routing entry from the routing table. In case where a write could not be replicated,
// however, it is important to ensure that the shard copy with the missing write is considered as stale from that point
// on, which is implemented by removing the allocation id of the shard copy from the in-sync allocations set.
// We check here that the primary to which the write happened was not already failed in an earlier cluster state update.
// This prevents situations where a new primary has already been selected and replication failures from an old stale
// primary unnecessarily fail currently active shards.
// mark shard copies without routing entries that are in in-sync allocations set only as stale if the reason why
// they were failed is because a write made it into the primary but not to this copy (which corresponds to
// the check "primaryTerm > 0").
// tasks that correspond to non-existent shards are marked as successful
// failing a shard also possibly marks it as stale (see IndexMetaDataUpdater)
// failures are communicated back to the requester
// cluster state will not be updated in this case
// visible for testing
// The reroute called after failing some shards will not assign any shard back to the node on which it failed. If there were
// no other options for a failed shard then it is left unassigned. However, absent other options it's better to try and
// assign it again, even if that means putting it back on the node on which it previously failed:
// Exclude message and exception from equals and hashCode
// to prevent duplicates
// tasks that correspond to non-existent shards are marked as successful. The reason is that we resend shard started
// events on every cluster state publishing that does not contain the shard as started yet. This means that old stale
// requests might still be in flight even after the shard has already been started or failed on the master. We just
// ignore these requests for now.
// same as above, this might have been a stale in-flight request, so we just ignore.
// remove duplicate actions as allocation service expects a clean list without duplicates
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// let's remove all blocks for this index and add them back -- no need to remove all individual blocks....
// We copy the block sets here in case of the builder is modified after build is called
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// null if discoveryIsConfigured()
/*
//www.apache.org/licenses/LICENSE-2.0
// if no warning is scheduled
// TODO update this when we can bootstrap on only a quorum of the initial nodes
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// this could be used in many more places - TODO use this where appropriate
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/elastic/elasticsearch-formal-models/blob/master/ZenWithTerms/tla/ZenWithTerms.tla">formal model</a>
// persisted state
// transient state
// persisted state
// transient state
// used for tests
/**
/**
/**
// We do not check for an election won on setting the initial configuration, so it would be possible to end up in a state where
// we have enough join votes to have won the election immediately on setting the initial configuration. It'd be quite
// complicated to restore all the appropriate invariants when setting the initial configuration (it's not just electionWon)
// so instead we just reject join votes received prior to receiving the initial configuration.
// we cannot go from won to not won
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// the timeout before emitting an info log about a slow-running publication
// the timeout for the publication of each value
// TODO: the following field is package-private as some tests require access to it
// These tests can be rewritten to use public methods once Coordinator is more feature-complete
// initialized on start-up (see doStart)
// the state that should be exposed to the cluster state applier
/**
// check if node has accepted a state in this term already. If not, this node has never committed a cluster state in this
// term and therefore never removed the NO_MASTER_BLOCK for this term. This logic ensures that we quickly turn a node
// into follower, even before receiving the first cluster state update, but also don't have to deal with the situation
// where we would possibly have to remove the NO_MASTER_BLOCK from the applierState when turning a candidate back to follower.
// master node applies the committed state at the end of the publication process, not here.
// Rare case in which we stood down as leader between starting this publication and receiving it ourselves. The publication
// is already failed so there is no point in proceeding.
// only do join validation if we have not accepted state from this master yet
// also updates preVoteCollector
// Bump our term. However if there is a publication in flight then doing so would cancel the publication, so don't do that
// since we check whether a term bump is needed at the end of the publication too.
// The preVoteCollector is only active while we are candidate, but it does not call this method with synchronisation, so we have
// to check our mode again here.
// handling of start join messages on the local node will be dispatched to the generic thread-pool
// explicitly move node to candidate state so that the next cluster state update task yields an onNoLongerMaster event
// updates followersChecker and preVoteCollector
// we do this in a couple of places including the cluster update thread. This one here is really just best effort
// to ensure we fail as fast as possible.
// package private for tests
// validate the join on the joining node, will throw a failure if it fails the validation
// ignore
// package-visible for testing
// package-visible for testing
// visible for testing
// package-visible for testing
// This looks like a race that might leak an unclosed CoordinationState if it's created while execution is here, but this method
// is synchronized on AbstractLifecycleComponent#lifestyle, as is the doStart() method that creates the CoordinationState, so
// it's all ok.
// cluster state update task to become master is submitted to MasterService, but publication has not started yet
// active publication in progress: followersChecker is up-to-date with nodes that we're actively publishing to
// no active publication: followersChecker is up-to-date with the nodes of the latest publication
// followersChecker excludes local node
/**
// automatically generate a UID for the metadata if we need to
// TODO generate UUID in bootstrapping tool?
// pick up the change to last-accepted version
// Package-private for testing
// exclude any nodes whose ID is in the voting config exclusions list ...
// ... and also automatically exclude the node IDs of master-ineligible nodes that were previously master-eligible and are still in
// the voting config. We could exclude all the master-ineligible nodes here, but there could be quite a few of them and that makes
// the logging much harder to follow.
// exposed for tests
// If we have already won the election then the actual join does not matter for election purposes, so swallow any exception
// If we haven't completely finished becoming master then there's already a publication scheduled which will, in turn,
// schedule a reconfiguration if needed. It's benign to schedule a reconfiguration anyway, but it might fail if it wins the
// race against the election-winning publication and log a big error message, which we can prevent by checking this here:
// this might fail and bubble up the exception
/**
// expose last accepted cluster state as base state upon which the master service
// speculatively calculates the next cluster state update
// the master service checks if the local node is the master node in order to fail execution of the state update early
// remove block if it already exists before adding new one
// there is no equals on cluster state, so we just serialize it to XContent and compare Maps
// deserialized from the resulting JSON
// TODO variable grace period
// TODO everyone takes this and adds the local node. Maybe just add the local node here?
/**
// We may not have accepted our own state before receiving a join from another node, causing its join to be rejected (we cannot
// safely accept a join whose last-accepted term/version is ahead of ours), so store them up and process them at the end.
// acking and cluster state application for local node is handled specially
// check if node has not already switched modes (by bumping term)
// checks if this publication can still influence the mode of the current publication
// trigger term bump if new term was found during publication
// if necessary, abdicate to another node or improve the voting configuration
// committed state
// check if master candidate would be able to get an election quorum if we were to
// abdicate to it. Assume that every node that completed the publication can provide
// a vote in that next election and has the latest state.
// other nodes have acked, but not the master.
// a late response may arrive after the state has been locally applied, meaning that receivedJoins has already been
// processed, so we have to handle this late response here.
// The remote node did not include a join vote in its publish response. We do not persist joins, so it could be that the remote
// node voted for us and then rebooted, or it could be that it voted for a different node in this term. If we don't have a copy
// of a join from this node then we assume the latter and bump our term to obtain a vote from this node.
/*
//www.apache.org/licenses/LICENSE-2.0
// package-private for tests
//package-private for tests
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
//package-private for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/**
/**
// package-private for testing
// to overflow here would take over a million years of failed election attempts, so we won't worry about that:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// the time between checks sent to each node
// the timeout for each check sent to each node
// the number of failed checks that must happen before the follower is considered to have failed.
// protects writes to this state; read access does not need sync
/**
/**
/**
// TODO trigger a term bump if we voted for a different leader in this term
// TODO in the PoC a faulty node was considered non-faulty again if it sent us a PeersRequest:
// - node disconnects, detected faulty, removal is enqueued
// - node reconnects, pings us, finds we are master, requests to join, all before removal is applied
// - join is processed before removal, but we do not publish to known-faulty nodes so the joining node does not receive this publication
// - it doesn't start its leader checker since it receives nothing to cause it to become a follower
// Apparently this meant that it remained a candidate for too long, leading to a test failure.  At the time this logic was added, we did
// not have gossip-based discovery which would (I think) have retried this joining process a short time later. It's therefore possible
// that this is no longer required, so it's omitted here until we can be sure if it's necessary or not.
/**
// For assertions
// For assertions
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO add support and tests for behaviour with persistence-layer failures
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// the timeout for each join attempt
// This is called when preparing the next cluster state for publication. There is no guarantee that the term we see here is
// the term under which this state will eventually be published: the current term may be increased after this check due to
// some other activity. That the term is correct is, however, checked properly during publication, so it is sufficient to
// check it here on a best-effort basis. This is fine because a concurrent change indicates the existence of another leader
// in a higher term which will cause this node to stand down.
// package-private for testing
// CandidateJoinAccumulator is only closed when becoming leader or follower, otherwise it accumulates all joins received
// regardless of term.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// use these joins to try and become the master.
// Note that we don't have to do any validation of the amount of joining nodes - the commit
// during the cluster state publishing guarantees that we have enough
// we only enforce major version transitions on a fully formed clusters
// processing any joins
// noop
// we do this validation quite late to prevent race conditions between nodes joining and importing dangling indices
// we have to reject nodes that don't support all indices we have in this cluster
// we must return a new cluster state instance to force publishing. This is important
// for the joining node to finalize its join and set us as a master
// noop
// now trim any left over dead nodes - either left there when the previous master stepped down
// or removed by us above
// we validate that we are allowed to change the cluster state during cluster state processing
/**
/**
// we ensure that all indices in the cluster we join are compatible with us no matter if they are
// closed or not we can't read mappings of these indices so we need to reject the join...
/** ensures that the joining node has a version that's compatible with all current nodes*/
/** ensures that the joining node has a version that's compatible with a given version range */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// the timeout for each node to apply a cluster state update after the leader has applied it, before being removed from the cluster
// Received an ack from a node that a later publication has removed (or we are no longer master). No big deal.
// for assertions
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// the time between checks sent to the leader
// the timeout for each check sent to the leader
// the number of failed checks that must happen before the leader is considered to have failed.
/**
/**
// For assertions
// logs trace message indicating success
/*
//www.apache.org/licenses/LICENSE-2.0
// no nodes to remove, keep the current cluster state
// visible for testing
// hook is used in testing to ensure that correct cluster state is used to test whether a
// rejoin or reroute is needed
/*
//www.apache.org/licenses/LICENSE-2.0
// NodeToolCli does not extend LoggingAwareCommand, because LoggingAwareCommand performs logging initialization
// after LoggingAwareCommand instance is constructed.
// It's too late for us, because before UnsafeBootstrapMasterCommand is added to the list of subcommands
// log4j2 initialization will happen, because it has static reference to Logger class.
// Even if we avoid making a static reference to Logger class, there is no nice way to avoid declaring
// UNSAFE_BOOTSTRAP, which depends on ClusterService, which in turn has static Logger.
// TODO execute CommandLoggingConfigurator.configureLoggingWithoutConfig() in the constructor of commands, not in beforeMain
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Tuple for simple atomic updates. null until the first call to `update()`.
// DiscoveryNode component is null if there is currently no known leader.
// TODO does this need to be on the generic threadpool or can it use SAME?
/**
// only for testing
// only for testing
// This is a _rare_ case where our leader has detected a failure and stepped down, but we are still a follower. It's possible
// that the leader lost its quorum, but while we're still a follower we will not offer joins to any other node so there is no
// major drawback in offering a join to our old leader. The advantage of this is that it makes it slightly more likely that the
// leader won't change, and also that its re-election will happen more quickly than if it had to wait for a quorum of followers
// to also detect its failure.
// create a fake VoteCollection based on the pre-votes and check if there is an election quorum
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// set when state is committed
// set when publication is completed
// set when publication is cancelled
// fail all current publications
// For assertions only: verify that this invariant holds
// For assertions
// TODO Can this ^ fail with an exception? Target should be failed if so.
/*
//www.apache.org/licenses/LICENSE-2.0
// the master needs the original non-serialized state as the cluster state contains some volatile information that we
// don't want to be replicated because it's not usable on another node (e.g. UnassignedInfo.unassignedTimeNanos) or
// because it's mostly just debugging info that would unnecessarily blow up CS updates (I think there was one in
// snapshot code).
// TODO: look into these and check how to get rid of them
// -> no need to put a timeout on the options here, because we want the response to eventually be received
//  and not log an error if it arrives after the timeout
// we build these early as a best effort not to commit in the case of error.
// sadly this is not water tight as it may that a failed diff based publishing to a node
// will cause a full serialization based on an older version, which may fail after the
// change has been committed.
// if publishing to self, use original request instead (see currentPublishRequestToSelf for explanation)
// we might override an in-flight publication to self in case where we failed as master and became master again,
// and the new publication started before the previous one completed (which fails anyhow because of higher current term)
// only clean-up our mess
// only clean-up our mess
// will send a diff
// If true we received full cluster state - otherwise diffs
// might throw IncompatibleClusterStateVersionException
// if the state is coming from the current node, use original request instead (see currentPublishRequestToSelf for explanation)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
// new configuration should have a quorum
// If there are not enough live nodes to form a quorum in the newly-proposed configuration, it's better to do nothing.
// prefer current master
// prefer nodes that are live
// prefer nodes that are in current config for stability
// tiebreak by node id to have stable ordering
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Can be absent if LEVEL == 'indices' or 'cluster'
// update the index status
// do not override an existing red
// might be since none has been created yet (two phase index creation)
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// the shard is relocating, the one it is relocating to will be in initializing state, so we don't count it
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// shortcut on green
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
// Check if this alias already exists
// It already exists, ignore it
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no data...
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// the value we recognize in the "max" position to mean all the nodes
// Only start using new logic once all nodes are migrated to 7.6.0, avoiding disruption during an upgrade
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// diffs also become upserts
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// the default maximum number of tombstones
/**
/**
/**
/**
/**
/**
/**
/**
/**
// first, purge the necessary amount of entries
/**
// nothing will have been removed, and all entries in current are new
// nothing will have been added, and all entries in previous are removed
// look through the back, starting from the end, for added tombstones
// look from the front for the removed tombstones
// the first tombstone in the current list wasn't found in the previous list,
// which means all tombstones from the previous list have been deleted.
// all previous are removed, so the current list must be the same as the added
/** The index tombstones that were added between two states */
/** The number of index tombstones that were removed between two states */
/**
// create from stream
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/* This is a safety limit that should only be exceeded in very rare and special cases. The assumption is that
/**
// this is only setable internally not a registered setting!!
/**
/**
/**
/**
/**
/**
/**
/**
// uuid will come as part of settings
/**
/**
/**
/**
/**
// fill missing slots in inSyncAllocationIds with empty set if needed and make all entries immutable
// fresh parser? move to the first token
// on a start object move to next token
// TODO: do this in 6.0:
// throw new IllegalArgumentException("Warmers are not supported anymore - are you upgrading from 1.x?");
// ignore: warmers have been removed in 5.0 and are
// simply ignored when upgrading from 2.x
// assume it's custom index metadata
/**
/**
/**
/**
/**
/**
// now we verify that the numRoutingShards is valid in the source index
// note: if the number of shards is 1 in the source index we can just assume it's correct since from 1 we can split into anything
// this is important to special case here since we use this to validate this in various places in the code but allow to split form
// 1 to N but we never modify the sourceIndexMetadata to accommodate for that
// this is just an additional assertion that ensures we are a factor of the routing num shards.
// special case - we can split into anything from 1 shard
/**
/**
/**
// split
// shrink
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// If only one index is specified then whether we fail a request if an index is missing depends on the allow_no_indices
// option. At some point we should change this, because there shouldn't be a reason why whether a single index
// or multiple indices are specified yield different behaviour.
/**
/**
/**
/**
/**
// The data math expression resolver doesn't rely on cluster state or indices options, because
// it just resolves the date math to an actual date.
/**
/**
/**
// pkg-private for testing
/**
// Shouldn't happen
// faster to iterate indexAliases
// faster to iterate resolvedExpressions
// If required - add it to the list of aliases
// If not, we have a non required alias for this index - no further checking needed
/**
// TODO: it appears that this can never be true?
// List of indices that don't require any routing
// Routing alias
// Non-routing alias
// Index
/**
/**
/**
/**
// if we end up matching on all indices, check, if its a wildcard parameter, or a "-something" structure
//we might have something like /-test1,+test1 that would identify all indices
//or something like /-test1 with test1 index missing and IndicesOptions.lenient()
//otherwise we check if there's any simple regex
/**
/**
/**
/**
// TODO: Fix API to work with sets rather than lists since we need to convert to sets
// internally anyway.
// add all the previous ones...
//TODO why does wildcard resolver throw exceptions regarding non wildcarded expressions? This should not be done here.
// Expressions can not start with an underscore. This is reserved for APIs. If the check gets here, the API
// does not exist and the path is interpreted as an expression. If the expression begins with an underscore,
// throw a specific error that is different from the [[IndexNotFoundException]], which is typically thrown
// if the expression can't be found.
//treat aliases as unavailable indices when ignoreAliases is set to true (e.g. delete index and update aliases api)
// Can only happen if the expressions was initially: '-*'
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// the mapping source should always include the type as top level
/**
/**
/**
// The parameter include_type_name is only ever used in the REST API, where reduce_mappings is
// always set to true. We therefore only check for include_type_name in this branch.
// the type name is the root value, reduce it
// crap, no mapping source, warn?
// we just hit the template name, which should be ignored and we move on
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//TODO revisit missing and unknown constants once Zen2 BWC is ready
/**
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// XContent exception, should never happen
/**
// the type name is the root value, reduce it
/**
// routing
/*
//www.apache.org/licenses/LICENSE-2.0
/* Custom metadata should be returns as part of API call */
/* Custom metadata should be stored as part of the persistent cluster state */
/* Custom metadata should be stored as part of a snapshot */
/**
/**
/**
/**
// Transient ? not serializable anyway?
/**
/**
/**
/**
/**
// Make the list order deterministic
/**
//only remove a field if it has no sub-fields left and it has to be excluded
//multi fields that should be excluded but hold subfields that don't have to be excluded are converted to objects
//return true if the ancestor may be removed, as it has no sub-fields left
/**
/**
// Alias routing overrides the parent routing (if any).
/**
// TODO: This can be moved to IndexNameExpressionResolver too, but this means that we will support wildcards and other expressions
// in the index,bulk,update and delete apis.
// Alias routing overrides the parent routing (if any).
/** Returns true iff existing index has the same {@link IndexMetaData} instance */
/**
/**
/**
/**
/**
/**
/**
// Check if any persistent metadata needs to be saved
// filter out custom states not supported by the other node
// create new empty index graveyard to initialize
// we know its a new one, increment the version and store
// if we put a new index metadata, increment its version
/**
// TODO: We should move these datastructures to IndexNameExpressionResolver, this will give the following benefits:
// 1) The datastructures will only be rebuilded when needed. Now during serializing we rebuild these datastructures
//    while these datastructures aren't even used.
// 2) The aliasAndIndexLookup can be updated instead of rebuilding it all the time.
// iterate again and constructs a helpful message
// build all concrete indices arrays:
// TODO: I think we can remove these arrays. it isn't worth the effort, for operations on all indices.
// When doing an operation across all indices, most of the time is spent on actually going to all shards and
// do the required operations, the bottleneck isn't resolving expressions into concrete indices.
// we might get here after the meta-data element, or on a fresh parser
// move to the field name (meta-data)
// move to the next object
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// UTF-8 should always be supported, but rethrow this if it is not for some reason
/**
// we do validate here - index setting must be consistent
/**
// we only find a template when its an API call (a new index)
// find templates, highest order are better matching
// remove the setting it's temporary and is only relevant once we create the index
// create the index here (on the master) to validate it can be created, as well as adding the mapping
// the context is only used for validation so it's fine to pass fake values for the shard id and the current
// timestamp
// Index was already partially created - need to clean up
/**
// apply templates, merging the mappings into the request mapping if exists
// Templates are wrapped with their _type names, which for pre-8x templates may not
// be _doc.  For now, we unwrap them based on the _type name, and then re-wrap with
// _doc
// TODO in 9x these will all have a _type of _doc so no re-wrapping will be necessary
/**
// apply templates, here, in reverse order, since first ones are better matching
// now, put the request settings, so they override templates
/*
/**
// in this case we either have no index to recover from or
// we have a source index with 1 shard and without an explicit split factor
// or one that is valid in that case we can split into whatever and auto-generate a new factor.
/**
// handle aliases
// if an alias with same name came with the create index request itself,
// ignore this one taken from the index template
// if an alias with same name was already processed, ignore this one
// Allow templatesAliases to be templated by replacing a token with the
// name of the index that we are applying it to
/**
// now, update the mappings with the actual source
// apply the aliases in reverse order as the lower index ones have higher order
/**
/*
// now that the mapping is merged we can validate the index sort.
// we cannot validate for index shrinking since the mapping is empty
// at this point. The validation will take place later in the process
// (when all shards are copied in a single place).
// Set up everything, now locally create the index to see that things are ok, and apply
/**
/**
/**
// now check that index is all on one node
// ensure index is read-only
// this method applies all necessary checks ie. if the target shards are less than the source shards
// of if the source shards are divisible by the number of target shards
// we use "i.r.a.initial_recovery" rather than "i.r.a.require|include" since we want the replica to allocate right away
// once we are allocated.
// copy all settings and non-copyable settings and settings that have already been set (e.g., from the request)
// do not override settings that have already been set (for example, from the request)
/**
// only select this automatically for indices that are created on or after 7.0 this will prevent this new behaviour
// until we have a fully upgraded cluster. Additionally it will make integratin testing easier since mixed clusters
// will always have the behavior of the min node in the cluster.
//
// We use as a default number of routing shards the higher number that can be expressed
// as {@code numShards * 2^x`} that is less than or equal to the maximum number of shards: 1024.
// logBase2(1024)
// ceil(logBase2(numShards))
// Ensure the index can be split at least once
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Check if index deletion conflicts with any running snapshots
// add tombstones to the cluster state for each deleted index
// the new graveyard set on the metadata
// update snapshot restore entries
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Gather all the indexes that must be removed first so:
// 1. We don't cause error when attempting to replace an index with a alias of the same name.
// 2. We don't allow removal of aliases from indexes that we're just going to delete anyway. That'd be silly.
// Remove the indexes if there are any to remove
// Run the remaining alias actions
// Handled above
/* It is important that we look up the index using the metadata builder we are modifying so we can remove an
// temporarily create the index and add mappings so we can parse the filter
// the context is only used for validation so it's fine to pass fake values for the shard id and the current
// timestamp
// only increment the aliases version if the aliases actually changed for this index
// even though changes happened, they resulted in 0 actual changes to metadata
// i.e. remove and add the same alias to the same index
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// acknowledged maybe be false but some indices may have been correctly closed, so
// we maintain a kind of coherency by overriding the shardsAcknowledged value
// (see ShardsAcknowledgedResponse constructor)
/**
// Check if index closing conflicts with any running restores
// Check if index closing conflicts with any running snapshots
// Reuse the existing index closed block
// Create a new index closed block
/**
/**
// Remove the index routing table of closed indices if the cluster is in a mixed version
// that does not support the replication of closed indices
// we should report error in this case as the index can be left as open.
// Check if index closing conflicts with any running restores
// Check if index closing conflicts with any running snapshots
//no explicit wait for other nodes needed as we use AckedClusterStateUpdateTask
// The index might be closed because we couldn't import it due to old incompatible version
// We need to check that this index can be upgraded to the current version
// Always removes index closed blocks (note: this can fail on-going close index actions)
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if its a match all pattern, and no templates are found (we have none), don't
// fail with index missing...
/**
// use the provided values, otherwise just pick valid dummy values
//create index service for parsing and validating "mappings"
// templates must be consistent with regards to dependencies
//we validate the alias only partially, as we don't know yet to which index it'll get applied to
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Throws an exception if there are too-old segments:
/*
// we have to run this first otherwise in we try to create IndexSettings
// with broken settings and fail in checkMappingsCompatibility
// only run the check with the upgraded settings!!
/**
/**
/*
/**
// We cannot instantiate real analysis server or similarity service at this point because the node
// might not have been started yet. However, we don't really need real analyzers or similarities at
// this stage - so we can fake it using constant maps accepting every key.
// This is ok because all used similarities and analyzers for this index were known before the upgrade.
// Missing analyzers and similarities plugin will still trigger the appropriate error during the
// actual upgrade.
// this entrySet impl isn't fully correct but necessary as SimilarityService will iterate
// over all similarities
// this entrySet impl isn't fully correct but necessary as IndexAnalyzers will iterate
// over all analyzers to close them
// Wrap the inner exception so we have the index name in the exception message
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// break down to tasks per index, so we can optimize the on demand index service creation
// to only happen for the duration of a single index processing of its respective events
// index got deleted on us, ignore...
// the tasks lists to iterate over, filled with the list of mapping tasks, trying to keep
// the latest (based on order) update mapping one per node
// construct the actual index if needed, and make sure the relevant mappings are there
// we need to create the index here, and add the current mapping to it, so we can merge
// if the mapping is not up-to-date, re-send everything
/**
// add mappings for all types, we need them for cross-type validation
// IMPORTANT: always get the metadata from the state since it get's batched
// and if we pull it from the indexService we might miss an update etc.
// this is paranoia... just to be sure we use the exact same metadata tuple on the update that
// we used for the validation, it makes this mechanism little less scary (a little)
// try and parse it (no need to add it here) so we can bail early in case of parsing exception
// first, simulate: just call merge and ignore the result
// do the actual merge here on the master, and update the mapping source
// we use the exact same indexService and metadata we used to validate above here to actually apply the update
// same source, no changes, ignore it
// use the merged mapping source
// Mapping updates on a single type may have side-effects on other types so we need to
// update mapping metadata on all types
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// don't validate wildcards
// don't validate dependencies here we check it below never allow to change the number of shards
// validate internal or private index settings
// we already validated the normalized settings
// allow to change any settings to a close index, and only allow dynamic settings to be changed
// on an open index
// Verify that this won't take us over the cluster shard limit.
// we do *not* update the in sync allocation ids as they will be removed upon the first index
// operation which make these copies stale
// TODO: update the list once the data is deleted by the node?
// increment settings versions
// now, reroute in case things change that require it (like number of replicas)
// Verifies that the current index settings can be updated with the updated dynamic settings.
// Now check that we can create the index with the updated settings (dynamic and non-dynamic).
// This step is mandatory since we allow to update non-dynamic settings on closed indices.
/**
// no reason to pollute the settings, we didn't really upgrade anything
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// wait until the gateway has recovered from disk, otherwise we think may not have the index templates,
// while they actually do exist
// we are already running some upgrades - skip this cluster state update
// we already checked these sets of templates - no reason to check it again
// we can do identity check here because due to cluster state diffs the actual map will not change
// if there were no changes
// we might attempt to delete the same template from different nodes - so that's ok if template doesn't exist
// otherwise we need to warn
// this is the last upgrade, the templates should now be in the desired state
// Check upgraders are satisfied after the update completed. If they still
// report that changes are required, this might indicate a bug or that something
// else tinkering with the templates during the upgrade.
// collect current templates
// upgrade global custom meta data
// remove templates if needed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: make this a proper setting validation logic, requiring multi-settings validation
/**
/**
/**
/**
//verify that no node roles are being provided as attributes
/** Creates a DiscoveryNode representing the local node. */
/** extract node roles from the given settings */
/**
// an old node will only send us legacy roles since pluggable roles is a new concept
// an old node will only understand legacy roles since pluggable roles is a new concept
/**
/**
/**
/**
/**
/**
/**
/**
/**
// we only need to hash the id because it's highly unlikely that two nodes
// in our system will have the same id but be different
// This is done so that this class can be used efficiently as a key in a map
// collect the abbreviation names into a map to ensure that there are not any duplicate abbreviations
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We check both the host_ip or the publish_ip
// If we match, we can check to the next filter
// We check explicitly only the host_ip
// If we match, we can check to the next filter
// We check explicitly only the publish_ip
// If we match, we can check to the next filter
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// since this setting is not registered, it will always return false when testing if the local node has the role
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// we don't know yet the local node id, return false
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// ignore ourselves when reporting on nodes being added
// reuse the same instance of our address and local node id for faster equality
// some one already built this and validated it's OK, skip the n2 scans
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// null if no reroute is currently pending
/**
/**
// no big deal, the new master will reroute again
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// package private to access from tests
/**
// delay until submitting the reroute command
// timestamp (in nanos) upon which delay was calculated
// no state changed, check when we should remove the delay flag from the shards the next time.
// if cluster state changed, we can leave the scheduling of the next delay up to the clusterChangedEvent
// this should not be needed, but we want to be extra safe here
/** override this to control time based decisions during delayed allocation */
/**
// we need an earlier delayed reroute
// protected so that it can be overridden (and disabled) by unit tests
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// note, we assume that when the index routing is created, ShardRoutings are created for all possible number of
// shards with state set to UNASSIGNED
/**
// check index exists
// check the number of shards
// check the replicas
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// This shards wasn't completely snapshotted - restore it as new shard
/**
// we have previous valid copies for this shard. use them for recovery
// this is a new index but the initial shards should merged from another index
// a freshly created index with no restriction
// version 0, will get updated when reroute will happen
// nothing to do here!
// re-add all the current ones
// first check if there is one that is not assigned to a node, and remove it
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// create the target initializing shard routing on the node the shard is relocating to
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Retrieve which nodes we can potentially send the query to
// Retrieve all the nodes the shards exist on
// sort all shards based on the shard rank
// adjust the non-winner nodes' stats so they will get a chance to receive queries
// If the winning shard is not started we are ranking initializing
// shards, don't bother to do adjustments
// Increase the number of searches for the "winning" node by one.
// Note that this doesn't actually affect the "real" counts, instead
// it only affects the captured node search counts, which is
// captured once for each query in TransportSearchAction
// these shards on the same node
// place non-nulls after null values
// place nulls before non-null values
// Both nodes do not have stats, they are equal
/**
/**
// fill it in a randomized fashion
/**
// don't allow more than one shard copy with same id to be allocated to same node
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//no instance
// no information loss
/*
//www.apache.org/licenses/LICENSE-2.0
// just use an empty map
// we use set here and not list since we might get duplicates
// starts with _shards, so execute on specific ones
// no more preference
// update the preference and continue
// if not, then use it as the index
// we would have still got 0 above but this check just saves us an unnecessary hash calculation
// we don't use IMD#getNumberOfShards since the index might have been shrunk such that we need to use the size
// of original index to hash documents
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Calls to nextOrNull might be performed on different threads in the transport actions so we need the volatile
// keyword in order to ensure visibility. Note that it is fine to use `volatile` for a counter in that case given
// that although nextOrNull might be called from different threads, it can never happen concurrently.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// LinkedHashMap to preserve order
/**
/**
/**
// Shard was already removed by routing nodes iterator
// TODO: change caller logic in RoutingNodes so that this check can go away
/**
/**
/**
/**
// initializingShards must consistent with that in shards
// relocatingShards must consistent with that in shards
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// fill in the nodeToShards with the "live" nodes
// LinkedHashMap to preserve order
// fill in the inverse of node -> shards allocated
// also fill replicaSet information
// to get all the shards belonging to an index, including the replicas,
// we define a replica set and keep track of it. A replica set is identified
// by the ShardId, as this is common for primary and replicas.
// A replica Set might have one (and not more) replicas with the state of RELOCATING.
// LinkedHashMap to preserve order
// LinkedHashMap to preserve order.
// Add the counterpart shard with relocatingNodeId reflecting the source from which
// it's relocating from.
// TODO: check primary == null || primary.active() after all tests properly add ReplicaAfterPrimaryActiveAllocationDecider
// add/remove corresponding outgoing recovery on node with primary shard
// primary is done relocating, move non-primary recoveries from old primary to new primary
/**
/**
/**
/**
/**
// It's possible for replicaNodeVersion to be null, when disassociating dead nodes
// that have been removed, the shards are failed, and part of the shard failing
// calls this method with an out-of-date RoutingNodes, where the version might not
// be accessible. Therefore, we need to protect against the version being null
// (meaning the node will be going away).
/**
// if we are empty nothing is active if we have less than total at least one is unassigned
// TODO these are used on tests only - move into utils class
// TODO these are used on tests only - move into utils class
/**
/**
/**
// relocation target has been started, remove relocation source
// if this is a primary shard with ongoing replica recoveries, reinitialize them as their recovery source changed
// copy list to prevent ConcurrentModificationException
// find the relocation source
// cancel relocation and start relocation to same node again
/**
// if this is a primary, fail initializing replicas first (otherwise we move RoutingNodes into an inconsistent state)
// copy list to prevent ConcurrentModificationException
// re-resolve replica as earlier iteration could have changed source/target of replica relocation
// find the shard that is initializing on the target node
// cancel and remove target shard
// promote to initializing shard without relocation source and ensure that removed relocation source
// is not added back as unassigned shard
// fail actual shard
// promote active replica to primary if active replica exists (only the case for shadow replicas)
// initializing shard that is not relocation target, just move to unassigned
// The shard is a target of a relocating shard. In that case we only need to remove the target shard and cancel the source
// relocation. No shard is left unassigned
// promote active replica to primary if active replica exists
// if the activeReplica was relocating before this call to failShard, its relocation was cancelled earlier when we
// failed initializing replica shards (and moved replica relocation source back to started)
/**
// if this is not a target shard for relocation, we need to update statistics
/**
/**
/**
/**
// relocation targets are not counted as inactive shards whereas initializing shards are
// yes we check identity here
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Assert that the active shard routing are identical.
// node might have dropped out of the cluster
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// index to IndexRoutingTable map
/**
/**
/**
/**
/**
/**
/**
/**
// use list here since we need to maintain identity across shards
// we simply ignore indices that don't exists (make sense for operations that use it currently)
// we need this for counting properly, just make it an empty one
// use list here since we need to maintain identity across shards
// we simply ignore indices that don't exists (make sense for operations that use it currently)
/**
// use list here since we need to maintain identity across shards
// we need this for counting properly, just make it an empty one
/**
// this is being called without pre initializing the routing table, so we must copy over the version as well
// every relocating shard has a double entry, ignore the target one.
/**
// ignore index missing failure, its closed...
// remove the required primary
// re-add all the shards
// now, add "empty" ones
// ignore, can't remove below the current one...
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// relocation target
/**
/**
/**
/**
/** returns true if the routing is the relocation target of the given routing */
/** returns true if the routing is the relocation source for the given routing */
/** returns true if the current routing is identical to the other routing in all but meta fields, i.e., unassigned info */
/**
// default to 0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// used for display and log messages, in milliseconds
// in nanoseconds, used to calculate delay for delayed shard allocation
// if allocation of this shard is delayed due to INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING
// result of the last allocation attempt for this shard
/**
/**
// As System.nanoTime() cannot be compared across different JVMs, reset it to now.
// This means that in master fail-over situations, elapsed delay time is forgotten.
// Do not serialize unassignedTimeNanos as System.nanoTime() cannot be compared across different JVMs
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// calculate next time to schedule
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** a constant representing a shard decision where no decision was taken */
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// shuffle the unassigned nodes, just so we won't have things like poison failed shards
// as starting a primary relocation target can reinitialize replica shards, start replicas first
// validates the routing table is coherent with the cluster state metadata
// Used for testing
// Used for testing
/**
// shuffle the unassigned nodes, just so we won't have things like poison failed shards
// failing a primary also fails initializing replica shards, re-resolve ShardRouting
/**
// shuffle the unassigned nodes, just so we won't have things like poison failed shards
// first, clear from the shards any node id they used to belong to that is now dead
/**
// we do *not* update the in sync allocation ids as they will be removed upon the first index
// operation which make these copies stale
// update settings version for each index
/**
/**
/**
// we don't shuffle the unassigned shards here, to try and get as close as possible to
// a consistent result of the effect the commands have on the routing
// this allows systems to dry run the commands, see the resulting cluster state, and act on it
// don't short circuit deciders, we want a full explanation
// we ignore disable allocation, because commands are explicit
// we revert the ignore disable flag, since when rerouting, we want the original setting to take place
// the assumption is that commands will move / act on shards (or fail through exceptions)
// so, there will always be shard "movements", so no need to check on reroute
/**
// shuffle the unassigned nodes, just so we won't have things like poison failed shards
// try to allocate existing shard copies first
// its a live node, continue
// now, go over all the shards routing on the node, and fail them
// its a dead node, remove it, note, its important to remove it *after* we apply failed shard
// since it relies on the fact that the RoutingNode exists in the list of nodes
// this is a costly operation - only call this once!
/** override this to control time based decisions during allocation */
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Clean up nodes that have been removed from the cluster
// might be temporarily null if the ClusterInfoService and the ClusterService are out of step
// might be temporarily null if the ClusterInfoService and the ClusterService are out of step
// will log about this node when the reroute completes
// The node has previously been over the low watermark, but is no longer, so it may be possible to allocate more shards
// if we reroute now.
// might be temporarily null if the ClusterInfoService and the ClusterService are out of step
// exposed for tests to override
// set read-only block but don't block on the response
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// this check is unnecessary in v9
// early return so that we do not try to parse as bytes
// swallow as we are now going to try to parse as bytes
// Watermark is expressed in terms of used data, but we need "free" data watermark
// Watermark is expressed in terms of used data, but we need "free" data watermark
// Watermark is expressed in terms of used data, but we need "free" data watermark
/**
/**
/**
/**
// NOTE: this is not end-user leniency, since up above we check that it's a valid byte or percentage, and then store the two
// cases separately
/**
/**
// NOTE: this is not end-user leniency, since up above we check that it's a valid byte or percentage, and then store the two
// cases separately
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// started shard has to have null recoverySource; have to pick up recoverySource from its initializing state
// more than one primary can be failed (because of batching, primary can be failed, replica promoted and then failed...)
/**
/**
// check if we have been force-initializing an empty primary or a stale primary
// we're not reusing an existing in-sync allocation id to initialize a primary, which means that we're either force-allocating
// an empty or a stale primary (see AllocateEmptyPrimaryAllocationCommand or AllocateStalePrimaryAllocationCommand).
// forcing an empty primary resets the in-sync allocations to the empty set (ShardRouting.allocatedPostIndexCreate)
// forcing a stale primary resets the in-sync allocations to the singleton set with the stale id
// standard path for updating in-sync ids
// Prevent set of inSyncAllocationIds to grow unboundedly. This can happen for example if we don't write to a primary
// but repeatedly shut down nodes that have active replicas.
// We use number_of_replicas + 1 (= possible active shard copies) to bound the inSyncAllocationIds set
// Only trim the set of allocation ids when it grows, otherwise we might trim too eagerly when the number
// of replicas was decreased while shards were unassigned.
// +1 for the primary
// trim entries that have no corresponding shard routing in the cluster state (i.e. trim unavailable copies)
// values with routing entries first
// only remove allocation id of failed active primary if there is at least one active shard remaining. Assume for example that
// the primary fails but there is no new primary to fail over to. If we were to remove the allocation id of the primary from the
// in-sync set, this could create an empty primary on the next allocation.
// add back allocation id of failed primary
// be extra safe here and only update in-sync set if it is non-empty
/**
// group staleShards entries by index
// group staleShards entries by shard id
// be extra safe here: if the in-sync set were to become empty, this would create an empty primary on the next allocation
// (see ShardRouting#allocatedPostIndexCreate)
/**
/**
/**
/**
// whether primary term should be increased
// allocation ids that should be added to the in-sync set
// allocation ids that should be removed from the in-sync set
// primary that was initialized from unassigned
// first active primary that was failed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** a constant representing no decision taken */
/** cached decisions so we don't have to recreate objects for common decisions when not in explain mode. */
/**
/**
// the final decision is NO (no node to move the shard to) and we are not in explain mode, return a cached version
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// it was a decision to rebalance the shard, because the shard was allowed to remain on its current node
// it was a decision to force move the shard
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/** A class that captures metadata about a shard store on a node. */
/**
/**
/**
/**
/**
// dealing with a primary shard
// there was no information we could obtain of any shard data on the node
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** returns the nano time captured at the beginning of the allocation. used to make sure all time based decisions are aligned */
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/* deltas close to the threshold are "rounded" to the threshold manually
/**
/*
/* skip if we only have one node */
/**
// we can only rebalance started shards
// find currently assigned node
// balance the shard, if a better node can be found
// skip over node we're currently allocated to
// the current weight of the node in the cluster, as computed by the weight function;
// this is a comparison of the number of shards on this node to the number of shards
// that should be on each node on average (both taking the cluster as a whole into account
// as well as shards per index)
// if the node we are examining has a worse (higher) weight than the node the shard is
// assigned to, then there is no way moving the shard to the node with the worse weight
// can make the balance of the cluster better, so we check for that here
// get the delta between the weights of the node we are checking and the node that holds the shard
// checks if the weight delta is above a certain threshold; if it is not above a certain threshold,
// then even though the node we are examining has a better weight and may make the cluster balance
// more even, it doesn't make sense to execute the heavyweight operation of relocating a shard unless
// the gains make it worth it, as defined by the threshold
// simulate the weight of the node if we were to relocate the shard to it
// calculate the delta of the weights of the two nodes if we were to add the shard to the
// node in question and move it away from the node that currently holds it.
// if the simulated weight delta with the shard moved away is better than the weight delta
// with the shard remaining on the current node, and we are allowed to allocate to the
// node in question, then allow the rebalance
// rebalance to the node, only will get overwritten if the decision here is to
// THROTTLE and we get a decision with YES on another node
/**
// find nodes that have a shard of this index or where shards of this index are allowed to be allocated to,
// move these nodes to the front of modelNodes so that we can only balance based on these nodes
// swap nodes at position i and relevantNodes
// is there a chance for a higher delta?
// check if we need to break at all
/* This is a special case if allocations from the "heaviest" to the "lighter" nodes is not possible
/* pass the delta to the replication function to prevent relocations that only swap the weights of the two nodes.
/*
/* Shrinking the window from MIN to MAX
/* Shrinking the window from MAX to MIN
/* we are done here, we either can't relocate anymore or we are balanced */
/**
/**
// Iterate over the started shards interleaving between nodes, and check if they can remain. In the presence of throttling
// shard movements, the goal of this iteration order is to achieve a fairer movement of shards from the nodes that are
// offloading the shards.
/**
// we can only move started shards
/*
// don't use canRebalance as we want hard filtering rules to apply. See #17698
// TODO maybe we can respect throttling here too?
// we are not in explain mode and already have a YES decision on the best weighted node,
// no need to continue iterating
/**
/* we skip relocating shards here since we expect an initializing shard with the same id coming in */
/**
/*
// this comparator is more expensive than all the others up there
// that's why it's added last even though it could be easier to read
// if we'd apply it earlier. this comparator will only differentiate across
// indices all shards of the same index is treated equally.
/*
// copy over the same replica shards to the secondary array so they will get allocated
// in a subsequent iteration, allowing replicas of other shards to be allocated first
// did *not* receive a YES decision
// throttle decision scenario
// we could not allocate it and we are a replica - check if we can ignore the other replicas
// clear everything we have either added it or moved to ignoreUnassigned
/**
// we only make decisions for unassigned shards here
// NO decision for allocating the shard, irrespective of any particular node, so exit early
/* find an node with minimal weight we can allocate on*/
// all nodes are throttled, so we know we won't be able to allocate this round,
// so if we are not in explain mode, short circuit
/* Don't iterate over an identity hashset here the
// decision is NO without needing to check anything further, so short circuit
// simulate weight if we would add shard to node
// moving the shard would not improve the balance, and we are not in explain mode, so short circuit
/*  we have an equal weight tie breaking:
// decision was not set and a node was not assigned, so treat it as a NO decision
// fill in the correct weight ranking, once we've been through all nodes
/**
// skip initializing, unassigned and relocating shards we can't relocate them anyway
// simulate moving shard from maxNode to minNode
/* this last line is a tie-breaker to make the shard allocation alg deterministic
/* allocate on the model even if not throttled */
/* only allocate on the cluster if we are not throttled */
/* now allocate on the cluster */
// expect few shards of same index to be allocated on same node
/* the nodes weights with respect to the current weight function / index */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Override equals and hashCode for testing
// Override equals and hashCode for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// don't use explainOrThrowRejectedCommand to keep the original "NO" decision
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
// ok...
// move to the command name
// move to the end object one
// Override equals and hashCode for testing
// Override equals and hashCode for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// only allow cancelling initializing shard of primary relocation without allowPrimary flag
// TODO: We don't have to remove a cancelled shard from in-sync set once we have a strict resync implementation.
// Override equals and hashCode for testing
// Override equals and hashCode for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// TODO we can possibly support also relocating cases, where we cancel relocation and move...
// its being throttled, maybe have a flag to take it into account and fail? for now, just do it since the "user" wants it...
// Override equals and hashCode for testing
// Override equals and hashCode for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// On a NO decision, by default, we allow force allocating the primary.
// On a THROTTLE/YES decision, we use the same decision instead of forcing allocation
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// short track if a NO is returned.
// short track if a NO is returned.
// short circuit only if debugging is not enabled
// the assumption is that a decider that returns the static instance Decision#ALWAYS
// does not really implements canAllocate
// short track if a NO is returned.
// short track if a NO is returned.
// short track if a NO is returned.
// short track if a NO is returned.
// short track if a NO is returned.
// short track if a NO is returned.
// short track if a NO is returned.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// 1 for primary
// the node the shard exists on must be associated with an awareness attribute
// build attr_value -> nodes map
// build the count of shards per attribute value
// Note: this also counts relocation targets as that will be the new location of the shard.
// Relocation sources should not be counted as the shard is moving away
// we work on different nodes, move counts around
// TODO should we remove ones that are not part of full list?
// ceil(shardCount/numberOfAttributes)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// check if there are unassigned primaries.
// check if there are initializing primaries that don't have a relocatingNodeId entry.
// check if there are unassigned shards.
// in case all indices are assigned, are there initializing shards which
// are not relocating?
// type == Type.ALWAYS
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Determine whether to read a Single or Multi Decision
/**
/**
/**
/**
/**
/**
/**
/**
/**
// flag specifying its a single decision
// Flatten explanation on serialization, so that explanationParams
// do not need to be serialized
/**
/**
// Multi decisions have no labels
// flag indicating it is a multi decision
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// in practice the only initializing-but-not-relocating shards with a nonzero expected shard size will be ones created
// by a resize (shrink/split/clone) operation which we expect to happen using hard links, so they shouldn't be taking
// any additional space and can be ignored here
// if we don't yet know the actual path of the incoming shard then conservatively assume it's going to the path with the least
// free space
// we might know the path of this shard from before when it was relocating
// subtractLeavingShards is passed as false here, because they still use disk space, and therefore should we should be extra careful
// and take the size into account
// First, check that the node currently over the low watermark
// Cache the used disk percentage for displaying disk percentages consistent with documentation
// flag that determines whether the low threshold checks below can be skipped. We use this for a primary shard that is freshly
// allocated and empty.
// checks for exact byte comparisons
// Allow the shard to be allocated because it is primary that
// has never been allocated if it's under the high watermark
// Even though the primary has never been allocated, the node is
// above the high watermark, so don't allow allocating the shard
// checks for percentage comparisons
// If the shard is a replica or is a non-empty primary, check the low threshold
// Allow the shard to be allocated because it is primary that
// has never been allocated if it's under the high watermark
// Even though the primary has never been allocated, the node is
// above the high watermark, so don't allow allocating the shard
// Secondly, check that allocating the shard to this node doesn't put it above the high watermark
// subtractLeavingShards is passed as true here, since this is only for shards remaining, we will *eventually* have enough disk
// since shards are moving away. No new shards will be incoming since in canAllocate we pass false for this check.
// If this node is already above the high threshold, the shard cannot remain (get it off!)
// If there is no usage, and we have other nodes in the cluster,
// use the average usage for all nodes as the usage for this node
/**
/**
// Always allow allocation if the decider is disabled
// Allow allocation regardless if only a single data node is available
// Fail open there is no info available
// Fail open if there are no disk usages available
/**
// in the shrink index case we sum up the source index shards since we basically make a copy of the shard in
// the worst case
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// only for unassigned - we filter allocation right after the index creation (for shard shrinking) to ensure
// that once it has been allocated post API the replicas can be allocated elsewhere without user interaction
// this is a setting that can only be set within the system!
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// check if we have passed the maximum retry threshold through canAllocate,
// if so, we don't want to force the primary allocation here
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// restoring from a snapshot - check that the node can handle the version
// existing or fresh primary on the node
// relocating primary, only migrate to newer host
// check that active primary has a newer version so that peer recovery works
// ReplicaAfterPrimaryActiveAllocationDecider should prevent this case from occurring
/* we can allocate if we can recover from a node that is younger or on the same version
/* we can allocate if we can restore from a snapshot that is older or on the same version */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we only make decisions here if we have an unassigned info and we have to recover from another index ie. split / shrink
// this only handles splits and clone so far.
// we might get called from the 2 param canAllocate method..
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// if its already a NO decision looking at the node, or we aren't configured to look at the host, return the decision
// check if its on the same host as the one we want to allocate to
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Capture the limit here in case it changes during this method's
// execution
// don't count relocating shards...
// Only checks the node-level limit, not the index-level
// Capture the limit here in case it changes during this method's
// execution
// don't count relocating shards...
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Only primary shards are snapshotted
// Snapshots are not running
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// primary is unassigned, means we are going to do recovery from store, snapshot or local shards
// count *just the primaries* currently doing recovery on the node and check against primariesInitialRecoveries
// when a primary shard is INITIALIZING, it can be because of *initial recovery* or *relocation from another node*
// we only count initial recoveries here, so we need to make sure that relocating node is null
// TODO: Should index creation not be throttled for primary shards?
// Peer recovery
// Allocating a shard to this node will increase the incoming recoveries
// search for corresponding recovery source (= primary shard) and check number of outgoing recoveries on that node
/**
// unassigned shards must have unassignedInfo (initializing shards might not)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// last applied state
// close timeout listeners that did not have an ongoing timeout
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// call the post added notification on the same event thread
// ignore cases where we are shutting down..., there is really nothing interesting
// to be done here...
/** asserts that the current thread is <b>NOT</b> the cluster state update thread */
/** asserts that the current stack trace does <b>NOT</b> involve a cluster state applier */
// people may start an observer from an applier
// failing to apply a cluster state with an exception indicates a bug in validation or in one of the appliers; if we
// continue we will retry with the same cluster state but that might not help.
// new cluster state, notify all listeners
// nothing to do until we actually recover from the gateway or any other block indicates we need to disable persistency
// can't wait for an ActionFuture on the cluster applier thread, but we do want to block the thread here, so use a CountDownLatch.
// note, we rely on the listener to remove itself in case of timeout if needed
// this one is overridden in tests so we can control time
// overridden by tests that need to check behaviour in the event of an application failure
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Add a no-op update consumer so changes are logged
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// new cluster state, notify all listeners
// indefinitely wait for publication to complete
// TODO: do we want to call updateTask.onFailure here?
// only the master controls the version numbers
/**
/**
/**
// fail all tasks that have failed
//no need to wait for ack if nothing changed, the update can be counted as acknowledged
/**
/**
/**
//we always wait for at least the master node
// we also wait for onCommit to be called
// re-check if onNodeAck has not completed while we were scheduling the timeout
// may be expensive => construct message lazily
/**
/**
// ignore cases where we are shutting down..., there is really nothing interesting
// to be done here...
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// package visible for tests
// convert to an identity map to check for dups based on task identity
// check that there won't be two tasks with the same identity for the same batching key
/**
// if this task is already processed, it shouldn't execute other tasks with same batching key that arrived later,
// to give other tasks with different batching key a chance to execute.
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Pick the first valid non loopback address we find
// Could not find a mac address
// If any of the bytes are non zero assume a good address
// address will be set below
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/** Returns true if value is neither NaN nor infinite. */
/** Return the long that {@code n} stores, or throws an exception if the
// weak bounds on the BigDecimal representation to allow for coercion
/** Return the long that {@code stringValue} stores or throws an exception if the
// we will try again with BigDecimal
/** Return the int that {@code n} stores, or throws an exception if the
/** Return the short that {@code n} stores, or throws an exception if the
/** Return the byte that {@code n} stores, or throws an exception if the
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// only do this if it doesn't exists we get a better exception further down
// if there are security issues etc. this also doesn't work if the parent exists
// and is a soft-link like on many linux systems /var/run can be a link and that should
// not prevent us from writing the PID
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.ietf.org/rfc/rfc4122.txt
/**
//www.ietf.org/rfc/rfc4122.txt
/**
//www.ietf.org/rfc/rfc4122.txt
//www.ietf.org/rfc/rfc4122.txt)
/* Set the version to version 4 (see http://www.ietf.org/rfc/rfc4122.txt)
/* clear the 4 most significant bits for the version  */
/* set the version to 0100 / 0x40 */
/* Set the variant:
/* clear the 2 most significant bits */
/* set the variant (MSB is set)*/
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// unexpected, bail
/**
// tests, so just use a seed from the non secure random
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/** Since, there is no offset of -1 ms, it is safe to use -1 for non-fixed timezones */
/** For fixed offset time zones, this is the offset in milliseconds, otherwise TZ_OFFSET_NON_FIXED */
// This works as long as the tz offset doesn't change. It is worth getting this case out of the way first,
// as the calculations for fixing things near to offset changes are a little expensive and unnecessary
// in the common case of working with fixed offset timezones (such as UTC).
// truncateAsLocalTime cannot have failed if there were no previous transitions
// There was a transition in between the input time and the truncated time. Return to the transition time and
// round that down instead.
// Now work out what localMidnight actually means
// There is at least one midnight on this day, so choose the first
// There were no midnights on this day, so we must have entered the day via an offset transition.
// Use the time of the transition as it is the earliest time on the right day.
// at least one possibilities - choose the latest one that's still no later than the input time
// The chosen local time didn't happen. This means we were given a time in an hour (or a minute) whose start
// is missing due to an offset transition, so the time cannot be truncated.
/** Since, there is no offset of -1 ms, it is safe to use -1 for non-fixed timezones */
/** For fixed offset timezones, this is the offset in milliseconds, otherwise TZ_OFFSET_NON_FIXED */
// This works as long as the tz offset doesn't change. It is worth getting this case out of the way first,
// as the calculations for fixing things near to offset changes are a little expensive and unnecessary
// in the common case of working with fixed offset timezones (such as UTC).
// a millisecond value with the same local time, in UTC, as `utcMillis` has in `timeZone`
// Now work out what roundedLocalDateTime actually means
// There is at least one instant with the desired local time. In general the desired result is
// the latest rounded time that's no later than the input time, but this could involve rounding across
// a timezone transition, which may yield the wrong result
// Rounding down across the transition can yield the wrong result. It's best to return to the transition time
// and round that down.
// TODO or throw something?
// The desired time isn't valid because within a gap, so just return the gap time.
// Versions before 7.6.0 will never send this type of rounding.
/*
//www.apache.org/licenses/LICENSE-2.0
// class loading is atomic - this is a lazy & safe singleton to be used by this package
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ERROR, or let it go?
//---------------------------------------------------------------------
// General convenience methods for working with Strings
//---------------------------------------------------------------------
/**
/**
/**
/**
/**
/**
/**
/**
/**
// our position in the old string
// the index of an occurrence we've found, or -1
// remember to append any characters to the right of a match
/**
/**
//---------------------------------------------------------------------
// Convenience methods for working with formatted Strings
//---------------------------------------------------------------------
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Add rest of String, but not in case of empty input.
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/* Create associative structure for columns that
// get the attributes of the header cell we are going to add to
// get the attributes of the header cell we are going to add
// If we're in a value row, also populate the named column.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//archive.fo/2015.07.08-082503/http://www.boundary.com/blog/2012/01/flake-a-decentralized-k-ordered-unique-id-generator-in-erlang/
// We only use bottom 3 bytes for the sequence number.  Paranoia: init with random int so that if JVM/OS/machine goes down, clock slips
// backwards, and JVM comes back up, we are less likely to be on the same sequenceNumber at the same time:
// Used to ensure clock moves forward:
// protected for testing
// protected for testing
// Don't let timestamp go backwards, at least "on our watch" (while this JVM is running).  We are still vulnerable if we are
// shut down, clock goes backwards, and we restart... for this we randomize the sequenceNumber on init to decrease chance of
// collision:
// Always force the clock to increment whenever sequence number is 0, in case we have a long time-slip backwards:
// We have auto-generated ids, which are usually used for append-only workloads.
// So we try to optimize the order of bytes for indexing speed (by having quite
// unique bytes close to the beginning of the ids so that sorting is fast) and
// compression (by making sure we share common prefixes between enough ids),
// but not necessarily for lookup speed (by having the leading bytes identify
// segments whenever possible)
// Blocks in the block tree have between 25 and 48 terms. So all prefixes that
// are shared by ~30 terms should be well compressed. I first tried putting the
// two lower bytes of the sequence id in the beginning of the id, but compression
// is only triggered when you have at least 30*2^16 ~= 2M documents in a segment,
// which is already quite large. So instead, we are putting the 1st and 3rd byte
// of the sequence number so that compression starts to be triggered with smaller
// segment sizes and still gives pretty good indexing speed. We use the sequenceId
// rather than the timestamp because the distribution of the timestamp depends too
// much on the indexing rate, so it is less reliable.
// changes every 65k docs, so potentially every second if you have a steady indexing rate
// Now we start focusing on compression and put bytes that should not change too often.
// changes every ~65 secs
// changes every ~4.5h
// changes every ~50 days
// changes every 35 years
// Finally we put the remaining bytes, which will likely not be compressed at all.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Generates a time-based UUID (similar to Flake IDs), which is preferred when generating an ID to be indexed into a Lucene index as
//www.ietf.org/rfc/rfc4122.txt, using the
/** Returns a Base64 encoded version of a Version 4.0 compatible UUID as defined here: http://www.ietf.org/rfc/rfc4122.txt, using the
//www.ietf.org/rfc/rfc4122.txt, using a
/** Returns a Base64 encoded version of a Version 4.0 compatible UUID as defined here: http://www.ietf.org/rfc/rfc4122.txt, using a
//www.ietf.org/rfc/rfc4122.txt,
/** Returns a Base64 encoded {@link SecureString} of a Version 4.0 compatible UUID as defined here: http://www.ietf.org/rfc/rfc4122.txt,
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// The file was concurrently deleted between listing files and trying to get its attributes so we skip it here
// If the target file exists then Files.move() behaviour is implementation specific
// the existing file might be replaced or this method fails by throwing an IOException.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// nothing to do here...
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// short-circuit on no data allowed, immediately throwing an exception
// If there is no limit (-1), we can optimize a bit by using
// .addAndGet() instead of looping (because we don't have to check a
// limit), which makes the RamAccountingTermsEnum case faster.
// Additionally, we need to check that we haven't exceeded the parent's limit
// If the parent breaker is tripped, this breaker has to be
// adjusted back down because the allocation is "blocked" but the
// breaker has already been incremented
// Otherwise, check the addition and commit the addition, looping if
// there are conflicts. May result in additional logging, but it's
// trace logging and shouldn't be counted on for additions.
// Attempt to set the new used value, but make sure it hasn't changed
// underneath us, if it has, keep trying until we are able to set it
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// A regular or ChildMemoryCircuitBreaker
// A special parent-type for the hierarchy breaker service
// A breaker where every action is a noop, it never breaks
// The condition that tripped the circuit breaker fixes itself eventually.
// The condition that tripped the circuit breaker requires manual intervention.
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// noop
/*
//www.apache.org/licenses/LICENSE-2.0
// we cache the hash of this reference since it can be quite costly to re-calculated it
// only return it once...
// this is a call to BytesRef#bytesEquals - this method is the hot one in the comparison
/**
// we use the iterators since it's a 0-copy comparison where possible!
// do we have any data?
// we clone since we modify the offsets and length in the iteration below
// is it only one array slice we are comparing?
// must be non null otherwise we have a bug
// must be non null otherwise we have a bug
// shrink to the same length and use the fast compare in lucene
// now we move to the fast comparison - this is the hot part of the loop
// One is a prefix of the other, or, they are equal:
/**
// can't use FilterStreamInput it needs to reset the delegate
// readLimit is optional it only guarantees that the stream remembers data upto this limit but it can remember more
// which we do in our case
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// this is really an error since we don't do IO in our bytesreferences
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// the total size of the stream
// the current position of the stream
// copy the full length or the remaining part
// do nothing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we use the offsets to seek into the right BytesReference for random access and slicing
// offsets
// references
// length
// ramBytesUsed
// for slices we only need to find the start and the end reference
// adjust them and pass on the references in between as they are fully contained
// now adjust slices in front and at the end
// this is really an error since we don't do IO in our bytesreferences
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if length <= pagesize this will dereference the page, or materialize the byte[]
// this iteration is page aligned to ensure we do NOT materialize the pages from the ByteArray
// we calculate the initial fragment size here to ensure that if this reference is a slice we are still page aligned
// across the entire iteration. The first page is smaller if our offset != 0 then we start in the middle of the page
// otherwise we iterate full pages until we reach the last chunk which also might end within a page.
// this BytesRef is reused across the iteration on purpose - BytesRefIterator interface was designed for this
// we are done with this iteration
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// positive if entries have an expiration
// true if entries can expire after access
// positive if entries have an expiration after write
// true if entries can expire after initial insertion
// the number of entries in the cache
// the weight of the entries in the cache
// the maximum weight that this cache supports
// the weigher of entries
// the removal callback
// use CacheBuilder to construct
// pkg-private for testing
// pkg-private for testing
/**
// System.nanoTime takes non-negligible time, so we only use it if we need it
// use System.nanoTime because we want relative time, not absolute time
// the state of an entry in the LRU list
/**
// read/write lock protecting mutations to the segment
/**
/**
/**
/**
// lock protecting mutations to the LRU list
/**
/**
// we have to eagerly evict expired entries or our putIfAbsent call below will fail
// we need to synchronize loading of a value for a given key; however, holding the segment lock while
// invoking load can lead to deadlock against another thread due to dependent key loading; therefore, we
// need a mechanism to ensure that load is invoked at most once, but we are not invoking load while holding
// the segment lock; to do this, we atomically put a future in the map that can load the value, and then
// get the value from this future on the thread that won the race to place the future into the segment map
// check to ensure the future hasn't been completed with an exception
// call get to force the exception to be thrown for other concurrent callers
/**
// ok
/**
/**
/**
/**
/**
/**
/**
/**
/**
// removing the head
// removing inner element
// removing tail
// removing inner element
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//en.wikipedia.org/wiki/Hash_array_mapped_trie">the wikipedia page</a>
/**
// no need to copy in that case
/**
/**
/**
/**
/**
/**
/**
/**
// the bitmap
// subNodes[slot] is either a value or a sub node in case of a hash collision
// only used in assert
/**
/**
// keys don't make sense on inner nodes
// we have an entry for this hash, but the value is different
// insert recursively
// replace the existing entry
// hash collision
// not in sub-nodes
// remove entry
// hash collision, nothing to remove
/**
// works fine since null values are not supported
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// Hppc has forEach, but this means we need to build an intermediate set, with this method we just iterate
// over each unique value without creating a third set.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// nullify the map, so any operation post build will fail! (hackish, but safest)
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// nullify the map, so any operation post build will fail! (hackish, but safest)
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// explicit generic type argument needed for type inference
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: replace with usages of Map.of and Map.ofEntries
/**
// TODO: follow the directions in the Javadoc for this method
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// continue with stop logic
// perform close logic here
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no-op
// no-op
// cannot happen
// Used for serialization
/**
/**
// already compressed...
/** Return the compressed bytes. */
/** Return the compressed bytes as a {@link BytesReference}. */
/** Return the uncompressed bytes. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// bytes should be either detected as compressed or as xcontent,
// if we have bytes that can be either detected as compressed or
// as a xcontent, we have a problem
/** true if the bytes were compressed with LZF: only used before elasticsearch 2.0 */
/**
/** Decompress the provided {@link BytesReference}. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// An arbitrary header that we use to identify compressed streams
// It needs to be different from other compressors and to not be specific
// enough so that no stream starting with these bytes could be detected as
// a XContent
// 3 is a good trade-off between speed and compression ratio
// We use buffering on the input and output of in/def-laters in order to
// limit the number of JNI calls
// important to release native memory
// important to release native memory
/*
//www.apache.org/licenses/LICENSE-2.0
/** Exception indicating that we were expecting something compressed, which
/*
//www.apache.org/licenses/LICENSE-2.0
/** Exception indicating that we were expecting some {@link XContent} but could
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// this call doesn't really need to support writing any kind of object.
// Stored fields values are converted using MappedFieldType#valueForDisplay.
// As a result they can either be Strings, Numbers, or Booleans, that's
// all.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Creates a GeoDistance instance from an input stream */
/** Writes an instance of a GeoDistance object to an output stream */
/**
/** compute the distance between two points using the selected algorithm (PLANE, ARC) */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// top left
// bottom right
// We expect to have coordinates for all the rest
// handle possible null in orientation
// handle possible null in orientation
/**
/**
// Base cases
/**
// Add support for coerce here
// alt (for storing purposes only - future use includes 3d shapes)
// do not support > 3 dimensions
/**
/**
/**
// verify coordinate bounds, correct if necessary
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Orientation for BWC with ShapeBuilder
// Number of points For BWC with Shape Builder
// Orientation for BWC with ShapeBuilder
// top left
// bottom right
// For BWC with Shape Builder
// orientation for BWC
// top left
// bottom right
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// We don't know the format of the original geometry - so going with default
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// todo this is a crutch because LatLonPoint doesn't have a helper for returning .stringValue()
// todo remove with next release of lucene
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// build shell
// build polygon with shell and holes
// close linear ring iff coerce is set and ring is open, otherwise throw parse exception
/**
// noop; todo validate at least 1 polygon to ensure valid multipolygon
// verify coordinate bounds, correct if necessary
// validate the coordinate array for envelope type
// noop
// noop, handled in parser
// noop
/** wkt shape name */
/*
//www.apache.org/licenses/LICENSE-2.0
/** Maximum valid latitude in degrees. */
/** Minimum valid latitude in degrees. */
/** Maximum valid longitude in degrees. */
/** Minimum valid longitude in degrees. */
/** Earth ellipsoid major axis defined by WGS 84 in meters */
// meters (WGS 84)
/** Earth ellipsoid minor axis defined by WGS 84 in meters */
// meters (WGS 84)
/** Earth mean radius defined by WGS 84 in meters */
// meters (WGS 84)
/** Earth axis ratio defined by WGS 84 (0.996647189335) */
/** Earth ellipsoid equator length in meters */
/** Earth ellipsoid polar distance in meters */
/** rounding error for quantized latitude and longitude values */
/** Returns true if latitude is actually a valid latitude value.*/
/** Returns true if longitude is actually a valid longitude value. */
/**
// Geohash cells are split into 32 cells at each level. the grid
// alternates at each level between a 8x4 and a 4x8 grid
/**
/**
// Geohash cells are split into 32 cells at each level. the grid
// alternates at each level between a 8x4 and a 4x8 grid
/**
/**
/**
/**
// cell ratio
// convert to cell width
// (log_2)
// adjust level
/**
/**
// cell ratio
// convert to cell width
// number of 5 bit subdivisions
// bit representing the last level
// number of even levels
// number of odd levels
/**
/**
// avoid -0.0
/**
// avoid -0.0
/**
/**
// No need to shift the longitude, and the latitude is normalized
// Longitude won't be normalized,
// keep it in the form x+k*360 (with x in ]-180;180])
// by only changing x, assuming k is meaningful for the user application.
/**
/**
// start object
// field name
// field value
/**
/**
/**
/**
/**
// we want to treat simple integer strings as precision levels, not distances
// try to parse as a distance value
// this happens when distance too small, so precision > 12. We'd like to see the original string
/**
/** Returns the maximum distance/radius (in meters) from the point 'center' before overlapping */
/** Return the distance (in meters) between 2 lat,lon geo points using the haversine method implemented by lucene */
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Maps ShapeRelation to Lucene's LatLonShapeRelation */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// minX, maxX, maxY, minY
// TODO support Z??
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
//note: ShapeCollection is probably faster than a Multi* geom.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//geojson.org/geojson-spec.html#linestring)
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
//Could wrap JtsGeometry but probably slower due to conversions to/from JTS in relate()
//MultiPoint geometry = FACTORY.createMultiPoint(points.toArray(new Coordinate[points.size()]));
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
//note: ShapeCollection is probably faster than a Multi* geom.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//super(new ArrayList<>(1));
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// line string defining the shell of the polygon
// List of line strings defining the holes of the polygon
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Last point is repeated
/**
/**
// find a coordinate that is not part of the dateline
// run along the border of the component, collect the
// edges, shift them according to the dateline and
// update the component id
// if there are two connected components, splitIndex keeps track of where to split the edge array
// start at 1 since the source coordinate is shared
// bookkeep the source and sink of each visited coordinate
// found a closed loop - we have two connected components so we need to slice into two distinct components
// a negative id flags the edge as visited for the edges(...) method.
// since we're splitting connected components, we want the edges method to visit
// the newly separated component
// correct the graph pointers by correcting the 'next' pointer for both the
// first appearance and this appearance of the edge
// backtrack until we get back to this coordinate, setting the visit id to
// a non-visited value (anything positive)
/**
// First and last coordinates must be equal
// mark as visited by inverting the sign
// Assign Hole to related components
// To find the new component the hole belongs to all intersections of the
// polygon edges with a vertical line are calculated. This vertical line
// is an arbitrary point of the hole. The polygon edge next to this point
// is part of the polygon the hole belongs to.
// To do the assignment we assume (and later, elsewhere, check) that each hole is within
// a single component, and the components do not overlap. Based on this assumption, it's
// enough to find a component that contains some vertex of the hole, and
// holes[i].coordinate is such a vertex, so we use that one.
// First, we sort all the edges according to their order of intersection with the line
// of longitude through holes[i].coordinate, in order from south to north. Edges that do
// not intersect this line are sorted to the end of the array and of no further interest
// here.
// There were no edges that intersect the line of longitude through
// holes[i].coordinate, so there's no way this hole is within the polygon.
// Next we do a binary search to find the position of holes[i].coordinate in the array.
// The binary search returns the index of an exact match, or (-insertionPoint - 1) if
// the vertex lies between the intersections of edges[insertionPoint] and
// edges[insertionPoint+1]. The latter case is vastly more common.
// The binary search returned an exact match, but we checked again using compareTo()
// and it didn't match after all.
// TODO Can this actually happen? Needs a test to exercise it, or else needs to be removed.
// holes[i].coordinate lies exactly on an edge.
// TODO Should this be pos instead of 0? This assigns exact matches to the southernmost component.
// holes[i].coordinate is strictly south of all intersections. Assign it to the
// southernmost component, and allow later validation to spot that it is not
// entirely within the chosen component.
// holes[i].coordinate is strictly north of at least one intersection. Assign it to
// the component immediately to its south.
// Intersections appear pairwise. On the first edge the inner of
// of the polygon is entered. On the second edge the outer face
// is entered. Other kinds of intersections are discard by the
// intersection function
// If two segments are connected maybe a hole must be deleted
// Since Edges of components appear pairwise we need to check
// the second edge only (the first edge is either polygon or
// already handled)
//TODO: Check if we could save the set null step
// only connect edges if intersections are pairwise
// 1. per the comment above, the edge array is sorted by y-value of the intersection
// with the dateline.  Two edges have the same y intercept when they cross the
// dateline thus they appear sequentially (pairwise) in the edge array. Two edges
// do not have the same y intercept when we're forming a multi-poly from a poly
// that wraps the dateline (but there are 2 ordered intercepts).
// The connect method creates a new edge for these paired edges in the linked list.
// For boundary conditions (e.g., intersect but not crossing) there is no sibling edge
// to connect. Thus the first logic check enforces the pairwise rule
// 2. the second logic check ensures the two candidate edges aren't already connected by an
//    existing edge along the dateline - this is necessary due to a logic change in
//    ShapeBuilder.intersection that computes dateline edges as valid intersect points
//    in support of OGC standards
// Connecting two Edges by inserting the point at
// dateline intersection and connect these by adding
// two edges between this points. One per direction
// NOTE: the order of the object creation is crucial here! Don't change it!
// first edge has no point on dateline
// second edge has no point on dateline
// second edge intersects with dateline
// first edge intersects with dateline
// second edge has no point on dateline
// second edge intersects with dateline
// inner rings (holes) have an opposite direction than the outer rings
// XOR will invert the orientation for outer ring cases (Truth Table:, T/T = F, T/F = T, F/T = T, F/F = F)
// set the points array accordingly (shell or hole)
/**
// OGC requires shell as ccw (Right-Handedness) and holes as cw (Left-Handedness)
// since GeoJSON doesn't specify (and doesn't need to) GEO core will assume OGC standards
// thus if orientation is computed as cw, the logic will translate points across dateline
// and convert to a right handed system
// compute the bounding box and calculate range
// translate the points if the following is true
//   1.  shell orientation is cw and range is greater than a hemisphere (180 degrees) but not spanning 2 hemispheres
//       (translation would result in a collapsed poly)
//   2.  the shell of the candidate hole has been translated (to preserve the coordinate system)
// flip the translation bit if the shell is being translated
// correct the orientation post translation (ccw for shell, cw for holes)
/**
// calculate the direction of the points: find the southernmost point
// and check its neighbors orientation.
// Points are collinear, but `top` is not in the middle if so, so the edges either side of `top` are intersecting.
/**
// we start at 1 here since top points to 0
// compute the bounding coordinates (@todo: cleanup brute force)
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if asserts are enabled we run the debug statements even if they are not logged
// to prevent exceptions only present if debug enabled
/**
// TODO how might we use JtsSpatialContextFactory to configure the context (esp. for non-geo)?
/** We're expecting some geometries might cross the dateline. */
/** It's possible that some geometries in a MULTI* shape might overlap. With the possible exception of GeometryCollection,
/** @see org.locationtech.spatial4j.shape.jts.JtsGeometry#validate() */
/** @see org.locationtech.spatial4j.shape.jts.JtsGeometry#index() */
//may want to turn off once SpatialStrategy impls do it.
/** default ctor */
/** ctor from list of coordinates */
/** ctor from serialized stream input */
/**
/**
/**
/**
/**
//dateline180Check is false because ElasticSearch does it's own dateline wrapping
/**
/**
/**
/** tracks number of dimensions for this shape */
/**
/**
/**
// coordinate of the start point
// next segment
// potential intersection with dateline
// id of the component this edge belongs to
// use setter to catch duplicate point cases
// don't bother setting next if its null
// self-loop throws an invalid shape
/**
// walk through coordinates:
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//tools.ietf.org/html/rfc7946
/**
// Base cases
// alt (for storing purposes only - future use includes 3d shapes)
// do not support > 3 dimensions
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//docs.opengeospatial.org/is/12-063r5/12-063r5.html
// no instance
/** throws an exception if the parsed geometry type does not match the expected shape type */
// setup the tokenizer; configured to read words w/o numbers
/** parse geometry from the stream tokenizer */
// A LinearRing is closed LineString with 4 or more positions. The first and last positions
// are equivalent (they represent equivalent points).
/** next word in the stream */
/** next word in the stream */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// start object
// field name
// field value
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** lower 64 bits part **/
/** higher 64 bits part **/
/**
// Intentionally uses fallthrough to implement a well known hashing algorithm
// higher multiple of 16 that is lower than or equal to length
// Advance offset to the unprocessed tail of the data.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// or, alternatively
// or, alternatively
// A member variable (field)
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Error: Missing implementation.
// Example: bind(Date.class).annotatedWith(Red.class);
// We can't assume abstract types aren't injectable. They may have an
// @ImplementedBy annotation or something.
// This cast is safe after the preceding check.
// leave the private elements for the PrivateElementsProcessor to handle
// prevent the parent from creating a JIT binding for this key
/**
// It's unfortunate that we have to maintain a blacklist of specific
// classes, but we can't easily block the whole package because of
// all our unit tests.
// TODO(jessewilson): fix BuiltInModule, then add Stage
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// this is *extremely* unsafe. We trust the caller here.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// This may not actually be safe because it could return a super type of T (if that's all the
// client needs), but it should be OK in practice thanks to the wonders of erasure.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// We have a circular reference between constructors. Return a proxy.
// TODO (crazybob): if we can't proxy this object, can we proxy the other object?
// If we're re-entering this factory while injecting fields or methods,
// return the same instance. This prevents infinite loops.
// First time through...
// Store reference. If an injector re-enters this factory, they'll get the same reference.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// the ConstructorInjector type always agrees with the passed type
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// the injection point is for a constructor of T
// shouldn't happen, we know this is a concrete type
// a security manager is blocking us, we're hosed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// lazy
// lazy
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// validate that the child injector has its own factory. If the getInternalFactory() returns
// this, then that child injector doesn't have a factory (and getExplicitBinding has returned
// its parent's binding instead
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Now just bootstrap the application and you're done
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Must be a linked hashmap in order to preserve order of bindings in Modules.
// we only put in BindingImpls that match their key types
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// short circuit if the object has no injections
/**
/**
// loop over a defensive copy since ensureInjected() mutates the set. Unfortunately, that copy
// is made complicated by a bug in IBM's JDK, wherein entrySet().toArray(Object[]) doesn't work
// the type of 'T' is a TypeLiteral<T>
/**
// just wait for everything to be injected by another thread
// Give up, since we don't know if our injection is ready
// toInject needs injection, do it right away. we only do this once, even if it fails
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Synchronize while we're building up the bindings and other injector state. This ensures that
// the JIT bindings in the parent injector don't change while we're being built
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Check explicit bindings, i.e. bindings created by modules.
// Look for an on-demand binding.
/**
// first try to find a JIT binding that we've already created
// we only store bindings that match their key
/**
/**
// safe because T came from Key<MembersInjector<T>>
/**
// If the Provider has no type parameter (raw Provider)...
// safe because T came from Key<Provider<T>>
/**
// Find a constant string binding.
// Find a matching type converter.
// No converter can handle the given type.
// Try to convert the string. A failed conversion results in an error.
// This cast is safe because we double check below.
// Put the partially constructed binding in the map a little early. This enables us to handle
// circular dependencies. Example: FooImpl -> BarImpl -> FooImpl.
// Note: We don't need to synchronize on state.lock() during injector creation.
// TODO: for the above example, remove the binding for BarImpl if the binding for FooImpl fails
/**
// Don't try to inject arrays, or enums.
// Handle TypeLiteral<T> by binding the inner type
// we have to fudge the inner type as Object
// Handle @ImplementedBy
// Handle @ProvidedBy.
// We can't inject abstract classes.
// TODO: Method interceptors could actually enable us to implement
// abstract types. Should we remove this restriction?
// Error: Inner class.
/**
// this is unfortunate. We don't support building TypeLiterals for type variable like 'T'. If
// this proves problematic, we can probably fix TypeLiteral to support type variables
// by definition, innerType == T, so this is safe
/**
// Make sure it's not the same type. TODO: Can we check for deeper loops?
// Assume the provider provides an appropriate type. We double check at runtime.
// protected by isInstance() check above
/* source */,
/**
// Make sure it's not the same type. TODO: Can we check for deeper cycles?
// Make sure implementationType extends type.
// After the preceding check, this cast is safe.
// Look up the target binding.
/* source */,
/**
/**
// Handle cases where T is a Provider<?>.
// These casts are safe. We know T extends Provider<X> and that given Key<Provider<X>>,
// createProviderBinding() will return BindingImpl<Provider<X>>.
// Handle cases where T is a MembersInjector<?>
// These casts are safe. T extends MembersInjector<X> and that given Key<MembersInjector<X>>,
// createMembersInjectorBinding() will return BindingImpl<MembersInjector<X>>.
// Try to convert a constant string binding to the requested type.
// If the key has an annotation...
// Look for a binding without annotation attributes or return null.
// throw with a more appropriate message below
// safe because we only put matching entries into the map
/**
// rethrown below
/**
/**
/**
// the members injector type is consistent with instance's type
// ES: optimize for a common case of read only instance getting from the parent...
// ignore
// should never happen...
/**
// Only clear the context if this call created it.
// Someone else will clean up this context.
// ES_GUICE: clear caches
// ES_GUICE: make all registered bindings act as eager singletons
// reindex the bindings
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// bind Stage and Singleton if this is a top-level injector
// recursively build child shells
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// no usages, not test-covered
/**
/**
/**
/**
/**
/**
// not test-covered
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// this class not test-covered
// Keep the instance around if we have it so the client can request it.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: source
// ensure the provider can be created
// TODO: source
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// optimization: use manual for/each to save allocating an iterator here
// optimization: use manual for/each to save allocating an iterator here
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// the MembersInjector type always agrees with the passed type
/**
/**
// ignored for now
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//private static final Logger logger = Logger.getLogger(Guice.class.getName());
// ES_GUICE: don't log failures using jdk logging
//        if (message.getCause() != null) {
//            String rootMessage = getRootMessage(message.getCause());
//            logger.log(Level.INFO,
//                    "An exception was caught and reported. Message: " + rootMessage,
//                    message.getCause());
//        }
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// in ES, we always create all instances as if they are eager singletons
// this allows for considerable memory savings (no need to store construction info) as well as cycles
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//docs.google.com/Doc?id=dd2fhx4z_5df5hw8">User's Guide</a>
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Guice treats PrivateModules specially and passes in a PrivateBinder automatically.
/**
/**
/**
/**
// everything below is copied from AbstractModule
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Go ahead and bind anyway so we don't get collateral errors.
// Go ahead and bind anyway so we don't get collateral errors.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// DCL on a volatile is safe as of Java 5, which we obviously require.
/*
/**
/**
// TODO: use diamond operator once JI-9019884 is fixed
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// a security manager is blocking us, we're hosed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We can't use FastMethod if the method is private.
// a security manager is blocking us, we're hosed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// optimization: use manual for/each to save allocating an iterator here
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
// ES_GUICE: clean blacklist keys
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Configure type converters.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// prints "Set<Integer>"}
/**
/**
/**
/**
/**
/**
/**
// This cast is safe and wouldn't generate a warning if Type had a type
// parameter.
/**
/**
/**
/**
// this implementation is made a little more complicated in an attempt to avoid object-creation
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// categorize params as @Assisted or @Injected
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
// pass methods from Object.class to the proxy
// we imprecisely treat the class literal of T as a Class<T>
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Scoping isn't allowed when we have only one instance.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// lookup the injection points, adding any errors to the binder's errors list
// lookup the injection points, adding any errors to the binder's errors list
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// constant bindings start out with T unknown
// this type will define T, so these assignments are safe
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: if I create a proxy which implements all the interfaces of
// the implementation type, I'll be able to get away with one proxy
// instance (as opposed to one per caller).
// ES: Replace, since we don't use bytecode gen, just get the type class loader, or system if its null
//ClassLoader classLoader = BytecodeGen.getClassLoader(expectedType);
// This appears to be not test-covered
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// lazy, use getErrorsForAdd()
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we can't use cache.computeIfAbsent since this might be recursively call this API
// create returned a non-error result, so this is safe
/*
//www.apache.org/licenses/LICENSE-2.0
// instance bindings aren't scoped
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// type is either serializable as-is or unsupported
// type is a normal class.
// I'm not exactly sure why getRawType() returns Type instead of Class.
// Neal isn't either but suspects some pathological case related
// to nested classes exists.
// TODO: Is this sufficient?
// we could use the variable's bounds, but that'll won't work if there are multiple.
// having a raw type that's more general than necessary is okay
/**
// also handles (a == null && b == null)
// Class already specifies equals().
// TODO: save a .clone() call
// This isn't a type we support. Could be a generic array type, wildcard type, etc.
/**
// Class specifies hashCode().
// This isn't a type we support. Probably a type variable
/**
/**
/**
// we skip searching through interfaces if unknown is an interface
// check our supertypes
// we can't resolve this further
// we can't reduce this further
/**
// require an owner type if the raw type needs it
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//groups.google.com/group/jsr-305/web/proposed-annotations">
//www.jetbrains.com/idea/documentation/howto.html">
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/**
/**
// ensure exposedKeysToSources is populated
// ensure exposedKeysToSources is populated
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// exposed for GIN
// the cast is safe 'cause the only binder we have implements PrivateBinder. If there's a
// misplaced @Exposed, calling this will add an error to the binder's error queue
// We know this cast is safe because T is the method's return type.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
//code.google.com/p/google-gin/">GIN</a>
// avoid infinite recursion, since installing a module always installs itself
// prepare the parameter providers
// Define T as the method's return type.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// do nothing
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Linked hash map ensures ordering.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
// TODO minJava >= 9 : use ClassLoader.getDefinedPackage and remove @SuppressForbidden
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// a map of <K, V> is safely a Map<K, V>
// a provider map <K, V> is safely a Map<K, Provider<V>>
// a provider entry <K, V> is safely a Map.Entry<K, Provider<V>>
/**
/**
/* the target injector's binder. non-null until initialization, null afterwards */
/**
// code is silly stupid with generics
// code is silly stupid with generics
// code is silly stupid with generics
// code is silly stupid with generics
// binds a Map<K, Provider<V>> from a collection of Map<Entry<K, Provider<V>>
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// wrapping a T in a Set safely returns a Set<T>
/**
/**
/* the target injector's binder. non-null until initialization, null afterwards */
/* a provider for each element in the set. null until initialization, non-null afterwards */
/**
// protected by findBindingsByType()
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// This is specified in java.lang.Annotation.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// use enumeration to include the default properties
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// javac says it's an error to cast ProviderBinding<? extends T> to Binding<? extends T>
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
// it is safe to use the type literal for the raw type
// NOTE: This is not in the original guice. We rethrow here to expose any explicit errors in configure()
// if a source is specified explicitly, we don't need to skip sources
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// This method is necessary to create a Dependency<T> with proper generic type information
/**
/**
/**
/**
// If no annotated constructor is found, look for a no-arg constructor instead.
// Disallow private constructors on non-private classes (unless they have @Inject)
/**
/**
/**
/**
// TODO (crazybob): Filter out overridden members.
/**
// don't warn about misplaced binding annotations on methods when there's a field with the same
// name. In Scala, fields always get accessor methods (that we need to ignore). See bug 242.
// Add injectors for superclass first.
// Add injectors for all members next
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// NOTE: this class is not part of guice and was added so the provider lookup's key can be accessible for tests
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
// execute the overrides module, keeping track of which keys and scopes are bound
// execute the original module, skipping all scopes and overridden keys. We only skip each
// overridden binding once so things still blow up if the module binds the same thing
// multiple times.
// Record when a scope instance is used in a binding
// we're not skipping deep exposes, but that should be okay. If we ever need to, we
// have to search through this set of elements for PrivateElements, recursively
// execute the scope bindings, skipping scopes that have been overridden. Any scope that
// is overridden and in active use will prompt an error
// TODO: bind the overridden keys using multibinder
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
// for other custom collections types, use newParameterizedType()
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// duplicate the buffer in order to be able to change the limit
// make sure we update byteBuffer to indicate how far we came..
/**
/**
/**
// duplicate the buffer in order to be able to change the limit
// make sure we update byteBuffer to indicate how far we came..
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// only static methods
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//---------------------------------------------------------------------
// Copy methods for java.io.InputStream / java.io.OutputStream
//---------------------------------------------------------------------
/**
// Leverage try-with-resources to close in and out so that exceptions in close() are either propagated or added as suppressed
// exceptions to the main exception
/**
//---------------------------------------------------------------------
// Copy methods for java.io.Reader / java.io.Writer
//---------------------------------------------------------------------
/**
// Leverage try-with-resources to close in and out so that exceptions in close() are either propagated or added as suppressed
// exceptions to the main exception
/**
/**
/**
// noop
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// High surrogate.
// Low surrogate.
/**
// Writes more than one byte.
// 2 bytes.
// 3 bytes.
// 4 bytes.
// 5 bytes.
// 6 bytes.
/**
/**
/**
/**
/**
/**
// Implements Reusable.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// since this impl is not recycling anyway, don't bother aligning to
// the page size, this will even save memory
/**
// nothing to copy
// illegal args: offset and/or length exceed array size
// get enough pages for new size
// bulk copy
// advance
// shrink list of pages
// go back to start
// nothing to do
// empty for now.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// nothing to do there...
// nothing to do there...
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** An entry in the registry, made up of a category class and name, and a reader for that category class. */
/** The superclass of a {@link NamedWriteable} which will be read by {@link #reader}. */
/** A name for the writeable which is unique to the {@link #categoryClass}. */
/** A reader capability of reading*/
/** Creates a new entry which can be stored by the registry. */
/**
/**
// we've seen the last of this category, put it into the big map
// handle the last category
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// re-create the releasable with the new reference
// re-create the releasable with the new reference
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// use StringAndBytes so we can cache the string if its ever converted to it
// Maximum char-count to de-serialize via the thread-local CharsRef buffer
// Reusable bytes for deserializing strings
// Thread-local buffer for smaller strings
// Larger buffer used for long strings that can't fit into the thread-local buffer
// We don't use a CharsRefBuilder since we exactly know the size of the character array up front
// this prevents calling grow for every character since we don't need this
// we don't use ArrayUtils.grow since there is no need to copy the array
// Determine the minimum amount of bytes that are left in the string
// One byte for each remaining char except for the already partially read char
// Each char has at least a single byte
// We don't have enough space left in the byte array to read as much as we'd like to so we free up as many bytes in the
// buffer by moving unused bytes that didn't make up a full char in the last iteration to the beginning of the buffer,
// if there are any
// We only have 0, 1 or 2 => no need to bother with a native call to System#arrayCopy
// As long as we at least have three bytes buffered we don't need to do any bounds checking when getting the next char since we
// read 3 bytes per char/iteration at most
// try to extract chars from remaining bytes with bounds checks for multi-byte chars
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// skip the msg - it's composed from file, other and reason
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// lets do a sanity check that if we are reading an array size that is bigger that the remaining bytes we can safely
// throw an exception instead of allocating the array based on the size. A simple corrutpted byte can make a node go OOM
// if the size is large and for perf reasons we allocate arrays ahead of time
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// zig-zag encoding cf. https://developers.google.com/protocol-buffers/docs/encoding?hl=en
/**
// make sure any possible char can fit into the buffer in any possible iteration
// we need at most 3 bytes so we flush the buffer once we have less than 3 bytes
// left before we start another iteration
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// write the joda compatibility datetime as joda datetime
// joda does not understand "Z" for utc, so we must special case
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// in this case, we have a separate parser and printer since the dataOptionalTimeParser can't print
// this sucks we should use the root local by default and not be dependent on the node
// strict date formats here, must be at least 4 digits for year and two for months and two for day
// in this case, we have a separate parser and printer since the dataOptionalTimeParser can't print
// this sucks we should use the root local by default and not be dependent on the node
// 2014/10/10
// 2014/10/10 12:12:12
// check for deprecations, but after it has parsed correctly so invalid values aren't counted as deprecated
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Note: we take a callable here for the timestamp in order to be able to figure out
// if it has been used. For instance, the request cache does not cache requests that make
// use of `now`.
// we want to go up to the next whole value, even if we are already on a rounded value
// subtract 1 millisecond to get the largest inclusive value
// We use 01/01/1970 as a base date so that things keep working with date
// fields that are filled with times without dates
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Utility methods to work with {@link Releasable}s. */
// this does the right thing with respect to add suppressed and not wrapping errors etc.
/** Release the provided {@link Releasable}s. */
/** Release the provided {@link Releasable}s. */
/** Release the provided {@link Releasable}s, ignoring exceptions. */
/** Release the provided {@link Releasable}s, ignoring exceptions. */
/** Release the provided {@link Releasable}s, ignoring exceptions if <code>success</code> is {@code false}. */
/** Release the provided {@link Releasable}s, ignoring exceptions if <code>success</code> is {@code false}. */
/** Wrap several releasables into a single one. This is typically useful for use with try-with-resources: for example let's assume
// do something
// the resources will be released when reaching here
/** @see #wrap(Iterable) */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// add returning false means it _did_ have it already
/**
// remove returning false means it did not have it already
/**
/**
// LRU set of keys used to determine if a deprecation message should be emitted to the deprecation logs
/**
/*
/**
// warn code
// warn agent
// warn agent
// warn agent
// quoted warning value, captured
// quoted RFC 1123 date format
// opening quote
// weekday
// 2-digit day
// month
// 4-digit year
// (two-digit hour):(two-digit minute):(two-digit second)
// GMT
// closing quote (optional, since an older version can still send a warn-date)
/**
/*
/**
/**
// ignored; it should be removed shortly
/**
/**
// Assume that the common scenario won't have a string to escape and encode.
/**
/**
/*
// we have to skip '%' which is 0x25 so that it is percent-encoded too
/**
// first check if the string needs any encoding; this is the fast path and we want to avoid creating a string builder and copying
/*
// append directly and move to the next character
// noinspection ForLoopReplaceableByForEach
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// overridden fields are expected to be present in a log message
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//taken from super.asJson without the wrapping '{' '}'
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/**
/**
// we initialize the status logger immediately otherwise Log4j will complain when we try to get the context
/**
// we are about to configure logging, check that the status logger did not log any error-level messages
// whether or not the error listener check failed we can remove the listener now
/**
/**
// we initialize the status logger immediately otherwise Log4j will complain when we try to get the context
/*
// Hack the new pattern into place
// Null is weird here but we can't do anything with it so ignore it
// Tests don't need to be changed
/*
// end hack
/**
// do not set a log level for a logger named level (from the default log setting)
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no more variables
// this is a simple string
// add the tail string which contains no variables and return
// the result.
// DELIM_START was escaped, thus should not be incremented
// The escape character preceding the delimiter start is
// itself escaped: "abc x:\\{}"
// we have to consume one backward slash
// normal case
// append the characters following the last {} pair.
// check for primitive array types because they
// unfortunately cannot be cast to Object[]
// allow repeats in siblings
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// we have to descend the hierarchy
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// nodeId/clusterUuid not received yet, not appending
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/**
/**
/**
/**
// markers is not thread-safe, so we synchronize access
/*
// noinspection RedundantStringConstructorCall
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** An InfoStream (for Lucene's IndexWriter) that redirects
// TP is a special "test point" component; we don't want
// to log it:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Using commit.getSegmentsFileName() does NOT work here, have to
// manually create the segment filename
/**
/**
/*
// remove all segment_N files except of the one we wanna keep
// do nothing and close this will kick off IndexFileDeleter which will remove all pending files
/**
/**
// remove all segment_N files
// no merges
// no commits
// force creation - don't append...
// do nothing and close this will kick of IndexFileDeleter which will remove all pending files
/**
// the scorer API should be more efficient at stopping after the first
// match than the bulk scorer API
// LUCENE 4 UPGRADE: We might want to maintain our own ordinal, instead of Lucene's ordinal
// for geo sorting, we replace the SortField with a SortField that assumes a double field.
// this works since the SortField is only used for merging top docs
// for multi-valued sort field, we replace the SortedSetSortField with a simple SortField.
// It works because the sort field is only used to merge results from different shards.
// for multi-valued sort field, we replace the SortedSetSortField with a simple SortField.
// It works because the sort field is only used to merge results from different shards.
/**
// one more try after all retries
/**
/**
/**
// hard fail - we can't get a SegmentReader
// pass to default
/**
/**
// Since we want bits, we need random-access
// this never returns null
// we cache whether it matched because it is illegal to call
// twoPhase.matches() twice
/**
// early termination is possible if fields1 is a prefix of fields2
/**
// Modifying liveDocs
// Once soft-deletes is enabled, we no longer hard-update or hard-delete documents directly.
// Two scenarios that we have hard-deletes: (1) from old segments where soft-deletes was disabled,
// (2) when IndexWriter hits non-aborted exceptions. These two cases, IW flushes SegmentInfos
// before exposing the hard-deletes, thus we can use the hard-delete count of SegmentInfos.
// Modifying liveDocs
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Do this check before entering the synchronized block in order to
// avoid taking the mutex if possible (which should happen most of
// the time).
// Only add the core key to the map as a last operation so that
// if another thread sees that the core key is already in the
// map (like the check just before this synchronized block),
// then it means that the closed listener has already been
// registered.
/**
/**
// we have to copy otherwise we risk ConcurrentModificationException
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// safe to delegate since this reader does not alter the index
/**
/**
/**
// We need to use FilterDirectoryReader#getDelegate and not FilterDirectoryReader#unwrap, because
// If there are multiple levels of filtered leaf readers then with the unwrap() method it immediately
// returns the most inner leaf reader and thus skipping of over any other filtered leaf reader that
// may be instance of ElasticsearchLeafReader. This can cause us to miss the shardId.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// We need to use FilterLeafReader#getDelegate and not FilterLeafReader#unwrap, because
// If there are multiple levels of filtered leaf readers then with the unwrap() method it immediately
// returns the most inner leaf reader and thus skipping of over any other filtered leaf reader that
// may be instance of ElasticsearchLeafReader. This can cause us to miss the shardId.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// fully filtered, none matching, no need to iterate on this
// we want to force apply deleted docs
// 2 choices for performing same heavy loop - one attempts to calculate totalTermFreq and other does not
// docsEnum.freq() returns 1 if doc indexed with IndexOptions.DOCS_ONLY so no way of knowing if value
// is really 1 or unrecorded when filtering like this
// docsEnum.freq() behaviour is undefined if docsEnumFlag==PostingsEnum.FLAG_NONE so don't bother with call
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//Check cache
// already seen, initialize instance data with the cached frequencies
//Cache miss - gather stats
//Cache the result - found or not. 
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//LUCENE 4 UPGRADE this mapps the 3.6 behavior (only use the first field)
// handle like text
// only use the first field to be consistent
// handle like fields
//LUCENE 4 UPGRADE we need TFIDF similarity here so I only set it if it is an instance of it
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// no prefix and the phrase query is empty
// if the terms does not exist we could return a MatchNoDocsQuery but this would break the unified highlighter
// which rewrites query with an empty reader.
// SlowCompositeReaderWrapper could be used... but this would merge all terms from each segment into one terms
// instance, which is very expensive. Therefore I think it is better to iterate over each leaf individually.
/**
/**
// Breakout calculation of the termArrays hashcode
// Breakout calculation of the termArrays equals
/*
//www.apache.org/licenses/LICENSE-2.0
/** Return a query that matches no document. */
/**
/** Return a query that matches all documents but those that match the given query. */
/**
/* we have conditional spec(s) */
/* otherwise, simple expression */
/* percentage - assume the % was the last char.  If not, let Integer.parseInt fail. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// field does not exist
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: Remove this class when https://github.com/elastic/elasticsearch/issues/32981 is addressed.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// orig source of doc you want to find similarities to
// now the usual iteration thru 'hits' - the only thing to watch for is to make sure
//you ignore the doc if it matches your 'target' document, as it should be similar to itself
//    static {
//        assert Version.CURRENT.luceneVersion == org.apache.lucene.util.Version.LUCENE_4_9:
//                   "Remove this class once we upgrade to Lucene 5.0";
//    }
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// = new ClassicSimilarity();
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// gather list of valid fields from lucene
/**
/**
/**
// get all field names
// term selection is per field, then appended to a single boolean query
/**
/**
/**
/**
// have collected all words in doc and their freqs
// will order words by score
// for every word
// term freq in the source doc
// filter out words that don't occur enough times in the source
// go through all the fields and find the largest document frequency
// filter out words that don't occur in enough docs
// filter out words that occur in too many docs
// index update problem?
// there is still space in the queue
// update the smallest in the queue in place and update the queue.
/**
/**
// field does not store term vector info
/**
/**
// increment frequency
/**
// for every token
// increment frequency
/**
/**
/**
/**
// have to be careful, retrieveTerms returns all words but that's probably not useful to our caller...
// we just want to return the top words
// the 1st entry is the interesting word
/**
// have to be careful, retrieveTerms returns all words but that's probably not useful to our caller...
// we just want to return the top words
// the 1st entry is the interesting word
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// Highlighters must visit the child query to extract terms
// First: Gather explanations for all functions/filters
// it is a little weird to add a match although no function matches but that is the way function_score behaves right now
// If minScore is not null, then matches depend on statistics of the
// top-level reader.
// Even if the weight is created with needsScores=false, it might
// be costly to call score(), so we explicitly check if scores
// are needed
/*
// Avg / Total
// TODO: what would be a good upper bound?
/*
//www.apache.org/licenses/LICENSE-2.0
/** Per-leaf {@link ScoreFunction}. */
/*
//www.apache.org/licenses/LICENSE-2.0
/** A {@link Scorer} that filters out documents that have a score that is
// we need to check the two-phase iterator first
// otherwise calling score() is illegal
// random constant for the score computation
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// field has no value
// only use the lower 24 bits to construct a float from 0.0-1.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// info about params already included in sScript
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// subquery should have already matched above
// no explanation provided by user; give a simple one
// If minScore is not null, then matches depend on statistics of the top-level reader.
// Highlighters must visit the child query to extract terms
// TODO: what would be a good upper bound?
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Utility class to do efficient primary-key (only 1 doc contains the
// TODO: do we really need to store all this stuff? some if it might not speed up anything.
// we keep it around for now, to reduce the amount of e.g. hash lookups by field and stuff
/** terms enum for uid field */
/** Reused for iteration (when the term exists) */
/** used for assertions to make sure class usage meets assumptions */
/**
// If a segment contains only no-ops, it does not have _uid but has both _soft_deletes and _tombstone fields.
// this is a special case when we pruned away all IDs in a segment since all docs are deleted.
/** Return null if id is not found.
/**
// termsEnum can possibly be null here if this leaf contains only no-ops.
// there may be more than one matching docID, in the case of nested docs, so we want the last one:
/** Return null if id is not found. */
/*
//www.apache.org/licenses/LICENSE-2.0
/** used to indicate the write operation should succeed regardless of current version **/
/** indicates that the current document was not found in lucene and in the version map */
// -2 was used for docs that can be found in the index but do not have a version
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Utility class to resolve the Lucene doc ID, version, seqNo and primaryTerms for a given uid. */
// Evict this reader from lookupStates once it's closed:
// We cache on the top level
// This means cache entries have a shorter lifetime, maybe as low as 1s with the
// default refresh interval and a steady indexing rate, but on the other hand it
// proved to be cheaper than having to perform a CHM and a TL get for every segment.
// See https://github.com/elastic/elasticsearch/pull/19856.
// First time we are seeing this reader's core; make a new CTL:
// Our CTL won, we must remove it when the reader is closed:
// Another thread beat us to it: just use their CTL:
/** Wraps an {@link LeafReaderContext}, a doc ID <b>relative to the context doc base</b> and a version. */
/** Wraps an {@link LeafReaderContext}, a doc ID <b>relative to the context doc base</b> and a seqNo. */
/**
// iterate backwards to optimize for the frequently updated documents
// which are likely to be in the last segments
/**
// iterate backwards to optimize for the frequently updated documents
// which are likely to be in the last segments
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// do not try to parse IPv4-mapped IPv6 address
// validation
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// The CloseableChannel#close method does not throw IOException, so this should not occur.
// Ignore as we are only interested in waiting for the close process to complete. Logging
// close exceptions happens elsewhere.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** log interface configuration at debug level, if its enabled */
/** perform actual logging: might throw exception if things go wrong */
// ordinary name
// display name (e.g. on windows)
// addresses: v4 first, then v6
// hardware address
// attributes
/** format internet address: java's default doesn't include everything useful */
/** format network interface flags */
/*
//www.apache.org/licenses/LICENSE-2.0
// Make a first pass to categorize the characters in this string.
// Colons must not appear after dots.
// Everything else must be a decimal or hex digit.
// Now decide which address family to parse.
// Note: we already verified that this string contains only hex digits.
// Disallow leading zeroes, because no clear standard exists on
// whether these should be interpreted as decimal or octal.
// An address can have [2..8] colons, and N colons make N+1 parts.
// Disregarding the endpoints, find "::" with nothing in between.
// This indicates that a run of zeroes has been skipped.
// Can't have more than one ::
// Number of parts to copy from above/before the "::"
// Number of parts to copy from below/after the "::"
// If we found a "::", then check if it also covers the endpoints.
// ^: requires ^::
// :$ requires ::$
// Otherwise, allocate the entire address to partsHi.  The endpoints
// could still be empty, but parseHextet() will check for that.
// If we found a ::, then we must have skipped at least one part.
// Otherwise, we must have exactly the right number of parts.
// Now parse the hextets into a byte array.
// Note: we already verified that this string contains only hex digits.
/**
//tools.ietf.org/html/rfc3986#section-3.2.2"
//tools.ietf.org/html/rfc3986</a>,
//[2001:db8::1]:8888/index.html"}.
/**
//tools.ietf.org/html/rfc5952">RFC 5952</a>
// For IPv4, Java's formatting is good enough.
/**
/**
/*
/**
// The argument was malformed, i.e. not an IP string literal.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** No instantiation */
/**
/**
// note, we don't validate port, because we only allow InetSocketAddress
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** Adds a transport implementation that can be selected by setting {@link #TRANSPORT_TYPE_KEY}. */
/** Adds an http transport implementation that can be selected by setting {@link #HTTP_TYPE_KEY}. */
// TODO: consider moving this to the ClusterModule
// this lives here instead of the more aptly named ClusterModule because it used to be used by the Transport client
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** By default, we bind to loopback interfaces */
/**
/**
/**
/**
// we know it's not here. get the defaults
// try to deal with some (mis)configuration
// check if its multicast: flat out mistake
// check if its a wildcard address: this is only ok if its the only address!
/**
// TODO: needs to be InetAddress[]
// we know it's not here. get the defaults
// TODO: allow publishing multiple addresses
// for now... the hack begins
// 1. single wildcard address, probably set by network.host: expand to all interface addresses.
// 2. try to deal with some (mis)configuration
// check if its multicast: flat out mistake
// check if its a wildcard address: this is only ok if its the only address!
// (if it was a single wildcard address, it was replaced by step 1 above)
// 3. if we end out with multiple publish addresses, select by preference.
// don't warn the user, or they will get confused by bind_host vs publish_host etc.
/** resolves (and deduplicates) host specification */
// deduplicate, in case of resolver misconfiguration
// stuff like https://bugzilla.redhat.com/show_bug.cgi?id=496300
/** resolves a single host specification */
// next check any registered custom resolvers if any
/* an interface specification */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** no instantiation */
/**
/**
// be optimistic, you misconfigure, then you get noise to your screen
/** Sorts an address by preference. This way code like publishing can just pick the first one */
/** 
// only public because of silly multicast
/** Return all interfaces (and subinterfaces) on the system */
/** Helper for getInterfaces, recursively adds subinterfaces to {@code target} */
/** Returns system default for SO_REUSEADDR */
/** Returns all interface-local scope (loopback) addresses for interfaces that are up. */
/** Returns all site-local scope (private) addresses for interfaces that are up. */
/** Returns all global scope addresses for interfaces that are up. */
/** Returns all addresses (any scope) for interfaces that are up. 
/** Returns addresses for the given interface (it must be marked up) */
/** Returns only the IPV4 addresses in {@code addresses} */
/** Returns only the IPV6 addresses in {@code addresses} */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// Supports initial delimiter.
/**
// Supports initial delimiter.
// Supports initial delimiter.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// by default we simply drop the object for GC.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we maintain size separately because concurrent deque implementations typically have linear-time size() impls
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Called before releasing an object, returns true if the object should be recycled and false otherwise. */
/** Called after a release. */
// nothing to do
/*
//www.apache.org/licenses/LICENSE-2.0
/** Get the delegate instance to forward calls to. */
/** Wrap a recycled reference. */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Create a new empty instance of the given size. */
/** Recycle the data. This operation is called when the data structure is released. */
/** Destroy the data. This operation allows the data structure to release any internal resources before GC. */
/** Reference to the value. */
/** Whether this instance has been recycled (true) or newly allocated (false). */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// don't trust Thread.hashCode to have equiprobable low bits
// make positive, otherwise % may return negative numbers
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// supported in JAVA7
/**
/** Return an {@link Automaton} that matches the given pattern. */
/**
/**
// str.endsWith(pattern.substring(1)), but avoiding the construction of pattern.substring(1):
// Double wildcard "**" - skipping the first "*"
// only wildcard in pattern is at the end, so no need to look at the rest of the string
/**
/**
// #simpleMatch(String[], String) is likely to be inlined into this method
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// pkg private for tests
/**
// ensure running this through the updater / dynamic validator
// don't check if the value has changed we wanna test this anyways
// here we are exhaustive and record all settings that failed.
/**
// nothing changed in the settings, ignore
/**
/**
/**
/**
// it would be awesome to have a generic way to do that ie. a set of settings that map to an object with a builder
// down the road this would be nice to have!
/**
/**
/**
/**
/**
/**
/**
/**
/**
// settings iterate in deterministic fashion
/**
/**
// validate the dependent setting is set
// validate the dependent setting value
// the only time that validateInternalOrPrivateIndex should be true is if this call is coming via the update settings API
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// it's not a final setting
// it's a dynamicSetting and we only do dynamic settings
// the setting is not registered AND it's been archived
// if it's not dynamic AND we have a key
/**
// it's not a final setting
// this either accepts null values that suffice the canUpdate test OR wildcard expressions (key ends with *)
// we don't validate if there is any dynamic setting with that prefix yet we could do in the future
// we don't set changed here it's set after we apply deletes below if something actually changed
// we might not have a full picture here do to a dependency validation
// we have to re-check with canRemove here since we might have a wildcard expression foo.* that matches
// dynamic as well as static settings if that is the case we might remove static settings since we resolve the
// wildcards late
// we return null here because we use a putIfAbsent call when inserting into the map, so if it exists then we already checked
// the setting to make sure there are no overlapping settings.
/**
// track if any settings were upgraded
// the setting does not have an upgrader, copy the setting
// the setting has an upgrader, so mark that we have changed a setting and apply the upgrade logic
// noinspection ConstantConditions
// we only return a new instance if there was an upgrade
/**
/*
/*
/**
// TODO this should be replaced by Setting.Property.HIDDEN or something like this.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// this is used to compute the PBKDF2 hash (the published one)
/**
// eagerly compute hashes to be published
/**
// consistency of missing
// setting missing on master but present locally
// setting missing locally but present on master
// another case of settings missing locally, when group settings have not expanded to all the keys published
// setting missing locally but present on master
/**
// eagerly compute hashes to be published
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// validate that built-in similarities don't get redefined
// this allows similarity settings to be passed
// this allows analysis settings to be passed
// we keep the shrink settings for BWC - this can be removed in 8.0
// we can't remove in 7 since this setting might be baked into an index coming in via a full cluster restart from 6.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** An identifier for the type of data that may be stored in a keystore entry. */
/** An entry in the keystore. The bytes are opaque and interpreted based on the entry type. */
/**
/** Characters that may be used in the bootstrap seed setting added to all keystores. */
/** The name of the keystore file to read and write. */
/** The version of the metadata written before the keystore data. */
/** The oldest metadata format version that can be read. */
/** The algorithm used to derive the cipher key from a password. */
/** The number of iterations to derive the cipher key. */
/**
//www.oracle.com/technetwork/java/javase/terms/readme/jdk9-readme-3852447.html#jce
/** The number of bits for the GCM tag. */
/** The cipher used to encrypt the keystore data. */
/** The mode used with the cipher algorithm. */
/** The padding used with the cipher algorithm. */
// format version changelog:
// 1: initial version, ES 5.3
// 2: file setting, ES 5.4
// 3: FIPS compliant algos, ES 6.3
// 4: remove distinction between string/files, ES 6.8/7.1
/** The metadata format version used to read the current keystore wrapper. */
/** True iff the keystore has a password needed to read. */
/** The raw bytes of the encrypted keystore. */
/** The decrypted secret data. See {@link #decrypt(char[])}. */
/**
/** Returns a path representing the ES keystore in the given config dir. */
/** Constructs a new keystore with the given password. */
/** Add the bootstrap seed setting, which may be used as a unique, secure, random value by the node */
// Generate 20 character passwords
/**
// For v2 we had a map of strings containing the types for each setting. In v3 this map is now
// part of the encrypted bytes. Unfortunately we cannot seek backwards with checksum input, so
// we cannot just read the map and find out how long it is. So instead we read the map and
// store it back using java's builtin DataOutput in a byte array, along with the actual keystore bytes
/** Upgrades the format of the keystore, if necessary. */
// add keystore.seed if necessary
/** Return true iff calling {@link #decrypt(char[])} requires a non-empty password. */
/**
// legacy, the keystore format would previously store the entry type
/** Encrypt the keystore entries and return the encrypted data. */
// v1 and v2 keystores never had passwords actually used, so we always use an empty password
// first read the setting types map
// then read the actual keystore
// verify the settings metadata matches the keystore entries
// verify integrity: keys in keystore match what the metadata thinks exist
// fill in the entries now that we know all the types to expect
// The PBE keyspec gives us chars, we convert to bytes
// PBE only stores the lower 8 bits, so this narrowing is ok
/** Write the keystore to the given config directory. */
// write to tmp file first, then overwrite
// new cipher params
// use 64 bytes salt, which surpasses that recommended by OWASP
// see https://www.owasp.org/index.php/Password_Storage_Cheat_Sheet
// use 96 bits (12 bytes) for IV as recommended by NIST
// see http://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-38d.pdf section 5.2.1.1
// encrypted data
// size of data block
// don't rely on umask: ensure the keystore has minimal permissions
/**
// TODO: make settings accessible only to code that registered the setting
/**
/**
/** Set a string setting. */
/** Set a file setting. */
/** Remove the given setting from the keystore. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Recursive invocation, parsing placeholders contained in the placeholder key.
// Now obtain the value for the fully resolved key...
// Recursive invocation, parsing placeholders contained in the
// previously resolved placeholder value.
// Proceed with unprocessed value.
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** Returns the secret setting from the keyStoreReader store. */
/** Returns the value from a fallback setting. Returns null if no fallback exists. */
// TODO: override toXContent
/**
/**
/**
// this means "setting does not exist"
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Returns true iff the settings are loaded and retrievable. */
/** Returns the names of all secure settings available. */
/** Return a string setting. The {@link SecureString} should be closed once it is used. */
/** Return a file setting. The {@link InputStream} should be closed once it is used. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/** Constant time equality to avoid potential timing attacks. */
/**
/**
/**
// pass thee char[] to a external API
/**
/** Throw an exception if this string has been closed, indicating something is trying to access the data after being closed. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// we have to disable validation or we will stack overflow
/**
/**
/**
/** Logs a deprecation warning if the setting is deprecated and used. */
// They're using the setting, so we need to tell them to stop
// It would be convenient to show its replacement key, but replacement is often not so simple
/**
/**
// we use startsWith here since the key might be foo.bar.0 if it's an array
/**
/**
/**
/**
/**
/**
/**
/**
// we collect all concrete keys and then delegate to the actual setting for validation and settings extraction
// only the ones that have changed otherwise we might get too many updates
// the hasChanged above checks only if there are any changes
// we collect all concrete keys and then delegate to the actual setting for validation and settings extraction
// only the ones that have changed otherwise we might get too many updates
// the hasChanged above checks only if there are any changes
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// TODO this one's two argument get is still broken
// fromXContent doesn't use named xcontent or deprecation.
/**
/**
/**
// the last part of this regexp is to support both list and group keys
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The raw settings from the full key to raw string value. */
/** The secure settings storage associated with these settings. */
/** The first level of setting names. This is constructed lazily in {@link #names()}. */
/**
// we use a sorted map for consistent serialization when using getAsMap()
/**
// pkg private so it can only be accessed by local subclasses of SecureSetting
// It supposed to be a value, but we already have a map stored, need to convert this map to "." notation
// It supposed to be a map, but we already have a value stored, which is not a map
// fall back to "." notation
// Something went wrong. Different format?
// Bailout!
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// pull settings to exclude secure settings in size()
/**
/**
// ensure we reached the end of the stream
// just use the string representation here
/**
/** Returns the number of settings in this settings object. */
/** Returns the fully qualified setting names contained in this settings object. */
// uniquify, since for legacy reasons the same setting name may exist in both
/**
// we use a sorted map for consistent serialization when using getAsMap()
/**
/**
/** Return the current secure settings, or {@code null} if none have been set. */
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// let's only look at the head of the list and convert in order starting there.
/**
/**
// NOTE: loadFromStream will close the input stream
/**
// fromXContent doesn't use named xcontent or deprecation.
// empty file
/**
// visible for testing
// a null value obviously can't be replaced
// if the values exists and has length, we should maintain it  in the map
// otherwise, the replace process resolved into removing it
/**
/**
// TODO We could use an FST internally to make things even faster and more compact
// we cache that size since we have to iterate the entire set
// this is safe to do since this map is only used with unmodifiable maps
// protect against calling hasNext twice
// early terminate
// we didn't find anything
// protect against no #hasNext call or not respecting it
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//localhost:9200/_all/_settings?preserve_existing=true' -d '");
// by now we are fully configured, lets check node level settings for unregistered index settings
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// TODO: TextBytesOptimization we can use a buffer here to convert it? maybe add a
// request to jackson to support InputStream as well?
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// support the 6.x BWC compatible way of parsing java 8 dates
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/////////////////////////////////////////
//
// BEGIN basic time formatters
//
// these formatters to not have any splitting characters between hours, minutes, seconds, milliseconds
// this means they have to be strict with the exception of the last element
//
/////////////////////////////////////////
/*
/*
/*
/*
/*
/*
/*
/*
/*
/////////////////////////////////////////
//
// END basic time formatters
//
/////////////////////////////////////////
/////////////////////////////////////////
//
// start strict formatters
//
/////////////////////////////////////////
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// NOTE: this is not a strict formatter to retain the joda time based behaviour, even though it's named like this
/*
/*
// this one here is lenient as well to retain joda time based bwc compatibility
//  this one here is lenient as well to retain joda time based bwc compatibility
/*
/*
/*
// Note: milliseconds parsing is not strict, others are
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/////////////////////////////////////////
//
// end strict formatters
//
/////////////////////////////////////////
/////////////////////////////////////////
//
// start lenient formatters
//
/////////////////////////////////////////
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// only the formatter, nothing optional here
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/////////////////////////////////////////
//
// end lenient formatters
//
/////////////////////////////////////////
// strict date formats here, must be at least 4 digits for year and two for months and two for day
/**
// the first two cases are the most common, so this allows us to exit early when parsing dates
// missing year, falling back to the epoch and then filling
// we should not reach this piece of code, everything being parsed we should be able to
// convert to a zoned date time! If not, we have to extend the above methods
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Note: we take a callable here for the timestamp in order to be able to figure out
// if it has been used. For instance, the request cache does not cache requests that make
// use of `now`.
// exists for backcompat, do not use!
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// the id for zoneoffset is not ISO compatible, so cannot be read by ZoneId.of
// pkg private for tests
// eastern time without daylight savings
// Map of deprecated timezones and their recommended new counterpart
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/JodaOrg/joda-time/tree/v2.10.1
// see org.joda.time.chrono.BasicGJChronology
/**
// see org.joda.time.chrono.GregorianChronology.calculateFirstDayOfYearMillis
// Initial value is just temporary.
// Add 3 before shifting right since /4 and >>2 behave differently
// on negative numbers. When the expression is written as
// (year / 4) - (year / 100) + (year / 400),
// it works for both positive and negative values, except this optimization
// eliminates two divisions.
// millis per day
// see org.joda.time.chrono.BasicChronology
// Get an initial estimate of the year, and the millis value that
// represents the start of that year. Then verify estimate and fix if
// necessary.
// Initial estimate uses values divided by two to avoid overflow.
// One year may need to be added to fix estimate.
// Didn't go too far, so actually add one year.
// see org.joda.time.chrono.BasicGJChronology
// Perform a binary search to get the month. To make it go even faster,
// compare using ints instead of longs. The number of milliseconds per
// year exceeds the limit of a 32-bit int's capacity, so divide by
// 1024. No precision is lost (except time of day) since the number of
// milliseconds per day contains 1024 as a factor. After the division,
// the instant isn't measured in milliseconds, but in units of
// (128/125)seconds.
// There are 86400000 milliseconds per day, but divided by 1024 is
// 84375. There are 84375 (128/125)seconds per day.
// see org.joda.time.chrono.BasicGJChronology
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if there is already a milli of second, we need to overwrite it
// this supports seconds without any fraction
// optional is used so isSupported will be called when printing
// this supports seconds ending in dot
// this supports milliseconds without any fraction
// this supports milliseconds ending in dot
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// base fields which should be used for default parsing, when we round up for date math
// named formatters use default roundUpParser
// subclasses override roundUpParser
//this is when the RoundUp Formatter is created. In further merges (with ||) it will only append this one to a list.
/**
//bugs.openjdk.java.net/browse/JDK-8188771
// shortcurt to not create new objects unnecessarily
// shortcurt to not create new objects unnecessarily
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO only millisecond granularity here!
// subtract 1 millisecond to get the largest inclusive value
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// UGLY!, this exception messages seems to represent closed connection
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// 4 bytes (IPv4) or 16 bytes (IPv6)
// the host string was serialized so we can ignore the passed in version
// 4 bytes (IPv4) or 16 bytes (IPv6)
// 1 byte
// don't serialize scope ids over the network!!!!
// these only make sense with respect to the local machine, and will only formulate
// the address incorrectly remotely.
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// For testing
// For testing
/**
// Allow this special value to be unit-less:
// Allow this special value to be unit-less:
// Missing units:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// 'm' is a suffix of 'nmi' so it must follow 'nmi'
// since 'm' is suffix of other unit
// it must be the last entry of unit
// names ending with 'm'. otherwise
// parsing would fail
/**
/**
/**
/**
/**
/** 
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// check if it is one of the "AUTO" variants
// should be a float or int representing a valid edit distance, otherwise throw error
/**
// we cannot serialize the low/high bounds since the other node does not know about them.
// This is a best-effort to not fail queries in case the cluster is being upgraded and users
// start using features that are not available on all nodes.
//AUTO
// 5 avg term length in english
/*
//www.apache.org/licenses/LICENSE-2.0
/** Utility methods to get memory sizes. */
/** Parse the provided string as a memory size. This method either accepts absolute values such as
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Common implementation for array lists that slice data into fixed-size blocks. */
/** Given the size of the array, estimate the number of bytes it will use. */
// rough approximate, we only take into account the size of the values, not the overhead of the array objects
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// IDs are internally stored as id + 1 so that 0 encodes for an empty slot
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Open addressing typically requires having smaller load factors compared to linked lists because
// collisions may result into worse lookup performance.
// Don't use the value directly. Under some cases eg dates, it could be that the low bits don't carry much value and we would like
// all bits of the hash to carry as much value
// next power of two
/**
/**
// linear probing
/** Resize to the given capacity. */
/** Remove the entry at the given index and add it back */
// The difference of this implementation of grow() compared to standard hash tables is that we are growing in-place, which makes
// the re-mapping of keys to slots a bit more tricky.
// Resize arrays
// power of 2
// First let's remap in-place: most data will be put in its final position directly
// The only entries which have not been put in their final position in the previous loop are those that were stored in a slot that
// is < slot(key, mask). This only happens when slot(key, mask) returned a slot that was close to the end of the array and collision
// resolution has put it back in the first slots. This time, collision resolution will have put them at the beginning of the newly
// allocated slots. Let's re-add them to make sure they are in the right slot. This 2nd loop will typically exit very early.
// add it back
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// first index of a value that is > value
// last index of a value that is < value
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Base abstraction of an array. */
/** Return the length of this array. */
/*
//www.apache.org/licenses/LICENSE-2.0
/** Utility class to work with arrays. */
/** Returns the next size to grow when working with parallel arrays that
/** Return the next size to grow to that is &gt;= <code>minTargetSize</code>.
// round to a multiple of pageSize
// Checking the breaker is disabled if not specified
/**
// checking breaker means potentially tripping, but it doesn't
// have to if the delta is negative
// since we've already created the data, we need to
// add it so closing the stream re-adjusts properly
// re-throw the original exception
// even if we are not checking the breaker, we need to adjust
// its' totals, so add without breaking
/**
/**
// when allocating big arrays, we want to first ensure we have the capacity by
// checking with the circuit breaker before attempting to allocate
/**
/** Resize the array to the exact provided size. */
/** Grow an array to a size that is larger than <code>minSize</code>,
/** @see Arrays#hashCode(byte[]) */
/** @see Arrays#equals(byte[], byte[]) */
/**
// when allocating big arrays, we want to first ensure we have the capacity by
// checking with the circuit breaker before attempting to allocate
/**
/** Resize the array to the exact provided size. */
/** Grow an array to a size that is larger than <code>minSize</code>,
/**
// when allocating big arrays, we want to first ensure we have the capacity by
// checking with the circuit breaker before attempting to allocate
/**
/** Resize the array to the exact provided size. */
/** Grow an array to a size that is larger than <code>minSize</code>,
/**
// when allocating big arrays, we want to first ensure we have the capacity by
// checking with the circuit breaker before attempting to allocate
/** Allocate a new {@link DoubleArray} of the given capacity. */
/** Resize the array to the exact provided size. */
/** Grow an array to a size that is larger than <code>minSize</code>,
/**
// when allocating big arrays, we want to first ensure we have the capacity by
// checking with the circuit breaker before attempting to allocate
/** Allocate a new {@link FloatArray} of the given capacity. */
/** Resize the array to the exact provided size. */
/** Grow an array to a size that is larger than <code>minSize</code>,
/**
// when allocating big arrays, we want to first ensure we have the capacity by
// checking with the circuit breaker before attempting to allocate
/** Resize the array to the exact provided size. */
/** Grow an array to a size that is larger than <code>minSize</code>,
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Constructor. */
/** Change the size of this array. Content between indexes <code>0</code> and <code>min(size(), newSize)</code> will be preserved. */
/** Estimates the number of bytes that would be consumed by an array of the given size. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Constructor. */
/** Change the size of this array. Content between indexes <code>0</code> and <code>min(size(), newSize)</code> will be preserved. */
/** Estimates the number of bytes that would be consumed by an array of the given size. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Constructor. */
/** Change the size of this array. Content between indexes <code>0</code> and <code>min(size(), newSize)</code> will be preserved. */
/** Estimates the number of bytes that would be consumed by an array of the given size. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Constructor. */
/** Change the size of this array. Content between indexes <code>0</code> and <code>min(size(), newSize)</code> will be preserved. */
/** Estimates the number of bytes that would be consumed by an array of the given size. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Constructor. */
/** Change the size of this array. Content between indexes <code>0</code> and <code>min(size(), newSize)</code> will be preserved. */
// empty range
/** Estimates the number of bytes that would be consumed by an array of the given size. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Constructor. */
/** Change the size of this array. Content between indexes <code>0</code> and <code>min(size(), newSize)</code> will be preserved. */
/** Estimates the number of bytes that would be consumed by an array of the given size. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we cache hashes for faster re-hashing
// Constructor with configurable capacity and default maximum load factor.
//Constructor with configurable capacity and load factor.
// BytesRef has a weak hashCode function so we try to improve it by rehashing using Murmur3
// Feel free to remove rehashing if BytesRef gets a better hash function
/**
/**
/** Sugar for {@link #find(BytesRef, int) find(key, key.hashCode()} */
// means unset
// means unset
/**
/** Sugar to {@link #add(BytesRef, int) add(key, key.hashCode()}. */
/*
//www.apache.org/licenses/LICENSE-2.0
/** Utility methods to do byte-level encoding. These methods are biased towards little-endian byte order because it is the most
/** Zig-zag decode. */
/** Zig-zag encode: this helps transforming small signed numbers into small positive numbers. */
/** Write a long in little-endian format. */
/** Write a long in little-endian format. */
/** Write an int in little-endian format. */
/** Read an int in little-endian format. */
/** Write a double in little-endian format. */
/** Read a double in little-endian format. */
/** Write a float in little-endian format. */
/** Read a float in little-endian format. */
/** Same as DataOutput#writeVLong but accepts negative values (written on 9 bytes). */
/** Same as DataOutput#readVLong but can read negative values (read on 9 bytes). */
// unwinded because of hotspot bugs, see Lucene's impl
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// needs to be volatile as it is also read outside of synchronized blocks.
// fallback to the default exception
// capture and clean the interrupted thread before we start, so we can identify
// our own interrupt. we do so under lock so we know we don't clear our own.
/**
/**
// ignore, this interrupt has been triggered by us in #cancel()...
// we can only reach here if assertions are disabled. If we reach this code and cancelled is false, this means that we've
// been interrupted externally (which we don't support).
// we are now out of threads collection so we can't be interrupted any more by this class
// restore old flag and see if we need to fail
// clear the flag interrupted flag as we are checking for failure..
// if we're not canceling, we throw the original exception
// if we're not canceling, we throw the original exception
// restore interrupt flag to at least adhere to expected behavior
/** cancel all current running operations. Future calls to {@link #checkForCancel()} will be failed with the given reason */
// we were already cancelled, make sure we don't interrupt threads twice
// this is important in order to make sure that we don't mark
// Thread.interrupted without handling it
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Collections-related utility methods. */
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: This rate limiter has some concurrency issues between the two maybePause operations
// Time to pause
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf
/**
// puts the bits at the right side of the mask, e.g. `0000000000001111` for bitsPerEntry = 4
/**
// This shouldn't happen, but as a sanity check
// TODO this is probably super slow, but just used for testing atm
/**
/**
/**
/**
/**
/**
/**
// check all entries for both buckets and the evicted slot
/**
/**
// Each bucket needs 32 bits, so we truncate for the first bucket and shift/truncate for second
/**
// If we already have an evicted fingerprint we are full, no need to try
// overwrite our alternate bucket, and a random entry
// replace details and start again
// Only try to insert into alternate bucket
// If we get this far, we failed to insert the value after MAX_EVICTION rounds,
// so cache the last evicted value (so we don't lose it) and signal we failed
/**
// TODO implement semi-sorting
// Already have the fingerprint, no need to save
/**
/**
/*
//github.com/efficient/cuckoofilter/blob/master/src/cuckoofilter.h#L78
// NOTE(binfan): originally we use:
// index ^ HashUtil::BobHash((const void*) (&tag), 4)) & table_->INDEXMASK;
// now doing a quick-n-dirty way:
// 0x5bd1e995 is the hash constant from MurmurHash2
/**
/**
// we use 0 as "empty" so if the hash actually hashes to zero... return 1
// Some other impls will re-hash with a salt but this seems simpler
/**
/**
/*
/**
/*
/**
// Rounds up to nearest power of 2
// (numBuckets, bitsPerEntry, fingerprintMask, entriesPerBucket, count, evictedFingerprint) * 4b == 24b
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Read volatile just once...
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// the locale uses - as a separator, as expected
// lang, country, variant
// lang, country
// lang
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// IDs are internally stored as id + 1 so that 0 encodes for an empty slot
// Constructor with configurable capacity and default maximum load factor.
//Constructor with configurable capacity and load factor.
/**
/**
// means unset
// means unset
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// repair and continue
// slot was free
// we just updated the value
// not the right key, repair and continue
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** A recycler of fixed-size pages. */
// object pages are less useful to us so we give them a lower weight by default
/** Page size in bytes: 16KB */
// We have a global amount of memory that we need to divide across data types.
// Since some types are more useful than other ones we give them different weights.
// Trying to store all of them in a single stack would be problematic because eg.
// a work load could fill the recycler with only byte[] pages and then another
// workload that would work with double[] pages couldn't recycle them because there
// is no space left in the stack/queue. LRU/LFU policies are not an option either
// because they would make obtain/release too costly: we really need constant-time
// operations.
// Ultimately a better solution would be to only store one kind of data and have the
// ability to interpret it either as a source of bytes, doubles, longs, etc. eg. thanks
// to direct ByteBuffers or sun.misc.Unsafe on a byte[] but this would have other issues
// that would need to be addressed such as garbage collection of native memory or safety
// of Unsafe writes.
// nothing to do
// nothing to do
// nothing to do
// we need to remove the strong refs on the objects stored in the array
// object pages are cleared on release anyway
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
//noop
// cached here for performance reasons
// True if we are tracking inserts with a set, false otherwise
/**
// We have to ensure that, in the worst case, two full sets can be converted into
// one cuckoo filter without overflowing.  This keeps merging logic simpler
/**
/**
/**
/**
// We calculate these once up front for all the filters and use the expert API
/**
/**
/**
// filter is full, create a new one and insert there
// make sure we account for the new filter
/**
// Cache the chosen numBuckets for later use
// this zeros out the overhead of the set
// this adds back in the new overhead of the cuckoo filters
/**
// fpp (double), threshold (int), isSetMode (boolean)
/**
// Some basic sanity checks to make sure we can merge
// Both in sets, merge collections then see if we need to convert to cuckoo
// Other is in cuckoo mode, so we convert our set to a cuckoo, then
// call the merge function again.  Since both are now in set-mode
// this will fall through to the last conditional and do a cuckoo-cuckoo merge
// Rather than converting the other to a cuckoo first, we can just
// replay the values directly into our filter.
// Both are in cuckoo mode, merge raw fingerprints
// The iterator returns an array of longs corresponding to the
// fingerprints for buckets at the current position
// We check to see if the fingerprint is present in any of the existing filters
// (in the same bucket/alternate bucket), or if the fingerprint is empty.  In these cases
// we can skip the fingerprint
// Try to insert into the last filter in our list
// if we failed, the filter is now saturated and we need to create a new one
// make sure we account for the new filter
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// check again!
/** Return the potentially stale cached entry. */
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//*.local/some_path/*?*#* will match all uris with schema foobar in local domain
/*.local/some_path/*?*#* will match all uris with schema foobar in local domain
/**
/**
// This url only has scheme, scheme-specific part and fragment
// If the pattern is empty or matches anything - it's a match
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Currently running counts as scheduled to avoid an oscillating return value
// from this method when a task is repeatedly running and rescheduling itself.
/**
/**
// prevent the annoying fact of logging the same stuff all the time with an interval of 1 sec will spam all your logs
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// prevent execution if the service is stopped
/**
/**
/**
// nothing by default
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// nothing by default
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// the algorithm here tires to reduce the load on each individual caller.
// we try to have only one caller that processes pending items to disc while others just add to the queue but
// at the same time never overload the node by pushing too many items into the queue.
// we first try make a promise that we are responsible for the processing
// in this case we are not responsible and can just block until there is space
// here we have to try to make the promise again otherwise there is a race when a thread puts an entry without making the promise
// while we are draining that mean we might exit below too early in the while loop if the drainAndSync call is fast.
// we are responsible for processing we don't need to add the tuple to the queue we can just add it to the candidates
// no need to preserve context for listener since it runs in current thread.
// since we made the promise to process we gotta do it here at least once
// yet if the queue is not empty AND nobody else has yet made the promise to take over we continue processing
// if this fails we are in deep shit - fail the request
// this exception is passed to all listeners - we don't retry. if this doesn't work we are in deep shit
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// read first, lighter, and most times it will be null...
// read first, lighter, and most times it will be null...
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/**
/*
/**
// protected so that it can be overridden in specific instances
/**
/**
/**
// If it's an Error, we want to make sure it reaches the top of the
// call stack, so we rethrow it.
// we want to notify the listeners we have with errors as well, as it breaks
// how we work in ES in terms of using assertions
//        if (throwable instanceof Error) {
//            throw (Error) throwable;
//        }
/**
/* Valid states. */
/*
/*
/**
// Attempt to acquire the shared lock with a timeout.
/**
// Acquire the shared lock allowing interruption.
/**
/**
/**
/**
/**
/**
/**
// If this thread successfully transitioned to COMPLETING, set the value
// and exception and then release to the final state.
// If some other thread is currently completing the future, block until
// they are done so we can guarantee completion.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// MAP DELEGATION
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
// throw this error where it will propagate to the uncaught exception handler
// restore the interrupt status
/**
// TODO this should only be allowed in tests
// TODO missing node names should only be allowed in tests
/**
// first try to transfer to a waiting worker thread
// check if there might be spare capacity in the thread
// pool executor
// reject queuing the task to force the thread pool
// executor to add a worker if it can; combined
// with ForceQueuePolicy, this causes the thread
// pool to always scale up to max pool size and we
// only queue when there is no spare capacity
/**
// force queue policy should only be used with a scaling queue
// a scaling queue never blocks so a put to it can never be interrupted
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// If we are an abstract runnable we can handle the rejection
// directly and don't need to rethrow it.
/**
/*
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// this method is a forbidden API since it interrupts threads
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// ok we got it - make sure we increment it accordingly otherwise release it again
// we have to do this in a loop here since even if the count is > 0
// there could be a concurrent blocking acquire that changes the count and then this CAS fails. Since we already got
// the lock we should retry and see if we can still get it or if the count is 0. If that is the case and we give up.
// make sure we unlock and don't leave the lock in a locked state
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// run the callback directly, we don't hold the lock and don't need to fork!
// check done under lock since it could have been modified and protect modifications
// to the list under lock
// run the callback directly, we don't hold the lock and don't need to fork!
// release references to any listeners as we no longer need them and will live
// much longer than the listeners in most cases
// call get in a non-blocking fashion as we could be on a network thread
// or another thread like the scheduler, which we should never block!
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** innerRunnable can be null if task is finished but not removed from executor yet,
// We really shouldn't be here. The only way we can get here if somebody created PrioritizedFutureTask
// and passed it to execute, which doesn't make much sense
// it might be a callable wrapper...
// these two variables are protected by 'this'
// make the task as stared. This is needed for synchronization with the timeout handling
// see  #scheduleTimeout()
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// package visible for testing
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// This is a random starting point alpha. TODO: revisit this with actual testing and/or make it configurable
// The amount the queue size is adjusted by for each calcuation
/**
// There is no set execution time, instead we adjust the time window based on the
// number of completed tasks, so there is no background thread required to update the
// queue size at a regular interval. This means we need to calculate our λ by the
// total runtime, rather than a fixed interval.
// λ = total tasks divided by measurement time
/**
// L = λ * W
/**
/**
// A task has been completed, it has left the building. We should now be able to get the
// total time as a combination of the time in the queue and time spent running the task. We
// only want runnables that did not throw errors though, because they could be fast-failures
// that throw off our timings, so only check when t is null.
// taskExecutionNanos may be -1 if the task threw an exception
// Reset the start time for all tasks. At first glance this appears to need to be
// volatile, since we are reading from a different thread when it is set, but it
// is protected by the taskCount memory barrier.
// See: https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/package-summary.html
// Calculate the new desired queue size
// Adjust the queue size towards the desired capacity using an adjust of
// QUEUE_ADJUSTMENT_AMOUNT (either up or down), keeping in mind the min and max
// values the queue size can have.
// There was an integer overflow, so just log about it, rather than adjust the queue size
// Finally, decrement the task count and time back to their starting values. We
// do this at the end so there is no concurrent adjustments happening. We also
// decrement them instead of resetting them back to zero, as resetting them back
// to zero causes operations that came in during the adjustment to be uncounted
// Start over, because we can potentially reach a "never adjusting" state,
//
// consider the following:
// - If the frame window is 10, and there are 10 tasks, then an adjustment will begin. (taskCount == 10)
// - Prior to the adjustment being done, 15 more tasks come in, the taskCount is now 25
// - Adjustment happens and we decrement the tasks by 10, taskCount is now 15
// - Since taskCount will now be incremented forever, it will never be 10 again,
//   so there will be no further adjustments
// Do a regular adjustment
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// a per-thread count indicating how many times the thread has entered the lock; only works if assertions are enabled
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Resize the limit for the queue, returning the new size limit */
// Yahtzee!
// adjust up
// adjust down
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// note, we can't call #remove on the iterator because we need to know
// if it was removed or not
/**
// note, not used in ThreadPoolExecutor
// note, not used in ThreadPoolExecutor
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// current context is stashed and replaced with a default context
// read headers into current context
// inherits context
// previous context is restored on StoredContext#close()
/**
/**
/**
/**
// If the node and thus the threadLocal get closed while this task
// is still executing, we don't want this runnable to fail with an
// uncaught exception
/**
/**
/**
/**
// execute with the parents context and restore the threads context afterwards
/**
/**
// use a linked hash set to preserve order
/**
/**
/**
/**
/**
/**
/**
/**
// (T)object
/**
/**
/**
/**
/**
/**
/**
//saving current warning headers' size not to recalculate the size with every new warning header
/**
//check if we can add another warning header - if max size within limits
//if size is NOT unbounded, check its limits
// if max size has already been reached before
// preserve insertion order
//check if we can add another warning header - if max count within limits
//if count is NOT unbounded, check its limits
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// There must have been an exception thrown, the total time is unknown (-1)
/**
// There must have been an exception thrown, the total time is unknown (-1)
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Flattens the two level {@code Iterable} into a single {@code Iterable}.  Note that this pre-caches the values from the outer {@code
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Singleton
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO it'd be nice to combine this with BaseRestHandler's implementation.
// sort by distance in reverse order, then parameter name for equal distances
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Fully-qualified here to reduce ambiguity around our (ES') Version class
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// It is safe to use EMPTY here because this never uses namedObject
/**
// It is safe to use EMPTY here because this never uses namedObject
/**
// It is safe to use EMPTY here because this never uses namedObject
/**
// safe to copy, change does not exist in source
// recursive merge maps
// update the field
/**
// copy it over, it does not exists in the content
// in the content and in the default, only merge compound ones (maps)
// all are in the form of [ {"key1" : {}}, {"key2" : {}} ], merge based on keys
// put the default entries after the content ones.
// if both are lists, simply combine them, first the defaults, then the content
// just make sure not to add the same value twice
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
//binary values will be parsed back and returned as base64 strings when reading from json and yaml
//binary values will be parsed back and returned as BytesArray when reading from cbor and smile
/**
// if we didn't find a delimiter we ignore the object or array for forward compatibility instead of throwing an error
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// NOTE: We cannot use Operations.minus because of the special case that
// we want all sub properties to match as soon as an object matches
/** Make matches on objects also match dots in field names.
// the exclude has no chances to match inner properties
// the object matched, so consider that the include matches every inner property
// we only care about excludes now
// leaf property
// #22557: only accept this array value if the key we are on is accepted:
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// we use a zero length array, because if we try to pre allocate we may need to remove trailing
// nulls if some nodes responded in the meanwhile
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// for bwc purposes, add settings provider even if not explicitly specified
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// lines starting with `#` are comments
/*
//www.apache.org/licenses/LICENSE-2.0
// connection timeout for probes
// handshake timeout for probes
// TODO if transportService is already connected to this address then skip the handshaking
// generated deterministically for reproducible tests
// use NotifyOnceListener to make sure the following line does not result in onFailure being called when
// the connection is closed in the onResponse handler
// success means (amongst other things) that the cluster names match
// TODO cache this result for some time? forever?
// TODO cache this result for some time?
// we opened a connection and successfully performed a low-level handshake, so we were definitely
// talking to an Elasticsearch node, but the high-level handshake failed indicating some kind of
// mismatched configurations (e.g. cluster name) that the user should address
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// the time between attempts to find all peers
// return value discarded: there are no known peers, so none can be disconnected
// trigger a check for a quorum already
// exposed to subclasses for testing
// exposed for checking invariant in o.e.c.c.Coordinator (public since this is a different package)
// exposed for checking invariant in o.e.c.c.Coordinator (public since this is a different package)
/**
/**
/**
/**
/**
// may be null if connection not yet established
// Must not hold lock here to avoid deadlock
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// create tasks to submit to the executor service; we will wait up to resolveTimeout for these tasks to complete
// ExecutorService#invokeAll guarantees that the futures are returned in the iteration order of the tasks so we can associate the
// hostname with the corresponding task by iterating together
// no point in pinging ourselves
// ignore
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if unicast hosts are not specified, fill with simple defaults on the local machine
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: move PathUtils to be package-private here instead of
// public+forbidden api!
/** location of bin/, used by plugin manager */
/** location of lib/, */
/** Path to the PID file (can be null if no PID file is configured) **/
/** Path to the temporary file directory used by the JDK */
// Should only be called directly by this class's unit tests
// this is trappy, Setting#get(Settings) will get a fallback setting yet return false for Settings#exists(Settings)
/**
/**
/**
/**
/**
/**
// only local file urls are supported
// Couldn't resolve against known repo locations
// Normalize URL
// It's not file or jar url and it didn't match the white list - reject
// cannot make sense of this file url
// TODO: rename all these "file" methods to "dir"
/**
/**
/** Path to the default temp directory used by the JDK */
/** Ensure the configured temp directory is a valid directory */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** 
/** Underlying filestore */
// TODO: move PathUtils to be package-private here instead of 
// public+forbidden api!
// these are hacks that are not guaranteed
// see https://bugs.openjdk.java.net/browse/JDK-8162520:
// see https://bugs.openjdk.java.net/browse/JDK-8162520:
// see https://bugs.openjdk.java.net/browse/JDK-8162520:
// for the partition
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/* ${data.paths} */
/* ${data.paths}/indices */
/** Cached FileStore from path */
/**
/**
/**
/**
/**
// release all the ones that were obtained up until now
/**
/**
// check if we can do an auto-upgrade
// acquire locks on legacy path for duration of upgrade (to ensure there is no older ES version running on this path)
// move contents from legacy path to new path
// determine folders to move and check that there are no extra files/folders
// node state directory, containing MetaDataStateFormat-based node metadata as well as cluster state
// indices
// ignore
// now do the actual upgrade. start by upgrading the node metadata file before moving anything, since a downgrade in an
// intermediate state would be pretty disastrous
// upgrade successfully completed, remove legacy nodes folders
// We do some I/O in here, so skip this if DEBUG/INFO are not enabled:
// Log one line per path.data:
// Just log a 1-line summary:
/**
// load legacy metadata
// load legacy metadata
/**
/**
// resolve the directory the shard actually lives in
// open a directory (will be immediately closed) on the shard's location
// create a lock for the "write.lock" file
/**
// Relaxed assertion for the special case where only the empty state directory exists after deleting
// the shard directory because it was created again as a result of a metadata read action concurrently.
/**
/**
/**
/**
/**
// new instance prevents double closing
/**
/**
/*
// guarded by shardLocks
/**
/**
/**
// we currently only return the ID and hide the underlying nodeMetaData implementation in order to avoid
// confusion with other "metadata" like node settings found in elasticsearch.yml. In future
// we can encapsulate both (and more) in one NodeMetaData (or NodeSettings) object ala IndexSettings
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// This assert is because this should be caught by MetaDataCreateIndexService
/**
/**
/**
// Sanity check:
/**
// check node-paths are writable
// check index paths are writable
// package private for testing
// delete any lingering file from a previous failure
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// clean-up shard dirs
// clean-up all metadata dirs
// clean-up shard dirs
// equals on Path is good enough here due to the way these are collected.
//package-private for testing
/*
//www.apache.org/licenses/LICENSE-2.0
// ok, means the version change is not supported
//package-private for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// mark all node as fetching and go ahead and async fetch them
// use a unique round id to detect stale responses in processAsyncFetch
// if we are still fetching, return null to indicate it
// nothing to fetch, yay, build the return value
// if its failed, remove it from the list of nodes, so if this run doesn't work
// we try again next round to fetch it again
// clear the nodes to ignore, we had a successful run in fetching everything we can
// we need to try them if another full run is needed
// if at least one node failed, make sure to have a protective reroute
// here, just case this round won't find anything, and we need to retry fetching data
/**
// we are closed, no need to process this async fetch at all
// if the entry is there, for the right fetching round and not marked as failed already, process it
// if the entry is there, for the right fetching round and not marked as failed already, process it
// if the request got rejected or timed out, we need to try it again next time...
/**
/**
/**
// verify that all current data nodes are there
// remove nodes that are not longer part of the data nodes set
/**
/**
/**
// visible for testing
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// no decision was taken by this allocator
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// initialize all index routing tables as empty
// start with 0 based versions for routing table
// automatically generate a UID for the metadata if we need to
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// This might be a good use case for CopyOnWriteHashMap
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// update the global state, and clean the indices, we elect them in the next phase
// TODO if this logging statement is correct then we are missing an else here
/*
//www.apache.org/licenses/LICENSE-2.0
// for tests
// allow for testing infra to change shard allocators implementation
// sort for priority ordering
// cancel existing recoveries if we have a better match
/**
/**
// Invalidate the cache if a data node has been added to the cluster. This ensures that we do not cancel a recovery if a node
// drops out, we fetch the shard data, then some indexing happens and then the node rejoins the cluster again. There are other
// ways we could decide to cancel a recovery based on stale data (e.g. changing allocation filters or a primary failure) but
// making the wrong decision here is not catastrophic so we only need to cover the common case.
// recalc to also (lazily) clear out old nodes.
// explicitely type lister, some IDEs (Eclipse) are not able to correctly infer the function type
// explicitely type lister, some IDEs (Eclipse) are not able to correctly infer the function type
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Set by calling start()
// unreference legacy files (only keep them for dangling indices functionality)
// delete legacy files
// write legacy node metadata to prevent accidental downgrades from spawning empty cluster state
// write empty cluster state just so that we have a persistent node id. There is no need to write out global metadata with
// cluster uuid as coordinating-only nodes do not snap into a cluster as they carry no state
// delete legacy cluster state files
// write legacy node metadata to prevent downgrades from spawning empty cluster state
// exposed so it can be overridden by tests
// exposed so it can be overridden by tests
/**
// upgrade index meta data
// upgrade current templates
// collect current data
// upgrade global custom meta data
// remove all data first so a plugin can remove custom metadata or templates if needed
// visible for testing
// write current term before last accepted state so that it is never below term in last accepted state
/**
// As the close method can be concurrently called to the other PersistedState methods, this class has extra protection in place.
// Write the whole state out to be sure it's fresh and using the latest format. Called during initialisation, so that
// (1) throwing an IOException is enough to halt the node, and
// (2) the index is currently empty since it was opened with IndexWriterConfig.OpenMode.CREATE
// In the common case it's actually sufficient to commit() the existing state and not do any indexing. For instance,
// this is true if there's only one data path on this master node, and the commit we just loaded was already written out
// by this version of Elasticsearch. TODO TBD should we avoid indexing when possible?
// In a new currentTerm, we cannot compare the persisted metadata's lastAcceptedVersion to those in the new state,
// so it's simplest to write everything again.
// Within the same currentTerm, we _can_ use metadata versions to skip unnecessary writing.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// allow to control a delay of when indices will get created
// default the recover after master nodes to the minimum master nodes in the discovery
// use post applied so that the state will be visible to the background recovery thread we spawn in performStateRecovery
// not our job to recover
// already recovered
// no expected is set, honor the setting if they are there
// one of the expected is set, see if all of them meet the need, and ignore the timeout in this case
// does not meet the expected...
// does not meet the expected...
// does not meet the expected...
// reset flag even though state recovery completed, to ensure that if we subsequently become leader again based on a
// not-recovered state, that we again do another state recovery.
// used for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We call updateClusterState on the (unique) cluster applier thread so there's no need to synchronize access to these fields.
/**
/**
// exposed for tests
// exposed for tests
/**
/**
/**
/**
// we prefer not to clean-up index metadata in case of rollback,
// if it's not referenced by previous manifest file
// not to break dangling indices functionality
// If the Manifest write results in a dirty WriteStateException it's not safe to roll back, removing the new metadata files,
// because if the Manifest was actually written to disk and its deletion fails it will reference these new metadata files.
// On master-eligible nodes a dirty WriteStateException here is fatal to the node since we no longer really have any idea
// what the state on disk is and the only sensible response is to start again from scratch.
/*
//www.apache.org/licenses/LICENSE-2.0
// The dangled index might be from an older version, we need to make sure it's compatible
// with the current version and upgrade it if needed.
// upgrade failed - adding index as closed
// now, reroute
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// this is important since some of the XContentBuilders write bytes on close.
// in order to write the footer we need to prevent closing the actual index input.
/**
/**
/**
/**
/**
// We checksum the entire file before we even go and parse it. If it's corrupted we barf right here.
// we trick this into a dedicated exception with the original stacktrace
/**
/**
/**
// if we reach this something went wrong
// We have some state files but none of them gave us a usable state
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we allow subclasses in tests to redefine formats, e.g. to inject failures
/**
/**
// TODO https://github.com/elastic/elasticsearch/issues/38556
// assert Version.CURRENT.major < 8 : "failed to find manifest file, which is mandatory staring with Elasticsearch version 8.0";
// TODO https://github.com/elastic/elasticsearch/issues/38556
// assert Version.CURRENT.major < 8 : "failed to find manifest file, which is mandatory staring with Elasticsearch version 8.0";
// this index folder is cleared up when state is recovered
/**
/**
/**
/**
/**
/**
/**
/**
/**
// write empty file so that indices become unreferenced
/**
// To ensure that the metadata is never reimported by loadFullStateBWC in case where the deletions here fail mid-way through,
// we first write an empty manifest file so that the indices become unreferenced, then clean up the indices, and only then delete
// the manifest file.
// delete meta state directories of indices
// finally delete manifest
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// start empty since we re-write the whole cluster state to ensure it is all using the same format version
// only commit when specifically instructed, we must not write any intermediate states
// most of the data goes into stored fields which are not buffered, so we only really need a tiny buffer
// merge on the write thread (e.g. while flushing)
/**
// exposed for tests
// it is possible to disable the use of MMapDirectory for indices, and it may be surprising to users that have done so if we still
// use a MMapDirectory here, which might happen with FSDirectory.open(path). Concurrency is of no concern here so a
// SimpleFSDirectory is fine:
/**
/**
/**
// We use a write-all-read-one strategy: metadata is written to every data path when accepting it, which means it is mostly
// sufficient to read _any_ copy. "Mostly" sufficient because the user can change the set of data paths when restarting, and may
// add a data path containing a stale copy of the metadata. We deal with this by using the freshest copy we can find.
/**
/**
/**
/**
/**
// Flush, to try and expose a failure (e.g. out of disk space) before committing, because we can handle a failure here more
// gracefully than one that occurs during the commit process.
/**
/**
// Flush, to try and expose a failure (e.g. out of disk space) before committing, because we can handle a failure here more
// gracefully than one that occurs during the commit process.
// The commit() call has similar semantics to a fsync(): although it's atomic, if it fails then we've no idea whether the
// data on disk is now the old version or the new version, and this is a disaster. It's safest to fail the whole node and
// retry from the beginning.
// closing the XContentBuilder should not release the bytes yet
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// must be primary
// must be unassigned
// only handle either an existing store or a snapshot recovery
// this allocator is not responsible for allocating this shard
// don't create a new IndexSetting object for every shard as this could cause a lot of garbage
// on cluster restart if we allocate a boat load of shards
// use in-sync allocation ids to select nodes
// let BalancedShardsAllocator take care of allocating this shard
// We have a shard that was previously allocated, but we could not find a valid shard copy to allocate the primary.
// We could just be waiting for the node that holds the primary to start back up, in which case the allocation for
// this shard will be picked up when the node joins and we do another allocation reroute
// The deciders returned a NO decision for all nodes with shard copies, so we check if primary shard
// can be force-allocated to one of the nodes.
// we are throttling this, since we are allowed to allocate to this node but there are enough allocations
// taking place on the node currently, ignore it for now
/**
// there were no shard copies that were eligible for being assigned the allocation,
// so all fetched shard data are ineligible shards
/**
// allocation preference
// prefer shards with matching allocation ids
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if we are allocating a replica because of index creation, no need to go and find a copy, there isn't one...
// still fetching
// if we can't find the primary data, it is probably because the primary shard is corrupted (and listing failed)
// just let the recovery find it out, no need to do anything about it for the initializing shard
// current node will not be in matchingNodes as it is filtered away by SameShardAllocationDecider
// we found a better match that can perform noop recovery, cancel the existing allocation.
// don't cancel shard in the loop as it will cause a ConcurrentModificationException
/**
// must be a replica
// must be unassigned
// if we are allocating a replica because of index creation, no need to go and find a copy, there isn't one...
// this allocator is not responsible for deciding on this shard
// pre-check if it can be allocated to any node that currently exists, so we won't list the store for it for nothing
// only return early if we are not in explain mode, or we are in explain mode but we have not
// yet attempted to fetch any shard data
// if we can't find the primary data, it is probably because the primary shard is corrupted (and listing failed)
// we want to let the replica be allocated in order to expose the actual problem with the primary that the replica
// will try and recover from
// Note, this is the existing behavior, as exposed in running CorruptFileTest#testNoPrimaryData
// we only check on THROTTLE since we checked before before on NO
// we are throttling this, as we have enough other shards to allocate to this node, so ignore it for now
// we found a match
// if we didn't manage to find *any* data (regardless of matching sizes), and the replica is
// unassigned due to a node leaving, so we delay allocation of this replica to see if the
// node with the shard copy will rejoin so we can re-use the copy it has
/**
// if we can't allocate it on a node, ignore it, for example, this handles
// cases for only allocating a replica after a primary
/**
/**
// we don't have any files at all, it is an empty index
// check if we can allocate on that node...
// we only check for NO, since if this node is THROTTLING and it has enough "same data"
// then we will try and assign it next time
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: Fallback for BWC with older ES versions. Remove once request.getCustomDataPath() always returns non-null
// we don't have an open shard on the store, validate the files on disk are openable
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we can't make the network.bind_host a fallback since we already fall back to http.host hence the extra conditional here
// we can't make the network.publish_host a fallback since we already fall back to http.host hence the extra conditional here
// Bind and start to accept incoming connections.
/**
// package private for tests
// if no matching boundAddress found, check if there is a unique port for all bound addresses
// just close and ignore - we are already stopped and just need to make sure we release all resources
/**
/**
// Visible for testing
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: Ideally we should move the setting of Cors headers into :server
// NioCorsHandler.setCorsResponseHeaders(nettyRequest, resp, corsConfig);
// Add all custom headers
// If our response doesn't specify a content-type header, set one
// If our response has no content-length, calculate and set one
// Determine if the request connection should be closed on completion.
// In case we fail to parse the http protocol version out of the request we always close the connection
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we intentionally use a different compression level as Netty here as our benchmarks have shown that a compression level of 3 is the
// best compromise between reduction in network traffic and added latency. For more details please check #7309.
// don't reset cookies by default, since I don't think we really need to
// note, parsing cookies was fixed in netty 3.5.1 regarding stack allocation, but still, currently, we don't need cookies
// A default of 0 means that by default there is no read timeout
// Tcp socket settings
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//issues.apache.org/jira/browse/LUCENE-7976.
// unlimited
// only setter that must NOT delegate to the forced merge policy
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
// allow for _na_ uuid
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
//message for json logs is provided by jsonFields
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** On which extensions to load data into the file-system cache upon opening of files.
// whether to use the query cache
// for test purposes only
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// pkg private for testing
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// don't convert to Setting<> and register... we only set this in tests and register via a plugin
// we parse all percolator queries as they would be parsed on shard 0
// we delay the actual creation of the sort order for this index because the mapping has not been merged yet.
// The sort order is validated right after the merge of the mapping later in the process.
// initialize this last -- otherwise if the wrapper requires any other member to be non-null we fail with an NPE
// kick off async ops for the first shard in this index
// metadata verification needs a mapper service
/**
/**
// method is synchronized so that IndexService can't be closed while we're writing out dangling indices information
// method is synchronized so that IndexService can't be closed while we're deleting dangling indices information
// NOTE: O(numShards) cost, but numShards should be smallish?
/*
// TODO: we should, instead, hold a "bytes reserved" of how large we anticipate this shard will be, e.g. for a shard
// that's being relocated/replicated we know how large it will become once it's done copying:
// Count up how many shards are currently on each data path:
// if we are on a shared FS we only own the shard (ie. we can safely delete it) if we are the primary.
// this logic is tricky, we want to close the engine so we rollback the changes done to it
// and close the shard so no operations are allowed to it
// only flush we are we closed (closed index or shutdown) and if we are not deleted
// ignore
// call this before we close the store, so we can release resources for it
// we remove that shards content if this index has been deleted
/**
/**
/**
/**
// pkg private for testing
// pkg private for testing
// once we change the refresh interval we schedule yet another refresh
// to ensure we are in a clean and predictable state.
// it doesn't matter if we move from or to <code>-1</code>  in both cases we want
// docs to become visible immediately. This also flushes all pending indexing / search requests
// that are waiting for a refresh.
// pkg private for testing
// pkg private for testing
// fine - continue;
// fine - continue;
// fine - continue;
// the shard was closed concurrently, continue
// don't re-schedule if the IndexService instance is closed or if the index is closed
/**
// this setting is intentionally not registered, it is only used in tests
// this setting is intentionally not registered, it is only used in tests
/**
// index.global_checkpoint_sync_interval is not a real setting, it is only registered in tests
// for tests
// for tests
// for tests
/**
// only clear caches relating to the specified fields
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
// always flush after merge
// never flush after merge
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// volatile fields are updated via #updateIndexMetaData(IndexMetaData) under lock
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// ignore the translog retention settings if soft-deletes enabled
// ignore the translog retention settings if soft-deletes enabled
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// nothing to update, same settings
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// visible for tests
/**
/**
/** We only allow index sorting on these types */
/*
//www.apache.org/licenses/LICENSE-2.0
// get a handle on pending tasks
// wait for termination
/** A handle on the execution of  warm-up action. */
/** Wait until execution of the warm-up action completes. */
/** Queue tasks to warm-up the given segments and return handles that allow to wait for termination of the
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// don't convert to Setting<> and register... we only set this in tests and register via a plugin
// percentage
// TODO is this really a good default number for max_merge_segment, what happens for large indices,
// won't they end up with many segments?
// fixing maxMergeAtOnce, see TieredMergePolicy#setMaxMergeAtOnce
// max merge at once should be at least 2
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// message for json logs is overridden from json Fields
// Message will be used in plaintext logs
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// not allowing Versions.NOT_FOUND as it is not a valid input value.
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// check for explicit version on the specific analyzer component
// check for explicit version on the index itself as default for all analysis components
// resolve the analysis version based on the version the index was created with
// LUCENE 4 UPGRADE: Should be settings.getAsBoolean("stem_exclusion_case", false)?
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// custom component, so we build it from scratch
// no index provided, so we use prebuilt analysis components
// if there's no prebuilt component, try loading a global one to build with no settings
// get the component from index settings
/**
/**
/**
/**
/**
/**
// TODO: Have pre-built normalizers
/**
/**
/**
// go over the char filters in the bindings and register the ones that are not configured
// we don't want to re-register one that already exists
// check, if it requires settings, then don't register it, we know default has no settings...
// Pre-build analyzers
// check for deprecations
/*
/*
// if we got a named analyzer back, use it...
// unless the positionIncrementGap needs to be overridden
// Some analysis components emit deprecation warnings or throw exceptions when used
// with the wrong version of elasticsearch.  These exceptions and warnings are
// normally thrown when tokenstreams are constructed, which unless we build a
// tokenstream up-front does not happen until a document is indexed.  In order to
// surface these warnings or exceptions as early as possible, we build an empty
// tokenstream and pull it through an Analyzer at construction time.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// merge and transfer token filter analysis modes with analyzer
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Don't be lenient here and return the default analyzer
// Fields need to be explicitly added
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// everything allowed if this analyzer is in ALL mode
/** It is an error if this is ever used, it means we screwed up! */
/*
//www.apache.org/licenses/LICENSE-2.0
// exactly one of these two members is not null
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we create the named analyzer here so the resources associated with it will be shared
// and we won't wrap a shared analyzer with named analyzer each time causing the resources
// to not be shared...
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// No need to put, because we delegate in get() directly to PreBuiltAnalyzers which already caches.
// Wrap the analyzer instance in a PreBuiltAnalyzerProvider, this is what PreBuiltAnalyzerProviderFactory#close expects
// (other caches are not directly caching analyzers, but analyzer provider instead)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// insanity
// if null then this means the shard has already been removed and the stats are 0 anyway for the shard this key belongs to
// this is from a different index
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// nothing to do here
// nothing to do here
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** the raw unfiltered lucene default. useful for testing */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Number of snapshots held against each commit point.
// the most recent safe commit point - its max_seqno at most the persisted global checkpoint.
// the most recent commit point
// This is protected from concurrent calls by a lock on the IndexWriter, but this assertion makes sure that we notice if that ceases
// to be true in future. It is not disastrous if safeCommitInfo refers to an older safeCommit, it just means that we might retain a
// bit more history and do a few more ops-based recoveries than we would otherwise.
/**
// increase refCount
/**
// release refCount
// The commit can be clean up only if no pending snapshot and it is neither the safe commit nor last commit.
/**
/**
// Commits are sorted by age (the 0th one is the oldest commit).
// Ignore index commits with different translog uuid.
// If an index was created before 6.2 or recovered from remote, we might not have a safe commit.
// In this case, we return the oldest index commit instead.
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Use -1 for docs which don't have primary term. The caller considers those docs as nested docs.
/*
//www.apache.org/licenses/LICENSE-2.0
/** a class the returns dynamic information with respect to the last commit point of this shard */
// lucene commit id in base 64;
// clone the map to protect against concurrent changes
// lucene calls the current generation, last generation.
/** base64 version of the commit id (see {@link SegmentInfos#getId()} */
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Holds a deleted version, which just adds a timestamp to {@link VersionValue} so we know when we can expire the deletion. */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if more than 20 seconds, DEBUG log it
/**
/**
// Lucene IW makes a clone internally but since we hold on to this instance
// the clone will just be the identity.
// Don't stall here, because we do our own index throttling (in InternalEngine.IndexThrottle) when merges can't keep up
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// we use the engine class directly here to make sure all subclasses have the same logger name
/** Returns 0 in the case where accountable is null, otherwise returns {@code ramBytesUsed()} */
/** returns the history uuid for the engine */
/** Returns how many bytes we are currently moving from heap to disk */
/**
// TODO: currently we load up the suggester for reporting its size
/**
// we calculate the doc stats based on the internal searcher that is more up-to-date and not subject
// to external refreshes. For instance we don't refresh an external searcher if we flush and indices with
// index.refresh_interval=-1 won't see any doc stats updates at all. This change will give more accurate statistics
// when indexing but not refreshing in general. Yet, if a refresh happens the internal searcher is refresh as well so we are
// safe here.
// we don't wait for a pending refreshes here since it's a stats call instead we mark it as accessed only which will cause
// the next scheduled refresh to go through and refresh the stats as well
// we go on the segment level here to get accurate numbers
/**
/**
/** Activate throttling, which switches the lock to be a real lock */
/** Deactivate throttling, which switches the lock to be an always-acquirable NoOpLock */
// Paranoia (System.nanoTime() is supposed to be monotonic): time slip may have occurred but never want
// to add a negative number
// Paranoia (System.nanoTime() is supposed to be monotonic): time slip must have happened, have to ignore this value
/**
/**
/**
/** A Lock implementation that always allows the lock to be acquired */
/**
/**
/**
/** whether the operation was successful, has failed or was aborted due to a mapping update */
/** get the updated document version */
/**
/**
/** get the translog location after executing the operation */
/** get document failure while executing the operation {@code null} in case of no failure */
/** get total time in nanoseconds */
/**
/**
/**
//TODO: A better exception goes here
// don't release the searcher on this path, it is the
// responsibility of the caller to call GetResult.release
/**
/**
/* Acquire order here is store -> manager since we need
/* In general, readers should never be released twice or this would break reference counting. There is one rare case
// success - hand over the reference to the engine reader
// throw EngineCloseException here if we are already closed
/**
/**
/**
/**
/**
/**
/**
/**
/**
/** get commits stats for the last commit */
/**
/**
/**
/**
// TODO: consider moving this to StoreStats
// by default we don't have a writer here... subclasses can override this
/** How much heap is used that would be freed by a refresh.  Note that this may throw {@link AlreadyClosedException}. */
// first, go over and compute the search ones...
// now, correlate or add the committed ones...
// TODO: add more fine grained mem stats values to per segment info here
/**
/*
/**
/**
/**
// NOTE: do NOT rename this to something containing flush or refresh!
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// this must happen before we close IW or Translog such that we can check this state to opt out of failing the engine
// again on any caught AlreadyClosedException
// we just go and close this engine - no way to recover
// we must set a failure exception, generate one if not supplied
// we first mark the store as corrupted before we notify any listeners
// this must happen first otherwise we might try to reallocate so quickly
// on the same node that we don't see the corrupted marker file when
// the shard is initializing
// don't bubble up these exceptions up
/** Check whether the engine should be failed */
/**
/**
// This means there's a bug somewhere: don't suppress it
/** type of operation (index, delete), subclasses use static types */
/**
// TEST ONLY
// TEST ONLY
/**
/**
/**
/**
// TODO we might force a flush in the future since we have the write lock already even though recoveries
// are running.
// double close is not a problem
// don't acquire the write lock if we are already closed
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/**
/**
// we don't error message the not officially supported ones
/**
// We need to make the indexing buffer for this shard at least as large
// as the amount of memory that is available for all engines on the
// local node so that decisions to flush segments to disk are made by
// IndexingMemoryController rather than Lucene.
// Add an escape hatch in case this change proves problematic - it used
// to be a fixed amound of RAM: 256 MB.
// TODO: Remove this escape hatch in 8.x
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// A uid (in the form of BytesRef) to the version map
// we use the hashed variant since we iterate over it and check removal and additions on existing keys
// How many callers are currently requesting index throttling.  Currently there are only two situations where we do this: when merges
// are falling behind and when writing indexing buffer to disk is too slow.  When this is 0, there is no throttling, else we throttling
// incoming indexing ops to a single thread:
// max_seq_no_of_updates_or_deletes tracks the max seq_no of update or delete operations that have been processed in this engine.
// An index request is considered as an update if it overwrites existing documents with the same docId in the Lucene index.
// The value of this marker never goes backwards, and is tracked/updated differently on primary and replica.
// Lucene operations since this engine was opened - not include operations from existing segments.
// IndexWriter throws AssertionError on init, if asserts are enabled, if any files don't exist, but tests that
// randomly throw FNFE/NSFE can also hit this:
// don't allow commits until we are done with recovering
// failure we need to dec the store reference
/**
//guarded by refreshLock
// steal the reference without warming up
// we simply run a blocking refresh on the internal reference manager and then steal it's reader
// it's a save operation since we acquire the reader which incs it's reference but then down the road
// steal it by calling incRef on the "stolen" reader
// nothing has changed - both ref managers share the same instance so we can use reference equality
// steal the reference
// we can access segment_stats while a shard is still in the recovering state.
/* leap-frog the local checkpoint */) {
// to persist noops associated with the advancement of the local checkpoint
// just play safe and never allow commits on this see #ensureCanFlush
// we are good - now we can commit
// flush if we recovered something or if we have references to older translogs
// note: if opsRecovered == 0 and we have older translogs it means they are corrupted or 0 length.
// we are good - now we can commit
// We expect that this shard already exists, so it must already have an existing translog else something is badly wrong!
// Package private for testing purposes only
// Package private for testing purposes only
/**
/**
/** Returns how many bytes we are currently moving from indexing buffer to segments on disk */
/**
/**
// iw is closed below
// release everything we created on a failure
// we need to lock here to access the version map to do this truly in RT
// this is only used for updates - API _GET calls will always read form a reader for consistency
// the update call doesn't need the consistency since it's source only + _parent but parent can go away in 7.0
// in the case of a already pruned translog generation we might get null here - yet very unlikely
// lets check if the translog has failed with a tragic event
// we expose what has been externally expose in a point in time snapshot via an explicit refresh
// no version, get the version from the index, we know that we refresh on flush
/**
/** the op is more recent than the one that last modified the doc found in lucene*/
/** the op is older or the same as the one that last modified the doc found in lucene*/
/** no doc was found in lucene */
// load from index
/** resolves the current version of the document, returning null if not found */
// used for asserting in tests
// used for asserting in tests
// we are switching from an unsafe map to a safe map. This might happen concurrently
// but we only need to do this once since the last operation per ID is to add to the version
// map so once we pass this point we can safely lookup from the version map.
// allow to optimize in order to update the max safe time stamp
// sequence number should be set when operation origin is not primary
// sequence number should not be set when operation origin is primary
/**
/* A NOTE ABOUT APPEND ONLY OPTIMIZATIONS:
// generate or register sequence number
// if we have document failure, record it as a no-op in the translog and Lucene with the generated seq_no
// the op is coming from the translog (and is hence persisted already) or it does not have a sequence number
// needs to maintain the auto_id timestamp in case this replica becomes primary
// unlike the primary, replicas don't really care to about creation status of documents
// this allows to ignore the case where a document was found in the live version maps in
// a delete state and return false for the created flag in favor of code simplicity
// the operation seq# was processed and thus the same operation was already put into lucene
// this can happen during recovery where older operations are sent from the translog that are already
// part of the lucene commit (either from a peer recovery or a local translog)
// or due to concurrent indexing & recovery. For the former it is important to skip lucene as the operation in
// question may have been deleted in an out of order op that is not replayed.
// See testRecoverFromStoreWithOutOfOrderDelete for an example of local recovery
// See testRecoveryWithOutOfOrderDelete for an example of peer recovery
// see Engine#getMaxSeqNoOfUpdatesOrDeletes for the explanation of the optimization using sequence numbers
// non-primary mode (i.e., replica or recovery)
// resolve an external operation into an internal one which is safe to replay
// resolves incoming version
/* Update the document's sequence number and primary term; the sequence number here is derived here from either the sequence
// document does not exists, we can optimize for create, but double check if assertions are running
/* There is no tragic event recorded so this must be a document failure.
/**
// TODO: can we enable this check for all origins except primary on the leader?
/**
// in this case we force
// soft-deleted every document before adding to Lucene
/**
// NOTE this uses direct access to the version map since we are in the assertion code where we maintain a secondary
// map in the version map such that we don't need to refresh if we are unsafe;
// NOTE: we don't throttle this when merges fall behind because delete-by-id does not create new segments:
// generate or register sequence number
// the op is coming from the translog (and is hence persisted already) or does not have a sequence number (version conflict)
// non-primary mode (i.e., replica or recovery)
// the operation seq# was processed thus this operation was already put into lucene
// this can happen during recovery where older operations are sent from the translog that are already
// part of the lucene commit (either from a peer recovery or a local translog)
// or due to concurrent indexing & recovery. For the former it is important to skip lucene as the operation in
// question may have been deleted in an out of order op that is not replayed.
// See testRecoverFromStoreWithOutOfOrderDelete for an example of local recovery
// See testRecoveryWithOutOfOrderDelete for an example of peer recovery
// resolve operation from external to internal
/*
// of a rare double delete
// It's expensive to prune because we walk the deletes map acquiring dirtyLock for each uid so we only do it
// every 1/4 of gcDeletesInMillis:
// A noop tombstone does not require a _version but it's added to have a fully dense docvalues for the version
// field. 1L is selected to optimize the compression because it might probably be the most common value in
// version field.
/*
// the op is coming from the translog (and is hence persisted already) or it does not have a sequence number
/**
// both refresh types will result in an internal refresh but only the external will also
// pass the new reader reference to the external reader manager.
// refresh does not need to hold readLock as ReferenceManager can handle correctly if the engine is closed in mid-way.
// increment the ref just to ensure nobody closes the store during a refresh
// even though we maintain 2 managers we really do the heavy-lifting only once.
// the second refresh will only do the extra work we have to do for warming caches etc.
// it is intentional that we never refresh both internal / external together
// TODO: maybe we should just put a scheduled job in threadPool?
// We check for pruning in each delete request, but we also prune here e.g. in case a delete burst comes in and then no more deletes
// for a long time:
// best effort attempt before we acquire locks
// lets do a refresh to make sure we shrink the version map. This refresh will be either a no-op (just shrink the version map)
// or we also have uncommitted changes and that causes this syncFlush to fail.
// refresh outside of the write lock
// we have to refresh internal reader here to ensure we release unreferenced segments.
/*
/*
// if we can't get the lock right away we block if needed otherwise barf
// Only flush if (1) Lucene has uncommitted docs, or (2) forced by caller, or (3) the
// newly created commit points to a different translog generation (can free translog)
// a temporary debugging to investigate test failure - issue#32827. Remove when the issue is resolved
// we need to refresh in order to clear older version values
// We don't have to do this here; we do it defensively to make sure that even if wall clock time is misbehaving
// (e.g., moves backwards) we will at least still sometimes prune deleted tombstones:
/*
// reread the last committed segment infos
/*
// testing
// for testing
/*
// increment the ref just to ensure nobody closes the store while we optimize
/* blocks and waits for merges*/);
/* blocks and waits for merges*/);
/* in this case we first check if the engine is still open. If so this exception is just fine
// reset it just to make sure we reset it in a case of an error
// we have to flush outside of the readlock otherwise we might have a problem upgrading
// the to a write lock when we fail the engine in this operation
// Revisit the deletion policy if we can clean up the snapshotting commit.
// Here we don't have to trim translog because snapshotting an index commit
// does not lock translog or prevents unreferenced files from trimming.
// if we are already closed due to some tragic exception
// we need to fail the engine. it might have already been failed before
// but we are double-checking it's failed and closed
// we are closed but the engine is not failed yet?
// this smells like a bug - we only expect ACE if we are in a fatal case ie. either translog or IW is closed by
// a tragic event or has closed itself. if that is not the case we are in a buggy state and raise an assertion error
// Check for AlreadyClosedException -- ACE is a very special
// exception that should only be thrown in a tragic event. we pass on the checks to failOnTragicEvent which will
// throw and AssertionError if the tragic event condition is not met.
// this spot on - we are handling the tragic event exception here so we have to fail the engine
// right away
// We don't guard w/ readLock here, so we could throw AlreadyClosedException
// fill in the merges flag
/**
// no need to commit in this case!, we snapshot before we close the shard, so translog and all sync'ed
// pkg-private for testing
// if we are using MMAP for term dics we force all off heap unless it's the ID field
// always force ID field on-heap for fast updates
// we by default don't commit on close
// with tests.verbose, lucene sets this up: plumb to align with filesystem stream
// Give us the opportunity to upgrade old segments while performing
// background merges
// always configure soft-deletes field so an engine with soft-deletes disabled can open a Lucene index with soft-deletes.
// We wrap the merge policy for all indices even though it is mostly useful for time-based indices
// but there should be no overhead for other type of indices so it's simpler than adding a setting
// to enable it.
// always use compound on flush - reduces # of file-handles on refresh
/** A listener that warms the segments if needed when acquiring a new reader */
// NEVER do this on a merge thread since we acquire some locks blocking here and if we concurrently rollback the writer
// we deadlock on engine#close for instance.
// if we have no pending merges and we are supposed to flush once merges have finished
// we try to renew a sync commit which is the case when we are having a big merge after we
// are inactive. If that didn't work we go and do a real flush which is ok since it only doesn't work
// if we either have records in the translog or if we don't have a sync ID at all...
// maybe even more important, we flush after all merges finish and we are inactive indexing-wise to
// free up transient disk usage of the (presumably biggish) segments that were just merged
// we hit a significant merge which would allow us to free up memory if we'd commit it hence on the next change
// we should execute a flush on the next operation if that's a flush after inactive or indexing a document.
// we could fork a thread and do it right away but we try to minimize forking and piggyback on outside events.
/*
/**
/*
/*
// translog recovery happens after the engine is fully constructed.
// If we are in this stage we have to prevent flushes from this
// engine otherwise we might loose documents if the flush succeeds
// and the translog recovery fails when we "commit" the translog on flush.
// config().isEnableGcDeletes() or config.getGcDeletesInMillis() may have changed:
/**
/**
/**
// for testing
/**
// for testing
// only used by asserts
/**
/**
/**
// avoid scanning translog if not necessary
/**
/**
/**
/**
// all changes until this point should be visible after refresh
// We treat a delete on the tombstones on replicas as a regular document, then use updateDocument (not addDocument).
// Operations can be processed on a replica in a different order than on the primary. If the order on the primary is index-1,
// delete-2, index-3, and the order on a replica is index-1, index-3, delete-2, then the msu of index-3 on the replica is 2
// even though it is an update (overwrites index-1). We should relax this assertion if there is a pending gap in the seq_no.
/**
// skip children docs which do not have primary term
// use 0L for the start time so we can prune this delete tombstone quickly
// when the local checkpoint advances (i.e., after a recovery completed).
// remove live entries in the version map
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Maps _uid value to its version information. */
/** Tracks bytes used by current map, i.e. what is freed on refresh. For deletes, which are also added to tombstones,
// each version map has a notion of safe / unsafe which allows us to apply certain optimization in the auto-generated ID usecase
// where we know that documents can't have any duplicates so we can skip the version map entirely. This reduces
// the memory pressure significantly for this use-case where we often get a massive amount of small document (metrics).
// if the version map is in safeAccess mode we track all version in the version map. yet if a document comes in that needs
// safe access but we are not in this mode we force a refresh and make the map as safe access required. All subsequent ops will
// respect that and fill the version map. The nice part here is that we are only really requiring this for a single ID and since
// we hold the ID lock in the engine while we do all this it's safe to do it globally unlocked.
// NOTE: these values can both be non-volatile since it's ok to read a stale value per doc ID. We serialize changes in the engine
// that will prevent concurrent updates to the same document ID and therefore we can rely on the happens-before guanratee of the
// map reference itself.
// minimum timestamp of delete operations that were made while this map was active. this is used to make sure they are kept in
// the tombstone
// All writes (adds and deletes) go into here:
// Used while refresh is running, and to hold adds/deletes until refresh finishes.  We read from both current and old on lookup:
// this is not volatile since we don't need to maintain a happens before relation ship across doc IDs so it's enough to
// have the volatile read of the Maps reference to make it visible even across threads.
// we haven't seen any ops and map before needed it so we maintain it
/**
/**
// we also need to remove it from the old map here to make sure we don't read this stale value while
// we are in the middle of a refresh. Most of the time the old map is an empty map so we can skip it there.
// All deletes also go here, and delete "tombstones" are retained after refresh:
// we maintain a second map that only receives the updates that we skip on the actual map (unsafe ops)
// this map is only maintained if assertions are enabled
/**
// shallow memory usage of the BytesRef object
// header of the byte[] array
// with an alignment size (-XX:ObjectAlignmentInBytes) of 8 (default),
// there could be between 0 and 7 lost bytes, so we account for 3
// lost bytes on average
/**
// use the same impl as the Maps does
// assume a load factor of 50%
// for each entry, we need two object refs, one for the entry itself
// and one for the free space that is due to the fact hash tables can
// not be fully loaded
/**
// Start sending all updates after this point to the new
// map.  While reopen is running, any lookup will first
// try this new map, then fallback to old, then to the
// current searcher:
// This is not 100% correct, since concurrent indexing ops can change these counters in between our execution of the previous
// line and this one, but that should be minor, and the error won't accumulate over time:
// We can now drop old because these operations are now visible via the newly opened searcher.  Even if didRefresh is false, which
// means Lucene did not actually open a new reader because it detected no changes, it's possible old has some entries in it, which
// is fine: it means they were actually already included in the previously opened reader, so we can still safely drop them in that
// case.  This is because we assign new maps (in beforeRefresh) slightly before Lucene actually flushes any segments for the
// reopen, and so any concurrent indexing requests can still sneak in a few additions to that current map that are in fact
// reflected in the previous reader.   We don't touch tombstones here: they expire on their own index.gc_deletes timeframe:
/**
// First try to get the "live" value:
/**
// Even though we don't store a record of the indexing operation (and mark as unsafe),
// we should still remove any previous delete for this uuid (avoid accidental accesses).
// Not this should not hurt performance because the tombstone is small (or empty) when unsafe is relevant.
// Also enroll the delete into tombstones, and account for its RAM too:
// Deduct tombstones bytes used for the version we just removed or replaced:
/**
// check if the value is old enough and safe to be removed
// version value can't be removed it's
// not yet flushed to lucene ie. it's part of this current maps object
/**
// we do check before we actually lock the key - this way we don't need to acquire the lock for tombstones that are not
// prune-able. If the tombstone changes concurrently we will re-read and step out below since if we can't collect it now w
// we won't collect the tombstone below since it must be newer than this one.
// we use tryAcquire here since this is a best effort and we try to be least disruptive
// this method is also called under lock in the engine under certain situations such that this can lead to deadlocks
// if we do use a blocking acquire. see #28714
// did we get the lock?
// Must re-get it here, vs using entry.getValue(), in case the uid was indexed/deleted since we pulled the iterator:
/**
// NOTE: we can't zero this here, because a refresh thread could be calling InternalEngine.pruneDeletedTombstones at the same time,
// and this will lead to an assert trip.  Presumably it's fine if our ramBytesUsedTombstones is non-zero after clear since the
// index is being closed:
//ramBytesUsedTombstones.set(0);
/**
/**
// TODO: useful to break down RAM usage here?
/**
/** Iterates over all deleted versions, including new ones (not yet exposed via reader) and old ones
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// we have processed all docs in the current search - fetch the next batch
// for better loading performance we sort the array by docID and
// then visit all leaves in order.
// now sort back based on the shardIndex. we use this to store the previous index
// We don't have to read the nested child documents - those docs don't have primary terms.
// Only pick the first seen seq#
// TODO: Callers should ask for the range that source should be retained. Thus we should always
// check for the existence source once we make peer-recovery to send ops after the local checkpoint.
// TODO: pass the latest timestamp from engine.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// a translog deletion policy that retains nothing but the last translog generation from safe commit
// refresh the translog stats
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no deleted docs - we are good!
// short-cut this if we don't match anything
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Construct a list of the previous segment readers, we only want to track memory used
// by new readers, so these will be exempted from the circuit breaking accounting.
//
// The Core CacheKey is used as the key for the set so that deletions still keep the correct
// accounting, as using the Reader or Reader's CacheKey causes incorrect accounting.
// don't add the segment's memory unless it is not referenced by the previous reader
// (only new segments)
// add the segment memory to the breaker (non-breaking)
// and register a listener for when the segment is closed to decrement the
// breaker accounting
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// we obtain the IW lock even though we never modify the index.
// yet this makes sure nobody else does. including some testing tools that try to be messy
// this is stupid
// Before 8.0 the global checkpoint is not known and up to date when the engine is created after
// peer recovery, so we only check the max seq no / global checkpoint coherency when the global
// checkpoint is different from the unassigned sequence number value.
// In addition to that we only execute the check if the index the engine belongs to has been
// created after the refactoring of the Close Index API and its TransportVerifyShardBeforeCloseAction
// that guarantee that all operations have been flushed to Lucene.
// the value of the global checkpoint is verified when the read-only engine is opened,
// and it is not expected to change during the lifecycle of the engine. We could also
// check this value before closing the read-only engine but if something went wrong
// and the global checkpoint is not in-sync with the max. sequence number anymore,
// checking the value here again would prevent the read-only engine to be closed and
// reopened as an internal engine, which would be the path to fix the issue.
// we can do operation-based recovery if we don't have to replay any operation.
// we could allow refreshes if we want down the road the reader manager will then reflect changes to a rw-engine
// opened side-by-side
// we can't do synced flushes this would require an indexWriter which we don't have
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// early terminate - nothing to do here since non of the docs has a recovery source anymore.
// calculating the cardinality is significantly cheaper than skipping all bulk-merging we might do
// if retentions are high we keep most of it
// keep all source
// we can't return null here lucenes DocIdMerger expects an instance
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// verbose mode
/**
/**
/**
/**
// the ram tree is written recursively since the depth is fairly low (5 or 6)
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// This lock count is used to prevent `minRetainedSeqNo` from advancing.
// The extra number of operations before the global checkpoint are retained
// The min seq_no value that is retained - ops after this seq# should exist in the Lucene index.
// provides the retention leases used to calculate the minimum sequence number to retain
/**
/**
/**
/**
/*
// do not advance if the retention lock is held
/*
// calculate the minimum sequence number to retain based on retention leases
/*
/*
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** the version of the document. used for versioned indexed operations and as a BWC layer, where no seq# are set yet */
/** the seq number of the operation that last changed the associated uuid */
/** the term of the operation that last changed the associated uuid */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// reset the iterator on the current doc
/**
/**
/** Fill the list of charsquences with the list of values for the current document. */
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// we need this extended source we we have custom comparators to reuse our field data
// in this case, we need to reduce type that will be used when search results are reduced
// on another node (we don't have the custom source them...)
/**
/**
/**
/** Whether missing values should be sorted first. */
/** Whether missing values should be sorted last, this is the default. */
/** Return the missing object value according to the reduced type of the comparator. */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// the below map needs to be modified under a lock
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// process value
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Sole constructor. (For invocation by subclass
// TODO: this interaction with sort comparators is really ugly...
/** Returns numeric docvalues view of raw double bits */
// yes... this is doing what the previous code was doing...
/** Returns numeric docvalues view of raw float bits */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Flush every 5mb
/**
/**
/**
// We have reached the end of the termsEnum, flush the buffer
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Throw meaningful exceptions if someone tries to modify the ScriptDocValues.
/**
/**
/**
/**
/**
// Happens for the document. We delay allocating dates so we can allocate it with a reasonable size.
/**
/**
/**
// We need to make a copy here, because BytesBinaryDVAtomicFieldData's SortedBinaryDocValues
// implementation reuses the returned BytesRef. Otherwise we would end up with the same BytesRef
// instance for all slots in the values array.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Return the wrapped {@link NumericDoubleValues} */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Return the wrapped values. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Return the wrapped values. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Return the wrapped values. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Return the wrapped values. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: Should it expose a count (current approach) or return null when there are no more values?
/**
/** 
/** 
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Sole constructor. (For invocation by subclass
/** Advance the iterator to exactly {@code target} and return whether
/** 
/** 
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// otherwise we fill missing values ourselves
/**
// TODO: move this out if we need it for other reasons
// we let termsenum etc fall back to the default implementation
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// NOTE: it's important to pass null as a missing value in the constructor so that
// the comparator doesn't check docsWithField since we replace missing values in select()
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// NOTE: it's important to pass null as a missing value in the constructor so that
// the comparator doesn't check docsWithField since we replace missing values in select()
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// NOTE: it's important to pass null as a missing value in the constructor so that
// the comparator doesn't check docsWithField since we replace missing values in select()
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: break down ram usage?
/**
/**
// segment ordinals match global ordinals
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Compute the worst-case number of bits per value for offsets in the worst case, eg. if no docs have a value at the
// beginning of the block and all docs have one at the end of the block
// +1 because of the sign
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Current position
// First level (0) of ordinals and pointers to the next level
// Ordinals and pointers for other levels +1
// over allocate in order to never worry about the array sizes, 24 entries would allow
// to store several millions of ordinals per doc...
// reserve the 1st slice on every level
/**
// Lazily allocate ordinals
// on the first level
// 0 or 1 ordinal
// current position is on the 1st level and not allocated yet
// reached the end of the slice, allocate a new one on the next level
// just go to the next slot
// First level
// Other levels
/**
/**
/**
/**
/**
/**
/**
/**
/**
// MultiOrdinals can be smaller than SinglePackedOrdinals for sparse fields
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// ordinals with value 0 indicates no value
// We don't reuse the builder as-is because it might have been built with a higher overhead ratio
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Some leaf readers may be wrapped and report different set of fields and use the same cache key.
// If a field can't be found then it doesn't mean it isn't there,
// so if a field doesn't exist then we don't cache it and just return an empty field data instance.
// The next time the field is found, we do cache.
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we create a new instance of the cached value for each consumer in order
// to avoid creating new TermsEnums for each segment in the cached instance
// ordinals are already global
// Some directory readers may be wrapped and report different set of fields and use the same cache key.
// If a field can't be found then it doesn't mean it isn't there,
// so if a field doesn't exist then we don't cache it and just return an empty field data instance.
// The next time the field is found, we do cache.
/*
//www.apache.org/licenses/LICENSE-2.0
/** helper: checks a fieldinfo and throws exception if its definitely not a LatLonDocValuesField */
// dv properties could be "unset", if you e.g. used only StoredField with this same name in the segment.
// ignore breaker
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// for now, dates and nanoseconds are treated the same, which also means, that the precision is only on millisecond level
/*
//www.apache.org/licenses/LICENSE-2.0
/** {@link AtomicFieldData} impl on top of Lucene's binary doc values. */
// no-op
// unknown
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// not exposed by Lucene
// no-op
/*
//www.apache.org/licenses/LICENSE-2.0
// Ignore breaker
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** {@link IndexFieldData} impl based on Lucene's doc values. Caching is done on the Lucene side. */
// can't do
// can't do
// Ignore Circuit Breaker
/*
//www.apache.org/licenses/LICENSE-2.0
// not exposed by lucene
// noop
/*
//www.apache.org/licenses/LICENSE-2.0
// PackedBytes
// PackedInts
/*
//www.apache.org/licenses/LICENSE-2.0
// Wrap the context in an estimator and use it to either estimate
// the entire set, or wrap the TermsEnum so it can be calculated
// per-term
// If something went wrong, unwind any current estimations we've made
// Call .afterLoad() to adjust the breaker now that we have an exact size
/**
/**
// 64 bytes for miscellaneous overhead
// Seems to be about a 1.5x compression per term/ord, plus 1 for some wiggle room
/**
/**
// If we weren't able to estimate, wrap in the RamAccountingTermsEnum
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// converts date values to nanosecond resolution
// converts date_nanos values to millisecond resolution
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// unknown
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we create a new instance of the cached value for each consumer in order
// to avoid creating new TermsEnums for each segment in the cached instance
// ordinals are already global
// Some directory readers may be wrapped and report different set of fields and use the same cache key.
// If a field can't be found then it doesn't mean it isn't there,
// so if a field doesn't exist then we don't cache it and just return an empty field data instance.
// The next time the field is found, we do cache.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Always load _ignored to be explicit about ignored fields
// This works because _ignored is added as the first metadata mapper,
// so its stored fields always appear first in the list.
// All these fields are single-valued so we can stop when the set is
// empty
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// need to join the fields and metadata fields
// seqNo may not be assigned if read from an old node
// TODO: can we avoid having an exception here?
//the original document gets slightly modified: whitespaces or pretty printing are not preserved,
//it all depends on the current builder settings
// skip potential inner objects for forward compatibility
// skip potential inner arrays for forward compatibility
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// This shouldn't happen...
/**
// break between having loaded it from translog (so we only have _source), and having a document to load
// check first if stored fields to be loaded don't contain an object field
// Only fail if we know it is a object field, missing paths / fields shouldn't fail.
// force fetching source if we read from translog and need to recreate stored fields
// in case we read from translog, some extra steps are needed to make _source consistent and to load stored fields
// Fast path: if only asked for the source or stored fields that have been already provided by TranslogLeafReader,
// just make source consistent by reapplying source filters from mapping (possibly also nulling the source)
// Slow path: recreate stored fields from original source
// update special fields
// retrieve stored fields from parsed doc
// retrieve source (with possible transformations, e.g. source filters
// put stored fields into result objects
// apply request-level source filtering
// TODO: The source might be parsed and available in the sourceLookup but that one uses unordered maps so different.
//  Do we care?
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/** default builder - used for external mapper*/
// field mapper handles this at build time
// but prefix tree strategies require a name, so throw a similar exception
/** parsing logic for geometry indexing */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we parse in post parse
// noop mapper
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Only add an entry to the field names field if the field is stored
// but has no doc values so exists query will work on a field with
// no doc values
// write total number of values
/*
//www.apache.org/licenses/LICENSE-2.0
// offset + length because copyOfRange wants a from and a to, not an offset & length
// TODO: Support for exclusive ranges, pending resolution of #40601
/**
// means positive
// Start by masking off the last three bits of the first byte - that's the start of our number
// the header is formed of:
// - 1 bit for the sign
// - 4 bits for the number of additional bytes
// - up to 3 bits of the value
// additional bytes are data bytes
// write data bytes
// byte 0 can't encode more than 3 bits
// reverse the order
// the first byte only uses 3 bits, we need the 5 upper bits for the header
// write the header
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Mapping field names
// Content field names
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// parse
// ignore null values
// index
// truncate input
/**
// always parse a long to make sure we don't get overflow
// no-op
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// expand if needed
/*
//www.apache.org/licenses/LICENSE-2.0
// used for binary, geo and range fields
/*
//www.apache.org/licenses/LICENSE-2.0
/** A {@link FieldMapper} for dates. */
// This check needs to be done after fromInclusive and toInclusive
// are resolved so we can throw an exception if they are invalid
// even if there are no points in the shard
// no points, so nothing matches
// the resolution here is always set to milliseconds, as aggregations use this formatter mainly and those are always in
// milliseconds. The only special case here is docvalue fields, which are handled somewhere else
/*
//www.apache.org/licenses/LICENSE-2.0
/** Full field name to mapper */
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// collect all the mappers for this type
// _id won't be used.
// Store the reason of a noop as a raw string in the _source field
/**
// We can pass down 'null' as acceptedDocs, because nestedDocId is a doc to be fetched and
// therefor is guaranteed to be a live doc.
/**
// no change
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// parse RootObjectMapper
// parse DocumentMapper
/*
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** A parser for documents, given mappings from a DocumentMapper */
// entire type is disabled
// will result in START_OBJECT
// only check for end of tokens if we created the parser here
// try to parse the next token, this should be null if the object is ended properly
// but will throw a JSON exception if the extra tokens is not valid JSON (this will be handled by the catch)
// empty doc, we can handle it...
// if its already a mapper parsing exception, no need to wrap it...
// Throw a more meaningful message if the document is empty.
// check if the field name contains only whitespace
/** Creates a Mapping containing any dynamically added fields, or returns null if there were no dynamic mappings. */
// We build a mapping by first sorting the mappers, so that all mappers containing a common prefix
// will be processed in a contiguous block. When the prefix is no longer seen, we pop the extra elements
// off the stack, merging them upwards into the existing mappers.
// We can see the same mapper more than once, for example, if we had foo.bar and foo.baz, where
// foo did not yet exist. This will create 2 copies in dynamic mappings, which should be identical.
// Here we just skip over the duplicates, but we merge them to ensure there are no conflicts.
// We first need the stack to only contain mappers in common with the previously processed mapper
// For example, if the first mapper processed was a.b.c, and we now have a.d, the stack will contain
// a.b, and we want to merge b back into the stack so it just contains a
// Then we need to add back mappers that may already exist within the stack, but are not on it.
// For example, if we processed a.b, followed by an object mapper a.c.d, and now are adding a.c.d.e
// then the stack will only have a on it because we will have already merged a.c.d into the stack.
// So we need to pull a.c, followed by a.c.d, onto the stack so e can be added to the end.
// If there are still parents of the new mapper which are not on the stack, we need to pull them
// from the existing mappings. In order to maintain the invariant that the stack only contains
// fields which are updated, we cannot simply add the existing mappers to the stack, since they
// may have other subfields which will not be updated. Instead, we pull the mapper from the existing
// mappings, and build an update with only the new mapper and its parents. This then becomes our
// "new mapper", and can be added to the stack.
// never remove the root mapper
// pop off parent mappers not needed by the current mapper,
// merging them backwards since they are immutable
/**
/**
/**
/** Creates an update for intermediate object mappers that are not on the stack, but parents of newMapper. */
// only prefix with parent mapper if the parent mapper isn't the root (which has a fake name)
/** Build an update for the parent which will contain the given mapper and any intermediate fields. */
// add the new mapper to the stack, and pop down to the original parent level
// the object is null ("obj1" : null), simply bail
// if we are at the end of the previous object, advance
// if we are just starting an OBJECT, advance, this is the object we are parsing, we need the name first
// restore the enable path flag
// don't add it twice, if its included in parent, and we are handling the master doc...
// We need to add the uid or id to this nested Lucene document too,
// If we do not do this then when a document gets deleted only the root Lucene document gets deleted and
// not the nested Lucene documents! Besides the fact that we would have zombie Lucene documents, the ordering of
// documents inside the Lucene index (document blocks) will be incorrect, as nested documents of different root
// documents are then aligned with other root documents. This will lead tothe nested query, sorting, aggregations
// and inner hits to fail or yield incorrect results.
// We just need to store the id as indexed field, so that IndexWriter#deleteDocuments(term) can then
// delete it when the root document is deleted too.
// the type of the nested doc starts with __, so we can identify that its a nested one in filters
// note, we don't prefix it with the type of the doc since it allows us to execute a nested query
// across types (for example, with similar nested objects)
// not dynamic, read everything up to end object
// There is a concrete mapper for this field already. Need to check if the mapper
// expects an array, if so we pass the context straight to the mapper and if not
// we serialize the array components
// TODO: shouldn't this skip, not parse?
// we can only handle null values if we have mappings for them
// TODO: passing null to an object seems bogus?
// not a long number
// not a double number
// We refuse to match pure numbers, which are too likely to be
// false positives with date formats that include eg.
// `epoch_millis` or `YYYY`
// failure to parse this, continue
// no templates are defined, we use float by default instead of double
// since this is much more space-efficient and should be enough most of
// the time
// TODO how do we identify dynamically that its a binary value?
// create a builder of the same type
// try to not introduce a conflict
/** Creates instances of the fields that the current field should be copied to */
// In case of a hierarchy of nested documents, we need to figure out
// which document the field should go to
/** Creates an copy of the current field with given field name and boost */
// The path of the dest field might be completely different from the current one so we need to reset it
// One mapping is missing, check if we are allowed to create a dynamic one.
// Should not dynamically create any more mappers so return the last mapper
// find what the dynamic setting is given the current parse context and parent
// no dot means we the parent is the root, so just delegate to the default outside the loop
// If parentMapper is ever null, it means the parent of the current mapper was dynamically created.
// But in order to be created dynamically, the dynamic setting of that parent was necessarily true
// looks up a child mapper, but takes into account field names that expand to objects
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Whether {@code value} matches {@code regex}. */
/** The type of a field as detected while parsing a json document. */
/** The default mapping type to use for fields of this {@link XContentFieldType}. */
// unknown parameters were ignored before but still carried through serialization
// so we need to ignore them at parsing time for old indices
// Validate that the pattern
// either the type was not set, or we updated it through replacements
// and the result is "text"
// now that string has been splitted into text and keyword, we use text for
// dynamic mappings. However before it used to be possible to index as a keyword
// by setting index=not_analyzed, so for now we will use a keyword field rather
// than a text field if index=not_analyzed and the field type was not specified
// explicitly
// TODO: remove this in 6.0
// TODO: how to do it in the future?
// use a sorted map for consistent serialization
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we have to store it the fieldType is mutable
/*
// can happen when an existing type on the same index has disabled indexing
// since we inherit the default field type from the first mapper that is
// created on an index
// don't set it to false, it is default and might be flipped by a more specific option
/** Set metadata on this field. */
/**
/**
/**
/**
// apply changeable values
// this field does not exist in the mappings yet
// this can happen if this mapper represents a mapping update
// no change
// ensure consistent order
// we disable the all in multi-field mappers
// TODO: multi fields are really just copy fields, we just need to expose "sub fields" or something that can be part
// of the mappings
// override previous definition
// sort the mappers so we get consistent serialization format
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Adding values to the _field_names field is handled by the mappers for each field type
// used as a sentinel - field names can't be empty
// Sometimes mappers create multiple Lucene fields, eg. one for indexing,
// one for doc values and one for storing. Deduplicating is not required
// for correctness but this simple check helps save utf-8 conversions and
// gives Lucene fewer values to deal with.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// If the mapping contains fields that support dynamic sub-key lookup, check
// if this could correspond to a keyed field of the form 'path_to_field.path_to_key'.
/**
/**
// Visible for testing.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if the mapping contains multifields then use the geohash string
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Flatten collection and convert each geometry to Lucene-friendly format
// decompose linestrings crossing dateline into array of Lines
/**
/**
/**
/**
// Check where the line is going east (+1), west (-1) or directly north/south (0)
// first point lon + shift is always between -180.0 and +180.0
// Jumping over anti-meridian - we need to start a new segment
// Check if new point intersects with anti-meridian
// Found intersection, all previous segments are now part of the linestring
// Didn't find intersection - just continue checking
/**
/**
// Last point is repeated
/**
// coordinate of the start point
// next segment
// potential intersection with dateline
// id of the component this edge belongs to
// use setter to catch duplicate point cases
// don't bother setting next if its null
// self-loop throws an invalid shape
/**
// inner rings (holes) have an opposite direction than the outer rings
// XOR will invert the orientation for outer ring cases (Truth Table:, T/T = F, T/F = T, F/T = T, F/F = F)
// set the points array accordingly (shell or hole)
/**
// OGC requires shell as ccw (Right-Handedness) and holes as cw (Left-Handedness)
// since GeoJSON doesn't specify (and doesn't need to) GEO core will assume OGC standards
// thus if orientation is computed as cw, the logic will translate points across dateline
// and convert to a right handed system
// compute the bounding box and calculate range
// translate the points if the following is true
//   1.  shell orientation is cw and range is greater than a hemisphere (180 degrees) but not spanning 2 hemispheres
//       (translation would result in a collapsed poly)
//   2.  the shell of the candidate hole has been translated (to preserve the coordinate system)
// flip the translation bit if the shell is being translated
// correct the orientation post translation (ccw for shell, cw for holes)
/**
/**
// calculate the direction of the points: find the southernmost point
// and check its neighbors orientation.
// Points are collinear, but `top` is not in the middle if so, so the edges either side of `top` are intersecting.
/**
// we start at 1 here since top points to 0
// compute the bounding coordinates (@todo: cleanup brute force)
// Intersections appear pairwise. On the first edge the inner of
// of the polygon is entered. On the second edge the outer face
// is entered. Other kinds of intersections are discard by the
// intersection function
// If two segments are connected maybe a hole must be deleted
// Since Edges of components appear pairwise we need to check
// the second edge only (the first edge is either polygon or
// already handled)
//TODO: Check if we could save the set null step
// only connect edges if intersections are pairwise
// 1. per the comment above, the edge array is sorted by y-value of the intersection
// with the dateline.  Two edges have the same y intercept when they cross the
// dateline thus they appear sequentially (pairwise) in the edge array. Two edges
// do not have the same y intercept when we're forming a multi-poly from a poly
// that wraps the dateline (but there are 2 ordered intercepts).
// The connect method creates a new edge for these paired edges in the linked list.
// For boundary conditions (e.g., intersect but not crossing) there is no sibling edge
// to connect. Thus the first logic check enforces the pairwise rule
// 2. the second logic check ensures the two candidate edges aren't already connected by an
//    existing edge along the dateline - this is necessary due to a logic change in
//    ShapeBuilder.intersection that computes dateline edges as valid intersect points
//    in support of OGC standards
// Connecting two Edges by inserting the point at
// dateline intersection and connect these by adding
// two edges between this points. One per direction
// NOTE: the order of the object creation is crucial here! Don't change it!
// first edge has no point on dateline
// second edge has no point on dateline
// second edge intersects with dateline
// first edge intersects with dateline
// second edge has no point on dateline
// second edge intersects with dateline
/**
/**
// Assign Hole to related components
// To find the new component the hole belongs to all intersections of the
// polygon edges with a vertical line are calculated. This vertical line
// is an arbitrary point of the hole. The polygon edge next to this point
// is part of the polygon the hole belongs to.
// To do the assignment we assume (and later, elsewhere, check) that each hole is within
// a single component, and the components do not overlap. Based on this assumption, it's
// enough to find a component that contains some vertex of the hole, and
// holes[i].coordinate is such a vertex, so we use that one.
// First, we sort all the edges according to their order of intersection with the line
// of longitude through holes[i].coordinate, in order from south to north. Edges that do
// not intersect this line are sorted to the end of the array and of no further interest
// here.
// There were no edges that intersect the line of longitude through
// holes[i].coordinate, so there's no way this hole is within the polygon.
// Next we do a binary search to find the position of holes[i].coordinate in the array.
// The binary search returns the index of an exact match, or (-insertionPoint - 1) if
// the vertex lies between the intersections of edges[insertionPoint] and
// edges[insertionPoint+1]. The latter case is vastly more common.
// The binary search returned an exact match, but we checked again using compareTo()
// and it didn't match after all.
// TODO Can this actually happen? Needs a test to exercise it, or else needs to be removed.
// holes[i].coordinate lies exactly on an edge.
// TODO Should this be pos instead of 0? This assigns exact matches to the southernmost component.
// holes[i].coordinate is strictly south of all intersections. Assign it to the
// southernmost component, and allow later validation to spot that it is not
// entirely within the chosen component.
// holes[i].coordinate is strictly north of at least one intersection. Assign it to
// the component immediately to its south.
/**
// find a coordinate that is not part of the dateline
// run along the border of the component, collect the
// edges, shift them according to the dateline and
// update the component id
// if there are two connected components, splitIndex keeps track of where to split the edge array
// start at 1 since the source coordinate is shared
// bookkeep the source and sink of each visited coordinate
// found a closed loop - we have two connected components so we need to slice into two distinct components
// a negative id flags the edge as visited for the edges(...) method.
// since we're splitting connected components, we want the edges method to visit
// the newly separated component
// correct the graph pointers by correcting the 'next' pointer for both the
// first appearance and this appearance of the edge
// backtrack until we get back to this coordinate, setting the visit id to
// a non-visited value (anything positive)
/**
// First and last coordinates must be equal
//We do not have holes on the dateline as they get eliminated
//when breaking the polygon around it.
//Lucene Tessellator treats different +180 and -180 and we should keep the sign.
//normalizeLon method excludes -180.
// mark as visited by inverting the sign
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// The _id field is always searchable.
// If the count is not 1 then the impl is not correct as the binary representation
// does not preserve order. But id fields only have one value per doc so we are good.
// do nothing here, no merging, but also no exception
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// This query is not performance sensitive, it only helps assess
// quality of the data, so we may use a slow query. It shouldn't
// be too slow in practice since the number of unique terms in this
// field is bounded by the number of fields in the mappings.
// done in post-parse
/*
//www.apache.org/licenses/LICENSE-2.0
// The _index field is always searchable.
/**
// No need to OR these clauses - we can only logically be
// running in the context of just one of these index names.
// No need to OR these clauses - we can only logically be
// running in the context of just one of these index names.
// None of the listed index names are this one
// nothing to do
/*
//www.apache.org/licenses/LICENSE-2.0
/** A {@link FieldMapper} for ip addresses. */
// the `terms` query contains some prefix queries, so we cannot create a set query
// and need to fall back to a disjunction of `term` queries
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// keywords are internally stored as utf8 bytes
// keyword analyzer with the default attribute source which encodes terms using UTF8
// in that case we skip normalization, which may be slow if there many terms need to
// parse (eg. large terms query) since Analyzer.normalize involves things like creating
// attributes through reflection
// This if statement will be used whenever a normalizer is NOT configured
/** Values that have more chars than the return value of this method will
// pkg-private for testing
// convert to utf8 only once before feeding postings/dv/stored fields
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// precision is only set iff: a. treeLevel is not explicitly set, b. its explicitly set
// setup prefix trees regardless of strategy (this is used for the QueryBuilder)
// recursive:
// term:
// set default (based on strategy):
// field mapper handles this at build time
// but prefix tree strategies require a name, so throw a similar exception
// setup the deprecated parameters and the prefix tree configuration
// these are built when the field type is frozen
// prevent user from changing strategies
// prevent user from changing trees (changes encoding)
// TODO we should allow this, but at the moment levels is used to build bookkeeping variables
// in lucene's SpatialPrefixTree implementations, need a patch to correct that first
// defaults only make sense if precision is not specified
// defaults only make sense if tree levels are not specified
// For TERMs strategy the defaults for points only change to true
/*
//www.apache.org/licenses/LICENSE-2.0
// index configured for pointsOnly
// MULTIPOINT data: index each point separately
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: remove this docvalues flag and use docValuesType
// for sending null value to _all field
/**
// TODO: we need to override freeze() and add safety checks that all settings are actually set
/** Returns the name of this type, as would be specified in mapping properties */
/** Checks this type is the same type as other. Adds a conflict if they are different. */
/**
// TODO: should be validating if index options go "up" (but "down" is ok)
// null and "default"-named index analyzers both mean the default is used
/** Returns the value that should be added when JSON null is found, or null if no value should be added */
/** Returns the null value stringified or null if there is no null value */
/** Sets the null value and initializes the string version */
/** Given a value that comes from the stored fields API, convert it to the
/** Returns true if the field is searchable.
/** Returns true if the field is aggregatable.
/** Generates a query that will only match documents that contain the given value.
// TODO: Standardize exception types
/** Build a constant-scoring query that matches all values. The default implementation uses a
/**
/**
/**
/** Return whether all values of the given {@link IndexReader} are within the range,
/** @throws IllegalArgumentException if the fielddata is not supported on this type.
// we throw an IAE rather than an ISE so that it translates to a 4xx code rather than 5xx code on the http layer
/** Return a {@link DocValueFormat} that can be used to display and parse
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Returns a newly built mapper. */
/** Returns the simple name, which identifies this mapper against other mappers at the same level in the mappers hierarchy
/** Returns the canonical name which uniquely identifies the mapper against other mappers in a type. */
/**
/** Return the merge of {@code mergeWith} into this.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// maximum allowed number of nested json objects across all fields in a single document
//TODO this needs to be cleaned up: _timestamp and _ttl are not supported anymore, _field_names, _seq_no, _version and _source are
//also missing, not sure if on purpose. See IndicesModule#getMetadataMappers
// updated dynamically to true when a nested object is added
/**
/**
// only update entries if needed
// refresh mapping can happen when the parsing/merging of the mapping from the metadata doesn't result in the same
// mapping, in this case, we send to the master to refresh its own version of the mappings (to conform with the
// merge version of it, which it does when refreshing the mappings), and warn log it.
// if the mapping version is unchanged, then there should not be any updates and all mappings should be the same
// if the mapping version is changed, it should increase, there should be updates, and the mapping should be different
// check naming
// compute the merged DocumentMapper
// check basic sanity of the new mapping
// update lookup data-structures
// first time through the loops
// this check will only be performed on the master node when there is
// a call to the update mapping API. For all other cases like
// the master node restoring mappings from disk or data nodes
// deserializing cluster state that was sent by the master node,
// this check will be skipped.
// Also, don't take metadata mappers into account for the field limit check
// this check will only be performed on the master node when there is
// a call to the update mapping API. For all other cases like
// the master node restoring mappings from disk or data nodes
// deserializing cluster state that was sent by the master node,
// this check will be skipped.
// only need to immutably rewrap these if the previous reference was changed.
// if not then they are already implicitly immutable.
// commit the change
// capture the source now, it may change due to concurrent parsing
/**
/**
/**
/**
/**
/**
// no wildcards
/**
/**
// There is no need to synchronize writes here. In the case of concurrent access, we could just
// compute some mappers several times, which is not a big deal
/**
/**
/** An analyzer wrapper that can lookup fields within the index mappings */
// refresh indexAnalyzers and search analyzers
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// root mapper isn't really an object mapper
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// keep root mappers sorted for consistent serialization
/** Return the root object mapper. */
/**
/** Get the root mapper with the given class. */
/** @see DocumentMapper#merge(Mapping) */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// TODO: remove the fieldType parameter which is only used for bw compat with pre-2.0
// since settings could be modified
/**
/**
// do nothing
/*
//www.apache.org/licenses/LICENSE-2.0
/** A {@link FieldMapper} for numeric types: byte, short, int, long, float and double. */
// if the lower bound is decimal:
// - if the bound is positive then we increment it:
//      if lowerTerm=1.5 then the (inclusive) bound becomes 2
// - if the bound is negative then we leave it as is:
//      if lowerTerm=-1.5 then the (inclusive) bound becomes -1 due to the call to longValue
// this check does not guarantee that value is inside MIN_VALUE/MAX_VALUE because values up to 9223372036854776832 will
// be equal to Long.MAX_VALUE after conversion to double. More checks ahead.
// longs need special handling so we don't lose precision while parsing
// if the lower bound is decimal:
// - if the bound is positive then we increment it:
//      if lowerTerm=1.5 then the (inclusive) bound becomes 2
// - if the bound is negative then we leave it as is:
//      if lowerTerm=-1.5 then the (inclusive) bound becomes -1 due to the call to longValue
/** Get the associated type name. */
/** Get the associated numeric type */
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// not set, inherited from root
// nothing to do here, empty (to support "properties: []" case)
// Should accept empty arrays, as a work around for when the
// user can't provide an empty Map. (PHP for example)
// lets see if we can derive this...
// if there is a single property with the enabled
// flag on it, make it an object
// (usually, setting enabled to false to not index
// any type, including core values, which
/**
// reset the sub mappers
/**
/**
// no mapping, simply add it
// root mappers can only exist here for backcompat, and are merged in Mapping
// only write the object content type if there are no properties, otherwise, it is automatically detected
// sort the mappers so we get consistent serialization format
/*
//www.apache.org/licenses/LICENSE-2.0
/** Fork of {@link org.apache.lucene.document.Document} with additional functionality. */
/**
/**
/**
// either a meta fields or starts with the prefix
/** Add fields so that they can later be fetched using {@link #getByKey(Object)}. */
/** Get back fields that have been previously added with {@link #addWithKey(Object, IndexableField)}. */
/**
// We preserve the order of the children while ensuring that parents appear after them.
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** A {@link FieldMapper} for indexing numeric and date ranges, and creating queries */
// this is private since it has a different default
// the resolution here is always set to milliseconds, as aggregations use this formatter mainly and those are always in
// milliseconds. The only special case here is docvalue fields, which are handled somewhere else
// create the lower value by zeroing out the host portion, upper value by filling it with all ones.
/** Class defining a range */
/*
//www.apache.org/licenses/LICENSE-2.0
/** Enum defining the type of range */
// TODO: Implement this.
// todo support half_float
// todo add BYTE support
// todo add SHORT support
/** Get the associated type name. */
/**
// wrong argument order, this is an error the user should fix
/** parses from value. rounds according to included flag */
/** parses to value. rounds according to included flag */
// No need to take into account Range#includeFrom or Range#includeTo, because from and to have already been
// rounded up via parseFrom and parseTo methods.
/**
// the first bit encodes the sign and the next 4 bits encode the number
// of additional bytes
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
// for dynamic updates, no need to carry root-specific options, we just
// set everything to they implicit default value so that they are not
// applied at merge time
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// no need ot parse here, we either get the routing in the sourceToParse
// or we don't have routing, if we get it in sourceToParse, we process it in preParse
// which will always be called
// if all are defaults, no sense to write it at all
// do nothing here, no merging, but also no exception
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// see InternalEngine.innerIndex to see where the real version value is set
// also see ParsedDocument.updateSeqID (called by innerIndex)
// fields are added in parseCreateField
// In the case of nested docs, let's fill nested docs with the original
// so that Lucene doesn't write a Bitset for documents that
// don't have the field. This is consistent with the default value
// for efficiency.
// we share the parent docs fields to ensure good compression
// nothing to do
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We do not fail on non-null time zones and date parsers
// The reasoning is that on query parsers, you might want to set a time zone or format for date fields
// but then the API has no way to know which fields are dates and which fields are not dates
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// not indexed
/** indicates whether the source will always exist and be complete, for use by features like the update API */
// Only stored.
// nothing to do here, we will call it in pre parse
// if we omitted source or modified it we add the _recovery_source to ensure we have it for ops based recovery
// Percolate and tv APIs may not set the source and that is ok, because these APIs will not index any data
// we don't update the context source if we filter, we want to keep it as is...
// all are defaults, no need to write it at all
/*
//www.apache.org/licenses/LICENSE-2.0
// we always convert back to byte array, since we store it and Field only supports bytes..
// so, we might as well do it here, and improve the performance of working with direct byte arrays
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Base class for {@link MappedFieldType} implementations that use the same
/*
//www.apache.org/licenses/LICENSE-2.0
/** Base {@link MappedFieldType} implementation for a field that is indexed
/** Returns the indexed value used to construct search "values".
/*
//www.apache.org/licenses/LICENSE-2.0
/** A {@link FieldMapper} for full-text fields. */
/**
/**
// Copy the index options of the main field to allow phrase queries on
// the prefix field.
// frequencies are not needed because prefix queries always use a constant score
// we can't use the index_phrases shortcut with slop, if there are gaps in the stream,
// or if the incoming token stream is the output of a token graph due to
// https://issues.apache.org/jira/browse/LUCENE-8916
/*
//www.apache.org/licenses/LICENSE-2.0
// type filters are expected not to match nested docs
/**
// Same threshold as TermInSetQuery
// this _type is not present in the reader
// strict equality should be enough ?
// Matches all docs since _type is a single value field
// Using a match_all query will help Lucene perform some optimizations
// For instance, match_all queries as filter clauses are automatically removed
// we parse in pre parse
// do nothing here, no merging, but also no exception
/*
//www.apache.org/licenses/LICENSE-2.0
// check analyzers are allowed to work in the respective AnalysisMode
/**
/**
// no meta
/**
/*
// For indices created prior to 8.0, we only emit a deprecation warning and do not fail type parsing. This is to
// maintain the backwards-compatibility guarantee that we can always load indexes from the previous major version.
/*
//www.apache.org/licenses/LICENSE-2.0
// We are not lenient about padding chars ('=') otherwise
// 'xxx=' and 'xxx' could be considered the same id
// the last 2 symbols (12 bits) are encoding 1 byte (8 bits)
// so the last symbol only actually uses 8-6=2 bits and can only take 4 values
// The last 3 symbols (18 bits) are encoding 2 bytes (16 bits)
// so the last symbol only actually uses 16-12=4 bits and can only take 16 values
// number & 0x03 is always in [0,3]
/** With numeric ids, we just fold two consecutive chars in a single byte
// end marker
/** With base64 ids, we decode and prepend an escape char in the cases that
// Prepend a byte that indicates that the content is an utf8 string
/** Encode an id for storage in the index. This encoding is optimized for
// common for ids that come from databases with auto-increments
// common since it applies to autogenerated ids
// no need to copy if it's not a slice
/** Decode an indexed id back to its original form.
/** Decode an indexed id back to its original form.
/*
//www.apache.org/licenses/LICENSE-2.0
/** Mapper for the _version field. */
// see InternalEngine.updateVersion to see where the real version value is set
// _version added in preparse
// In the case of nested docs, let's fill nested docs with version=1 so that Lucene doesn't write a Bitset for documents
// that don't have the field. This is consistent with the default value for efficiency.
// nothing to do
/*
//www.apache.org/licenses/LICENSE-2.0
/** Total millis that large merges were stopped so that smaller merges would finish. */
/** Total millis that we slept during writes so merge IO is throttled. */
// Added in 2.0:
/**
/**
/**
/**
/**
/**
/**
/**
// Added in 2.0:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** registry of content types this query can be used with */
/** The default value for ignore_unmapped. */
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/** list of content types this shape query is compatible with */
/** builds the appropriate lucene shape query */
/** returns expected content type for this query */
/** writes the xcontent specific to this shape query */
/** creates a new ShapeQueryBuilder from the provided field name and shape builder */
/** creates a new ShapeQueryBuilder from the provided field name, supplier and indexed shape id*/
/** returns true if the provided field type is valid for this query */
/**
// It is safe to use EMPTY here because this never uses namedObject
/** local class that encapsulates xcontent parsed shape parameters */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Default for boost to apply to resulting Lucene query. Defaults to 1.0*/
/**
/**
/**
/**
/**
/**
/**
/**
//default impl returns the same as writeable name, but we keep the distinction between the two just to make sure
// we inherit the name
/**
/**
// we encountered '{}' for a query clause, it used to be supported, deprecated in 5.0 and removed in 6.0
// move to the next START_OBJECT
// Preserve the error message from 5.0 until we have a compellingly better message so we don't break BWC.
// This intentionally doesn't include the causing exception because that'd change the "root_cause" of any unknown query errors
//end_object of the specific query (e.g. match, multi_match etc.) element
//end_object of the query object
// Like Objects.requireNotNull(...) but instead throws a IllegalArgumentException
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Name of field to match against. */
/** Value to find matches for. */
/**
/**
/**
/**
/**
/**
/**
/**
/** Returns the field name used in this query. */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// ignore
// early termination when must clause is empty and optional clauses is returning MatchNoneQueryBuilder
// lets do some early termination and prevent any kind of rewriting if we have a mandatory query that is a MatchNoneQueryBuilder
// no need to include must_not (since there will be no hits for it)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// we won't match anyway
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Default multiplication factor for breaking ties in document scores.*/
/**
/**
/**
/**
/**
// return null if there are no queries at all
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// origin: number or string for date and date_nanos fields; string, array, object for geo fields
// NANOSECONDS
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// can only happen when no types exist, so no docs exist either
// the _field_names field also indexes objects, so we don't have to
// do any more work to support exists queries on whole objects
// The field does not exist as a leaf but could be an object so
// check for an object mapper
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Default maximum edit distance. Defaults to AUTO. */
/** Default number of initial characters which will not be “fuzzified”. Defaults to 0. */
/** Default maximum number of terms that the fuzzy query will expand to. Defaults to 50. */
/** Default as to whether transpositions should be treated as a primitive edit operation,
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Default type for executing this query (memory as of this writing). */
/**
/** Name of field holding geo coordinates to compute the bounding box on.*/
/** How to deal with incorrect coordinates.*/
/** How the query should be run. */
/**
/**
/**
// all corners are valid after above checks - make sure they are in the right relation
// we do not check longitudes as the query generation code can deal with flipped left/right values
/**
/**
// get the bounding box of the geohash and set topLeft and bottomRight
/**
/** Returns the top left corner of the bounding box. */
/** Returns the bottom right corner of the bounding box. */
/**
/**
/**
/**
/**
/**
/** Returns the execution type of the geo bounding box.*/
/** Returns the name of the field to base the bounding box computation on. */
/**
/**
// For everything post 2.0 validate latitude and longitude unless validation was explicitly turned off
// Special case: if the difference between the left and right is 360 and the right is greater than the left, we are asking for
// the complete longitude range so need to set longitude to the complete longitude range
// bottom (minLat), top (maxLat), left (minLon), right (maxLon)
// ignore deprecated coerce/ignoreMalformed settings if validationMethod is set
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Default for distance unit computation. */
/** Default for geo distance computation. */
/**
/** Distance from center to cover. */
/** Point to use as center. */
/** Algorithm to use for distance computation. */
/** How strict should geo coordinate validation be? */
/**
/**
/** Name of the field this query is operating on. */
/** Sets the center point for the query.
/**
/** Returns the center point of the distance query. */
/** Sets the distance from the center using the default distance unit.*/
/** Sets the distance from the center for this query. */
/** Sets the distance from the center for this query. */
/** Returns the distance configured as radius. */
/** Sets the center point for this query. */
/** Which type of geo distance calculation method to use. */
/** Returns geo distance calculation type to use. */
/** Set validation method for geo coordinates. */
/** Returns validation method for geo coordinates. */
/**
/**
// the json in the format of -> field : { lat : 30, lon : 12 }
// a String
// a Number
// For everything post 2.0, validate latitude and longitude unless validation was explicitly turned off
/*
//www.apache.org/licenses/LICENSE-2.0
/** Specifies how a geo query should be run. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** Sets the validation method to use for geo coordinates. */
/** Returns the validation method to use for geo coordinates. */
/**
/**
// validation was not available prior to 2.x, so to support bwc
// percolation queries we only ignore_malformed on 2.x created indexes
// if GeoValidationMethod was explicitly set ignore deprecated coerce and ignoreMalformed settings
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Returns whether or not to skip bounding box validation. */
/** Returns whether or not to try and fix broken/wrapping bounding boxes. */
/** Returns validation method corresponding to given coerce and ignoreMalformed values. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// nothing to do
/**
// types no longer relevant so ignore
// types not supported so send an empty array to previous versions
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// phase 1: read through the stream and assess the situation:
// counting the number of tokens/positions and marking if we have any synonyms.
// phase 2: based on token count, presence of synonyms, and options
// formulate a single term, boolean, or phrase.
// single term
// graph
// phrase
// phrase with single-term synonyms
// simple phrase
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Be lenient with unmapped fields so that cross-index search will work nicely
// Be lenient with unmapped fields so that cross-index search will work nicely
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO Intervals.wildcard() should take BytesRef
/*
//www.apache.org/licenses/LICENSE-2.0
// this strategy doesn't support disjoint anymore: but it did
// before, including creating lucene fieldcache (!)
// in this case, execute disjoint as exists && !intersects
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// only superclass has state
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Returns the field name used in this query. */
/** Returns the value used in this query. */
/** Get the analyzer to use, if previously set, otherwise {@code null} */
/**
/** Sets the operator to use when using a boolean query. Defaults to {@code OR}. */
/** Returns the operator to use in a boolean query.*/
/** Sets optional minimumShouldMatch value to apply to the query */
/** Gets the minimumShouldMatch value */
/** Sets the fuzziness used when evaluated to a fuzzy query type. Defaults to "AUTO". */
/**  Gets the fuzziness used when evaluated to a fuzzy query type. */
/**
/**
/**
/**
/**
/** Gets the fuzzy query transposition setting. */
/** Sets the fuzzy_rewrite parameter controlling how the fuzzy query will get rewritten */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// all state is in the superclass
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** Returns the field name used in this query. */
/** Returns the value used in this query. */
/**
/** Get the analyzer to use, if previously set, otherwise {@code null} */
/** Sets a slop factor for phrase queries */
/** Get the slop factor for phrase queries. */
/**
/**
// validate context specific fields
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** Returns the field name used in this query. */
/** Returns the value used in this query. */
/**
/** Get the analyzer to use, if previously set, otherwise {@code null} */
/** Sets a slop factor for phrase queries */
/** Get the slop factor for phrase queries. */
/**
// validate context specific fields
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The name for the match query */
/** The default mode terms are combined in a match query */
/**
/**
// optional fields
// cutoff_frequency has been removed
// optional fields
// cutoff_frequency has been removed
/** Returns the field name used in this query. */
/** Returns the value used in this query. */
/** Sets the operator to use when using a boolean query. Defaults to {@code OR}. */
/** Returns the operator to use in a boolean query.*/
/**
/** Get the analyzer to use, if previously set, otherwise {@code null} */
/** Sets the fuzziness used when evaluated to a fuzzy query type. Defaults to "AUTO". */
/**  Gets the fuzziness used when evaluated to a fuzzy query type. */
/**
/**
/**
/**
/** Sets optional minimumShouldMatch value to apply to the query */
/** Gets the minimumShouldMatch value */
/** Sets the fuzzy_rewrite parameter controlling how the fuzzy query will get rewritten */
/**
/**
/** Gets the fuzzy query transposition setting. */
/**
/**
/**
/**
/**
// LUCENE 4 UPGRADE we need to document this & test this
// validate context specific fields
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no boost terms
// document inputs
// term selection parameters
// query formation parameters
// other parameters
/**
/**
/**
/**
// types no longer relevant so ignore
// types not supported so send an empty array to previous versions
/**
/**
// ensures these following parameters are never set
// for artificial docs to make sure that the id has changed in the item too
/**
// otherwise we are comparing pointers
/**
/**
// TODO we allow null here for the _all field, but this is forbidden in the parser. Re-check
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// document inputs
// term selection parameters
// query formation parameters
// other parameters
// set similarity
// set query parameters
// sets boost terms
// set analyzer
// set like text fields
// skip
// handle like texts
// handle items
// set default index, type and fields if not specified
// fetching the items with multi-termvectors API
// getting the Fields for liked items
// getting the Fields for unliked items
// exclude the items from the search
// default fields if not present but don't override for artificial docs
// no mappings, nothing to exclude
// artificial docs get assigned a random id and should be disregarded
// otherwise we are comparing pointers
// TODO this needs heavy cleanups before we can rewrite it
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// no fields provided, defaults to index.query.default_field
// Sets leniency to true if not explicitly
// set in the request
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// Lucene uses 'total' but 'sum' is more consistent with other elasticsearch APIs
// ToParentBlockJoinQuery requires that the inner query only matches documents
// in its child space
// With nested inner hits the nested docs are always in the same segement, so need to use the other segments
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Special-case optimisation for canMatch phase:  
// We can skip querying this shard if the index name doesn't match the value of this query on the "_index" field.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// BOOLEAN is the default
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/** Creates a new {@code span_within} builder.
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// make a copy to prevent concurrent modification exception
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// This might be a good use case for CopyOnWriteHashMap
/**
/**
/**
/**
/** Compile script using script service */
/**
/**
/**
/**
// we somebody uses a terms filter with lookup for instance can't be cached...
/**
/**
/** Return the current {@link IndexReader}, or {@code null} if no index reader is available,
/** Return the current {@link IndexSearcher}, or {@code null} if no index reader is available,
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
//issues.apache.org/jira/browse/LUCENE-6305 is fixed.
/** To limit effort spent determinizing regexp queries. */
/**
/**
/**
/**
/**
/** Returns the fields including their respective boosts to run the query against. */
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
//save the BoostQuery wrapped structure if present
//restore the previous BoostQuery wrapping
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// for testing
/**
// this just ensure that the pattern is actually valid, no need to keep it here
/**
// pkg private for testing
// Overridable for testing only
// If the context is null we are not on the shard and cannot
// rewrite so just pretend there is an intersection so that the rewrite is a noop
// no field means we have no values
// Percolator queries get rewritten and pre-processed at index time.
// If a range query has a date range using 'now' and 'now' gets resolved at index time then
// the pre-processing uses that to pre-process. This can then lead to mismatches at query time.
/**
// Exists query would fail if the fieldNames field is disabled.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/** Returns the field name used in this query. */
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// this is some protection against user provided queries if they don't obey the contract of rewrite we allow 16 rounds
// and then we fail to prevent infinite loops
/**
/**
// this is some protection against user provided queries if they don't obey the contract of rewrite we allow 16 rounds
// and then we fail to prevent infinite loops
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// also, when caching, since its isCacheable is false, will result in loading all bit set...
// TODO: how can we compute this?
// TODO: Change this to true when we can assume that scripts are pure functions
// ie. the return value is always the same given the same conditions and may not
// depend on the current timestamp, other documents, etc.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html"
/** Default for using lenient query parsing.*/
/** Default for wildcard analysis.*/
/** Default for default operator to use for linking boolean clauses.*/
/** Default for search flags to use. */
/** Default for prefix length in fuzzy queries.*/
/** Default number of terms fuzzy queries will expand to.*/
/** Default for using transpositions in fuzzy queries.*/
/** Name for (de-)serialization. */
/** Query text to parse. */
/**
/** If specified, analyzer to use to parse the query text, defaults to registered default in toQuery. */
/** Default operator to use for linking boolean clauses. Defaults to OR according to docs. */
/** If result is a boolean query, minimumShouldMatch parameter to apply. Ignored otherwise. */
/** Any search flags to be used, ALL by default. */
/** Whether or not the lenient flag has been set or not */
/** Further search settings needed by the ES specific query string parser only. */
/** Construct a new simple query with this query string. */
/**
/** Returns the text to parse the query from. */
/** Add a field to run the query against. */
/** Add a field to run the query against with a specific boost. */
/** Add several fields to run the query against with a specific boost. */
/** Returns the fields including their respective boosts to run the query against. */
/** Specify an analyzer to use for the query. */
/** Returns the analyzer to use for the query. */
/**
/** Returns the default operator for the query. */
/**
/** For testing and serialisation only. */
/** For testing only: Return the flags set for this query. */
/**
/**
/** Specifies whether query parsing should be lenient. Defaults to false. */
/** Returns whether query parsing should be lenient. */
/** Specifies whether wildcards should be analyzed. Defaults to false. */
/** Returns whether wildcards should by analyzed. */
/**
/**
/**
/**
// Possible options are:
// ALL, NONE, AND, OR, PREFIX, PHRASE, PRECEDENCE, ESCAPE, WHITESPACE, FUZZY, NEAR, SLOP
// ignore, deprecated setting
// ignore, deprecated setting
// Ignore deprecated option
// Query text is required
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// NEAR and SLOP are synonymous, since "slop" is a more familiar term than "near"
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Default for flag controlling whether matches are required to be in-order */
/** Default slop value, this is the same that lucene {@link SpanNearQuery} uses if no slop is provided */
/**
/**
/**
/**
/**
/**
/**
/*
/**
/** Name of field to match against. */
/** Width of the gap introduced. */
/**
//lucene has not coded any restriction on value of width.
//to-do : find if theoretically it makes sense to apply restrictions.
/**
/**
/**
//copied from AbstractQueryBuilder
/*
//www.apache.org/licenses/LICENSE-2.0
/** the default pre parameter size */
/** the default post parameter size */
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// utility class
/**
// if boost is unsupported it can't have been set
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, String) */
/** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, int) */
/** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, long) */
/** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, float) */
/** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, double) */
/** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, Object) */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, String) */
/** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, int) */
/** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, long) */
/** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, float) */
/** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, double) */
/** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, boolean) */
/** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, Object) */
/**
// Special-case optimisation for canMatch phase:  
// We can skip querying this shard if the index name doesn't match the value of this query on the "_index" field.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// we do not convert longs, all integer types are equivalent
// as far as this query is concerned
// extract terms only if the doc source exists
// Special-case optimisation for canMatch phase:  
// We can skip querying this shard if the index name doesn't match any of the search terms.
// We can match - at least one index name matches
// all index names are invalid - no possibility of a match on this shard.
/*
//www.apache.org/licenses/LICENSE-2.0
// package protected for testing purpose
// Fail before we attempt to create the term queries:
/**
// TODO: Change this to true when we can assume that scripts are pure functions
// ie. the return value is always the same given the same conditions and may not
// depend on the current timestamp, other documents, etc.
// Forked from LongValuesSource.FieldValuesSource and changed getValues() method to always use sorted numeric
// doc values, because that is what is being used in NumberFieldMapper.
/*
//www.apache.org/licenses/LICENSE-2.0
// CONTAINS queries are not supported by VECTOR strategy for indices created before version 7.5.0 (Lucene 8.3.0)
// wrap geoQuery as a ConstantScoreQuery
// all shapes must be disjoint / must be contained in relation to the indexed shape.
// at least one shape must intersect / contain the indexed shape.
// Flatten multi-points
// We do not support multi-point queries?
// contains and intersects are equivalent but the implementation of
// intersects is more efficient.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Special-case optimisation for canMatch phase:  
// We can skip querying this shard if the index name doesn't match the value of this query on the "_index" field.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// otherwise we compare pointers
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//parsing of origin, scale, offset and decay depends on the field type, delayed to the data node that has the mapping for it
/**
/**
/**
// EMPTY is safe because parseVariable doesn't use namedObject
/**
//the field must exist, else we cannot read the value for the doc later
// dates and time and geo need special handling
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// For better readability of error message
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// handle cases where only one score function and no filter was provided. In this case we create a FunctionScoreQuery.
// in all other cases we create a FunctionScoreQuery with filters
/**
/**
// Either define array of functions and filters or only one function
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// note that we already computed scale^2 in processScale() so we do
// not need to square it here.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// DocID-based random score generation
// no mappings: the index is empty anyway
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// require the supply of the query, even the explicit supply of "match_all" query
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
//nothing to do here, weight will be applied by the parent class, no score function
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Set the defaults which differ from SearchRequest's defaults.
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Parent task will store result
// Split requests per second between all slices
// Sub requests don't have workers
// maxDocs is split between workers. This means the maxDocs might round
// down!
// Set the parent task so this task is cancelled if we cancel the parent
// TODO It'd be nice not to refresh on every slice. Instead we should refresh after the sub requests finish.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// since the result of BulkByScrollResponse.Status are mixed we also parse that in this
/**
/**
/**
/**
/**
/**
// This field is the same as SearchFailure.index
// Do nothing
// Do nothing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
// Not present during deleteByQuery
// Not present during updateByQuery
/**
/**
/**
// No need for inner level fields for retries in the set of outer level fields
/**
// Hasn't returned yet.
// This slice failed catastrophically so it doesn't count towards the status
/**
// else if it is a value
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// This loop is present only to ignore unknown tokens. It breaks as soon as we find a field
// that is allowed.
// weird way to ignore unknown tokens
// Ignore unknown tokens
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Add the wait time into the scroll timeout so it won't timeout while we wait for throttling
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Delete-By-Query does not require the source
/**
/**
/**
/**
/**
/**
//delete by query deletes all documents that match a query. The indices and indices options that affect how
//indices are resolved depend entirely on the inner search request. That's why the following methods delegate to it.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// We only have access to the statuses of requests that have finished so we return them
/**
/**
/**
/* If the request isn't finished we could automatically rethrottle the sub-requests here but we would only want to do that if we
/**
// TODO cancel when a slice fails?
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// build source
// build destination
// Other fields
// Funky hack to work around Search not having a proper ObjectParser and us wanting to extract query if using remote.
// avoid silently accepting an ignored size.
/**
// URI has less stringent URL parsing than our code. We want to fail if all values are not provided.
//[host]:[port](/[pathPrefix])? but was ["
// We just checked....
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// public for testing
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// http is the default so it isn't worth taking up space if it is the scheme
//" + host + ":" + port +
/*
//www.apache.org/licenses/LICENSE-2.0
// schedule does not preserve context so have to do this manually
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// package private for tests.
// following is the SPI to be implemented.
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
//update by query updates all documents that match a query. The indices and indices options that affect how
//indices are resolved depend entirely on the inner search request. That's why the following methods delegate to it.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// Drop the throttle to 0, immediately rescheduling any throttle operation so it will wake up and cancel itself.
/**
// Synchronize so we are less likely to schedule the same request twice.
/**
//       requests
// ------------------- == seconds
// request per seconds
// nanoseconds per seconds * seconds == nanoseconds
/**
// No request has been queued so nothing to reschedule.
/* The user is attempting to slow the request down. We'll let the
// Actually reschedule the task
// Couldn't cancel, probably because the task has finished or been scheduled. Either way we have nothing to do here.
/* Strangely enough getting here doesn't mean that you actually
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** A {@link ToParentBlockJoinQuery} that allows to retrieve its nested path. */
/** Return the child query. */
/** Return the path of results of this query, or {@code null} if matches are at the root level. */
/** Return the score mode for the matching children. **/
// Right now ToParentBlockJoinQuery always rewrites to a ToParentBlockJoinQuery
// so the else block will never be used. It is useful in the case that
// ToParentBlockJoinQuery one day starts to rewrite to a different query, eg.
// a MatchNoDocsQuery if it realizes that it cannot match any docs and rewrites
// to a MatchNoDocsQuery. In that case it would be fine to lose information
// about the nested path.
// Highlighters must visit the child query to extract terms
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// this is used internally to make sure that query_string and simple_query_string
// ignores query part that removes all tokens.
// this field is a concrete field or an alias so we use the
// field type name directly
/*
/**
/**
/**
// Build an appropriate query based on the analysis chain.
// phase 1: read through the stream and assess the situation:
// counting the number of tokens/positions and marking if we have any synonyms.
// phase 2: based on token count, presence of synonyms, and options
// formulate a single term, boolean, or phrase.
// single term
// graph
// phrase
// complex phrase with synonyms
// simple phrase
// phrase prefix
// boolean
// only one position, with synonyms
// complex case: multiple positions
// Use the analyzer to get all the tokens, and then build an appropriate
// query based on the analysis chain.
/*
/**
// We don't apply prefix on synonyms
// We don't apply prefix on synonyms
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// all query fields are unmapped
// ignore unmapped fields
/*
// apply the field boost to groups that contain a single field
// best effort: add clauses that are not term queries so that they have an opportunity to match
// however their score contribution will be different
// TODO: can we improve this?
/*
//www.apache.org/licenses/LICENSE-2.0
/** Utility class to filter parent and children clauses when building nested
/** Returns true if the given query might match nested documents. */
// We only handle term(s) queries and range queries, which should already
// cover a high majority of use-cases
/** Returns true if a query on the given field might match nested documents. */
// meta field. Every meta field behaves differently, eg. nested
// documents have the same _uid as their parent, put their path in
// the _type field but do not have _field_names. So we just ignore
// meta fields and return true, which is always safe, it just means
// we might add a nested filter when it is nor required.
// field does not exist
/** Returns true if the given query might match parent documents or documents
/** Returns true if a query on the given field might match parent documents
// meta field. Every meta field behaves differently, eg. nested
// documents have the same _uid as their parent, put their path in
// the _type field but do not have _field_names. So we just ignore
// meta fields and return true, which is always safe, it just means
// we might add a nested filter when it is nor required.
// If the mapper does not include in its parent or in the root object then
// the query might only match nested documents with the given path
// the first parent nested mapper does not have the expected path
// It might be misconfiguration or a sub nested mapper
// the field is not a sub field of the nested path
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// handle duplicates
/**
/**
/**
// Note that we don't ignore unmapped fields.
// Ignore metadata fields
// field type is never searchable with term queries (eg. geo point): ignore
// other exceptions are parsing errors or not indexed fields: keep
// Deduplicate aliases and their concrete fields.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Force the tie breaker in the query builder too
/**
// "*" is the default field
// Filters unsupported fields if a pattern is requested
// Filters metadata fields if all fields are requested
// Detects additional operators '<', '<=', '>', '>=' to handle range query with one side unbounded.
// It is required to use a prefix field operator to enable the detection since they are not treated
// as logical operator by the query parser (e.g. age:>=10).
// the requested fields do not match any field in the mapping
// happens for wildcard fields only since we cannot expand to a valid field name
// if there is no match in the mappings.
// get Analyzer from superclass and tokenize the term
// build a boolean query with prefix on the last position only.
// build a synonym query for terms in the same position.
// The field_names_field is disabled so we switch to a wildcard query that matches all terms
// effectively, we check if a field exists or not
// effectively, we check if a field exists or not
//make sure that the boost hasn't been set beforehand, otherwise we'd lose it
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Creates a new parser with custom flags used to enable/disable certain features. */
/** Creates a new parser with custom flags used to enable/disable certain features. */
/**
/**
// ignore
// TODO: we should not ignore the exception and return a prefix query with the original term ?
// Bail on any exceptions, going with a regular prefix query
// build a boolean query with prefix on the last position only.
// build a synonym query for terms in the same position.
/**
/** Specifies whether lenient query parsing should be used. */
/** Specifies whether wildcards should be analyzed. */
/** Specifies a suffix, if any, to apply to field names for phrase matching. */
/** Whether phrase queries should be automatically generated for multi terms synonyms. */
/** Prefix length in fuzzy queries.*/
/** The number of terms fuzzy queries will expand to.*/
/** Whether transpositions are supported in fuzzy queries.*/
/**
/** Specifies whether to use lenient parsing, defaults to false. */
/** Returns whether to use lenient parsing. */
/** Specifies whether to analyze wildcards. Defaults to false if unset. */
/** Returns whether to analyze wildcards. */
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// for internal use, initializes all counts to 0
// need consider the count of the shard's current scroll
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/* We store scroll statistics in microseconds because with nanoseconds we run the risk of overflowing the total stats if there are
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Number of bits are set.
// Ignore set when bitset is full.
// Once all bits are set, we can simply just return YES for all indexes.
// This allows us to clear the internal bitset and use null check as the guard.
// Below methods are pkg-private for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// make sure we track highest seen sequence number
// this is possible during recovery where we might replay an operation that was also replicated
/**
/**
/**
/**
/**
// notified by updateCheckpoint
/**
// check again under lock
/**
// keep it simple for now, get the checkpoint one by one; in the future we can optimize and read words
// the bit set corresponding to the checkpoint has already been removed, set ourselves up for the next bit set
/*
// notifies waiters in waitForProcessedOpsToComplete
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// the primary calculates the non-expired retention leases and syncs them to replicas
// early out as no retention leases have expired
// NB safeCommitInfo.docCount is a very low-level count of the docs in the index, and in particular if this shard contains nested
// docs then safeCommitInfo.docCount counts every child doc separately from the parent doc. However every part of a nested document
// has the same seqno, so we may be overestimating the cost of a file-based recovery when compared to an ops-based recovery and
// therefore preferring ops-based recoveries inappropriately in this case. Correctly accounting for nested docs seems difficult to
// do cheaply, and the circumstances in which this matters should be relatively rare, so we use this naive calculation regardless.
// TODO improve this measure for when nested docs are in use
/**
/**
// Syncing here may not be strictly necessary, because this new lease isn't retaining any extra history that wasn't previously
// retained by the source lease; however we prefer to sync anyway since we expect to do so whenever creating a new lease.
/**
/**
/**
/**
/**
// TODO after backporting we expect this never to happen in 8.x, so adjust this to throw an exception instead.
/**
/**
/**
/**
/**
/**
/**
/*
/*
/*
// the retention lease is tied to the node, not the shard copy, so it's possible a copy was removed and now
// we are in the process of recovering it again, or maybe we were just promoted and have not yet received the
// global checkpoints from our peers.
/**
/**
/**
/**
/**
/**
// upper bound on the size
/**
/**
/**
/**
/**
// local checkpoints only set during primary mode
// global checkpoints only set during primary mode
// relocation handoff can only occur in primary mode
// a relocated copy is not in primary mode
// the current shard is marked as in-sync when the global checkpoint tracker operates in primary mode
// the routing table and replication group is set when the global checkpoint tracker operates in primary mode
// when in primary mode, the current allocation ID is the allocation ID of the primary or the relocation allocation ID
// during relocation handoff there are no entries blocking global checkpoint advancement
// entries blocking global checkpoint advancement can only exist in primary mode and when not having a relocation handoff
// the computed global checkpoint is always up-to-date
// when in primary mode, the global checkpoint is at most the minimum local checkpoint on all in-sync shard copies
// we have a routing table iff we have a replication group
// all assigned shards from the routing table are tracked
// blocking global checkpoint advancement only happens for shards that are not in-sync
// in-sync shard copies are tracked
// all pending in sync shards are tracked
// all tracked shard copies have a corresponding peer-recovery retention lease
/**
/**
/**
/**
/*
/**
/**
/**
// Safe to call innerAddRetentionLease() without a subsequent sync since there are no other members of this replication
// group.
/*
// Although this index is old enough not to have all the expected peer recovery retention leases, in fact it does, so we
// don't need to do any more work.
/**
// check that the master does not fabricate new in-sync entries out of thin air once we are in primary mode
// remove entries which don't exist on master
// add new initializingIds that are missing locally. These are fresh shard copies - and not in-sync
// notify any waiter for local checkpoint advancement to recheck that their shard is still being tracked.
/**
// can happen if replica was removed from cluster but recovery process is unaware of it yet
/**
// can happen if replica was removed from cluster but recovery process is unaware of it yet
// if it was already in-sync (because of a previously failed recovery attempt), global checkpoint must have been
// stuck from advancing
// a local checkpoint for a shard copy should be a valid sequence number
/**
// can happen if replica was removed from cluster but replication process is unaware of it yet
/**
// unassigned in-sync replica
/**
/**
// can happen if the relocation target was removed from cluster but the recovery process isn't aware of that.
// copy clusterStateVersion and checkpoints and return
// all the entries from checkpoints that are inSync: the reason we don't need to care about initializing non-insync entries
// is that they will have to undergo a recovery attempt on the relocation target, and will hence be supplied by the cluster state
// update on the relocation target once relocation completes). We could alternatively also copy the map as-is (it’s safe), and it
// would be cleaned up on the target by cluster state updates.
/**
/**
// forget all checkpoint information
/**
// capture current state to possibly replay missed cluster state update
// reapply missed cluster state update
// note that if there was no cluster state update between start of the engine of this shard and the call to
// initializeWithPrimaryContext, we might still have missed a cluster state update. This is best effort.
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// node shutting down
// the shard is closed
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// use a linked hash map to preserve order
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ignore
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// note that the global checkpoint can be higher from both maxSeqNo and localCheckpoint
// as we use this stats object to describe lucene commits as well as live statistic.
/** the maximum sequence number seen so far */
/** the maximum sequence number for which all previous operations (including) have been persisted */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// True if the next merge request should do segment upgrades:
// True if the next merge request should only upgrade ancient (an older Lucene major version than current) segments;
/** @param delegate the merge policy to wrap */
/** return the wrapped merge policy */
// Something seriously wrong if this trips:
// Always upgrade segment if Lucene's major version is too old
// If it's only a minor version difference, and we are not upgrading only ancient segments,
// also upgrade:
// Version matches, or segment is not ancient and we are only upgrading ancient segments:
// TODO: Use IndexUpgradeMergePolicy instead.  We should be comparing codecs,
// for now we just assume every minor upgrade has a new format.
// TODO: we could check IndexWriter.getMergingSegments and avoid adding merges that IW will just reject?
// hit our max upgrades, so return the spec.  we will get a cascaded call to continue.
// We must have less than our max upgrade merges, so the next return will be our last in upgrading mode.
// Only set this once there are 0 segments needing upgrading, because when we return a
// spec, IndexWriter may (silently!) reject that merge if some of the segments we asked
// to be merged were already being (naturally) merged:
// fall through, so when we don't have any segments to upgrade, the delegate policy
// has a chance to decide what to do (e.g. collapse the segments to satisfy maxSegmentCount)
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// guarded by this
/**
/**
// notify directly
/*
/**
/**
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//When combining if one is throttled set result to throttled.
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Package visible for testing
// ensure happens-before relation between addRefreshListener() and postRecovery()
// see JavaDocs for getPendingPrimaryTerm
// lock ordering: engineMutex -> mutex
// for primaries, we only allow to write when actually started (so the cluster has decided we started)
// in case we have a relocation of a primary, we also allow to write after phase 2 completed, where the shard may be
// in state RECOVERING or POST_RECOVERY.
// for replicas, replication is also allowed while recovering, since we index also during recovery to replicas and rely on
// version checks to make sure its consistent a relocated shard can also be target of a replication if the relocation target has not
// been marked as active yet and is syncing it's changes back to the relocation source
/**
/**
/* create engine config */
// the query cache is a node-level thing, however we want the most popular filters
// to be computed on a per-shard basis
/**
/**
/** Returns the primary term that is currently being used to assign to operations */
/**
// if the shard is not in primary mode anymore (after primary relocation) we have to fail when any changes in shard
// routing occur (e.g. due to recovery failure / cancellation). The reason is that at the moment we cannot safely
// reactivate primary mode without risking two active primaries.
// the master started a recovering primary, activate primary mode.
/* Note that due to cluster state batching an initializing primary shard term can failed and re-assigned
/*
// to prevent primary relocation handoff while resync is not completed
/*
/* Rolling the translog generation is not strictly needed here (as we will never have collisions between
// ignore, shutting down
// okay, the index was deleted
// set this last, once we finished updating all internal state.
// note that we use started and not active to avoid relocating shards
// if permits are blocked, we are still transitioning
// include relocation targets
/**
/**
// no shard operation permits are being held here, move state from started to relocated
/*
// make changes to primaryMode and relocated flag only under mutex
// This is really bad as ongoing replication operations are preventing this shard from completing relocation hand-off.
// Fail primary relocation source and target shards.
/*
/**
// We treat any exception during parsing and or mapping update as a document level failure
// with the exception side effects of closing the shard. Since we don't have the shard, we
// can not raise an exception that may block any replication of previous operations to the
// replicas
// don't use index.source().utf8ToString() here source might not be valid UTF-8
/**
/**
/**
/**
/**
/*
/**
/**
/**
// we just want to upgrade the segments, not actually forge merge to a single segment
// we need to flush at the end to make sure the upgrade is durable
// we just want to upgrade the segments, not actually optimize to a single segment
/**
// one time volatile read
// we allow snapshot on closed index shard, since we want to do one after we close the shard and before we close the engine
/**
// one time volatile read
// we allow snapshot on closed index shard, since we want to do one after we close the shard and before we close the engine
/**
// if the engine is not running, we can access the store directly, but we need to make sure no one starts
// the engine on us. If the engine is running, we can get a snapshot via the deletion policy of the engine.
/**
// fail the engine. This will cause this shard to also be removed from the node's index service.
/**
// prevent that somebody wraps with a non-filter reader
// we close the reader to make sure wrappers can release resources if needed....
// our NonClosingReaderWrapper makes sure that our reader is not closed
// this will close the wrappers excluding the NonClosingReaderWrapper
// this will run the closeable on the wrapped engine reader
// don't close here - mimic the MultiReader#doClose = false behavior that FilterDirectoryReader doesn't have
// playing safe here and close the engine even if the above succeeds - close can be called multiple times
// Also closing refreshListeners to prevent us from accumulating any more listeners
// we need to refresh again to expose all operations that were index until now. Otherwise
// we may not expose operations that were indexed with a refresh listener that was immediately
// responded to in addRefreshListener. The refresh must happen under the same mutex used in addRefreshListener
// and before moving this shard to POST_RECOVERY state (i.e., allow to read from this shard).
/**
/**
// check index here and won't do it again if ops-based recovery occurs
// adjust the total local to reflect the actual count
// we need to find the safe commit again as we should have created a new one during the local recovery
/**
/**
// If a translog op is replayed on the primary (eg. ccr), we need to use external instead of null for its version type.
// we set canHaveDuplicates to true all the time such that we de-optimze the translog case and ensure that all
// autoGeneratedID docs that are coming from the primary are updated correctly.
/**
// TODO: Don't enable this leniency unless users explicitly opt-in
// mainly for MapperParsingException and Failure to detect xcontent
// we have to set it before we open an engine and recover from the translog because
// acquiring a snapshot from the translog causes a sync which causes the global checkpoint to be pulled in,
// and an engine can be forced to close in ctor which also causes the global checkpoint to be pulled in.
/**
/**
// we disable deletes since we allow for operations to be executed against the shard while recovering
// but we need to make sure we don't loose deletes until we are done recovering
// we must create a new engine under mutex (see IndexShard#snapshotStoreMetadata).
// We set active because we are now writing operations to the engine; this way,
// we can flush if we go idle after some time and become inactive.
// time elapses after the engine is created above (pulling the config settings) until we set the engine reference, during
// which settings changes could possibly have happened, so here we forcefully push any config changes to the new engine.
/**
/**
/**
/**
/**
/**
// one time volatile read
// one time volatile read
/** returns true if the {@link IndexShardState} allows reading */
// one time volatile read
// one time volatile read
// one time volatile read
/**
/**
// we are the first primary, recover from the gateway
// if its post api allocation, the index should exists
// we are the first primary, recover from the gateway
// if its post api allocation, the index should exists
/**
// we are already closed, no need to flush or roll
/**
// we are already closed, no need to flush or roll
// Off to the generic threadPool as pruning the delete tombstones can be expensive.
/**
/**
/**
/**
/**
/**
// ignore
// ignore
// ignore
// ignore, we are being shutdown
// ignore, we are being shutdown
// ignore, we are being shutdown
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// only sync if there are no operations in flight, or when using async durability
// async durability means that the local checkpoint might lag (as it is only advanced on fsync)
// periodically ask for the newest local checkpoint by syncing the global checkpoint, so that ultimately the global
// checkpoint can be synced. Also take into account that a shard might be pending sync, which means that it isn't
// in the in-sync set just yet but might be blocked on waiting for its persisted local checkpoint to catch up to
// the global checkpoint.
// check if the persisted global checkpoint
// only sync if index is not closed and there is a shard lagging the primary
/**
/**
/*
/**
// make changes to primaryMode flag only under mutex
/**
/**
// physical verification only: verify all checksums for the latest commit
// full checkindex
// ignore if closed....
/**
// TODO: Create a proper object to encapsulate the recovery context
// all of the current methods here follow a pattern of:
// resolve context which isn't really dependent on the local shards and then async
// call some external method with this pointer.
// with a proper recovery context object we can simply change this to:
// startRecovery(RecoveryState recoveryState, ShardRecoverySource source ) {
//     markAsRecovery("from " + source.getShortDescription(), recoveryState);
//     threadPool.generic().execute()  {
//           onFailure () { listener.failure() };
//           doRun() {
//                if (source.recover(this)) {
//                  recoveryListener.onRecoveryDone(recoveryState);
//                }
//           }
//     }}
// }
// mark the shard as recovering on the cluster state thread
/**
// only needed for BWC reasons involving rolling upgrades from versions that do not support PRRLs:
/**
// called by the current engine
// only persist metadata if routing information that is persisted in shard state metadata actually changed
/**
/**
/**
/**
// ignore, shard is already closed
// indexShardOperationPermits doesn't guarantee that async submissions are executed
// in the order submitted. We need to guard against another term bump
// otherwise leave it to combineWithAction to release the permit
/**
/**
// This listener is used for the execution of the operation. If the operation requires all the permits for its
// execution and the primary term must be updated first, we can combine the operation execution with the
// primary term update. Since indexShardOperationPermits doesn't guarantee that async submissions are executed
// in the order submitted, combining both operations ensure that the term is updated before the operation is
// executed. It also has the side effect of acquiring all the permits one time instead of two.
// only roll translog and update primary term if shard has made it past recovery
// Having a new primary term here means that the old primary failed and that there is a new primary, which again
// means that the master will fail this shard as all initializing shards are failed when a primary is selected
// We abort early here to prevent an ongoing recovery from the failed primary to mess with the global / local checkpoint
/**
/**
// that's fine since we already synced everything on engine close - this also is conform with the methods
// documentation
// if this fails we are in deep shit - fail the request
/**
/**
/**
// we can not protect with a lock since we "release" on a different thread
/**
/*
/**
/**
// for tests
/**
// if we have a listener that is waiting for a refresh we need to force it
// it must be active otherwise we might not free up segment memory once the shard became inactive
// lets skip this refresh since we are search idle and
// don't necessarily need to refresh. the next searcher access will register a refreshListener and that will
// cause the next schedule to refresh.
// try to prune the deletes in the engine if we accumulated some
// try to prune the deletes in the engine if we accumulated some
/**
/**
/**
// move the shard into non-search idle
/**
// check again under postRecoveryMutex. this is important to create a happens before relationship
// between the switch to POST_RECOVERY + associated refresh. Otherwise we may respond
// to a listener before a refresh actually happened that contained that operation.
// we're not yet ready fo ready for reads, just ignore refresh cycles
/**
// persist the global checkpoint to disk
// flush to make sure the latest commit, which will be opened by the read-only engine, includes all operations.
// we must create both new read-only engine and new read-write engine under engineMutex to ensure snapshotStoreMetadata,
// acquireXXXCommit and close works.
// ignore flushFirst since we flushed above and we do not want to interfere with ongoing translog replay
// we successfully installed the new engine so do not close it.
// TODO: add a dedicate recovery stats for the reset translog
// We set active because we are now writing operations to the engine; this way,
// if we go idle after some time and become inactive, we still give sync'd flush a chance to run.
// time elapses after the engine is created above (pulling the config settings) until we set the engine reference, during
// which settings changes could possibly have happened, so here we forcefully push any config changes to the new engine.
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// fair to ensure a blocking thread is not starved
// operations that are delayed
// does not need to be volatile as all accesses are done under a lock on this
// only valid when assertions are enabled. Key is AtomicBoolean associated with each permit to ensure close once semantics.
// Value is a tuple, with a some debug information supplied by the caller and a stack trace of the acquiring thread
/**
/**
/**
// resume delayed operations as soon as possible
// since delayed is not volatile, we have to synchronize even here for visibility
/*
/**
// execute this outside the synchronized block!
// the un-timed tryAcquire methods do not honor the fairness setting
// this should never happen, if it does something is deeply wrong
/**
// This occurs when blockOperations() has acquired all the permits.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// previously, 4 was the RELOCATED state
// +1 for RELOCATED state
// for backward compatibility reasons (this was the RELOCATED state)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/* this directory will not be used for anything else but reading / copying files to another directory
/* we do explicitly a no-lock instance since we hold an index commit from a SnapshotDeletionPolicy so we
/*
//www.apache.org/licenses/LICENSE-2.0
// for tests
// only settable for tests
// Wrap translog snapshot to make it synchronized as it is accessed by different threads through SnapshotSender.
// Even though those calls are not concurrent, snapshot.next() uses non-synchronized state and is not multi-thread-compatible
// Also fail the resync early if the shard is shutting down
// We must capture the timestamp after snapshotting a snapshot of operations to make sure
// that the auto_id_timestamp of every operation in the snapshot is at most this value.
// it's not transport :-)
// to track progress
// check if this request is past bytes threshold, and if so, send it off
// have to send sync request even in case of there are no operations to sync - have to sync trimmedAboveSeqNo at least
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
// Location already visible, just call the listener
// We have a free slot so register the listener
// No free slot so force a refresh and call the listener in this thread
// Fire any listeners we might have had
/**
// A null list doesn't need a refresh. If we're closed we don't need a refresh either.
/**
// No need to synchronize here because we're doing a single volatile read
// A null list means we haven't accumulated any listeners. Otherwise we need the size.
/**
/**
// Increment refresh metric before communicating to listeners.
/* We intentionally ignore didRefresh here because our timing is a little off. It'd be a useful flag if we knew everything that made
/* The translog had an empty last write location at the start of the refresh so we can't alert anyone to anything. This
/* Set the lastRefreshedLocation so listeners that come in for locations before that will just execute inline without messing
/* Grab the current refresh listeners and replace them with null while synchronized. Any listeners that come in after this won't be
// No listeners to check so just bail early
// Iterate the list of listeners, copying the listeners to fire to one list and those to preserve to another list.
/* Now deal with the listeners that it isn't time yet to fire. We need to do this under lock so we don't miss a concurrent close or
// Lastly, fire the listeners that are ready on the listener thread pool
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Visible for testing
// SHARD-ID path element check
// `indices` check
// otherwise resolve shardPath based on the index name and shard id
// Visible for testing
// keep the index lock to block any runs of older versions of this tool
////////// Index
////////// Translog
// translog relies on data stored in an index commit so we have to have a recoverable index to check the translog
////////// Drop corrupted data
// newHistoryCommit obtains its own lock
// commit the new history id
// we don't want merges to happen here - we call maybe merge on the engine
// later once we stared it up otherwise we would need to wait for it here
// we also don't specify a codec here and merges should use the engines for this index
// IndexWriter acquires directory lock by its own
// In order to have a safe commit invariant, we have to assign the global checkpoint to the max_seqno of the last commit.
// We can only safely do it because we will generate a new history uuid this shard.
// Also advances the local checkpoint of the last commit to its max_seqno.
// commit the new history id
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// derived from the other fields
// derived from the other fields
// derived from the other fields
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// also strip the indices folder
/**
// also strip the indices folder
/**
/**
/**
// EMPTY is safe here because we never call namedObject
/**
// EMPTY is safe here because we never call namedObject
// TODO: this is a hack!!  We should instead keep track of incoming (relocated) shards since we know
// how large they will be once they're done copying, instead of a silly guess for such cases:
// Very rough heuristic of how much disk space we expect the shard will use over its lifetime, the max of current average
// shard size across the cluster and 5% of the total available free space on this node:
// TODO - do we need something more extensible? Yet, this does the job for now...
// If no better path is chosen, use the one with the most space by default
// Compute how much space there is on each path
// Filter out paths that have enough space
// Sort by the number of shards for this index
// if the number of shards is equal, tie-break with the number of total shards
// if the number of shards is equal, tie-break with the usable bytes
// Return the first result
// Or the existing best path if there aren't any that fit the criteria
// This path has been determined to be "better" based on the usable bytes
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// this is the common case - no partitioning and no _routing values
// in this case we also don't do anything special with regards to nested docs since we basically delete
// by ID and parent and nested all have the same id.
// no matches
// this is the heaviest invariant. Here we have to visit all docs stored fields do extract _id and _routing
// this this index is routing partitioned.
// here we potentially guard the docID consumers with our parent bitset if we have one.
// this ensures that we are only marking root documents in the nested case and if necessary
// we do a second pass to mark the corresponding children in markChildDocs
// in the _routing case we first go and find all docs that have a routing value and mark the ones we have to delete
// now if we have a mixed index where some docs have a _routing value and some don't we have to exclude the ones
// with a routing value from the next iteration an delete / select based on the ID.
// this is a special case where some of the docs have no routing values this sucks but it's possible today
// if nested docs are involved we also need to mark all child docs that belong to a matching parent doc.
// This is not a regular query, let's not cache it. It wouldn't help
// anyway.
/* this class is a stored fields visitor that reads _id and/or _routing from the stored fields which is necessary in the case
// we don't support 5.x so no need for the uid field
/**
// we iterate all live-docs
// that's obvious, right?
/**
// we iterate all live-docs
// the educated reader might ask why this works, it does because all live doc ids (root docs and nested docs) are evaluated in
// order and that way we don't need to seek backwards as we do in other nested docs cases.
// we only check once per nested/parent set
// never check a child document against the visitor, they neihter have _id nor _routing as stored fields
// that's obvious, right?
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// can be null if we read from legacy format (see fromXContent and MultiDataPathUpgrader)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// now that the mapping is merged we can validate the index sort configuration.
// don't close this directory!!
// just trigger a merge to do housekeeping on the
// copied segments - we will also see them in stats etc.
// we don't want merges to happen here - we call maybe merge on the engine
// later once we stared it up otherwise we would need to wait for it here
// we also don't specify a codec here and merges should use the engines for this index
/*
/**
// here we wrap the index input form the source directory to report progress of file copy for the recovery stats.
// we increment the num bytes recovered in the readBytes method below, if users pull statistics they can see immediately
// how much has been recovered.
// we rely on the fact that copyFrom uses a buffer
// hardlinked - we treat it as reused since the file was already somewhat there
/**
// got closed on us, just ignore this recovery
// Check that the gateway didn't leave the shard in init or recovering stage. it is up to the gateway
// to call post recovery.
// got closed on us, just ignore this recovery
// got closed on us, just ignore this recovery
// got closed on us, just ignore this recovery
/**
// it exists on the directory, but shouldn't exist on the FS, its a leftover (possibly dangling)
// its a "new index create" API, we have to do something, so better to clean it than use same data
// since we recover from local, just fill the files and size
// not loaded yet
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// public for testing
// Escape hatch
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Sole constructor. */
/** Compute the part of the score that does not depend on the current document using the init_script. */
// Sum scores across terms like a BooleanQuery would do
/** Scoring factors that come from the query. */
/** The boost of the query. It should typically be incorporated into the score as a multiplicative factor. */
/** Statistics that are specific to a given field. */
/** Return the number of documents that have a value for this field. */
/** Return the sum of {@link Term#getDocFreq()} for all terms that exist in this field,
/** Return the sum of {@link Term#getTotalTermFreq()} for all terms that exist in this field,
/** Statistics that are specific to a given term. */
/** Return the number of documents that contain this term in the index. */
/** Return the total number of occurrences of this term in the index, or {@code -1} if this statistic is not available. */
/** Statistics that are specific to a document. */
/** Return the number of tokens that the current document has in the considered field. */
// the length is computed lazily so that similarities that do not use the length are
// not penalized
/** Return the number of occurrences of the term in the current document for the considered field. */
/*
//www.apache.org/licenses/LICENSE-2.0
/** Provider of scripted similarities. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// no instantiation
// TODO: be and g and both based on the bose-einstein model.
// Is there a better replacement for d and p which use the binomial model?
// l is simpler than b, so this should be a better replacement for "no"
/**
/**
/**
/**
/**
// used to figure out which sim this is
/*
//www.apache.org/licenses/LICENSE-2.0
// We don't trust custom similarities
// like similarity but final
// TODO we can maybe factor out MapperService here entirely by introducing an interface for the lookup?
// for testing
// length = 20, no overlap
// length = 20, no overlap
// length = 20, no overlap
// esoteric similarity, skip this check
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// The snapshot is done which means the number of processed files is the same as total
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// First and last-but-one parts have a size equal to partBytes
// Last part size is deducted from the length and the number of parts
/**
/**
/**
/**
/**
/**
/**
// Verify that file information is complete
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// for the sake of BWC keep the actual property names as in 6.x
// + there is a constraint in #fromXContent() that leads to ElasticsearchParseException("unknown parameter [incremental_file_count]");
/**
/**
// fresh parser? move to the first token
// The index-version is needed for backward compatibility with v 1.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Map between blob names and file info
// Map between original physical names and file info
// First we build map between filenames in the repo and their original file info
// this map will be used in the next loop
// We are doing it in two loops here so we keep only one copy of the fileInfo per blob
// the first loop de-duplicates fileInfo objects that were loaded from different snapshots but refer to
// the same blob
/**
/**
/**
/**
// First we list all blobs with their file infos:
// Then we list all snapshots with list of all blobs that are used by the snapshot
// New parser
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// ignore, the file is not there no more; on Windows, if one thread concurrently deletes a file while
// calling Files.size, you can also sometimes hit AccessDeniedException
// Both these variables need to be accessed under `this` lock.
// It is ok for the size of the directory to be more recent than
// the mod count, we would just recompute the size of the
// directory on the next call as well. However the opposite
// would be bad as we would potentially have a stale cache
// entry for a long time. So we fetch the values of modCount and
// numOpenOutputs BEFORE computing the size of the directory.
// Compute this OUTSIDE of the lock
// The size was computed recently, don't recompute
// The cached entry was generated while there were pending
// writes, so the size might be stale: recompute.
// If there are pending writes or if new files have been
// written/deleted since last time: recompute
/** Return the cumulative size of all files in this directory. */
// we wrapped in the cache and unwrap here
// Don't write to atomicXXX here since it might be called in
// tight loops and memory barriers are costly
// Don't write to atomicXXX here since it might be called in
// tight loops and memory barriers are costly
// Close might cause some data to be flushed from in-memory buffers, so
// increment the modification counter too.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// can we set on both - node and index level, some nodes might be running on NFS so they might need simple rather than native
// Use Lucene defaults
/**
// we need to do these checks on the outer directory since the inner doesn't know about pending deletes
// we only use the mmap to open inputs. Everything else is managed by the NIOFSDirectory otherwise
// we might run into trouble with files that are pendingDelete in one directory but still
// listed in listAll() from the other. We on the other hand don't want to list files from both dirs
// and intersect for perf reasons.
// We are mmapping norms, docvalues as well as term dictionaries, all other files are served through NIOFS
// this provides good random access performance and does not lead to page cache thrashing.
// TODO it would be nice to share code between PreLoadMMapDirectory and HybridDirectory but due to the nesting aspect of
// directories here makes it tricky. It would be nice to allow MMAPDirectory to pre-load on a per IndexInput basis.
// we need to do these checks on the outer directory since the inner doesn't know about pending deletes
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// use the store...
// public is for test purposes
// close us once we are done
/**
/**
// TODO this should be caught by lucene - EOF is almost certainly an index corruption
// IOExceptions like too many open files are not necessarily a corruption - just bubble it up
/**
/**
/**
// if we lock the directory we also acquire the write lock since that makes sure that nobody else tries to lock the IW
// on this store at the same time.
/**
// this works just like a lucene commit - we rename all temp files and once we successfully
// renamed all the segments we rename the commit to ensure we don't leave half baked commits behind.
// we make sure that nobody fetches the metadata while we do this rename operation here to ensure we don't
// get exceptions if files are still open.
// first, go and delete the existing ones
// now, rename the files... and fail it it won't work
/**
/**
/**
/**
// only do this once!
// Leverage try-with-resources to close the shard lock for us
// this closes the distributorDirectory as well
/**
// that's fine - happens all the time no need to log
/**
/**
// first check the length no matter how old this file is
// throw exception if the file is corrupt
// throw exception if metadata is inconsistent
/* marking a store as corrupted is basically adding a _corrupted to all
/**
/**
// don't delete snapshot file, or the checksums file (note, this is extra protection since the Store won't delete
// checksum)
// FNF should not happen since we hold a write lock?
// TODO do we need to also fail this if we can't delete the pending commit file?
// if one of those files can't be deleted we better fail the cleanup otherwise we might leave an old commit
// point around?
// ignore, we don't really care, will get deleted later on
// pkg private for testing
// if we have different files then they must have no checksums; otherwise something went wrong during recovery.
// we have that problem when we have an empty index is only a segments_1 file so we can't tell if it's a Lucene 4.8 file
// and therefore no checksum is included. That isn't a problem since we simply copy it over anyway but those files
// come out as different in the diff. That's why we have to double check here again if the rest of it matches.
// all is fine this file is just part of a commit or a segment that is different
/**
/** Estimate the cumulative size of all files in this directory in bytes. */
/**
/**
/**
// we don't know which version was used to write so we take the max version.
// version is written since 3.1+: we should have already hit IndexFormatTooOld.
// we either know the index is corrupted or it's just not there
// Lucene checks the checksum after it tries to lookup the codec etc.
// in that case we might get only IAE or similar exceptions while we are really corrupt...
// TODO we should check the checksum in lucene if we hit an exception
// truncated files trigger IAE if we seek negative... these files are really corrupted though
// additional safety we checksum the entire file we read the hash for...
/**
// for safety we limit this to 1MB
// legacy delete file
// lucene 5 delete file
/**
// legacy
// we don't need that file at all
// only treat del files as per-commit files fnm files are generational but only for upgradable DV
// make sure all files are added - this can happen if only the deletes are different
/**
/**
/**
/**
// only for asserts
/**
/**
/**
/**
/**
/**
/**
/**
// this holds the actual footer checksum data written by to this output
// the last 8 bytes are the checksum - we store it in footerChecksum
// we are writing parts of the checksum....
// we have recorded the entire checksum
// fail if we write more than expected
// don't optimze writing the last block of bytes
/**
// Conversion to int is safe here because (verifiedPosition - pos) can be at most len, which is integer
// Conversion to int is safe here because checksumPosition is (file length - 8) so
// (pos - checksumPosition) cannot be bigger than 8 unless we are reading after the end of file
// going within verified region - just seek there
// portion of the skip region is verified and portion is not
// skipping the verified portion
// and checking unverified
// ignore :(
/**
/**
/**
/**
/**
/**
/**
/**
// this achieves two things:
// - by committing a new commit based on the starting commit, it make sure the starting commit will be opened
// - deletes any other commit (by lucene standard deletion policy)
//
// note that we can't just use IndexCommit.delete() as we really want to make sure that those files won't be used
// even if a virus scanner causes the files not to be used.
// The new commit will use segment files from the starting commit but userData from the last commit by default.
// Thus, we need to manually set the userData from the starting commit to the new commit.
/**
// all operations of the safe commit must be at most the global checkpoint.
// we don't want merges to happen here - we call maybe merge on the engine
// later once we stared it up otherwise we would need to wait for it here
// we also don't specify a codec here and merges should use the engines for this index
/*
//www.apache.org/licenses/LICENSE-2.0
// the actual file size on "disk", if compressed, the compressed size
/**
/**
/**
/**
/**
// we can't tell if either or is null so we return false in this case! this is why we don't use equals for this!
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// do NOT optimize this class for performance
/** Sole constructor */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/* handle potential wildcards in fields */
/* from an artificial document */
// if no document indexed in shard, take the queried document itself for stats
/* or from an existing document */
// fields with stored term vectors
// generate tvs for fields where analyzer is overridden
// fields without term vectors
/* no term vectors generated or found */
/* if there are term vectors, optional compute dfs and/or terms filtering */
// write term vectors
// must be a string
// and must be indexed
/* only keep valid fields */
// already retrieved, only if the analyzer hasn't been overridden at the field
/* generate term vectors from fetched document fields */
/* merge with existing Fields */
// some fields are returned even when not asked for, eg. _timestamp
/* store document in memory index */
/* and read vectors from it */
// parse the document, at the moment we do update the mapping, just like percolate
// select the right fields and generate term vectors
// Poached from Lucene ParallelLeafReader
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** read the size of the op (i.e., number of bytes, including the op size) written at the given position */
// read op size from disk
// Add an extra 4 to account for the operation size integer itself
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// ops
// offset
// generation
// minimum sequence number
// maximum sequence number
// global checkpoint
// minimum translog generation in the translog
// maximum reachable (trimmed) sequence number
/**
/**
// We checksum the entire file before we even go and parse it. If it's corrupted we barf right here.
// don't clone
// now go and write to the channel, in one go.
// no need to force metadata, file size stays the same and we did the full fsync
// when we first created the file, so the directory entry doesn't change as well
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// TODO: Read translog forward in 9.0+
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// to ensure there is no self-suppression
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
// the list of translog readers is guaranteed to be in order of translog generation
/**
// this is special handling for error condition when we create a new writer but we fail to bake
// the newly written file (generation+1) into the checkpoint. This is still a valid state
// we just need to cleanup before we continue
// we hit this before and then blindly deleted the new generation even though we managed to bake it in and then hit this:
// https://discuss.elastic.co/t/cannot-recover-index-because-of-missing-tanslog-files/38336 as an example
//
// For this to happen we must have already copied the translog.ckp file into translog-gen.ckp so we first check if that
// file exists. If not we don't even try to clean it up and wait until we fail creating it
// current checkpoint is already copied
// delete it and log a warning
// we have to close all the recovered ones otherwise we leak file handles here
// for instance if we have a lot of tlog and we can't create the writer we keep on holding
// on to all the uncommitted tlog files if we don't close
// close the opened translog files if we fail to create a new translog...
/** recover all translog files found on disk */
// we open files in reverse order in order to validate the translog uuid before we start traversing the translog based on
// the generation id we found in the lucene commit. This gives for better error messages if the wrong
// translog was found.
// when we clean up files, we first update the checkpoint with a new minReferencedTranslog and then delete them;
// if we crash just at the wrong moment, it may be that we leave one unreferenced file behind so we delete it if there
// a temp file to copy checkpoint to - note it must be in on the same FS otherwise atomic move won't work
// we first copy this into the temp-file and then fsync it followed by an atomic move into the target file
// that way if we hit a disk-full here we are still in an consistent state.
// we only fsync the directory the tempFile was already fsynced
/**
/** Returns {@code true} if this {@code Translog} is still open. */
//skip getStackTrace, current method and close method frames
//limit depth of analysis to 10 frames, it should be enough to catch closing with, e.g. IOUtils
//find all inner callers including Translog subclasses
//the list of inner callers should be either empty or should contain closeOnTragicEvent method
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
// no need to fsync here the read operation will ensure that buffers are written to disk
// if they are still in RAM and we are reading onto that position
// read backwards - it's likely we need to read on that is recent
/**
/**
/**
/** package private for testing */
/**
// we assume that the current translog generation doesn't have trimmable ops. Verify that.
// update all existed ones (if it is necessary) as checkpoint and reader are immutable
/**
// if we have a new one it's already synced
/**
// we only need to sync the max location since it will sync all other
// locations implicitly
/**
// we can not hold a read lock here because closing will attempt to obtain a write lock and that would result in self-deadlock
/*
//github.com/elastic/elasticsearch/issues/15941.
/**
// acquire lock to make the two numbers roughly consistent (no file change half way)
// public for testing
/**
/**
/**
/**
/**
// inclusive
// inclusive
/**
/**
// the de-serialization logic in Index was identical to that of Create when create was deprecated
/**
// the serialization logic in Index was identical to that of Create when create was deprecated
// since 7.0
// SERIALIZATION_FORMAT
// can't assert that this is _doc because pre-8.0 indexes can have any name for a type
// _parent
// _version_type
// _parent
// 6.0 - *
// since 7.0
// since 8.0
// SERIALIZATION_FORMAT
// Can't assert that this is _doc because pre-8.0 indexes can have any name for a type
// versionType
/** utility for testing */
/**
/**
// This absolutely must come first, or else reading the checksum becomes part of the checksum
/**
// 4byte for the checksum
// size is not part of the checksum!
// if we can we validate the checksum first
// we are sometimes called when mark is not supported this is the case when
// we are sending translogs across the network with LZ4 compression enabled - currently there is no way s
// to prevent this unfortunately.
/**
// This BufferedChecksumStreamOutput remains unclosed on purpose,
// because closing it closes the underlying stream, which we don't
// want to do here.
/**
/*
/**
// make sure we move most of the data to disk outside of the writeLock
// in order to reduce the time the lock is held since it's blocking all threads
// create a new translog file; this will sync it and update the checkpoint data;
/**
// move most of the data to disk to reduce the time the lock is held
// we're shutdown potentially on some tragic event, don't delete anything
// The checkpoint is used when opening the translog to know which files should be recovered from.
// We now update the checkpoint to ignore the file we are going to remove.
// Note that there is a provision in recoverFromFiles to allow for the case where we synced the checkpoint
// but crashed before we could delete the file.
// sync at once to make sure that there's at most one unreferenced generation.
/**
/**
/**
// for testing
/**
/** Reads and returns the current checkpoint */
/**
// We need to open at least one translog header to validate the translogUUID.
// just bubble up.
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/** returns the number of generations that were acquired for snapshots */
/**
/**
// both size and age are disabled;
// for the current writer
/** returns the translog generation that will be used as a basis of a future store/peer recovery */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// pre-2.0 - unsupported
// added checkpoints
// added primary term
/**
/**
/**
// uuid
// primary term
// checksum
/**
// This input is intentionally not closed because closing it will close the FileChannel.
// Read the translogUUID
// Read the primary term
// Verify the checksum
// verify UUID only after checksum, to ensure that UUID is not corrupted
// Lucene's CodecUtil writes a magic number of 0x3FD76C17 with the header, in binary this looks like:
// binary: 0011 1111 1101 0111 0110 1100 0001 0111
// hex   :    3    f    d    7    6    c    1    7
//
// With version 0 of the translog, the first byte is the Operation.Type, which will always be between 0-4,
// so we know if we grab the first byte, it can be:
// 0x3f => Lucene's magic number, so we can assume it's version 1 or later
// 0x00 => version 0 of the translog
// LUCENE_CODEC_HEADER_BYTE
// UNVERSIONED_TRANSLOG_HEADER_BYTE
/**
// This output is intentionally not closed because closing it will close the FileChannel.
// Write uuid
// Write primary term
// Checksum header
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** the size of the generations in the translog that weren't yet to committed to lucene */
/** the number of operations in generations of the translog that weren't yet to committed to lucene */
/*
//www.apache.org/licenses/LICENSE-2.0
// the last checkpoint that was written when the translog was last synced
/* the number of translog operations written to this file */
/* if we hit an exception that we can't recover from we assign it to this var and ship it with every AlreadyClosedException we throw */
/* A buffered outputstream what writes to the writers channel */
/* the total offset of this file including the bytes written to the file as well as into the buffer */
// callback that's called whenever an operation with a given sequence number is successfully persisted.
// lock order synchronized(syncLock) -> synchronized(this)
// if we fail to bake the file-generation into the checkpoint we stick with the file and once we recover and that
// file exists we remove it. We only apply this logic to the checkpoint.generation+1 any other file with a higher generation
// is an error condition
/**
/**
// nothing to do
// TODO: We haven't had timestamp for Index operations in Lucene yet, we need to loosen this check without timestamp.
/**
/**
/**
// make sure to acquire the sync lock first, to prevent dead locks with threads calling
// syncUpTo() , where the sync lock is acquired first, following by the synchronize(this)
//
// Note: While this is not strictly needed as this method is called while blocking all ops on the translog,
//       we do this to for correctness and preventing future issues.
// sync before we close..
// make sure to acquire the sync lock first, to prevent dead locks with threads calling
// syncUpTo() , where the sync lock is acquired first, following by the synchronize(this)
/**
// only one sync/checkpoint should happen concurrently but we wait
// double checked locking - we don't want to fsync unless we have to and now that we have
// the lock we should check again since if this code is busy we might have fsynced enough already
// now do the actual fsync outside of the synchronized block such that
// we can continue writing to the buffer etc.
// write protected by syncLock
// we only flush here if it's really really needed - try to minimize the impact of the read operation
// in some cases ie. a tragic event we might still be able to read the relevant value
// which is not really important in production but some test can make most strict assumptions
// if we don't fail in this call unless absolutely necessary.
// we don't have to have a lock here because we only write ahead to the file, so all writes has been complete
// for the requested location.
/**
// the stream is intentionally not closed because
// closing it will close the FileChannel
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Retrieve the generation and UUID from the existing data
// Hold the lock open for the duration of the tool running
// Hold the lock open for the duration of the tool running
// Retrieve the generation and UUID from the existing data
// Write empty checkpoint and translog to empty files
// Fsync the translog directory after rename
// perform clean check of translog instead of corrupted marker file
// We open translog to check for corruption, do not clean anything.
//noinspection StatementWithEmptyBody we are just checking that we can iterate through the whole snapshot
/** Write a checkpoint file to the given location with the given generation */
// fsync with metadata here to make sure.
/**
/** Show a warning about deleting files, asking for a confirmation if {@code batchMode} is false */
/** Return a Set of all files in a given directory */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** How much heap (% or bytes) we will share across all actively indexing shards on this node (default: 10%). */
/** Only applies when <code>indices.memory.index_buffer_size</code> is a %,
/** Only applies when <code>indices.memory.index_buffer_size</code> is a %,
/** If we see no indexing operations after this much time for a given shard,
/** How frequently we check indexing memory usage (default: 5 seconds). */
/** Contains shards currently being throttled because we can't write segments quickly enough */
// null means we used the default (10%)
// We only apply the min/max when % value was used for the index buffer:
// we need to have this relatively small to free up heap quickly enough
// Need to save this so we can later launch async "write indexing buffer to disk" on shards:
// it's fine to run it on the scheduler thread, no busy work
/**
/** returns how much heap this shard is using for its indexing buffer */
/** returns how many bytes this shard is currently writing to disk */
/** ask this shard to refresh, in the background, to free up heap */
/** force checker to run now */
/** Asks this shard to throttle indexing to one thread */
/** Asks this shard to stop throttling indexing to one thread */
/** called by IndexShard to record estimated bytes written to translog for the operation */
// Sort larger shards first:
/** not static because we need access to many fields/methods from our containing class (IMC): */
/** Shard calls this on each indexing/delete op */
// Must pull this again because it may have changed since we first checked:
// NOTE: this is only an approximate check, because bytes written is to the translog,
// vs indexing memory buffer which is typically smaller but can be larger in extreme
// cases (many unique terms).  This logic is here only as a safety against thread
// starvation or too infrequent checking, to ensure we are still checking periodically,
// in proportion to bytes processed by indexing:
// Must get it again since other threads could have increased it while we were in runUnlocked
// Another thread beat us to it: let them do all the work, yay!
// NOTE: even if we hit an errant exc here, our ThreadPool.scheduledWithFixedDelay will log the exception and re-invoke us
// again, on schedule
// First pass to sum up how much heap all shards' indexing buffers are using now, and how many bytes they are currently moving
// to disk:
// Give shard a chance to transition to inactive so we can flush
// How many bytes this shard is currently (async'd) moving from heap to disk:
// How many heap bytes this shard is currently using
// If the refresh completed just after we pulled shardWritingBytes and before we pulled shardBytesUsed, then we could
// have a negative value here.  So we just skip this shard since that means it's now using very little heap:
// If we are using more than 50% of our budget across both indexing buffer and bytes we are still moving to disk, then we now
// throttle the top shards to send back-pressure to ongoing indexing:
// OK we are now over-budget; fill the priority queue and ask largest shard(s) to refresh:
// How many bytes this shard is currently (async'd) moving from heap to disk:
// How many heap bytes this shard is currently using
// Only count up bytes not already being refreshed:
// If the refresh completed just after we pulled shardWritingBytes and before we pulled shardBytesUsed, then we could
// have a negative value here.  So we just skip this shard since that means it's now using very little heap:
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// builtin mappers
// Use a LinkedHashMap for metadataMappers because iteration order matters
// _ignored first so that we always load it, even if only _id is requested
// ID second so it will be the first (if no ignored fields) stored field to load
// (so will benefit from "fields: []" early termination
//_field_names must be added last so that it has a chance to see all the other mappers
// we register _field_names here so that it has a chance to see all the other mappers, including from plugins
/**
//the purpose of this method is to not chain no-op field predicates, so that we can easily find out when no plugins plug in
//a field filter, hence skip the mappings filtering part as a whole, as it requires parsing mappings into a map.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// mostly a way to prevent queries from being the main source of memory usage
// of the cache
// enables caching on all segments instead of only the larger ones, for testing only
// This is a hack for the fact that the close listener for the
// ShardCoreKeyMap will be called before onDocIdSetEviction
// See onDocIdSetEviction for more info
/** Get usage statistics for the given shard. */
// We also have some shared ram usage that we try to distribute to
// proportionally to their number of cache entries of each shard
// We wrap the weight to track the readers it sees and map them with
// the shards they belong to
/** Clear all entries that belong to the given index. */
// This cache stores two things: filters, and doc id sets. Calling
// clear only removes the doc id sets, but if we reach the situation
// that the cache does not contain any DocIdSet anymore, then it
// probably means that the user wanted to remove everything.
// This cache stores two things: filters, and doc id sets. At this time
// we only know that there are no more doc id sets, but we still track
// recently used queries, which we want to reclaim.
// It's ok to not protect these callbacks by a lock since it is
// done in LRUQueryCache
// don't throw away hit/miss
// onDocIdSetEviction might sometimes be called with a number
// of entries equal to zero if the cache for the given segment
// was already empty when the close listener was called
// We can't use ShardCoreKeyMap here because its core closed
// listener is called before the listener of the cache which
// triggers this eviction. So instead we use use stats2 that
// we only evict when nothing is cached anymore on the segment
// instead of relying on close listeners
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// NORELEASE The cacheKeyRenderer has been added in order to debug
// https://github.com/elastic/elasticsearch/issues/32827, it should be
// removed when this issue is solved
// see if its the first time we see this reader, and make sure to register a cleanup key
/**
/**
/**
/**
/**
/**
/**
/**
// use as identity equality
// TODO: more detailed ram usage?
// null indicates full cleanup, as does a closed shard
/**
// for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// pkg-private for testing
// pkg-private for testing
// Start thread that will manage cleaning the field data cache periodically
// ensure we pull an iter with new shards - flatten makes a copy
// do not allow any plugin-provided index store type to conflict with a built-in type
// doClose() is called when shutting down a node, yet there might still be ongoing requests
// that we need to wait for before closing some resources such as the caches. In order to
// avoid closing these resources while ongoing requests are still being processed, we use a
// ref count which will only close them when both this service and all index services are
// actually closed
// Copy indices because we modify it asynchronously in the body of the loop
// ignore
/**
// the cumulative statistics also account for shards that are no longer on this node, which is tracked by oldShardsStats
// we can safely ignore illegal state on ones that are closing for example
// shard is closed - no stats is fine
/**
/**
/**
/**
/**
// we ignore private settings since they are not registered settings
// NoOpEngine takes precedence as long as the index is closed
/**
/**
// this will also fail if some plugin fails etc. which is nice since we can verify that early
// concrete index - no name clash, it uses uuid
// now we are done - try to wipe data on disk if possible
// if this index was closed or deleted, we should eliminate the effect of the current scroll for this shard
/**
/**
// we are trying to delete the index store here - not a big deal if the lock can't be obtained
// the store metadata gets wiped anyway even without the lock this is just best effort since
// every shards deletes its content under the shard lock it owns.
// its safe to delete all index metadata and shard data
// this is a pure protection to make sure this index doesn't get re-imported as a dangling index.
// we should in the future rather write a tombstone rather than wiping the metadata.
/**
/**
// note that deleteIndexStore have more safety checks and may throw an exception if index was concurrently created.
// wrap the exception to indicate we already deleted the shard
/**
// index contents can be deleted if its an already closed index (so all its resources have
// already been relinquished)
/**
// this method should only be called when we know the index (name + uuid) is not part of the cluster state
// we just warn about the exception here because if deleteIndexStoreIfDeletionAllowed
// throws an exception, it gets added to the list of pending deletes to be tried again
/**
// shard data exists and can be deleted
// the shard is still allocated / active on this node
// the shards data locations do not exist
// node does not have local storage (see DiscoveryNode.nodeRequiresLocalStorage)
/**
// we are allocated - can't delete the shard
// lets see if it's on a custom path (return false if the shared doesn't exist)
// we don't need to delete anything that is not there
// lets see if it's path is available (return false if the shared doesn't exist)
// we don't need to delete anything that is not there
// play safe here and make sure that we take node level settings into account.
// we might run on nodes where we use shard FS and then in the future don't delete
// actual content.
/**
/**
/**
/**
/**
// make sure we delete indices first
// ensure we retry after 10 sec
// increase the sleep time gradually
/**
/**
// Reschedule itself to run again if not closed
/**
// Queries that create a scroll context cannot use the cache.
// They modify the search context during their execution so using the cache
// may invalidate the scroll for the next query.
// We cannot cache with DFS because results depend not only on the content of the index but also
// on the overridden statistics. So if you ran two queries on the same index with different stats
// (because an other shard was updated) you would get wrong results because of the scores
// (think about top_hits aggs or scripts using the score)
// Profiled queries should not use the cache
// if not explicitly set in the request, use the index setting, if not, use the request
// If no request cache query parameter and shard request cache
// is enabled in settings don't cache for requests with size > 0
// We use the cacheKey of the index reader as a part of a key of the IndicesRequestCache.
// if now in millis is used (or in the future, a more generic "isDeterministic" flag
// then we can't cache based on "now" key within the search request, as it is not deterministic
/**
// restore the cached query result into the context
// we have to invalidate the cache entry if we cached a query result form a request that timed out.
// we can't really throw exceptions in the loading part to signal a timed out search to the outside world since if there are
// multiple requests that wait for the cache entry to be calculated they'd fail all with the same exception.
// instead we all caching such a result for the time being, return the timed out result for all other searches with that cache
// key invalidate the result in the thread that caused the timeout. This will end up to be simpler and eventually correct since
// running a search that times out concurrently will likely timeout again if it's run while we have this `stale` result in the
// cache. One other option is to not cache requests with a timeout at all...
/**
/* BytesStreamOutput allows to pass the expected size but by default uses
// for now, keep the paged data structure, which might have unused bytes to fill a page, but better to keep
// the memory properly paged instead of having varied sized bytes
// No need to take the IndexShard into account since it is shared
// across many entities
/* Being static, parseAliasFilter doesn't have access to whatever guts it needs to parse a query. Instead of passing in a bunch
/**
/**
/**
/**
/**
/**
// Only enforce the shard limit if we have at least one data node, so that we don't block
// index creation during cluster setup
// ignore cases where we are shutting down..., there is really nothing interesting to be done here...
// visible for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//this.stats = stats;
// make a total common stats from old ones and current ones
// "node" level
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Add "standard" for old indices (bwc)
// No char filter are available in lucene-core so none are built in to Elasticsearch core
// Add filters available in lucene-core
// Add "standard" for old indices (bwc)
/* Note that "stop" is available in lucene-core but it's pre-built
// Temporary shim to register old style pre-configured tokenizers
// TODO: provide built-in normalizer providers?
// TODO: pluggability?
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// just making sure it's indeed a dictionary dir
// The cache loader throws unchecked exception (see #loadDictionary()),
// here we simply report the exception and continue loading the dictionaries
/**
// merging node settings with hunspell dictionary specific settings
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// by calling get analyzer we are ensuring reuse of the same STANDARD analyzer for DEFAULT!
// this call does not create a new instance
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Tripped count for when redistribution was attempted but wasn't successful
/**
// If the child is disabled, ignore it
// Gather the "estimated" count for the parent breaker by adding the
// estimations for each individual breaker
// Manually add the parent breaker settings since they aren't part of the breaker map
//package private to allow overriding it in tests
// This exception can happen (rarely) due to a race condition in the JVM when determining usage of memory pools. We do not want
// to fail requests because of this and thus return zero memory usage in this case. While we could also return the most
// recently determined memory usage, we would overestimate memory usage immediately after a garbage collection event.
/**
// derive durability of a tripped parent breaker depending on whether the majority of memory tracked by
// child circuit breakers is categorized as transient or permanent.
/**
// Validate the settings
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ignore
/*
//www.apache.org/licenses/LICENSE-2.0
// a list of shards that failed during recovery
// we keep track of these shards in order to prevent repeated recovery of these shards on each cluster state update
// for tests
// Doesn't make sense to manage shards on non-master and non-data nodes
// we need to clean the shards and indices we have on this node, since we
// are going to recover them again once state persistence is disabled (no master / not recovered)
// TODO: feels hacky, a block disables state persistence, and then we clean the allocated shards, maybe another flag in blocks?
// also cleans shards
// also deletes shards of deleted indices
// also removes shards of removed indices
// removes any local shards that doesn't match what the master expects
// can also fail shards, but these are then guaranteed to be in failedShardsCache
/**
// remove items from cache which are not in our routing table anymore and resend failures that have not executed on master yet
// TODO: can we remove this? Is resending shard failures the responsibility of shardStateAction?
// overrideable by tests
// overrideable by tests
/**
// The deleted index was part of the previous cluster state, but not loaded on the local node
// The previous cluster state's metadata also does not contain the index,
// which is what happens on node startup when an index was deleted while the
// node was not part of the cluster.  In this case, try reading the index
// metadata from disk.  If its not there, there is nothing to delete.
// First, though, verify the precondition for applying this case by
// asserting that the previous cluster state is not initialized/recovered.
// we are waiting until we can lock the index / all shards on the node and then we ack the delete of the store
// to the master. If we can't acquire the locks here immediately there might be a shard of this index still
// holding on to the lock due to a "currently canceled recovery" or so. The shard will delete itself BEFORE the
// lock is released so it's guaranteed to be deleted by the time we get the lock
/**
// null e.g. if we are not a data node
// if the cluster change indicates a brand new cluster, we only want
// to remove the in-memory structures for the index and not delete the
// contents on disk because the index will later be re-imported as a
// dangling index
/**
// the master thinks we are active, but we don't have this shard at all, mark it as failed
/**
// remove shards based on routing nodes (no deletion of data)
// we can just remove the shard without cleaning it locally, since we will clean it in IndicesStore
// once all shards are allocated
// this can happen if the node was isolated/gc-ed, rejoins the cluster and a new shard with the same allocation id
// is assigned to it. Batch cluster state processing or if shard fetching completes before the node gets a new cluster
// state may result in a new shard being initialized while having the same allocation id as the currently started shard.
// see above if clause
// this can happen when cluster state batching batches activation of the shard, closing an index, reopening it
// and assigning an initializing primary to this node
// we only create indices for shards that are allocated
// create map of indices to create with shards to fail if index creation fails
// fail shards that would be created or updated by createOrUpdateShards
// the master thinks we are initializing, but we are already started or on POST_RECOVERY and waiting
// for master to confirm a shard started message (either master failover, or a cluster event before
// we managed to tell the master we started), mark us as started
/**
// only recover from started primary, if we can't find one, we will do it next round
/**
/**
// package-private for testing
// the node got closed on us, ignore it
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// load anyway since listeners should not throw exceptions
/**
// load anyway since listeners should not throw exceptions
// load anyway since listeners should not throw exceptions
// don't call cache.cleanUp here as it would have bad performance implications
// force eviction
// we call refresh because this is a manual operation, should happen
// rarely and probably means the user wants to see memory returned as
// soon as possible
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// some shards may be unassigned, so we need this as state
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
// 1. send pre-sync flushes to all replicas
// 2. fetch in flight operations
// 3. now send the sync request to all the shards
/**
// count after the assert so we won't decrement twice in handleException
/**
// count after the assert so we won't decrement twice in handleException
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// add the _all field mapper for indices created in 6x
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// put a dummy item to start the processor
// These exceptions will be ignored as we record only the first failure, log them for debugging purpose.
// not an actual item
// While we are waiting for the responses, we can prepare the next request in advance
// so we can send it immediately when the responses arrive to reduce the transfer time.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Get a temporary name for the provided file name. */
/** remove and {@link IndexOutput} for a given file. It is the caller's responsibility to close it */
/**
// add first, before it's created
// we iterate over all pages - this is a 0-copy for all core impls
// we are done
// remove maybe null if we got finished
// clean open index outputs
// trash temporary files
/** renames all temporary files to their true name, potentially overriding existing files */
// chunks can be delivered out of order, we need to buffer chunks if there's a gap between them.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// exposed for testing
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// create a new recovery status, and process...
// we fork off quickly here and go async but this is called from the cluster state applier thread too and that can cause
// assertions to trip if we executed it on the same thread hence we fork off to the generic threadpool.
// this will be logged as warning later on...
// this can also come from the source wrapped in a RemoteTransportException
// unwrap an exception that was thrown as part of the recovery
// do it twice, in case we have double transport exception
// unwrap an exception that was thrown as part of the recovery
// here, we would add checks against exception that need to be retried (and not removeAndClean in this case)
// if the target is not ready yet, retry
// we still execute under cancelableThreads here to ensure we interrupt any blocking call to the network if any
// on the underlying transport. It's unclear if we need this here at all after moving to async execution but
// the issues that a missing call to this could cause are sneaky and hard to debug. If we don't need it on this
// call we can potentially remove it altogether which we should do it in a major release only with enough
// time to test. This shoudl be done for 7.0 if possible
// do this through ongoing recoveries to remove it from the collection
// we do some heavy work like refreshes in the response so fork off to the generic threadpool
/**
// Make sure that the current translog is consistent with the Lucene index; otherwise, we have to throw away the Lucene index.
// happens on an empty folder. no need to log
// in very rare cases a translog replay from primary is processed before a mapping update on this node
// which causes local mapping changes since the mapping (clusterstate) might not have arrived on this node.
// we do not need to use a timeout here since the entire recovery mechanism has an inactivity protection (it will be
// canceled)
// note that we do not use a timeout (see comment above)
// do not retry if the mapping on replica is at least as recent as the mapping
// that the primary used to index the operations in the request.
// How many bytes we've copied since we last called RateLimiter.pause
// Time to pause
// be safe
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** This is the single source of truth for ongoing recoveries. If it's not here, it was canceled or done */
/**
/**
// swap recovery targets in a synchronized block to ensure that the newly added recovery target is picked up by
// cancelRecoveriesForShard whenever the old recovery target is picked up
// Closes the current recovery target
// fail shard to be safe
/**
/** Similar to {@link #getRecovery(long)} but throws an exception if no recovery is found */
/** cancel the recovery with the given id (if found) and remove it from the recovery collection */
/**
/** mark the recovery with the given id as done (if found) */
/** the number of ongoing recoveries */
/**
/**
/**
// to be safe, we don't know what go stuck
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// was fileBasedRecovery
// was fileBasedRecovery
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** how long to wait before retrying after network related issues */
/** timeout value to use for requests made as part of the recovery process */
/**
/**
// choose 512KB-16B to ensure that the resulting byte[] is not a humongous allocation in G1.
// doesn't have to be fast as nodes are reconnected every 10s by default (see InternalClusterService.ReconnectToNodes)
// and we want to give the master time to remove a faulty node
// only settable for tests
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Shard that is going to be recovered (the "source")
// Request containing source and target node information
/**
// check if the shard got closed on us
// NB check hasCompleteHistoryOperations when computing isSequenceNumberBasedRecovery, even if there is a retention lease,
// because when doing a rolling upgrade from earlier than 7.4 we may create some leases that are initially unsatisfied. It's
// possible there are other cases where we cannot satisfy all leases, because that's not a property we currently expect to hold.
// Also it's pretty cheap when soft deletes are enabled, and it'd be a disaster if we tried a sequence-number-based recovery
// without having a complete history.
// all the history we need is retained by an existing retention lease, so we do not need a separate retention lock
// all the history we need is retained by the retention lock, obtained before calling shard.hasCompleteHistoryOperations()
// and before acquiring the safe commit we'll be using, so we can be certain that all operations after the safe commit's
// local checkpoint will be retained for the duration of this recovery.
// Try and copy enough operations to the recovering peer so that if it is promoted to primary then it has a chance of being
// able to recover other replicas using operations-based recoveries. If we are not using retention leases then we
// conservatively copy all available operations. If we are using retention leases then "enough operations" is just the
// operations from the local checkpoint of the safe commit onwards, because when using soft deletes the safe commit retains
// at least as much history as anything else. The safe commit will often contain all the history retained by the current set
// of retention leases, but this is not guaranteed: an earlier peer recovery from a different primary might have created a
// retention lease for some history that this primary already discarded, since we discard history when the global checkpoint
// advances and not when creating a new safe commit. In any case this is a best-effort thing since future recoveries can
// always fall back to file-based ones, and only really presents a problem if this primary fails before things have settled
// down.
// If the target previously had a copy of this shard then a file-based recovery might move its global
// checkpoint backwards. We must therefore remove any existing retention lease so that we can create a
// new one later on in the recovery.
// For a sequence based recovery, the target can keep its local translog
/*
// we have to capture the max_seen_auto_id_timestamp and the max_seq_no_of_updates to make sure that these values
// are at least as high as the corresponding values on the primary when any of these operations were executed on it.
// Recovery target can trim all operations >= startingSeqNo as we have sent all these operations in the phase 2
// TODO: return the actual throttle time
// check that the IndexShard still has the primary authority. This needs to be checked under operation permit to prevent
// races, as IndexShard will switch its authority only when it holds all operation permits, see IndexShard.relocated()
// just in case we got an exception (likely interrupted) while waiting for the get
/**
// TODO: We shouldn't use the generic thread pool here as we already execute this from the generic pool.
//       While practically unlikely at a min pool size of 128 we could technically block the whole pool by waiting on futures
//       below and thus make it impossible for the store release to execute which in turn would block the futures forever
/**
// Total size of segment files that are recovered
// Total size of segment files that were able to be re-used
// Generate a "diff" of all the identical, different, and missing
// segment files on the target node, using the existing files on
// the source node
// Establishes new empty translog on the replica with global checkpoint set to lastKnownGlobalCheckpoint. We want
// the commit we just copied to be a safe commit on the replica, so why not set the global checkpoint on the replica
// to the max seqno of this commit? Because (in rare corner cases) this commit might not be a safe commit here on
// the primary, and in these cases the max seqno would be too high to be valid as a global checkpoint.
// but we must still create a retention lease
// Clone the peer recovery retention lease belonging to the source shard. We are retaining history between the the local
// checkpoint of the safe commit we're creating and this lease's retained seqno with the retention lock, and by cloning an
// existing lease we (approximately) know that all our peers are also retaining history as requested by the cloned lease. If
// the recovery now fails before copying enough history over then a subsequent attempt will find this lease, determine it is
// not enough, and fall back to a file-based recovery.
//
// (approximately) because we do not guarantee to be able to satisfy every lease on every peer.
// it's possible that the primary has no retention lease yet if we are doing a rolling upgrade from a version before
// 7.4, and in that case we just create a lease using the local checkpoint of the safe commit which we're using for
// recovery as a conservative estimate for the global checkpoint.
// Send a request preparing the new shard's translog to receive operations. This ensures the shard engine is started and disables
// garbage collection (not the JVM's GC!) of tombstone deletes.
/**
// used to estimate the count of the subsequent batch.
// We need to synchronized Snapshot#next() because it's called by different threads through sendBatch.
// Even though those calls are not concurrent, Snapshot#next() uses non-synchronized state and is not multi-thread-compatible.
// check if this request is past bytes threshold, and if so, send it off
// send the leftover operations or if no operations were sent, request the target to respond with its local checkpoint
/*
// this global checkpoint is persisted in finalizeRecovery
// TODO: make relocated async
// this acquires all IndexShard operation permits and will thus delay new recoveries until it is done
/*
/**
// send smallest first
// InputStreamIndexInput's close is a noop
// Send the CLEAN_FILES request, which takes all of the files that
// were transferred and renames them from their temporary file
// names to the actual file names. It also writes checksums for
// the files after they have been renamed.
//
// Once the files have been renamed, any other files that are not
// related to this recovery (out of date segments, for example)
// are deleted
// check small files first
// we are corrupted on the primary -- fail!
// corruption has happened on the way to replica
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// synchronized is strictly speaking not needed (this is called by a single thread), but just to be safe
// reinitializing stop remove all state except for start time
/**
// write a snapshot of current time, which is not per se the time field
/** Returns start time in millis */
/** Returns elapsed time in millis, or 0 if timer was not started */
/** Returns stop time in millis */
// for tests
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// indicates we are still in init phase
/**
/**
/**
/**
// indicates we are still in init phase
// stream size first, as it matters more and the files section can be long
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// last time this status was accessed
// latch that can be used to blockingly wait for RecoveryTarget to be closed
/**
// make sure the store is not released until we are done.
/**
/** return the last time this RecoveryStatus was used (based on System.nanoTime() */
/** sets the lasAccessTime flag to now */
/**
// release the initial reference. recovery files will be cleaned as soon as ref count goes to zero, potentially now.
// once primary relocation has moved past the finalization step, the relocation source can put the target into primary mode
// and start indexing as primary into the target shard (see TransportReplicationAction). Resetting the target shard in this
// state could mean that indexing is halted until the recovery retry attempt is completed and could also destroy existing
// documents indexed and acknowledged before the reset.
/**
// release the initial reference. recovery files will be cleaned as soon as ref count goes to zero, potentially now
/**
// release the initial reference. recovery files will be cleaned as soon as ref count goes to zero, potentially now
/** mark the current recovery as done */
// this might still throw an exception ie. if the shard is CLOSED due to some other event.
// it's safer to decrement the reference in a try finally here.
// release the initial reference. recovery files will be cleaned as soon as ref count goes to zero, potentially now
// free store. increment happens in constructor
/*** Implementation of {@link RecoveryTargetHandler } */
// Persist the global checkpoint.
// We should erase all translog operations above trimAboveSeqNo as we have received either the same or a newer copy
// from the recovery source in phase2. Rolling a new translog generation is not strictly required here for we won't
// trim the current generation. It's merely to satisfy the assumption that the current generation does not have any
// operation that would be trimmed (see TranslogWriter#assertNoSeqAbove). This assumption does not hold for peer
// recovery because we could have received operations above startingSeqNo from the previous primary terms.
// the flush or translog generation threshold can be reached after we roll a new translog
/*
/*
/*
// update stats only after all operations completed (to ensure that mapping updates don't mess with stats)
// roll over / flush / trim if needed
// first, we go and move files that were created with the recovery id suffix to
// the actual names, its ok if we have a corrupted index here, since we have replicas
// to recover from in case of a full cluster shutdown just when this code executes...
// if empty, may be a fresh IndexShard, so write an empty leases file to disk
// this is a fatal exception at this stage.
// this means we transferred files from the remote that have not be checksummed and they are
// broken. We have to clean up this shard entirely, remove all files and bubble it up to the
// source shard since this index might be broken there as well? The Source can handle this and checks
// its content on disk if possible.
// clean up and delete all files
/** Get a temporary name for the provided file name. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/** writes a partial file chunk to the target store */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Pause using the rate limiter, if desired, to throttle the recovery
// always fetch the ratelimiter - it might be updated in real-time on the recovery settings
// Time to pause
/* we send estimateTotalOperations with every request since we collect stats on the target and that way we can
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO this class can be foled into either IndicesService and partially into IndicesClusterStateService
// there is no need for a separate public service
// Cache successful shard deletion checks to prevent unnecessary file system lookups
// Doesn't make sense to delete shards on non-data nodes
// we double check nothing has changed when responses come back from other nodes.
// it's easier to do that check when the current cluster state is visible.
// also it's good in general to let things settle down
// remove entries from cache that don't exist in the routing table anymore (either closed or deleted indices)
// - removing shard data of deleted indices is handled by IndicesClusterStateService
// - closed indices don't need to be removed from the cache but we do it anyway for code simplicity
// remove entries from cache which are allocated to this node
// Note, closed indices will not have any routing information, so won't be deleted
// nothing to do
// nothing to do
// a shard can be deleted if all its copies are active, and its not allocated on this node
// should not really happen, there should always be at least 1 (primary) shard in a
// shard replication group, in any case, protected from deleting something by mistake
// be conservative here, check on started, not even active
// check if shard is active on the current node
// make sure shard is really there before register cluster state observer
// create observer here. we need to register it here because we need to capture the current cluster state
// which will then be compared to the one that is applied when we call waitForNextChange(). if we create it
// later we might miss an update and wait forever in case no new cluster state comes in.
// in general, using a cluster state observer here is a workaround for the fact that we cannot listen on
// shard state changes explicitly. instead we wait for the cluster state changes because we know any
// shard state change will trigger or be triggered by a cluster state change.
// check if shard is active. if so, all is good
// shard is not active, might be POST_RECOVERY so check if cluster state changed inbetween or wait for next change
// the shard is not there in which case we want to send back a false (shard is not active),
// so the cluster state listener must be notified or the shard is active in which case we want to
// send back that the shard is active here we could also evaluate the cluster state and get the
// information from there. we don't do it because we would have to write another method for this
// that would have the same effect
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: Fallback for BWC with older ES versions. Remove this once request.getCustomDataPath() always returns non-null
// note that this may fail if it can't get access to the shard lock. Since we check above there is an active shard, this means:
// 1) a shard is being constructed, which means the master will not use a copy of this replica
// 2) A shard is shutting down and has not cleared it's content within lock timeout. In this case the master may not
//    reuse local resources.
// We use peer recovery retention leases from the primary for allocating replicas. We should always have retention leases when
// we refresh shard info after the primary has started. Hence, we can ignore retention leases if there is no active shard.
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Wraps all mutable types that the JSON parser can create by immutable wrappers.
// Any inputs not wrapped are assumed to be immutable
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// This check is here because the DEFAULT_TEMPLATE_LANG(mustache) is not
// installed for use by REST tests. `propertyValue` will not be
// modified if templating is not available so a script that simply returns an unmodified `propertyValue`
// is returned.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Contains all pipelines that have been executed for this document
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
//maybeList is already a list, we append the provided values to it
//maybeList is a scalar, we convert it to a list and append the provided values to it
// If there is a field in the source with the name '_ingest' it gets overwritten here,
// if access to that field is required then it get accessed via '_source._ingest'
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we use a treeset here to have a test-able / predictable order
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We can't use Pipeline class directly in cluster state, because we don't have the processor factories around when
// IngestMetadata is registered as custom metadata.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Ideally this should be in IngestMetadata class, but we don't have the processor factories around there.
// We know of all the processor factories when a node with all its plugin have been initialized. Also some
// processor factories rely on other node services. Custom metadata is statically registered when classes
// are loaded, so in the cluster state we just save the pipeline config and here we keep the actual pipelines around.
/**
/**
// Returning PipelineConfiguration instead of Pipeline, because Pipeline and Processor interface don't
// know how to serialize themselves.
// if we didn't ask for _any_ ID, then we get them all (this is the same as if they ask for '*')
/**
// validates the pipeline and processor configuration before submitting a cluster update task:
/**
/**
//only surface the top level non-failure processors, on-failure processor times will be included in the top level non-failure
//Prefer the conditional's metric since it only includes metrics when the conditional evaluated to true.
/**
//package private for testing
// conditionals are implemented as wrappers around the real processor, so get the real processor for the correct type for the name
// the pipeline specific stat holder may not exist and that is fine:
// (e.g. the pipeline may have been removed while we're ingesting a document
//it's fine to set all metadata fields all the time, as ingest document holds their starting values
//before ingestion, which might also get modified during ingestion.
// Publish cluster state to components that are used by processor factories before letting
// processor factories create new processor instances.
// (Note that this needs to be done also in the case when there is no change to ingest metadata, because in the case
// when only the part of the cluster state that a component is interested in, is updated.)
// Lazy initialize these variables in order to favour the most like scenario that there are no pipeline changes:
// Iterate over pipeline configurations in ingest metadata and constructs a new pipeline if there is no pipeline
// or the pipeline configuration has been modified
//Best attempt to populate new processor metrics using a parallel array of the old metrics. This is not ideal since
//the per processor metrics may get reset when the arrays don't match. However, to get to an ideal model, unique and
//consistent id's per processor and/or semantic equals for each processor will be needed.
// Iterate over the current active pipelines and check whether they are missing in the pipeline configuration and
// if so delete the pipeline from new Pipelines map:
// Update the pipelines:
// Rethrow errors that may have occurred during creating new pipeline instances:
/**
// break in the case of self referencing processors in the event a processor author creates a
// wrapping processor that has its inner processor refer to itself.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//package private for testing
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Store config as bytes reference, because the config is only used when the pipeline store reads the cluster state
// and the way the map of maps config is read requires a deep copy (it removes instead of gets entries to check for unused options)
// also the get pipeline api just directly returns this to the caller
// pkg-private for tests
// pkg-private for tests
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//runtime check for cycles against a copy of the document. This is needed to properly handle conditionals around pipelines
// do nothing, let the tracking processors throw the exception while recording the path up to the failure
//else do nothing, let the tracking processors throw the exception while recording the path up to the failure
//now that we know that there are no cycles between pipelines, decorate the processors for this pipeline and execute it
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// This check is here because the DEFAULT_TEMPLATE_LANG(mustache) is not
// installed for use by REST tests. `value` will not be
// modified if templating is not available
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** File system type from {@code java.nio.file.FileStore type()}, if available. */
/**
// total aggregates do not have a path
/**
// already added numbers for this device;
/*
//www.apache.org/licenses/LICENSE-2.0
// do not fail Elasticsearch if something unexpected
// happens here
//bugs.openjdk.java.net/browse/JDK-8162520 */
/* See: https://bugs.openjdk.java.net/browse/JDK-8162520 */
/**
// NOTE: we use already cached (on node startup) FileStore and spins
// since recomputing these once per second (default) could be costly,
// and they should not change:
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// NOTE: these are likely JVM dependent
// ignore our own thread...
// ignore our own thread...
// sort by delta CPU time on thread.
// skip that for now
// analyse N stack traces for M busiest threads
// NOTE, javadoc of getThreadInfo says: If a thread of the given ID is not alive or does not exist,
// null will be set in the corresponding element in the returned array. A thread is alive if it has
// been started and has not yet died.
// thread is not alive yet or died before the first snapshot - ignore it!
// for each snapshot (2nd array index) find later snapshot for same thread with max number of
// identical StackTraceElements (starting from end of each)
// print out trace maxSim levels of i, and mark similar ones as done
/*
//www.apache.org/licenses/LICENSE-2.0
// no collection has happened
/*
//www.apache.org/licenses/LICENSE-2.0
// ignore
// oracle java 9
// something else
/*
//the following members are only used locally for bootstrap checks, never serialized nor printed out
/**
/**
// 1.7.0_4
// 1.7.0-u2-b21
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// if we can't resolve it, its not interesting.... (Per Gen, Code Cache)
// buffer pools are not available
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/elastic/elasticsearch/pull/42725
/**
/**
/**
/**
/**
/**
/**
// this property is to support a hack to workaround an issue with Docker containers mounting the cgroups hierarchy inconsistently with
// respect to /proc/self/cgroup; for Docker containers this should be set to "/"
/**
/*
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// TODO: return a prettier name on non-Linux OS
/*
// we trim since some OS contain trailing space, for example, Oracle Linux Server 6.9 has a trailing space after the quote
/**
// fallback for older Red Hat-like OS
/**
// not available
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// These will be null for nodes running versions prior to 6.1.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// not available
/**
// not available
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: refactor this method out, it used to exist for the transport client
/**
// just create enough settings to build the environment, to get the config dir
// start with a fresh output
// re-initialize settings now that the config file has been loaded
/**
/**
// allow to force set properties based on configuration of the settings provided
// put the cluster and node name if they aren't set
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// register everything we need to release in the case of an error
// create the environment based on the finalized (processed) view of the settings
// this is just to makes sure that people get the same settings, no matter where they ask them from
// adds the context to the DeprecationLogger so that it does not need to be injected everywhere
// this is as early as we can validate settings at this point. we already pass them to ScriptModule as well as ThreadPool
// so we might be late here already
// collect engine factory providers from server and from plugins
// TODO hack around circular dependencies problems in AllocationService
// Noop in production, overridden by tests
/**
/**
/**
/**
/**
// Start the transport service now so the publish address will be added to the local disco node in ClusterService
// Load (and maybe upgrade) the metadata stored on disk
// we load the global state here (the persistent part of the cluster state stored on disk) to
// pass it to the bootstrap checks to allow plugins to enforce certain preconditions based on the recovered state.
// this is never null
// start after transport service so the local disco is known
// start before cluster service so that it can set initial state on ClusterApplierService
// stop any changes happening as a result of cluster state changes
// close discovery early to not react to pings anymore.
// This can confuse other nodes and delay things - mostly if we're the master and we're running tests.
// we close indices first, so operations won't be allowed on it
// we should stop this last since it waits for resources to get released
// if we had scroll searchers etc or recovery going on we wait for to finish.
// During concurrent close() calls we want to make sure that all of them return after the node has completed it's shutdown cycle.
// If not, the hook that is added in Bootstrap#setup() will be useless:
// close() might not be executed, in case another (for example api) call to close() has already set some lifecycles to stopped.
// In this case the process will be terminated even if the first call to close() has not finished yet.
// close filter/fielddata caches after indices
// Don't call shutdownNow here, it might break ongoing operations on Lucene indices.
// See https://issues.apache.org/jira/browse/LUCENE-7248. We call shutdownNow in
// awaitClose if the node doesn't finish closing within the specified time.
/**
// synchronized to prevent running concurrently with close()
// We don't want to shutdown the threadpool or interrupt threads on a node that is not
// closed yet.
// All threads terminated successfully. Because search, recovery and all other operations
// that run on shards run in the threadpool, indices should be effectively closed by now.
/**
/**
/** Writes a file to the logs dir containing the ports for the given transport type */
/**
/**
/**
/**
/**
/**
/** Constructs a ClusterInfoService which may be mocked for tests. */
/** Constructs a {@link org.elasticsearch.http.HttpServerTransport} which may be mocked for tests. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// for indices stats we want to include previous allocated shards stats as well (it will
// only be applied to the sensible ones to use, like refresh/merge/flush/indexing stats)
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Transform the mutable object internally used for accounting into the computed version
/**
/**
// We store timestamps with nanosecond precision, however, the
// formula specifies milliseconds, therefore we need to convert
// the values so the times don't unduely weight the formula
/**
//www.usenix.org/system/files/conference/nsdi15/nsdi15-paper-suresh.pdf
// the concurrency compensation is defined as the number of
// outstanding requests from the client to the node times the number
// of clients in the system
// Cubic queue adjustment factor. The paper chose 3 though we could
// potentially make this configurable if desired.
// EWMA of queue size
// EWMA of response time
// EWMA of service time
// The final formula
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// In case of persistent tasks we always need to return: `false`
// because in case of persistent task the parent task isn't a task in the task manager, but in cluster state.
// This instructs the task manager not to try to kill this persistent task when the task manager cannot find
// a fake parent node id "cluster" in the cluster state
/**
/**
// the task is currently running
// the task is cancelled on master, cancelling it locally
// the task is done running and trying to notify caller
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Cluster is not affected but we look up repositories in metadata
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// visible for testing only
/**
/**
// Using old state since in the new state the task is already gone
/**
// Using old state since in the new state the task is already gone
/**
/**
/**
// We want to avoid a periodic check duplicating this work
/**
// There must be a task that's worth rechecking because there was one
// that caused this method to be called and the method failed to assign it
/**
/**
/**
// We need to check if removed nodes were running any of the tasks and reassign them
/** Returns true if the persistent tasks are not equal between the previous and the current cluster state **/
/** Returns true if the task is not assigned or is assigned to a non-existing node */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: Implement custom Diff for tasks
// Tasks parser initialization
// Task description parser initialization
// Assignment parser
// Task parser initialization
/**
/**
/**
// These are transient values that shouldn't be persisted to gateway cluster state or snapshot
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// We don't have any task running yet, pick the first available node
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// wait until the gateway has recovered from disk, otherwise if the only master restarts
// we start cancelling all local tasks before cluster has a chance to recover.
// Cluster State   Local State      Local Action
//   STARTED         NULL          Create as STARTED, Start
//   STARTED         STARTED       Noop - running
//   STARTED         COMPLETED     Noop - waiting for notification ack
//   NULL            NULL          Noop - nothing to do
//   NULL            STARTED       Remove locally, Mark as PENDING_CANCEL, Cancel
//   NULL            COMPLETED     Remove locally
// Master states:
// NULL - doesn't exist in the cluster state
// STARTED - exist in the cluster state
// Local state:
// NULL - we don't have task registered locally in runningTasks
// STARTED - registered in TaskManager, requires master notification when finishes
// PENDING_CANCEL - registered in TaskManager, doesn't require master notification when finishes
// COMPLETED - not registered in TaskManager, notified, waiting for master to remove it from CS so we can remove locally
// When task finishes if it is marked as STARTED or PENDING_CANCEL it is marked as COMPLETED and unregistered,
// If the task was STARTED, the master notification is also triggered (this is handled by unregisterTask() method, which is
// triggered by PersistentTaskListener
// We have some changes let's check if they are related to our node
// New task - let's start it
// The task is still running
// Result was sent to the caller and the caller acknowledged acceptance of the result
// task is running locally, but master doesn't know about it - that means that the persistent task was removed
// cancel the task without notifying master
// Submit task failure
// something went wrong - unregistering task
/**
// Cancel the local task using the task manager
// There is really nothing we can do in case of failure here
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Cluster is not affected but we look up repositories in metadata
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Cluster is not affected but we look up repositories in metadata
/*
//www.apache.org/licenses/LICENSE-2.0
// We cannot really check if status has the same type as task because we don't have access
// to the task here. We will check it when we try to update the task
// Cluster is not affected but we look up repositories in metadata
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// TODO: since the plugins are unique by their directory name, this should only be a name check, version should not matter?
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: remove this indirection now that transport client is gone
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// sort permissions in a reasonable order
// print all permissions:
//docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html");
/** Format permission type, name, and actions into a string */
/**
// create a zero byte file for "comparison"
// this is necessary because the default policy impl automatically grants two permissions:
// 1. permission to exitVM (which we ignore)
// 2. read permission to the code itself (e.g. jar file of the code)
// parse the plugin's policy file into a set of permissions
// this method is supported with the specific implementation we use, but just check for safety.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// we need to build a List of plugins for checking mandatory plugins
// first we load plugins that are on the classpath. this is for tests
// load modules
// now, find all the ones that are in plugins/
// TODO: remove this leniency, but tests bogusly rely on it
// Checking expected plugins
// we don't log jars in lib/ we really shouldn't log modules,
// but for now: just be transparent so we can debug any potential issues
/**
// a "bundle" is a group of jars in a single classloader
// gather urls for jar files
// normalize with toRealPath to get symlinks out of our hair
/**
/**
/*
/** Get bundles for plugins installed in the given modules directory. */
/** Get bundles for plugins installed in the given plugins directory. */
// searches subdirectories under the given directory for plugin directories
// get a bundle for a single plugin dir
/**
// pkg private for tests
// add the given bundle to the sorted bundles, first adding dependencies
// already added this plugin, via a dependency
// jar-hell check the bundle against the parent classloader and extended plugins
// the plugin cli does it, but we do it again, in case lusers mess with jar files manually
// invariant: any plugins this plugin bundle extends have already been added to transitiveUrls
// check jarhell as we add each extended plugin's urls
// check jarhell of each extended plugin against this plugin
// check we don't have conflicting codebases with core
// check we don't have conflicting classes
// collect loaders of extended plugins
// create a child to load the plugin in this bundle
// reload SPI with any new services from the plugin
// note: already asserted above that extended plugins are loaded and extensible
/**
// do NOT change the order of these method calls!
// Codecs:
// Analysis:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Doesn't make sense to maintain repositories on non-master and non-data nodes
// Nothing happens there anyway
/**
// The response was acknowledged - all nodes should know about the new repository, let's verify them
// Trying to create the new repository on master to make sure it works
// Previous version is the same as this one no update is needed.
// repository is created on both master and data nodes
/**
// we use a wildcard so we don't barf if it's not present.
// repository was created on both master and data nodes
/**
// Check if repositories got changed
// First, remove repositories that are no longer there
// Now go through all repositories and update existing or create missing
// Found previous version of this repository
// Previous version is different from the version in settings
// TODO: this catch is bogus, it means the old repo is already closed,
// but we have nothing to replace it
/**
/** Closes the given repository. */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// if the snapshot id already exists in the repository data, it means an old master
// that is blocked from the cluster is trying to finalize a snapshot concurrently with
// the new master, so we make the operation idempotent
/**
/**
// removing the snapshot will mean no more snapshots
// have this index, so just skip over it
/**
/**
/**
/**
/**
// write the snapshots list
// write the indices map
// Add min version field to make it impossible for older ES versions to deserialize this object
/**
// the old format pre 5.4.1 which contains the snapshot name and uuid
// the new format post 5.4.1 that only contains the snapshot uuid,
// since we already have the name/uuid combo in the snapshots array
// A snapshotted index references a snapshot which does not exist in
// the list of snapshots. This can happen when multiple clusters in
// different versions create or delete snapshot in the same repository.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// If we had a previous generation that is different from an updated generation it's obsolete
// Since this method assumes only additions and no removals of shards, a null updated generation means no update
/**
/**
// Create a list that can hold the highest shard id as index and leave null values for shards that don't have
// a map entry.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// to close blobStore if blobStore initialization is started during close
// Inspects all cluster state elements that contain a hint about what the current repository generation is and updates
// #latestKnownRepoGen if a newer than currently known generation is found
// No need to waste cycles, no operations can run against a read-only repository
// Don't use generation from the delete task if we already found a generation for an in progress snapshot.
// In this case, the generation points at the generation the repo will be in after the snapshot finishes so it may not yet
// exist
// package private, only use for testing
// for test purposes only
/**
/**
/**
/**
/**
/**
// Cache the indices that were found before writing out the new index-N blob so that a stuck master will never
// delete an index that was created by another master node after writing this index-N blob.
/**
// It's always a possibility to not see the latest index-N in the listing here on an eventually consistent blob store, just
// debug log it. Any blobs leaked as a result of an inconsistent listing here will be cleaned up in a subsequent cleanup or
// snapshot delete run anyway.
/**
// First write the new shard state metadata (with the removed snapshot) and compute deletion targets
// Once we have put the new shard-level metadata into place, we can update the repository metadata as follows:
// 1. Remove the snapshot from the list of existing snapshots
// 2. Update the index shard generations of all updated shard folders
//
// Note: If we fail updating any of the individual shard paths, none of them are changed since the newly created
//       index-${gen_uuid} will not be referenced by the existing RepositoryData and new RepositoryData is only
//       written if all shard paths have been successfully updated.
// Once we have updated the repository, run the clean-ups
// Run unreferenced blobs cleanup in parallel to shard-level snapshot deletion
// Write the new repository data first (with the removed snapshot), using no shard generations
// Run unreferenced blobs cleanup in parallel to shard-level snapshot deletion
// updates the shard state metadata for shards of a snapshot that is to be deleted. Also computes the files to be cleaned up.
// Listener that flattens out the delete results for each index
// Just invoke the listener without any shard generations to count it down, this index will be cleaned up
// by the stale data cleanup in the end.
// TODO: Getting here means repository corruption. We should find a way of dealing with this instead of just ignoring
//       it and letting the cleanup deal with it.
// Listener for collecting the results of removing the snapshot from each shard's metadata in the current index
// Just passing null here to count down the listener instead of failing it, the stale data left behind
// here will be retried in the next delete or repository cleanup
/**
/**
// Nothing to clean up we return
// write new index-N blob to ensure concurrent operations will fail
// Finds all blobs directly under the repository root path that are not referenced by the current RepositoryData
// TODO: Include the current generation here once we remove keeping index-(N-1) around from #writeIndexGen
// TODO: We shouldn't be blanket catching and suppressing all exceptions here and instead handle them safely upstream.
//       Currently this catch exists as a stop gap solution to tackle unexpected runtime exceptions from implementations
//       bubbling up and breaking the snapshot functionality.
// TODO: We shouldn't be blanket catching and suppressing all exceptions here and instead handle them safely upstream.
//       Currently this catch exists as a stop gap solution to tackle unexpected runtime exceptions from implementations
//       bubbling up and breaking the snapshot functionality.
// Once we are done writing the updated index-N blob we remove the now unreferenced index-${uuid} blobs in each shard
// directory if all nodes are at least at version SnapshotsService#SHARD_GEN_IN_REPO_DATA_VERSION
// If there are older version nodes in the cluster, we don't need to run this cleanup as it will have already happened
// when writing the index-${N} to each shard directory.
// We ignore all FileAlreadyExistsException when writing metadata since otherwise a master failover while in this method will
// mean that no snap-${uuid}.dat blob is ever written for this snapshot. This is safe because any updated version of the
// index or global metadata will be compatible with the segments written in this snapshot as well.
// Failing on an already existing index-${repoGeneration} below ensures that the index.latest blob is not updated in a way
// that decrements the generation it points at
// Write Global MetaData
// write the index metadata for each index in the snapshot
// Delete all old shard gen blobs that aren't referenced any longer as a result from moving to updated repository data
/**
// It's readonly - so there is not much we can do here to verify it apart from reading the blob store metadata
// Tracks the latest known repository generation in a best-effort way to detect inconsistent listing of root level index-N blobs
// and concurrent modifications.
// Retry loading RepositoryData in a loop in case we run into concurrent modifications of the repository.
// We're only using #latestKnownRepoGen as a hint in this mode and listing repo contents as a secondary way of trying
// to find a higher generation
// We only rely on the generation tracked in #latestKnownRepoGen which is exclusively updated from the cluster state
// We did not find the expected index-N even though the cluster state continues to point at the missing value
// of N so we mark this repository as corrupted.
/**
// EMPTY is safe here because RepositoryData#fromXContent calls namedObject
// If we fail to load the generation we tracked in latestKnownRepoGen we reset it.
// This is done as a fail-safe in case a user manually deletes the contents of the repository in which case subsequent
// operations must start from the EMPTY_REPO_GEN again
/**
// can not write to a read only repository
// the index file was updated by a concurrent operation, so we were operating on stale
// repository data
// Step 1: Set repository generation state to the next possible pending generation
// If we run into the empty repo generation for the expected gen, the repo is assumed to have been cleared of
// all contents by an external process so we reset the safe generation to the empty generation.
// Regardless of whether or not the safe generation has been reset, the pending generation always increments so that
// even if a repository has been manually cleared of all contents we will never reuse the same repository generation.
// This is motivated by the consistency behavior the S3 based blob repository implementation has to support which does
// not offer any consistency guarantees when it comes to overwriting the same blob name with different content.
// Step 2: Write new index-N blob to repository and update index.latest
// BwC logic: Load snapshot version information if any snapshot is missing a version in RepositoryData so that the new
// RepositoryData contains a version for every snapshot
// write the index file
// write the current generation to the index-latest file
// Step 3: Update CS to reflect new repository generation.
// Delete all now outdated index files up to 1000 blobs back from the new generation.
// If there are more than 1000 dangling index-N cleanup functionality on repo delete will take care of them.
// Deleting one older than the current expectedGen is done for BwC reasons as older versions used to keep
// two index-N blobs around.
/**
// First, try listing all index-N blobs (there should only be two index-N blobs at any given
// time in a repository if cleanup is happening properly) and pick the index-N blob with the
// highest N value - this will be the latest index blob for the repository.  Note, we do this
// instead of directly reading the index.latest blob to get the current index-N blob because
// index.latest is not written atomically and is not immutable - on every index-N change,
// we first delete the old index.latest and then write the new one.  If the repository is not
// read-only, it is possible that we try deleting the index.latest blob while it is being read
// by some other operation (such as the get snapshots operation).  In some file systems, it is
// illegal to delete a file while it is being read elsewhere (e.g. Windows).  For read-only
// repositories, we read for index.latest, both because listing blob prefixes is often unsupported
// and because the index.latest blob will never be deleted and re-written.
// If its a read-only repository, listing blobs by prefix may not be supported (e.g. a URL repository),
// in this case, try reading the latest index generation from the index.latest blob
// package private for testing
// the index- blob wasn't of the format index-N where N is a number,
// no idea what this blob is but it doesn't belong in the repository!
// TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should
// a commit point file with the same name, size and checksum was already copied to repository
// we will reuse it for this snapshot
// create a new FileInfo
// now create and write the commit point
// build a new BlobStoreIndexShardSnapshot, that includes this one and all the saved ones
// Delete all previous index-N blobs
// Start as many workers as fit into the snapshot pool at once at the most
// Start as many workers as fit into the snapshot pool at once at the most
// restore the files from the snapshot to the Lucene store
// Stop uploading the remaining files if we run into any exception
// Not adding a real generation here as it doesn't matter to callers
/**
// Build a list of snapshots that should be preserved
// Unused blobs are all previous index-, data- and meta-blobs and that are not referenced by the new index- as well as all
// temporary blobs
/**
/**
/**
/**
// Make reads abortable by mutating the snapshotStatus object
/**
// Index that the snapshot was removed from
// Shard id that the snapshot was removed from
// Id of the new index-${uuid} blob that does not include the snapshot any more
// Blob names in the shard directory that have become unreferenced in the new shard generation
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Serialization parameters to specify correct context for metadata serialization
// when metadata is serialized certain elements of the metadata shouldn't be included into snapshot
// exclusion of these elements is done by setting MetaData.CONTEXT_MODE_PARAM to MetaData.CONTEXT_MODE_SNAPSHOT
// serialize SnapshotInfo using the SNAPSHOT mode
// The format version
/**
/**
/**
// we trick this into a dedicated exception with the original stacktrace
/**
/**
// this is important since some of the XContentBuilders write bytes on close.
// in order to write the footer we need to prevent closing the actual index input.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// this will throw an IOException if the store has no segments infos file. The
// store can still have existing files but they will be deleted just before being
// restored.
// happens when restore to an empty shard, not a big deal
// list of all existing store files
// if a file with a same physical name already exist in the store we need to delete it
// before restoring it from the snapshot. We could be lenient and try to reuse the existing
// store files (and compare their names/length/checksum again with the snapshot files) but to
// avoid extra complexity we simply delete them and restore them again like StoreRecovery
// does with dangling indices. Any existing store file that is not restored from the snapshot
// will be clean up by RecoveryTarget.cleanFiles().
// read the snapshot data persisted
/// now, go over and clean files that are in the store, but were not in the snapshot
//skip write.lock, checksum files and files that exist in the snapshot
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Disable filtering when building error responses
/**
/**
// try to determine the response content type from the media type or the format query string parameter, with the format parameter
// taking precedence over the Accept header
// if there was a parsed content-type for the incoming request use that since no format was specified using the query
// string parameter or the HTTP Accept header
// default to JSON output when all else fails
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// prepare the request for execution; has the side effect of touching the request parameters
// validate unconsumed params, but we must exclude params used to format the response
// use a sorted set so the unconsumed parameters appear in a reliable sorted order
// validate the non-response params
// execute the action
// sort by distance in reverse order, then parameter name for equal distances
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
//tools.ietf.org/html/rfc5987
// 32 = ' ' (31 = unit separator); 126 = '~' (127 = DEL)
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Rest headers that are copied to internal requests made during a rest request. */
// passthrough if no wrapper set
/**
/**
// remove deprecation in next major release
// e.g., [POST /_optimize] is deprecated! Use [POST /_forcemerge] instead.
/**
// iff we could reserve bytes for the request we need to send the response also over this channel
// TODO: Count requests double in the circuit breaker if they need copying?
// Get the map of matching handlers for a request, for the full set of HTTP methods.
// If an alternative handler for an explicit path is registered to a
// different HTTP method than the one supplied - return a 405 Method
// Not Allowed error.
// error_trace cannot be used when we disable detailed errors
// we consume the error_trace parameter first to ensure that it is always consumed
// Resolves the HTTP method and fails if the method is invalid
// Loop through all possible handlers, attempting to dispatch the request
// If request has not been handled, fallback to a bad request error.
// Between retrieving the correct path, we need to reset the parameters,
// otherwise parameters are parsed out of the URI that aren't actually handled.
// PathTrie modifies the request, so reset the params between each iteration
// we use rawPath since we don't want to decode it while processing the path resolution
// so we can handle things like:
// my_index/my_type/http%3A%2F%2Fwww.google.com
/**
//tools.ietf.org/html/rfc2616#section-10.4.6">HTTP/1.1 -
/**
//tools.ietf.org/html/rfc2616#section-9.2">HTTP/1.1 - 9.2
// When we have an OPTIONS HTTP request and no valid handlers, simply send OK by default (with the Access Control Origin header
// which gets automatically added).
/**
/**
// attempt to close once atomically
// We always obtain a fresh breaker to reflect changes to the breaker configuration.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// tchar pattern as defined by RFC7230 section 3.2.6
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Treat empty string as true because that allows the presence of the url parameter to mean "turn this on"
/**
/**
// will throw exception if body or content type missing
/**
/**
/**
/**
/**
/**
// TODO stop ignoring parameters such as charset...
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Beginning of the unprocessed region
// End of the unprocessed region
// Current character
// We haven't seen an `=' so far but moved forward.
// Must be a param of the form '&a&' so add it with
// an empty value.
// Are there characters we haven't dealt with?
// Yes and we haven't seen any `='.
// Yes and this must be the last value.
// Have we seen a name without value?
/**
/**
// We can skip at least one char, e.g. `%%'.
// position in `buf'.
// "+" -> " "
// "%%" -> "%"
// Fall through.
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we use static here so we won't have to pass the actual logger each time for a very rare case of logging
// where the settings don't matter that much
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// pkg private method that we can override for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Default to pretty printing, but allow ?pretty=false to disable
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
//would be nice if we could make default methods final
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//would be nice if we could make default methods final
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Empty request signals "explain the first unassigned shard you find"
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// wait_for_relocating_shards has been removed in favor of wait_for_no_relocating_shards
/*
//www.apache.org/licenses/LICENSE-2.0
// by default, return everything but metadata
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// do not ask for what we do not need.
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// this endpoint is used for metrics, not for node IDs, like /_nodes/fs
// added this endpoint to be aligned with stats
// special case like /_nodes/os (in this case os are metrics and not the nodeId)
// still, /_nodes/_local (or any other node id) should work and be treated as usual
// this means one must differentiate between allowed metrics and arbitrary node ids in the same place
// shortcut, don't do checks if only all is specified
/*
//www.apache.org/licenses/LICENSE-2.0
// use a sorted set so the unrecognized parameters appear in a reliable sorted order
// check for index specific metrics
// use a sorted set so the unrecognized parameters appear in a reliable sorted order
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// only display indices that have aliases
// compute explicitly requested aliases that have are not returned in the result
// first wildcard index, leading "-" as an alias name after this index means
// that it is an exclusion
// only explicitly requested aliases will be called out as missing (404)
// check if aliases[i] is subsequently excluded
// this is an exclude pattern
// aliases[i] is excluded by aliases[j]
// explicitly requested aliases[i] is not excluded by any subsequent "-" wildcard in expression
// aliases[i] is not in the result set
// The TransportGetAliasesAction was improved do the same post processing as is happening here.
// We can't remove this logic yet to support mixed clusters. We should be able to remove this logic here
// in when 8.0 becomes the new version in the master branch.
//we may want to move this logic to TransportGetAliasesAction but it is based on the original provided aliases, which will
//not always be available there (they may get replaced so retrieving request.aliases is not quite the same).
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// This is required so the "flat_settings" parameter counts as consumed
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//we cannot add POST for "/_aliases" because this is the _aliases api already defined in RestIndicesAliasesAction
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// short cut, if no metrics have been specified in URI
// use a sorted set so the unrecognized parameters appear in a reliable sorted order
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//register the same paths, but with plural form _mappings
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// need to do left-align always, so create new cells
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//if we don't know how much we use (non data nodes), it means 0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Sort ascending by shard id for readability
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Indices that were successfully resolved during the get settings request might be deleted when the subsequent cluster
// state, cluster health and indices stats requests execute. We have to distinguish two cases:
// 1) the deleted index was explicitly passed as parameter to the /_cat/indices request. In this case we want the
//    subsequent requests to fail.
// 2) the deleted index was resolved as part of a wildcard or _all. In this case, we want the subsequent requests not to
//    fail on the deleted index (as we want to ignore wildcards that cannot be resolved).
// This behavior can be ensured by letting the cluster state, cluster health and indices stats requests re-resolve the
// index names with the same indices options that we used for the initial cluster state request (strictExpand).
/**
// package private for testing
// the index exists in the Get Indices response but is not present in the cluster state:
// it is likely that the index was deleted in the meanwhile, so we ignore it.
// index health is known but does not match the one requested
// index health is unknown, skip if we don't explicitly request RED health
// TODO: expose docs stats for replicated closed indices
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// only needed in v8 to catch breaking usages
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// We know we need the header asked for:
// Look for accompanying sibling column
// ...link the sibling and check that its flag is set
/**
// check headers and aliases
// Ignores the leftover spaces if the cell is the last of the column.
// Add additional built in data points we can render based on request parameters?
/*
//www.apache.org/licenses/LICENSE-2.0
// Task main info
// Node info
// Task detailed info
// Node information. Note that the node may be null because it has left the cluster between when we got this response and now.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// collect all thread pool names that we see across the nodes
// collect all thread pool names that match the specified thread pool patterns
// we use a sorted map to ensure that thread pools are sorted by name
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// auto id creation
// default to op_type create
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//package private for testing
//In case the channel is already closed when we register the listener, the listener will be immediately executed which will
//remove the channel from the map straight-away. That is why we first create the CloseListener and later we associate it
//with the channel. This guarantees that the close listener is already in the map when the it gets registered to its
//corresponding channel, hence it is always found in the map when it gets invoked if the channel gets closed.
//When the channel gets closed it won't be reused: we can remove it from the map and forget about it.
// we stash any context here since this is an internal execution and should not leak any existing context information
//We don't wait for cancel tasks to come back. Task cancellation is just best effort.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// NOTE: if rest request with xcontent body has request parameters, values parsed from request body have the precedence
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// only set if we have the parameter since we auto adjust the max concurrency on the coordinator
// based on the number of nodes in the cluster
// preserve if it's set on the request
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/**
// only set if we have the parameter since we auto adjust the max concurrency on the coordinator
// based on the number of nodes in the cluster
// only set if we have the parameter passed to override the cluster-level default
// do not allow 'query_and_fetch' or 'dfs_query_and_fetch' search types
// from the REST layer. these modes are an internal optimization and should
// not be specified explicitly by the user.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// NOTE: if rest request with xcontent body has request parameters, values parsed from request body have the precedence
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** A scorer that will return the score for the current document when the script is run. */
/**
/**
/** Return the score of the current document. */
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// jdk classes
/** Typical set of classes for scripting: basic data types, math, dates, and simple collections */
// this is the list from the old grovy sandbox impl (+ some things like String, Iterator, etc that were missing)
// joda-time
/**
/**
// check for a special value of STANDARD to imply the basic set
// BasicPermissionCollection only handles wildcards, we expand <<STANDARD>> here
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/elastic/elasticsearch/issues/8561
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The generic runtime parameters for the script. */
/** A leaf lookup for the bound segment this script will operate on. */
// for expression engine
/** The leaf lookup for the Lucene segment this script was created for. */
/** Return the parameters for this script. */
/** The doc lookup for the Lucene segment this script was created for. */
/** Set the current document to run the script on next. */
/** A factory to construct {@link FieldScript} instances. */
/** The context used to compile {@link FieldScript} factories. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no parameters for execute, but constant still required...
/** The generic runtime parameters for the script. */
/** A leaf lookup for the bound segment this script will operate on. */
/** Return {@code true} if the current document matches the filter, or {@code false} otherwise. */
/** Return the parameters for this script. */
/** The doc lookup for the Lucene segment this script was created for. */
/** Set the current document to run the script on next. */
/** A factory to construct {@link FilterScript} instances. */
/** A factory to construct stateful {@link FilterScript} factories for a specific index. */
/** The context used to compile {@link FilterScript} factories. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The context used to compile {@link IngestConditionalScript} factories. */
/** The generic runtime parameters for the script. */
/** Return the parameters for this script. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The context used to compile {@link IngestScript} factories. */
/** The generic runtime parameters for the script. */
/** Return the parameters for this script. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// NOTE: we don't check SpecialPermission because this will be called (indirectly) from scripts
// access the underlying ZonedDateTime
// TODO: replace with bwc formatter
// TODO: replace with bwc formatter
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** A helper to take in an explanation from a script and turn it into an {@link org.apache.lucene.search.Explanation}  */
/**
/** The generic runtime parameters for the script. */
/** A leaf lookup for the bound segment this script will operate on. */
// null check needed b/c of expression engine subclass
/** Return the parameters for this script. */
/** The doc lookup for the Lucene segment this script was created for. */
/** Set the current document to run the script on next. */
/**
/**
/**
/**
/**
/**
/**
/**
/**
/** A factory to construct {@link ScoreScript} instances. */
/**
/** A factory to construct stateful {@link ScoreScript} factories for a specific index. */
/*
//www.apache.org/licenses/LICENSE-2.0
/****** STATIC FUNCTIONS that can be used by users for score calculations **/
/**
// random score based on the documents' values of the given field
// only use the lower 24 bits to construct a float from 0.0-1.0
// random score based on the internal Lucene document Ids
// only use the lower 24 bits to construct a float from 0.0-1.0
// **** Decay functions on geo field
// cached variables calculated once per script execution
// **** Decay functions on numeric field
// **** Decay functions on date field
/**
// as java.lang.Math#abs(long) is a forbidden API, have to use this comparison instead
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// This cannot default to an empty map because options are potentially added at multiple points.
/**
//this is really for search templates, that need to be converted to json format
/**
/**
/**
/**
// Defines the fields necessary to parse a Script as XContent using an ObjectParser.
/**
/**
/**
/**
// it should not happen since we are not actually reading from a stream but an in-memory byte[]
/**
// Exactly one of "id" or "source" must be specified
// OR
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** A unique identifier for this context. */
/** A factory class for constructing script or stateful factory instances. */
/** A factory class for construct script instances. */
/** A class that is an instance of a script. */
/** Construct a context with the related instance and compiled classes. */
/** Returns a method with the given name, or throws an exception if multiple are found. */
/*
//www.apache.org/licenses/LICENSE-2.0
// ScriptService constructor
// Deserialization constructor
// ignored instead of error, so future implementations can add methods.  Same as ScriptContextInfo(String, Class).
// Test constructor
// See ScriptContext.findMethod
// TODO(stu): ensure empty/no PARAMETERS if parameterTypes.length == 0?
// See ScriptClassInfo.readArgumentNamesConstant
// See ScriptClassInfo(PainlessLookup painlessLookup, Class<?> baseClass)
/*
//www.apache.org/licenses/LICENSE-2.0
// copy params so we aren't modifying input
// add lookup vars
// wrap with deprecations
// Return the doc as a map (instead of LeafDocLookup) in order to abide by type whitelisting rules for
// Painless scripts.
// get_score() is named this way so that it's picked up by Painless as '_score'
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Returns {@code true} if the result of the script will be deterministic, {@code false} otherwise. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// due to a bug (https://github.com/elastic/elasticsearch/issues/47593)
// scripts may have been retained during upgrade that include the old-style
// id of lang#id; these scripts are unreachable after 7.0, so they are dropped
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// a parsing function that requires a non negative int and a timevalue as arguments split by a slash
// this allows you to easily define rates
// protect against a too hard to check limit, like less than a minute
// the number format exception message is so confusing, that it makes more sense to wrap it with a useful one
/**
/**
// Reset the counter to allow new compilations
/**
// * lang and options will both be null when looking up a stored script,
// so we must get the source to retrieve them before checking if the
// context is supported
// * a stored script must be pulled from the cluster state every time in case
// the script has been updated since the last compilation
// Synchronize so we don't compile scripts many times during multiple shards all compiling a script
// Retrieve it again in case it has been put by a different thread
// Either an un-cached inline script or indexed script
// If the script type is inline the name will be the same as the code for identification in exceptions
// but give the script engine the chance to be better, give it separate name + source code
// for the inline case, then its anonymous: null.
// Check whether too many compilations have happened
// TODO: remove this try-catch completely, when all script engines have good exceptions!
// its already good
// Since the cache key is the script content itself we don't need to
// invalidate/check the cache if an indexed script changes.
/**
//en.wikipedia.org/wiki/Token_bucket
// It's been over the time limit anyway, readjust the bucket to be level
// If there is enough tokens in the bucket, allow the request and decrease the tokens by 1
// Otherwise reject the request
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** A script that is used to build {@link ScriptedSimilarity} instances. */
/** Compute the score.
/*
//www.apache.org/licenses/LICENSE-2.0
/** A script that is used to compute scoring factors that are the same for all documents. */
/** Compute the weight.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// This cannot default to an empty map because options are potentially added at multiple points.
/**
// this is really for search templates, that need to be converted to json format
/**
/**
// Defines the fields necessary to parse a Script as XContent using an ObjectParser.
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Return the parameters for this script. */
/** Run a template and return the resulting string, encoded in utf8 bytes. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The context used to compile {@link UpdateScript} factories. */
/** The generic runtime parameters for the script. */
/** The update context for the script. */
/** Return the parameters for this script. */
/** Return the update context for this script. */
/*
//www.apache.org/licenses/LICENSE-2.0
// terminate after count
// by default, we don't return versions
// when sorting, track scores as well...
// filter for sliced scroll
/**
/**
// SearchContexts use a BigArrays that can circuit break
/**
// initialize the filtering alias based on the provided filters
//it's ok to use the writeable name here given that we enforce it to be the same as the name of the element that gets
//parsed by the corresponding parser. There is one single name and one single way to retrieve the parsed object from the context.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** A formatter for values as returned by the fielddata/doc-values APIs. */
/** Format a long value. This is used by terms and histogram aggregations
/** Format a double value. This is used by terms and stats aggregations
/** Format a binary value. This is used by terms aggregations to format
/** Parse a value that was formatted with {@link #format(long)} back to the
/** Parse a value that was formatted with {@link #format(double)} back to
/** Parse a value that was formatted with {@link #format(BytesRef)} back
// Prefer parsing as a long to avoid losing precision
// retry as a double
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//These two fields normally get set when setting the shard target, so they hold the same values as the target thus don't get
//serialized over the wire. When parsing hits back from xcontent though, in most of the cases (whenever explanation is disabled)
//we can't rebuild the shard target object so we need to set these manually for users retrieval.
//used only in tests
// we call the setter here because that also sets the local index parameter
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// returns the fields without handling null cases
/**
/**
/**
/**
/**
/**
/**
/**
// public because we render hit as part of completion suggestion option
// For inner_hit hits shard is null and that is ok, because the parent search hit has all this information.
// Even if this was included in the inner_hit hits this would be the same, so better leave it out.
// _ignored is the only multi-valued meta field
// TODO: can we avoid having an exception here?
/**
//these fields get set anyways when setting the shard target,
//but we set them explicitly when we don't have enough info to rebuild the shard target
// the original document gets slightly modified: whitespaces or
// pretty printing are not preserved,
// it all depends on the current builder settings
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: consider using static final instance
// track_total_hits is false
/**
/**
/**
/**
/**
/**
/**
// For BWC with nodes pre 7.0
// NaN gets rendered as null-field
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/* Reuses result readers from SamplerAggregator*/);
// This bucket is used by many pipeline aggreations.
/* Uses InternalBucketMetricValue */);
// This bucket is used by many pipeline aggreations.
/* Uses InternalSimpleValue */);
// ScriptScoreFunctionBuilder has it own named writable because of a new script_score query
//weight doesn't have its own parser, so every function supports it out of the box.
//Can be a single function too when not associated to any other function, which is why it needs to be registered manually here.
// TODO remove funky contexts
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// TODO: this seems wrong, SearchPhaseResult should have a writeTo?
/*
//www.apache.org/licenses/LICENSE-2.0
// we can have 5 minutes here, since we make sure to clean with search requests and when shard/index closes
/**
// once an index is removed due to deletion or closing, we can just clean up all the pending search context information
// if we then close all the contexts we can get some search failures along the way which are not expected.
// it's fine to keep the contexts open if the index is still "alive"
// unfortunately we don't have a clear way to signal today why an index is closed.
// to release memory and let references to the filesystem go etc.
/**
// execution exception can happen while loading the cache, strip it
// no hits, we can release the context since there will be no fetch phase
// simple search, no scroll
// scroll request, but the scroll was not extended
// currently, the concrete listener is CompositeListener, which swallows exceptions, but here we anyway try to do the
// right thing by closing and notifying onFreeXXX in case one of the listeners fails with an exception in the future.
// if the from and size are still not set, default them
// pre process
// compute the context keep alive
// we clone the query shard context here just for rewriting otherwise we
// might end up with incorrect state since we are using now() or script services
// during rewrite and normalized / evaluate templates etc.
// we handle the case where the DefaultSearchContext constructor throws an exception since we would otherwise
// leak a searcher and this can have severe implications (unable to obtain shard lock exceptions).
// disable timeout while executing a search
// nothing to parse...
/**
// no more docs...
// process scroll
// update the context keep alive based on the new scroll value
/**
// Use the same value for both checks since lastAccessTime can
// be modified by another thread between checks!
// its being processed or timeout is disabled
/**
// we don't want to use the reader wrapper since it could run costly operations
// and we can afford false positives.
// null query means match_all
/**
/*
// now we need to check if there is a pending refresh and register
// we also do rewrite on the coordinating node (TransportSearchService) but we also need to do it here for BWC as well as
// AliasFilters that might need to be rewritten. These are edge-cases but we are every efficient doing the rewrite here so it's not
// adding a lot of overhead
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//original indices are only needed in the coordinating node throughout the search request execution.
//no need to serialize them as part of SearchShardTarget.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//we currently format only BytesRef but we may want to change that in the future
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// We always use the type of the aggregation as the writeable name
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** Return this aggregation's name. */
/** Internal: build an {@link AggregatorFactory} based on the configuration of this builder. */
/** Associate metadata with this {@link AggregationBuilder}. */
/** Return any associated metadata with this {@link AggregationBuilder}. */
/** Add a sub aggregation to this builder. */
/** Add a sub aggregation to this builder. */
/** Return the configured set of subaggregations **/
/** Return the configured set of pipeline aggregations **/
/**
/**
/**
/**
/** Common xcontent fields shared among aggregator builders */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: report on child aggs as well
// no need to compute the aggs twice, they should be computed on a per context basis
// optimize the global collector based execution
// TODO: report on sub collectors
// start a new profile with this collector
// disable aggregations so that they don't run on next pages in case of scrolling
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// IMPORTANT: DO NOT add methods to this class unless strictly required.
// On the other hand, if you can remove methods from it, you are highly welcome!
/**
/**
/**
/**
/**
/**
/**
/**
/**
/** Aggregation mode for sub aggregations. */
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The default "weight" that a bucket takes when performing an aggregation */
// 5kb
/**
// Register a safeguard to highlight any invalid construction logic (call to this constructor without subsequent preCollection call)
// unreachable but compiler does not agree
// unreachable
/**
// Only use the potential to circuit break if bytes are being incremented
/**
/**
/**
/**
/**
/**
/**
/**
// post-collect this agg before subs to make it possible to buffer and then replay in postCollection()
/** Called upon release of the aggregator. */
/** Release instance-specific data. */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// TODO: sometimes even sub aggregations always get called with bucket 0, eg. if
// you have a terms agg under a top-level filter agg. We should have a way to
// propagate the fact that only bucket 0 will be collected with single-bucket
// aggs
// These aggregators are going to be used with a single bucket ordinal, no need to wrap the PER_BUCKET ones
// top-level aggs only get called with bucket 0
/**
/**
// Using LinkedHashSets to preserve the order of insertion, that makes the results
// ordered nicely, although technically order does not matter
/**
/**
/**
// Check the non-pipeline sub-aggregator
// factories
// Check the pipeline sub-aggregator factories
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Passing a null scorer can cause unexpected NPE at a later time,
// which can't not be directly linked to the fact that a null scorer has been supplied.
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no-op
// no-op
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Concatenates the type and the name of the aggregation (ex: top_hits#foo)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// first we collect all aggregations of the same type and list them together
// now we can use the first aggregation of each list to handle the reduce of its list
// Sort aggregations so that unmapped aggs come last in the list
// If all aggs are unmapped, the agg that leads the reduction will just return itself
// the list can't be empty as it's created on demand
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// This is a bucket key, look through our buckets and see if we can find a match
// No key match, time to give up
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// Internal Optimization for terms aggregation to avoid constructing buckets for ordering purposes
/**
/**
/**
/**
/**
// add key order ascending as a tie-breaker to avoid non-deterministic ordering
// if all user provided comparators return 0.
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// check if its a compound order with the first element that matches
/**
/**
/**
/**
/**
/**
/**
// assume all other orders are sorting on a sub-aggregation. Validation occurs later.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no-op
// no-op
/**
// no-op by default
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** See {@link #wrap(Iterable)}. */
/**
// For the user's convenience, we allow NO_OP collectors to be passed.
// However, to improve performance, these null collectors are found
// and dropped from the array we save for actual collection time.
// only 1 Collector - return it.
// this leaf collector does not need this segment
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// aggregations execute in a single thread so no atomic here
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// the framework will automatically eliminate it
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Concatenates the type and the name of the aggregation (ex: top_hits#foo)
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Subclasses can override the getKeyAsString method to handle specific cases like
// keyed bucket with RAW doc value format where the key_as_string field is not printed
// out but we still need to have a string version of the key to use as the bucket's name.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** Return this aggregation's name. */
/** Return the consumed buckets paths. */
/**
/**
/** Associate metadata with this {@link PipelineAggregationBuilder}. */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** Set the deferred collectors. */
// allocates the builder lazily in case this segment doesn't contain any match
/**
// We don't need to check if the scorer is null
// since we are sure that there are documents to replay (entry.docDeltas it not empty).
// aggregations should only be replayed on matching documents
// collection was terminated prematurely
// continue with the following leaf
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Skip any in the map which have been "removed", signified with -1
/**
/**
// This may happen eg. if no document in the highest buckets is accepted by a sub aggregator.
// For example, if there is a long terms agg on 3 terms 1,2,3 with a sub filter aggregator and if no document with 3 as a value
// matches the filter, then the filter will never collect bucket ord 3. However, the long terms agg will call
// bucketAggregations(3) on the filter aggregator anyway to build sub-aggregations.
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Request 50% more buckets on the shards in order to improve accuracy
// as well as a small constant that should help with small values of 'size'
/*
//www.apache.org/licenses/LICENSE-2.0
// Default impl is a collector that selects the best buckets
// but an alternative defer policy may be based on best docs.
/**
// Being lenient here - ignore calls where there are no deferred
// collections to playback
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Sole constructor. */
/** Set the deferred collectors. */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Only merge in the ordinal if it hasn't been "removed", signified with -1
// we are skipping this ordinal, which means we need to accumulate the
// doc delta's since the last "good" delta
// Only create an entry if this segment has buckets after merging
// if there are buckets that have been collected in the current segment
// we need to update the bucket ordinals there too
// The current segment's deltas aren't built yet, so build to a temp object
// Only merge in the ordinal if it hasn't been "removed", signified with -1
// we are skipping this ordinal, which means we need to accumulate the
// doc delta's since the last "good" delta.
// The first is skipped because the original deltas are stored as offsets from first doc,
// not offsets from 0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Convert uniquely named objects into internal KeyedFilters
// internally we want to have a fixed order of filters, regardless of
// the order of the filters in the request
// internally we want to have a fixed order of filters, regardless of
// the order of the filters in the request
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// no need to provide deleted docs to the filter
// Check each of the provided filters
// Check all the possible intersections of the provided filters
// Skip checks on all the other filters given one half of the pairing failed
// Buckets are ordered into groups - [keyed filters] [key1&key2 intersects]
// Empty buckets are not returned because this aggregation will commonly be used under a
// a date-histogram where we will look for transactions over time and can expect many
// empty buckets.
// Empty buckets are not returned due to potential for very sparse matrices
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// check that the provided size is not greater than the search.max_buckets setting
// Replay all documents that contain at least one top bucket (collected during the first pass).
/** Return true if the provided field may have multiple values per document in the leaf **/
// we have no clue whether the field is multi-valued or not so we assume it is.
/**
// TODO: can we handle missing bucket when using index sort optimization ?
// the leading index sort matches the leading source field but the order is reversed
// so we don't check the other sources.
/**
// include all docs that belong to the partial bucket
// Visit documents sorted by the leading source of the composite definition and terminates
// when the leading source value is guaranteed to be greater than the lowest composite bucket
// in the queue.
// We can bypass search entirely for this segment, the processing is done in the previous call.
// Throwing this exception will terminate the execution of the search for this root aggregation,
// see {@link MultiCollector} for more details on how we handle early termination in aggregations.
// We have an after key and index sort is applicable so we jump directly to the doc
// that is after the index sort prefix using the rawAfterKey and we start collecting
// document from there.
/**
/**
// aggregations should only be replayed on matching documents
/**
// The candidate key is a top bucket.
// We can defer the collection of this document/bucket to the sub collector
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// the slot for the current candidate
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// checks if the candidate key is competitive
// this key is already in the top N, skip it
// the leading index sort is in the reverse order of the leading source
// so we can early terminate when we reach a document that is smaller
// than the after key (collected on a previous page).
// key was collected on a previous page, skip it (>= afterKey).
// the tree map is full, check if the candidate key should be kept
// index sort guarantees that there is no key greater or equal than the
// current one in the subsequent documents so we can early terminate.
// the candidate key is not competitive, skip it.
// the candidate key is competitive
// the queue is full, we replace the last key with this candidate
// and we recycle the deleted slot
// move the candidate key to its new slot
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/** Return the interval as a date time unit if applicable, regardless of how it was configured. If this returns
/**
/**
/**
/**
/**
// is specified in the builder.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// is specified in the builder.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// the top value is missing in this shard, the comparison is against
// the insertion point of the top value so equality means that the value
// is "after" the insertion point.
// convert negative insert position
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Visible for tests
// Visible for tests
/* Attach the formats from the last bucket to the reduced composite
/* Use the formats from the bucket because they'll be right to format
// returns the formatted key in a map
// get the raw key (without formatting to preserve the natural order).
// visible for testing
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// for date histogram source with "format", the after value is formatted
// as a string so we need to retrieve the original value in milliseconds.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no value for the field
// lower bucket is inclusive
// this bucket does not have any competitive composite buckets,
// we can early terminate the collection because the remaining buckets are guaranteed
// to be greater than this bucket.
// does not match the query
// check the current bounds
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// inverse of the natural order
// do not use the index if it has more than 50% of deleted docs
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// we need to add the matching document in the builder
// so we build a bulk adder from the approximate cost of the iterator
// and rebuild the adder during the collection if needed
// the cost approximation was lower than the real size, we need to grow the adder
// by some numbers (128) to ensure that we can add the extra documents
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no value for the field
// this bucket does not have any competitive composite buckets,
// we can early terminate the collection because the remaining buckets are guaranteed
// to be greater than this bucket.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// The field is unmapped so we use a value source that can parse any type of values.
// This is needed because the after values are parsed even when there are no values to process.
// defaults to the raw format on date fields (preserve timestamp as longs).
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no need to provide deleted docs to the filter
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// internally we want to have a fixed order of filters, regardless of the order of the filters in the request
/**
/**
/**
/**
/**
/**
/**
/**
// automatically enable the other bucket if a key is set, as per the doc
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no need to provide deleted docs to the filter
// other bucket
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// bucketMap gets lazily initialized from buckets in getBucketByKey()
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// in case this is not a keyed aggregation, we need to add numeric keys to the buckets
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//different GeoPoints could map to the same or different hashing cells.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/* recognized field names in JSON */
/**
/**
/**
// no validation done here, similar to geo_bounding_box query behavior.
// Use default heuristic to avoid any wrong-ranking caused by
// distributed counting
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// already seen
/**
// need a special function to keep the source bucket
// up-to-date so it can get the appropriate key
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// Mathematics for this code was adapted from https://wiki.openstreetmap.org/wiki/Slippy_map_tilenames#Java
// Number of tiles for the current zoom level along the X and Y axis
// Edge values may generate invalid values, and need to be clipped.
// For example, polar regions (above/below lat 85.05112878) get normalized.
/**
/**
// Zoom value is placed in front of all the bits used for the geotile
// e.g. when max zoom is 29, the largest index would use 58 bits (57th..0th),
// leaving 5 bits unused for zoom. See MAX_ZOOM comment above.
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// package protected for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Nothing to write
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Create a new builder with the given name. */
/** Read from a stream, for internal use only. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// already seen
// the contract of the histogram aggregation is that shards must return
// buckets ordered by key in ascending order
// value source will be null for unmapped fields
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Create a new builder with the given name. */
/** Read from a stream, for internal use only. */
// TODO: No idea how we'd support Range scripts here.
/** Get the current interval in milliseconds that is set on this builder. */
/** Set the interval on this builder, and return the builder so that calls can be chained.
/** Get the current date interval that is set on this builder. */
/** Set the interval on this builder, and return the builder so that calls can be chained.
/**
/**
/**
/**
/** Get the offset to use when rounding, which is a number of milliseconds. */
/** Set the offset on this builder, which is a number of milliseconds, and
/** Set the offset on this builder, as a time value, and
/**
/** Return extended bounds for this histogram, or {@code null} if none are set. */
/** Set extended bounds on this histogram, so that buckets would also be
/** Return the order to use to sort buckets of this histogram. */
/** Set a new order on this builder and return the builder so that calls
// if order already contains a tie-breaker we are good to go
// otherwise add a tie-breaker by using a compound order
/**
// if the list only contains one order use that to avoid inconsistent xcontent
/** Return whether buckets should be returned as a hash. In case
/** Set whether to return buckets as a hash or as an array, and return the
/** Return the minimum count of documents that buckets need to have in order
/** Set the minimum count of matching documents that buckets need to have
/*
// We need all not only values but also rounded values to be within
// [prevTransition, nextTransition].
// We're not sure what the interval was originally (legacy) so use old behavior of assuming
// calendar first, then fixed. Required because fixed/cal overlap in places ("1h")
// rounding rounds down, so 'nextTransition' is a good upper bound
// All values in this reader have the same offset despite daylight saving times.
// This is very common for location-based timezones such as Europe/Paris in
// combination with time-based indices.
// TODO use offset here rather than explicitly in the aggregation
// parse any string bounds to longs and round
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We can use shardRounding here, which is sometimes more efficient
// if daylight saving times are involved.
// already seen
// the contract of the histogram aggregation is that shards must return buckets ordered by key in ascending order
// value source will be null for unmapped fields
// Important: use `rounding` here, not `shardRounding`
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// NOTE: this field is deprecated and will be removed
/** Get the current interval in milliseconds that is set on this builder. */
/** Set the interval on this builder, and return the builder so that calls can be chained.
/** Get the current date interval that is set on this builder. */
/** Set the interval on this builder, and return the builder so that calls can be chained.
/**
/**
/**
/**
// Parse to make sure it is a valid fixed too
/** Return the interval as a date time unit if applicable, regardless of how it was configured. If this returns
/**
// We're not sure what the interval was originally (legacy) so use old behavior of assuming
// calendar first, then fixed.  Required because fixed/cal overlap in places ("1h")
// If we get here we have exhausted our options and are not able to parse this interval
// If we're the same or have no existing type, just use the provided type
// interval() method
// dateHistogramInterval() takes precedence over interval()
// dateHistogramInterval() takes precedence over interval()
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Is it possible for valuesCount to be > 1 here? Multiple ranges are encoded into the same BytesRef in the binary doc
// values, so it isn't clear what we'd be iterating over.
// The encoding should ensure that this assert is always true.
// Bucket collection identical to NumericHistogramAggregator, could be refactored
// already seen
// the contract of the histogram aggregation is that shards must return buckets ordered by key in ascending order
// value source will be null for unmapped fields
// Important: use `rounding` here, not `shardRounding`
/*
//www.apache.org/licenses/LICENSE-2.0
// nothing to do with it
// nothing to do with it
/**
/**
/**
/**
/**
/**
/**
// TODO: Should we rather pass roundUp=true?
// Extended bounds shouldn't be effected by the offset
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: No idea how we'd support Range scripts here.
/** Create a new builder with the given name. */
/** Read from a stream, for internal use only. */
/** Get the current interval that is set on this builder. */
/** Set the interval on this builder, and return the builder so that calls can be chained. */
/** Get the current offset that is set on this builder. */
/** Set the offset on this builder, and return the builder so that calls can be chained. */
/** Get the current minimum bound that is set on this builder. */
/** Get the current maximum bound that is set on this builder. */
/**
/** Return the order to use to sort buckets of this histogram. */
/** Set a new order on this builder and return the builder so that calls
// if order already contains a tie-breaker we are good to go
// otherwise add a tie-breaker by using a compound order
/**
// if the list only contains one order use that to avoid inconsistent xcontent
/** Return whether buckets should be returned as a hash. In case
/** Set whether to return buckets as a hash or as an array, and return the
/** Return the minimum count of documents that buckets need to have in order
/** Set the minimum count of matching documents that buckets need to have
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Implemented by histogram aggregations and used by pipeline aggregations to insert buckets. */
// public so that pipeline aggs can use this API: can we fix it?
/** Get the key for the given bucket. Date histograms must return the
/** Given a key returned by {@link #getKey}, compute the lowest key that is 
/** Create an {@link InternalAggregation} object that wraps the given buckets. */
/** Create a {@link MultiBucketsAggregation.Bucket} object that wraps the
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// No need to take the keyed and format parameters into account,
// they are already stored and tested on the InternalDateHistogram object
/**
/**
// First we need to find the highest level rounding used across all the
// shards
// This rounding will be used to reduce all the buckets
// list of buckets coming from different shards that have the same key
// the key changes, reduce what we already buffered and reset the buffer for current buckets
// merge buckets using the new rounding
// Add the empty buckets within the data,
// e.g. if the data series is [1,2,3,7] there're 3 empty buckets that will be created for 4,5,6
// Getting the accurate number of required buckets can be slow for large
// ranges at low roundings so get a rough estimate of the rounding first
// so we are at most 1 away from the correct rounding and then get the
// accurate rounding value
// The loop will increase past the correct rounding index here so we
// need to subtract one to get the rounding index we need
// adding empty buckets if needed
// Adding empty buckets may have tipped us over the target so merge the buckets again if needed
// Now finally see if we need to merge consecutive buckets together to make a coarser interval at the same rounding
// HistogramFactory method impls
// convert buckets to the right type
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// No need to take the keyed and format parameters into account,
// they are already stored and tested on the InternalDateHistogram object
/**
// list of buckets coming from different shards that have the same key
// the key changes, reduce what we already buffered and reset the buffer for current buckets
/**
// first adding all the empty buckets *before* the actual data (based on th extended_bounds.min the user requested)
// now adding the empty buckets within the actual data,
// e.g. if the data series is [1,2,3,7] there're 3 empty buckets that will be created for 4,5,6
// finally, adding the empty buckets *after* the actual data (based on the extended_bounds.max requested by the user)
// we just need to reverse here...
// nothing to do when sorting by key ascending, as data is already sorted since shards return
// sorted buckets and the merge-sort performed by reduceBuckets maintains order.
// otherwise, sorted by compound order or sub-aggregation, we need to fall back to a costly n*log(n) sort
// HistogramFactory method impls
// convert buckets to the right type
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// No need to take the keyed and format parameters into account,
// they are already stored and tested on the InternalHistogram object
/**
// list of buckets coming from different shards that have the same key
// The key changes, reduce what we already buffered and reset the buffer for current buckets.
// Using Double.compare instead of != to handle NaN correctly.
// first adding all the empty buckets *before* the actual data (based on th extended_bounds.min the user requested)
// fill with empty buckets
// fill with empty buckets until the first key
// now adding the empty buckets within the actual data,
// e.g. if the data series is [1,2,3,7] there're 3 empty buckets that will be created for 4,5,6
// finally, adding the empty buckets *after* the actual data (based on the extended_bounds.max requested by the user)
// we just need to reverse here...
// nothing to do when sorting by key ascending, as data is already sorted since shards return
// sorted buckets and the merge-sort performed by reduceBuckets maintains order.
// otherwise, sorted by compound order or sub-aggregation, we need to fall back to a costly n*log(n) sort
// HistogramFactory method impls
// convert buckets to the right type
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// already seen
// the contract of the histogram aggregation is that shards must return buckets ordered by key in ascending order
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Is it possible for valuesCount to be > 1 here? Multiple ranges are encoded into the same BytesRef in the binary doc
// values, so it isn't clear what we'd be iterating over.
// The encoding should ensure that this assert is always true.
// Bucket collection identical to NumericHistogramAggregator, could be refactored
// already seen
// TODO: buildAggregation and buildEmptyAggregation are literally just copied out of NumericHistogramAggregator.  We could refactor
// this to an abstract super class, if we wanted to.  Might be overkill.
// the contract of the histogram aggregation is that shards must return buckets ordered by key in ascending order
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// in case the path has been unmapped:
// "field" doesn't exist, so we fall back to the context of the ancestors
/*
//www.apache.org/licenses/LICENSE-2.0
// if parentDoc is 0 then this means that this parent doesn't have child docs (b/c these appear always before the parent
// doc), so we can skip:
// if parentDoc is 0 then this means that this parent doesn't have child docs (b/c these appear always before the parent
// doc), so we can skip:
// cache the score of the current parent
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// In ES if parent is deleted, then also the children are deleted, so the child docs this agg receives
// must belong to parent docs that is alive. For this reason acceptedDocs can be null here.
// fast forward to retrieve the parentDoc this childDoc belongs to
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** A range aggregator for values that are stored in SORTED_SET doc values. */
// inclusive
// all candidates are between these indexes
// no potential candidate
// binary search the lower bound
// binary search the upper bound
// all candidates are between these indexes
// no potential candidate
// binary search the lower bound
// binary search the upper bound
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// We need to call processRanges here so they are parsed and we know whether `now` has been used before we make
// the decision of whether to cache the request
// from/to provided as double should be converted to string and parsed regardless to support
// different formats like `epoch_millis` vs. `epoch_second` with numeric input
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// should not happen since we only parse geo points when we encounter a string, an object or an array
// ignore null value
/**
// for parsing
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// even if the geo points are unique, there's no guarantee the
// distances are
/*
//www.apache.org/licenses/LICENSE-2.0
/** A range aggregation for data that is encoded in doc values using a binary representation. */
// keyed and format are ignored since they are already tested on the InternalBinaryRange object
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// create the lower value by zeroing out the host portion, upper value by filling it with all ones.
/** Get the current list or ranges that are configured on this aggregation. */
/** Add a new {@link Range} to this aggregation. */
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// We need to call processRanges here so they are parsed before we make the decision of whether to cache the request
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ignore null value
// all candidates are between these indexes
// no potential candidate
// binary search the lower bound
// binary search the upper bound
// value source can be null in the case of unmapped fields
// value source can be null in the case of unmapped fields
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** Set the deferred collectors. */
// Deferring collector
// Designed to be overridden by subclasses that may score docs by criteria
// other than Lucene score
// Can be overridden by subclasses that have a different priority queue implementation
// and need different memory sizes
// Generic sentinel object
// no-op - deferred aggs processed in postCollection call
// ScoreDoc is 12b ([float + int + int])
// Sort the top matches by docID for the benefit of deferred collector
// done with allDocs now, reclaim some memory
// Add to CB based on the size and the implementations per-doc overhead
// A bit of a hack to (ab)use shardIndex property here to
// hold a bucket ID but avoids allocating extra data structures
// and users should have bigger concerns if bucket IDs
// exceed int capacity..
// The publisher behaviour for Reader/Scorer listeners triggers a
// call to this constructor with a null scorer so we can't call
// scorer.getWeight() and pass the Weight to our base class.
// However, passing null seems to have no adverse effects here...
// Doc ids from TopDocCollector are root-level Reader so
// need rebasing
// We stored the bucket ID in Lucene's shardIndex property
// for convenience.
// collection was terminated prematurely
// continue with the following leaf
// There are conditions where no docs are collected and the aggs
// framework still asks for doc count.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// In some cases using ordinals is just not supported: override
// it
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Make sure we do not allow size > maxDoc, to prevent accidental OOM
// This class extends the DiversifiedTopDocsCollector and provides
// a lookup from elasticsearch's ValuesSource
/*
//www.apache.org/licenses/LICENSE-2.0
// Need to use super class shardSize since it is limited to maxDoc
/**
// Make sure we do not allow size > maxDoc, to prevent accidental OOM
// This class extends the DiversifiedTopDocsCollector and provides
// a lookup from elasticsearch's ValuesSource
// already seen
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Make sure we do not allow size > maxDoc, to prevent accidental OOM
// This class extends the DiversifiedTopDocsCollector and provides
// a lookup from elasticsearch's ValuesSource
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Make sure we do not allow size > maxDoc, to prevent accidental OOM
// This class extends the DiversifiedTopDocsCollector and provides
// a lookup from elasticsearch's ValuesSource
// Check there isn't a second value for this
// document
/*
//www.apache.org/licenses/LICENSE-2.0
// InternalSampler and UnmappedSampler share the same parser name, so we use this when identifying the aggregation type
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Make sure we do not allow size > maxDoc, to prevent accidental OOM
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no context in this reader
// if minDocCount == 0 then we can end up with more buckets then maxBucketOrd() returns
// During shard-local down-selection we use subset/superset stats
// that are for this shard only
// Back at the central reducer these properties will be updated with
// global stats
// the terms are owned by the BytesRefHash, we need to pull a copy since the BytesRef hash data may be recycled at some point
// We need to account for the significance of a miss in our global stats - provide corpus size as context
/*
//www.apache.org/licenses/LICENSE-2.0
//There is a condition (presumably when only one shard has a bucket?) where reduce is not called
// and I end up with buckets that contravene the user's min_doc_count criteria in my reducer
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// TODO we should refactor to remove this, since buckets should be immutable after they are generated.
// This can lead to confusing bugs if the bucket is re-created (via createBucket() or similar) without
// the score
/**
// Compute the overall result set size and the corpus size using the
// top-level Aggregations from each shard
// Adjust the buckets with the global stats representing the
// total size of the pots from which the stats are drawn
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// During shard-local down-selection we use subset/superset stats that are for this shard only
// Back at the central reducer these properties will be updated with global stats
// We need to account for the significance of a miss in our global stats - provide corpus size as context
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// this method is needed for scripted numeric aggregations
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// During shard-local down-selection we use subset/superset stats
// that are for this shard only
// Back at the central reducer these properties will be updated with
// global stats
// the terms are owned by the BytesRefHash, we need to pull a copy since the BytesRef hash data may be
// recycled at some point
// We need to account for the significance of a miss in our global stats - provide corpus size as context
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Important - need to use the doc count that includes deleted docs
// or we have this issue: https://github.com/elastic/elasticsearch/issues/7951
/**
// for types that use the inverted index, we prefer using a caching terms
// enum that will do a better job at reusing index inputs
// otherwise do it the naive way
// The user has not made a shardSize selection .
// Use default heuristic to avoid any wrong-ranking caused by
// distributed counting
// but request double the usual amount.
// We typically need more than the number of "top" terms requested
// by other aggregations
// as the significance algorithm is in less of a position to
// down-select at shard-level -
// some of the things we want to find have only one occurrence on
// each shard and as
// such are impossible to differentiate from non-significant terms
// at that early stage.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Only update the circuitbreaker after
// already seen
// During shard-local down-selection we use subset/superset stats
// that are for this shard only
// Back at the central reducer these properties will be updated with
// global stats
// the terms are owned by the BytesRefHash, we need to pull a copy since the BytesRef hash data may be recycled at some point
// We need to account for the significance of a miss in our global stats - provide corpus size as context
/*
//www.apache.org/licenses/LICENSE-2.0
// Note that if the field is unmapped (its field type is null), we don't fail,
// and just use the given field name as a placeholder.
// Important - need to use the doc count that includes deleted docs
// or we have this issue: https://github.com/elastic/elasticsearch/issues/7951
/**
// for types that use the inverted index, we prefer using a caching terms
// enum that will do a better job at reusing index inputs
// otherwise do it the naive way
// The user has not made a shardSize selection.
// Use default heuristic to avoid any wrong-ranking caused by
// distributed counting but request double the usual amount.
// We typically need more than the number of "top" terms requested
// by other aggregations as the significance algorithm is in less
// of a position to down-select at shard-level - some of the things
// we want to find have only one occurrence on each shard and as
// such are impossible to differentiate from non-significant terms
// at that early stage.
//        TODO - need to check with mapping that this is indeed a text field....
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Nothing to write
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// here we check if the term appears more often in subset than in background without subset.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
//arxiv.org/pdf/cs/0412098v3.pdf
// no co-occurrence
// perfect co-occurrence
//we must invert the order of terms because GND scores relevant terms low
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Nothing to read.
/**
// avoid any divide by zero issues
// If we are using a background context that is not a strict superset, a foreground
// term may be missing from the background, so for the purposes of this calculation
// we assume a value of 1 for our calculations which avoids returning an "infinity" result
// Using absoluteProbabilityChange alone favours very common words e.g. you, we etc
// because a doubling in popularity of a common term is a big percent difference
// whereas a rare term would have to achieve a hundred-fold increase in popularity to
// achieve the same difference measure.
// In favouring common words as suggested features for search we would get high
// recall but low precision.
// Using relativeProbabilityChange tends to favour rarer terms e.g.mis-spellings or
// unique URLs.
// A very low-probability term can very easily double in popularity due to the low
// numbers required to do so whereas a high-probability term would have to add many
// extra individual sightings to achieve the same shift.
// In favouring rare words as suggested features for search we would get high
// precision but low recall.
// A blend of the above metrics - favours medium-rare terms to strike a useful
// balance between precision and recall.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// here we check if the term appears more often in subset than in background without subset.
/*  make sure that
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
//documents not in class and do not contain term
//documents in class and do not contain term
// documents not in class and do contain term
// documents in class and do contain term
//documents that do not contain term
//documents that contain term
//documents that are not in class
//documents that are in class
//all docs
//documents not in class and do not contain term
//documents in class and do not contain term
// documents not in class and do contain term
// documents in class and do contain term
//documents that do not contain term
//documents that contain term
//documents that are not in class
//documents that are in class
//all docs
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Nothing to read.
// move to the closing bracket
/**
// avoid a divide by zero issue
/*
//www.apache.org/licenses/LICENSE-2.0
// This class holds an executable form of the script with private variables ready for execution
// on a single search thread.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// sort by count ascending
// We seed the rng with the ShardID so results are deterministic and don't change randomly
/*
// already seen
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// reverse, since we reverse again when adding to a list
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: cache the acceptedglobalValues per aggregation definition.
// We can't cache this yet in ValuesSource, since ValuesSource is reused per field for aggs during the execution.
// If aggs with same field, but different include/exclude are defined, then the last defined one will override the
// first defined one.
// So currently for each instance of this aggregator the acceptedglobalValues will be computed, this is unnecessary
// especially if this agg is on a second layer or deeper.
// no context in this reader
// if minDocCount == 0 then we can end up with more buckets then maxBucketOrd() returns
// Get the top buckets
//replay any deferred collections
//Now build the aggs
/**
/**
// We use set(...) here, because we need to reset the slow to 0.
// segmentDocCounts get reused over the segments and otherwise counts would be too high.
// remember we do +1 when counting
// reset the iterator
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Needed to add this seed for a deterministic term hashing policy
// otherwise tests fail to get expected results and worse, shards
// can disagree on which terms hash to the required partition.
// for parsing purposes only
// TODO: move all aggs to the same package so that this stuff could be pkg-private
// The includeValue and excludeValue ByteRefs which are the result of the parsing
// process are converted into a LongFilter when used on numeric fields
// in the index.
// hash the value to keep even distributions
// Only used for the 'map' execution mode (ie. scripts)
/**
/**
/**
// TODO: specialize based on compiled.type: for ALL and prefixes (sinkState >= 0 ) we can avoid i/o and just set bits.
// default to all terms being acceptable
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Unmapped rare terms don't have a cuckoo filter so we'll skip all this work
// and save some type casting headaches later.
// control gets into this loop when the same field name against which the query is executed
// is of different types in different indices.
// this term has gone over threshold while merging, so add it to the filter.
// Note this may happen during incremental reductions too
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// No need to take format and showDocCountError, they are attributes
// of the parent terms aggregation object that are only copied here
// for serialization purposes
/**
// control gets into this loop when the same field name against which the query is executed
// is of different types in different indices.
// If there is an existing docCountError for this agg then
// use this as the error for this aggregation
// otherwise use the doc count of the last term in the
// aggregation
// If there is already a doc count error for this bucket
// subtract this aggs doc count error from it to make the
// new value for the bucket. This then means that when the
// final error for the bucket is calculated below we account
// for the existing error calculated in a previous reduce.
// Note that if the error is unbounded (-1) this will be fixed
// later in this method.
// For the per term doc count error we add up the errors from the
// shards that did not respond with the term. To do this we add up
// the errors from the shards that did respond with the terms and
// subtract that from the sum of the error from all shards
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// if the key is below threshold, reinsert into the new ords
// Make a note when one of the ords has been deleted
// Only merge/delete the ordinals if we have actually deleted one,
// to save on some redundant work
// Finalize the buckets
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// already seen
// we need to fill-in the blanks
// Get the top buckets
// Now build the aggs
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
//TODO review: what size cap should we put on this?
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//TODO global ords not implemented yet, only supports "map"
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// this method is needed for scripted numeric aggs
/*
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// SortedBinaryDocValues don't guarantee uniqueness so we
// need to take care of dups
/**
// if the key is below threshold, reinsert into the new ords
// Make a note when one of the ords has been deleted
// Only merge/delete the ordinals if we have actually deleted one,
// to save on some redundant work
// Finalize the buckets
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// this method is needed for scripted numeric aggs
/*
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// SortedBinaryDocValues don't guarantee uniqueness so we
// need to take care of dups
// already seen
// we need to fill-in the blanks
// brute force
// Get the top buckets
// replay any deferred collections
// Now build the aggs
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// automatically adds tie-breaker key asc order
/**
/**
/**
/**
/**
/**
/**
/**
/**
/** Set a new order on this builder and return the builder so that calls
// if order already contains a tie-breaker we are good to go
// otherwise add a tie-breaker by using a compound order
/**
// if the list only contains one order use that to avoid inconsistent xcontent
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// shard_size cannot be smaller than size as we need to at least fetch <size> entries from every shards in order to return
// <size>
// shard_min_doc_count should not be larger than min_doc_count because this can cause buckets to be removed that would match
// the min_doc_count criteria
/**
// Don't defer any child agg if we are dependent on it for pruning results
/**
// with only support single-bucket aggregators
// some metrics may return NaN (eg. avg, variance, etc...) in which case we'd like to push all of those to
// the bottom
// single-value metrics agg
// some metrics may return NaN (eg. avg, variance, etc...) in which case we'd like to push all of those to
// the bottom
/*
//www.apache.org/licenses/LICENSE-2.0
// even in the case of an unmapped aggregator, validate the
// order
// The user has not made a shardSize selection. Use default
// heuristic to avoid any wrong-ranking caused by distributed
// counting
// In some cases, using ordinals is just not supported: override it
// return the SubAggCollectionMode that this aggregation should use based on the expected size
// and the cardinality of the field
// return all buckets
// use breadth_first if the cardinality is bigger than the expected size or unknown (-1)
/**
// we use the static COLLECT_SEGMENT_ORDS to allow tests to force specific optimizations
/**
// We use REMAP_GLOBAL_ORDS to allow tests to force specific optimizations
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Nothing to write
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Nothing to write
/*
//www.apache.org/licenses/LICENSE-2.0
// Set the histogram to autosize so it can resize itself as
// the data range increases. Resize operations should be
// rare as the histogram buckets are exponential (on the top
// level). In the future we could expose the range as an
// option on the request so the histogram can be fixed at
// initialisation and doesn't need resizing.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we cannot use state.hashCode at the moment because of:
// https://github.com/HdrHistogram/HdrHistogram/issues/81
// TODO: upgrade the HDRHistogram library
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
// Compute the sum of double values with Kahan summation algorithm which is more
// accurate than naive summation.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*ignore*/}, REHASH);
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Expensive to initialize, so we only initialize it when we have an actual value source
// only use ordinals if they don't increase memory usage by more than 25%
// We need to build a copy because the returned Aggregation needs remain usable after
// this Aggregator (and its HLL++ counters) is released.
// no-op
// no-op
// no-op
// no-op
// no-op
/**
// 1 bit per ord
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//en.wikipedia.org/wiki/Kahan_summation_algorithm">Kahan Summation Algorithm</a>
/**
/**
/**
/**
/**
/**
// If the value is Inf or NaN, just add it to the running tally to "convert" to
// Inf/NaN. This keeps the behavior bwc from before kahan summing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Compute the sum and sum of squires for double values with Kahan summation algorithm
// which is more accurate than naive summation.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// increment by the number of points for this document
// Compute the sum of double values with Kahan summation algorithm which is more
// accurate than naive summation.
// update the sum
//latitude
//longitude
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//static.googleusercontent.com/media/research.google.com/fr//pubs/archive/40671.pdf and its appendix
//docs.google.com/document/d/1gyjfMHy43U9OWBXxfaeG-3MjGzejW1dlpyMwEYAAWEI/view?fullscreen
/**
/**
// these static tables come from the appendix of the paper
// precision 4
// precision 5
// precision 6
// precision 7
// precision 8
// precision 9
// precision 10
// precision 11
// precision 12
// precision 13
// precision 14
// precision 15
// precision 16
// precision 17
// precision 18
// precision 4
// precision 5
// precision 6
// precision 7
// precision 8
// precision 9
// precision 10
// precision 11
// precision 12
// precision 13
// precision 14
// precision 15
// precision 16
// precision 17
// precision 18
/**
/**
// because ints take 4 bytes
/**
// means unused, take it!
// k is already in the set
/** looks and smells like the old openbitset. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Compute the sum of double values with Kahan summation algorithm which is more
// accurate than naive summation.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// all empty
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if we are not an final reduce we have to maintain all the aggs from all the incoming one
// until we hit the final reduce phase.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Compute the sum of double values with Kahan summation algorithm which is more
// accurate than naive summation.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Compute the sum of double values with Kahan summation algorithm which is more
// accurate than naive summation.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// if we are not in the final reduce we need to ensure we maintain all possible elements during reduce
// hence for pagination we need to maintain all hits until we are in the final phase.
// Equals and hashcode implemented for testing round trips
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Compute the sum of double values with Kahan summation algorithm which is more
// accurate than naive summation.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
// we have no parent and the values source is empty so we can skip collecting hits.
/**
// the maximum value has been extracted, we don't need to collect hits on this segment.
/**
// we need to collect all values in this leaf (the sort is ascending) where
// the last live doc is guaranteed to contain the max value for the segment.
// we only check leaves that contain the max value for the segment.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//en.wikipedia.org/wiki/Median_absolute_deviation">https://en.wikipedia.org/wiki/Median_absolute_deviation</a>
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO better way to know if the scripted metric received documents?
// Could check for null too, but a script might return null on purpose...
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
// we have no parent and the values source is empty so we can skip collecting hits.
/**
// the minimum value has been extracted, we don't need to collect hits on this segment.
/**
/**
// this is the first leaf with a live doc so the value is the minimum for this segment.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// InternalAvg renders value only if the avg normalizer (count) is not 0.
// We parse back `null` as Double.POSITIVE_INFINITY so we check for that value here to get the same xContent output
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// A top of Double.NEGATIVE_INFINITY yields an empty xContent, so the bounding box is null
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// skip potential inner objects and arrays for forward compatibility
// skip potential inner objects and arrays for forward compatibility
/*
//www.apache.org/licenses/LICENSE-2.0
// see InternalScriptedMetric#aggregations() for why we can assume this
//binary values will be parsed back and returned as base64 strings when reading from json and yaml
//binary values will be parsed back and returned as BytesArray when reading from cbor and smile
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// InternalValueCount doesn't print "value_as_string", but you can get a formatted value using
// getValueAsString() using the raw formatter and converting the value to double
/*
//www.apache.org/licenses/LICENSE-2.0
// InternalWeightedAvg renders value only if the avg normalizer (count) is not 0.
// We parse back `null` as Double.POSITIVE_INFINITY so we check for that value here to get the same xContent output
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// The builder requires two parameters for the constructor: aggregation name and values array.  The
// agg name is supplied externally via the Parser's context (as a String), while the values array
// is parsed from the request and supplied to the ConstructingObjectParser as a ctor argument
// the aggregation name is supplied to the parser as a Context. See note at top of Parser for more details
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we need to return a PercentilesAggregationBuilder for equality checks to work
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Extract params from scripts and pass them along to ScriptedMetricAggregatorFactory, since it won't have
// access to them for the scripts it's given precompiled.
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: how can we know if the script relies on scores?
/*
//www.apache.org/licenses/LICENSE-2.0
// Start with script params
// Add in agg params, throwing an exception if any conflicts are detected
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
// Compute the sum of double values with Kahan summation algorithm which is more
// accurate than naive summation.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
// Compute the sum of double values with Kahan summation algorithm which is more
// accurate than naive summation.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// sort by score
// Create leaf collectors here instead of at the aggregator level. Otherwise in case this collector get invoked
// when post collecting then we have already replaced the leaf readers on the aggregator level have already been
// replaced with the next leaf readers and then post collection pushes docids of the previous segement, which
// then causes assertions to trip or incorrect top docs to be computed.
// In the QueryPhase we don't need this protection, because it is build into the IndexSearcher,
// but here we create collectors ourselves and we need prevent OOM because of crazy an offset and size.
// TODO: can we pass trackTotalHits=subSearchContext.trackTotalHits(){
// Note that this would require to catch CollectionTerminatedException
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// a count per bucket
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
// There should always be one weight if advanceExact lands us here, either
// a real weight or a `missing` weight
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// doc count never has missing values so gap policy doesn't apply here
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// Need to find the first agg name in the buckets path to check its a
// multi bucket agg: aggs are split with '>' and can optionally have a
// metric name after them by using '.' so need to split on both to get
// just the agg name
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// input is a string, name of the path set to '_value'.
// This is a bit odd as there is not constructor for it
// input is an array, name of the path set to '_value' + position
// input is an object, it should contain name / value pairs
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: can we use one instance of the script for all buckets? it should be stateless?
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// If no sorting needs to take place, we just truncate and return
// We just have to get as many elements as we expect in results and store them in the same order starting from
// the specified offset and taking currentSize into consideration.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Only increment the sum if it's a finite value, otherwise "increment by zero" is correct
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// "moving_fn"
// start_object
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Initialize the script
// Default is to reuse existing bucket.  Simplifies the rest of the logic,
// since we only change newBucket if we can add to it
// The custom context mandates that the script returns a double (not Double) so we
// don't need null checks, etc.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Smoothed value
// Trend value
/**
/**
// We need at least two full "seasons" to use HW
// This should have been caught earlier, we can't do anything now...bail
// Smoothed value
// Trend value
// Seasonal value
// Initial level value is average of first season
// Calculate the slopes between first and second season for each period
// Calculate first seasonal
// TODO if perf is a problem, we can specialize a subclass to avoid conditionals on each iteration
// TODO perhaps pad out seasonal to a power of 2 and use a mask instead of modulo?
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// check availability as unformatted value
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Perform the sorting and percentile collection now that all the data
// has been collected.
// todo need postCollection() to clean up temp sorted data?
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Still under the initial lag period, add nothing and move on
// Peek here, because we rely on add'ing to always move the window
// Normalize null's to NaN
// Both have values, calculate diff and replace the "empty" bucket
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do nothing, no extra state to write to stream
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO this could be incorrect... e.g. +1 + -1
// This is a coarse approximation, since some aggs use positive/negative infinity or NaN
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// the agg can only be a metrics agg, and a metrics agg must be at the end of the path
// we're left with a multi-value metric agg
/**
/**
/**
// we're in the middle of the path, so the aggregator can only be a single-bucket aggregator
// perfectly valid to sort on single-bucket aggregation (will be sored on its doc_count)
// perfectly valid to sort on single metric aggregation (will be sorted on its associated value)
// the aggregator must be of a multi-value metrics type
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ANY still has a lot of special handling in ValuesSourceConfig, and as such doesn't adhere to this interface yet
// TODO: Implement this or get rid of ANY
// TODO: Implement this or get rid of ANY
// TODO: Implement this or get rid of ANY
// TODO: Is this the correct exception type here?
// Value script case
// Again, what's the difference between WithScript and Script?
// TODO: Is this the correct exception type here?
// TODO: also support the structured formats of geo points
// TODO: Is this the correct exception type here?
// TODO: Is this the correct exception type here?
// TODO: Is this the correct exception type here?
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: we could specialize the single value case
// always return true because we want to return a value even if
// the document does not have a value
// always return true because we want to return a value even if
// the document does not have a value
// always return true because we want to return a value even if
// the document does not have a value
// The value already exists
// we want to return the next missing ord but set this to
// NO_MORE_ORDS so on the next call we indicate there are no
// more values
// always return true because we want to return a value even if
// the document does not have a value
// we want to return the next missing ord but set this to
// NO_MORE_ORDS so on the next call we indicate there are no
// more values
// always return true because we want to return a value even if
// the document does not have a value
// the missing value exists in the segment, nothing to do
// the missing value exists in another segment, but not the current one
// the missing value exists neither in this segment nor in another segment
// always return true because we want to return a value even if
// the document does not have a value
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// we can't figure it out
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Whether this values source needs scores. */
/**
/** Returns a mapping from segment ordinals to global ordinals. */
// segments and global ordinals are the same
/**
// No need to implement ReaderContextAware here, the delegate already takes care of updating data structures
/**
/** Whether the underlying data is floating-point or not. */
/** Get the current {@link SortedNumericDocValues}. */
/** Get the current {@link SortedNumericDoubleValues}. */
/**
// even if the underlying source produces longs, scripts can change them to doubles
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// TODO: Can we get rid of this constructor and always use the three value version? Does this assert provide any value?
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// the specific value source type is undefined, but for scripts,
// we need to have a specific value source
// type to know how to handle the script values, so we fallback
// on Bytes
// todo do we really need this for unmapped?
// we can't figure it out
/** Get a value source given its configuration. A return value of null indicates that
// otherwise we will have values because of the missing value
// TODO: Clean up special cases around CoreValuesSourceType.ANY
// TODO: Clean up special cases around CoreValuesSourceType.ANY
// falling back to bytes values
// TODO: Better docs for Scripts vs Scripted Fields
/*
//www.apache.org/licenses/LICENSE-2.0
// utility class, no instantiation
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: what is the difference between "number" and "numeric"?
// TODO: do not be lenient here
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Dates are exposed in scripts as ReadableDateTimes but aggregations want them to be numeric
// We do expose boolean fields as boolean in scripts, however aggregations still expect
// that scripts return the same internal representation as regular fields, so boolean
// values in scripts need to be converted to a number, and the value formatter will
// make sure of using true/false in the key_as_string field
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Dates are exposed in scripts as ReadableDateTimes but aggregations want them to be numeric
// We do expose boolean fields as boolean in scripts, however aggregations still expect
// that scripts return the same internal representation as regular fields, so boolean
// values in scripts need to be converted to a number, and the value formatter will
// make sure of using true/false in the key_as_string field
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// for object parser only
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** The field type used for collapsing **/
/** The inner hit options to expand the collapsed results **/
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// stats are always positive numbers
// stats are always positive numbers
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no fields specified, default to return source if no explicit indication
// disable stored fields entirely
// Only fail if we know it is a object field, missing paths / fields shouldn't fail.
// empty list specified, default to disable _source if no explicit indication
// Set _source if requested.
// Also if highlighting is requested on nested documents we need to fetch the _source from the root document,
// otherwise highlighting will attempt to fetch the _source from the nested doc, which will fail,
// because the entire _source is only stored with the root document.
// In case of nested inner hits we already know the uid, so no need to fetch it from stored fields again!
// Isolate the nested json array object that matches with nested hit and wrap it back into the same json
// structure with the nested json array object being the actual content. The latter is important, so that
// features like source filtering and highlighting work consistent regardless of whether the field points
// to a json object array for consistency reasons on how we refer to fields
// nested field has an array value in the _source
// nested field has an object value in the _source. This just means the nested field has just one inner object,
// which is valid, but uncommon.
// When one of the parent objects are not nested then XContentMapValues.extractValue(...) extracts the values
// from two or more layers resulting in a list of list being returned. This is because nestedPath
// encapsulates two or more object layers in the _source.
//
// This is why only the first element of nestedParsedSource needs to be checked.
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// client side counter
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/** The name of the field. */
/** The format of the field, or {@code null} if defaults should be used. */
/** Sole constructor. */
/** Serialization constructor. */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// retrieve the `doc_value` associated with the collapse field
// don't modify the incoming hits
// TODO: Remove in 8.x
// binary / string / ip fields
// int / date fields
// floating-point fields
// if the reader index has changed we need to get a new doc values reader instance
// by default nanoseconds are cut to milliseconds within aggregations
// however for doc value fields we need the original nanosecond longs
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we use the top level doc id, since we work with the top level searcher
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// If source is disabled in the mapping, then attempt to return early.
// If this is a parent document and there are no source filters, then add the source as-is.
// Otherwise, filter the source and add it to the hit.
// This happens if the source filtering could not find the specified in the _source.
// Just doing `builder.value(null)` is valid, but the xcontent validation can't detect what format
// it is. In certain cases, for example response serialization we fail if no xcontent type can't be
// detected. So instead we just return an empty top level object. Also this is in inline with what was
// being return in this situation in 5.x and earlier.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// use low leadCost since this scorer will be consumed on a minority of documents
// use low loadCost since this scorer will be consumed on a minority of documents
// Just setting the innerHitQueryScorer is ok, because that is the actual scoring part of the query
// ignore and continue
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// in case the request has only suggest, parsed query is null
// don't modify the incoming hits
// scorers can be costly to create, so reuse them across docs of the same segment
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// scores were already computed since they are needed on the coordinated node to merge top hits
// don't modify the incoming hits
// random-access
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// don't modify the incoming hits
/*
//www.apache.org/licenses/LICENSE-2.0
// don't modify the incoming hits
// we have to check the primary term field as it is only assigned for non-nested documents
/*
//www.apache.org/licenses/LICENSE-2.0
// don't modify the incoming hits
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// skip has_child or has_parent queries, see: https://github.com/elastic/elasticsearch/issues/14999
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// parameters to FVH are not requires since:
// first two booleans are not relevant since they are set on the CustomFieldQuery
// (phrase and fieldMatch) fragment builders are used explicitly
// a HACK to make highlighter do highlighting, even though its using the single frag list builder
// we highlight against the low level reader and docId, because if we load source, we want to reuse it if possible
// Only send matched fields if they were requested to save time.
// Essentially we just request that a fragment is built from 0 to noMatchSize using
// the normal fragmentsBuilder
/*ignored*/);
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no instance
/**
/* This is a special case where broken analysis like WDF is used for term-vector creation at index-time
// TODO maybe we need a getter on Namedanalyzer that tells if this uses broken Analysis
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** default for whether to highlight fields based on the source even if stored separately */
/** default for whether a field should be highlighted only if a query matches that field */
/** default for whether {@code fvh} should provide highlighting on filter clauses */
/** default for highlight fragments being ordered by score */
/** the default encoder setting */
/** default for the maximum number of phrases the fvh will consider */
/** default for fragment size when there are no matches */
/** the default number of fragments for highlighting */
/** the default number of fragments size in characters */
/** the default opening tag  */
/** the default closing tag  */
/** the default opening tags when {@code tag_schema = "styled"}  */
/** the default closing tags when {@code tag_schema = "styled"}  */
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// create template global options that are later merged with any partial field options
// overwrite unset global options by default values
// create field options
/**
// first write common options
// special options for top-level highlighter
/**
/**
// write common options
// write special field-highlighter options
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// We should prevent highlighting if a field is anything but a text or keyword field.
// However, someone might implement a custom field type that has text and still want to
// highlight on that. We cannot know in advance if the highlighter will be able to
// highlight such a field and so we do the following:
// If the field is only highlighted because the field matches a wildcard we assume
// it was a mistake and do not process it.
// If the field was explicitly given we assume that whoever issued the query knew
// what they were doing and try to highlight anyway.
// if several fieldnames matched the wildcard then we want to skip those that we cannot highlight
// Note that we make sure to use the original field name in the response. This is because the
// original field could be an alias, and highlighter implementations may instead reference the
// concrete field it points to.
/*
//www.apache.org/licenses/LICENSE-2.0
//U+2029 PARAGRAPH SEPARATOR (PS): each value holds a discrete passage for highlighting (unified highlighter)
/**
//percolator needs to always load from source, thus it sets the global force source to true
// Can happen if the document doesn't have the field to highlight
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// always highlight across all data
// a HACK to make highlighter do highlighting, even though its using the single frag list builder
// can't perform highlighting if the stream has no terms (binary token stream) or no offsets
// this can happen if for example a field is not_analyzed and ignore_above option is set.
// the field will be ignored when indexing but the huge term is still in the source and
// the plain highlighter will parse the source and try to analyze it.
// number_of_fragments is set to 0 but we have a multivalued field
// refine numberOfFragments if needed
// Pull an excerpt from the beginning of the string but make sure to split the string on a term boundary.
// Can't split on term boundaries without offsets
// Jump to the end of this token if it wouldn't put us past the boundary
// We've exhausted the token stream so we should just highlight everything.
/*
//www.apache.org/licenses/LICENSE-2.0
// Field options that default to null or -1 are often set to their real default in HighlighterParseElement#parse
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we know its low level reader, and matching docId, since that's how we call the highlighter with
/*
//www.apache.org/licenses/LICENSE-2.0
// we know its low level reader, and matching docId, since that's how we call the highlighter with
/*
//www.apache.org/licenses/LICENSE-2.0
// non-tokenized fields should not use any break iterator (ignore boundaryScannerType)
// we use a control char to separate values, which is the only char that the custom break iterator
// breaks the text on, so we don't lose the distinction between the different values of a field and we
// get back a snippet per value
//using paragraph separator we make sure that each field value holds a discrete passage for highlighting
// ignore terms that targets the _id field since they use a different encoding
// that is not compatible with utf8
//let's sort the snippets by score if needed
// ignore maxLen
//postings highlighter accepts all values in a single string, as offsets etc. need to match with content
//loaded from stored fields, we merge all values using a proper separator
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we use the BooleanScorer window size as a base interval in order to make sure that we do not
// slow down boolean queries
// No point in having intervals that are larger than 1M
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// createWeight() is called for each query in the tree, so we tell the queryProfiler
// each invocation so that it can build an internal representation of the query
// tree
// Lucene sets shards indexes during merging of topDocs from different collectors
// We need to reset shard index; ES will set shard index later during reduce stage
// we have already precalculated totalHits for the whole index
// search each subreader
/**
// there is no doc of interest in this reader context
// continue with the following leaf
// collection was terminated prematurely
// continue with the following leaf
// if the role query result set is sparse then we should use the SparseFixedBitSet for advancing:
// collection was terminated prematurely
// continue with the following leaf
// if the underlying role bitset is sparse
// ConjunctionDISI uses the DocIdSetIterator#cost() to order the iterators, so if roleBits has the lowest cardinality it should
// be used first:
// we are either executing the dfs phase or the search_type doesn't include the dfs phase.
// we don't have stats for this - this might be a must_not clauses etc. that doesn't allow extract terms on the query
// we are either executing the dfs phase or the search_type doesn't include the dfs phase.
// we don't have stats for this - this might be a must_not clauses etc. that doesn't allow extract terms on the query
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Wrapper around information that needs to stay around when scrolling. */
/**
// (T)object
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// For reference why we use RefCounted here see #20095
// prevent double closing
/**
/** Automatically apply all required filters to the given query such as
/**
/**
/**
/**
/**
/**
/** indicates whether the sequence number and primary term of the last modification to each hit should be returned */
/** controls whether the sequence number and primary term of the last modification to each hit should be returned */
/**
/**
/**
/**
/**
/** Return a view of the additional query collectors that should be run for this context. */
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//these are the only two mutable fields, as they are subject to rewriting
// If allowPartialSearchResults is unset (ie null), the cluster-level default should have been substituted
// at this stage. Any NPEs in the above are therefore an error in request preparation logic.
// types no longer relevant so ignore
// types not supported so send an empty array to previous versions
/**
// copy it over, most requests are small, we might as well copy to make sure we are not sliced...
// we could potentially keep it without copying, but then pay the price of extra unused bytes up to a page
// do a deep copy
// Shard id is enough here, the request itself can be found by looking at the parent task description
/**
// This shouldn't happen unless alias disappeared after filteringAliases was called.
// we need to bench here a bit, to see maybe it makes sense to use OrFilter
// This shouldn't happen unless alias disappeared after filteringAliases was called.
// The filter might be null only if filter was removed after filteringAliases was called
/*
//www.apache.org/licenses/LICENSE-2.0
// By default return 3 hits per bucket. A higher default would make the response really large by default, since
// the to hits are returned per bucket.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we can cached fieldType completely per name, since its on an index/shard level (the lookup, and it does not change within the scope
// of a search request)
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// deprecate _type
// assume its a string...
// load fielddata on behalf of the script: otherwise it would need additional permissions
// to deal with pagedbytes/ramusagestimator/etc
// assume its a string...
/*
//www.apache.org/licenses/LICENSE-2.0
// if we are called with the same docId, don't invalidate source
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// if we are called with the same document, don't invalidate source
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Maps the Query to it's list of children.  This is basically the dependency tree */
/** A list of the original queries, keyed by index position */
/** A list of top-level "roots".  Each root can have its own tree of profiles */
/** A temporary stack used to record where we are in the dependency tree. */
/**
// If the stack is empty, we are a new root query
// We couldn't find a rewritten query to attach to, so just add it as a
// top-level root. This is just a precaution: it really shouldn't happen.
// We would only get here if a top-level query that never rewrites for some reason.
// Increment the token since we are adding a new node, but notably, do not
// updateParent() because this was added as a root
// Increment the token since we are adding a new node
/**
// Add a new slot in the dependency tree
// Save our query for lookup later
/**
/**
/**
// TODO this would be better done bottom-up instead of top-down to avoid
// calculating the same times over and over...but worth the effort?
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** Sole constructor. */
/** Convert this record to a map from timingType to times. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// not Vlong because can be negative
/**
/**
/**
/**
/**
// skip, total time is calculate by adding up 'timings' values in ProfileResult ctor
// skip, total time is calculate by adding up 'timings' values in ProfileResult ctor
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Wrapper around all the profilers that makes management easier. */
/** Sole constructor. This {@link Profilers} instance will initially wrap one {@link QueryProfiler}. */
/** Switch to a new profile. */
/** Get the current profiler. */
/** Return the list of all created {@link QueryProfiler}s so far. */
/** Return the {@link AggregationProfiler}. */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// shardResults is a map, but we print entries in a json array, which is ordered.
// we sort the keys of the map, so that toXContent always prints out the same array order
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Helps measure how much time is spent running some methods.
// code to time
/** pkg-private for testing */
/** Start the timer. */
// We measure the timing of each method call for the first 256
// calls, then 1/2 call up to 512 then 1/3 up to 768, etc. with
// a maximum interval of 1024, which is reached for 1024*2^8 ~= 262000
// This allows to not slow down things too much because of calls
// to System.nanoTime() when methods are called millions of time
// in tight loops, while still providing useful timings for methods
// that are only called a couple times per search execution.
/** Stop the timer. */
/** Return the number of times that {@link #start()} has been called. */
/** Return an approximation of the total time spent between consecutive calls of #start and #stop. */
// We don't have timings for the last `count-lastCount` method calls
// so we assume that they had the same timing as the lastCount first
// calls. This approximation is ok since at most 1/256th of method
// calls have not been timed.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Anonymous classes (such as NonCollectingAggregator in TermsAgg) won't have a name,
// we need to get the super class
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// we need to consume this value, but we use the raw nanosecond value
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/** The wrapped collector */
/**
/**
/**
/**
/**
// MutiCollector which wraps multiple BucketCollectors is generated
// via an anonymous class, so this corrects the lack of a name by
// asking the enclosingClass
// Aggregation collector toString()'s include the user-defined agg name
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Rewrite time */
// Anonymous classes won't have a name,
// we need to get the super class
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** A collector that profiles how much time is spent calling it. */
/** Sole constructor. */
/** Return the wrapped collector. */
/** Return the total time spent on this collector. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// We use the default bulk scorer instead of the specialized one. The reason
// is that Lucene's BulkScorers do everything at once: finding matches,
// scoring them and calling the collector, so they make it impossible to
// see where time is spent, which is the purpose of query profiling.
// The default bulk scorer will pull a scorer and iterate over matches,
// this might be a significantly different execution path for some queries
// like disjunctions, but in general this is what is done anyway
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Sole constructor. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** Set the collector that is associated with this profiler. */
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: remove this property in 8.0
// Pre-process aggregations as late as possible. In the case of a DFS_Q_T_F
// request, preProcess is called on the DFS phase phase, this is why we pre-process them
// here to make sure it happens during the QUERY phase
// only if we do a regular search
/**
// already rewritten
// first round
// there is not much that we can optimize here since we want to collect all
// documents in order to get the total number of hits
// now this gets interesting: since we sort in index-order, we can directly
// skip to the desired doc
// ... and stop collecting after ${size} matches
// now this gets interesting: since the search sort is a prefix of the index sort, we can directly
// skip to the desired doc
// whether the chain contains a collector that filters documents
// add terminate_after before the filter collectors
// it will only be applied on documents accepted by these filter collectors
// this collector can filter documents during the collection
// add post filters before aggregations
// it will only be applied to top hits
// this collector can filter documents during the collection
// plug in additional collectors, like aggregations
// apply the minimum score after multi collector so we filter aggs as well
// this collector can filter documents during the collection
// try to rewrite numeric or date sort to the optimized distanceFeatureQuery
// modify sorts: add sort on _score as 1st sort, and move the sort on the original field as the 2nd sort
// stash SortAndFormats to restore it later
// if we are optimizing sort and there are no other collectors
// if we rewrote numeric long or date sort, restore fieldDocs based on the original sort
// restore SortAndFormats
// create the top docs collector last when the other collectors are known
// add the top docs collector, the first collector context in the chain
// Can't rethrow TimeExceededException because not serializable
/*
// will be computed via the collector
// don't compute hit counts via the collector
// Can't rethrow TimeExceededException because not serializable
// no rescoring when sorting by field
//TODO: handle sort optimization with search after
// disable this optimization if index sorting matches the query sort since it's already optimized by index searcher
// check if this is a field of type Long or Date, that is indexed and has doc values
// happens when _score or _doc is the 1st sort field
// mapperService can be null in tests
// for unmapped fields, default behaviour depending on "unmapped_type" flag
//TODO: change to pointDataDimensionCount() when implemented
// check that all sorts are actual document fields or _doc
//TODO: find out how to cover _script sort that don't use _score
// could be _script sort that uses _score
// check that setting of missing values allows optimization
// is not worth to run optimization on small index
// check for multiple values
//TODO: handle multiple values
// check if the optimization makes sense with the track_total_hits setting
// with filter, we can't pre-calculate hitsCount, we need to explicitly calculate them => optimization does't make sense
// if we can't pre-calculate hitsCount based on the query type, optimization does't make sense
// division by 2 on the unsigned representation to avoid overflow
// 0 if maxValue = (minValue + 1)
// filter for original query
//should for rewrittenQuery
/**
/**
/**
// sort by score
// queries that return constant scores will return docs in index
// order since Lucene tie-breaks on the doc id
/**
/**
// number of docs in segments with NO duplicate data that would benefit optimization
// number of docs in segments with duplicate data that would NOT benefit optimization
// skipping small segments as estimateMedianCount doesn't work well on them
// TODO: modify the code to handle multiple values
// expected doc count of duplicate data
// to avoid overflow first divide each value by 2
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
//Earlier versions serialize sibling pipeline aggs separately as they used to be set to QuerySearchResult directly, while
//later versions include them in InternalAggregations. Note that despite serializing sibling pipeline aggs as part of
//InternalAggregations is supported since 6.7.0, the shards set sibling pipeline aggs to InternalAggregations only from 7.1.
//Earlier versions expect sibling pipeline aggs separately as they used to be set to QuerySearchResult directly,
//while later versions expect them in InternalAggregations. Note that despite serializing sibling pipeline aggs as part of
//InternalAggregations is supported since 6.7.0, the shards set sibling pipeline aggs to InternalAggregations only from 7.1 on.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// for bwc hit count is set to 0, it will be converted to -1 by the coordinating node
// implicit total hit counts are valid only when there is no filter collector in the chain
/**
/**
// disable max score optimization since we have a mandatory clause
// that doesn't track the maximum score
// don't compute hit counts via the collector
// implicit total hit counts are valid only when there is no filter collector in the chain
// don't compute hit counts via the collector
// first round
// subsequent round: the total number of hits and
// the maximum score were computed on the first round
// if we fetch the document in the same roundtrip, we already know the last emitted doc
// set the last emitted doc
/**
// remove wrappers that don't matter for counts
// this is necessary so that we don't only optimize match_all
// queries but also match_all queries that are nested in
// a constant_score query
// no shortcut possible for fields that are not indexed
/**
// top collectors don't like a size of 0
// no matter what the value of from is
// we can disable the tracking of total hits after the initial scroll query
// since the total hits is preserved in the scroll context.
// no matter what the value of from is
/**
// boolean queries can skip documents even if they have some should
// clauses that don't track maximum scores
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: shouldn't this be up to the ScoreMode?  I.e., we should just invoke ScoreMode.combine, passing 0.0f for the
// secondary score?
// First take top slice of incoming docs, to be rescored:
// Save doc IDs for which rescoring was applied to be used in score explanation
// Rescore them:
// Splice back to non-topN hits and resort all of them:
// this should not happen but just in case
// NOTE: we don't use Lucene's Rescorer.explain because we want to insert our own description with which ScoreMode was used.
//  Maybe we should add QueryRescorer.explainCombine to Lucene?
/** Returns a new {@link TopDocs} with the topN from the incoming one, or the same TopDocs if the number of hits is already &lt;=
/** Modifies incoming TopDocs (in) by replacing the top hits with resorted's hits, and then resorting all hits. */
// These hits were not rescored (beyond the rescore window), so we treat them the same as a hit that did get rescored but did
// not match the 2nd pass query:
// TODO: shouldn't this be up to the ScoreMode?  I.e., we should just invoke ScoreMode.combine, passing 0.0f for the
// secondary score?
// TODO: this is wrong, i.e. we are comparing apples and oranges at this point.  It would be better if we always rescored all
// incoming first pass hits, instead of allowing recoring of just the top subset:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
// query is rewritten at this point already
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//doc Ids for which rescoring was applied
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// It is the responsibility of the rescorer to sort the resulted top docs,
// here we only assert that this condition is met.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/**
// for geo distance sorting
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// BitMixer.mix seems to be about 10 ops
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Name of field to slice against (_id by default) */
/** The id of the slice */
/** Max number of slices */
/**
/**
/**
/**
/**
/*
// remap the original shard id with its index (position) in the sorted shard iterator.
// the number of slices is greater than the number of shards
// in such case we can reduce the number of requested shards by slice
// first we check if the slice is responsible of this shard
// the shard is not part of this slice, we can skip it.
// compute the number of slices where this shard appears
// this shard has only one slice so we must check all the documents
// get the new slice id for this shard
// the number of shards is greater than the number of slices
// check if the shard is assigned to the slice
// the shard is not part of this slice, we can skip it.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Returns true if the value matches the predicate
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Fixed seed for computing term hashCode
/**
// use a fixed seed instead of term.hashCode() otherwise this query may return inconsistent results when
// running on another replica (StringHelper sets its default seed at startup with current time)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/** Copy constructor. */
/**
/**
/** Returns the document field this sort should be based on. */
/**
/** Returns the value used when a field is missing in a doc. */
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// already in nested context
// we are in a nested context that matches the path of the provided field so the nested path
// is not required
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/** Returns which distance to use for sorting in the case a document contains multiple geo points. */
/**
/**
/**
/**
// the json in the format of -> field : { lat : 30, lon : 12 }
// validation was not available prior to 2.x, so to support bwc percolation queries we only ignore_malformed
// on 2.x created indexes
// only works with 5.x geo_point
// LatLonDocValuesField internally picks the closest point
// we might get here if the geo point is " number, number] " and the parser already moved over the
// opening bracket in this case we cannot use GeoUtils.parseGeoPoint(..) because this expects an opening
// bracket
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// order defaults to desc when sorting on the _score
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/** script sort for a string value **/
/** script sort for a numeric value **/
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// parse fields common to more than one SortBuilder
// TODO: this can deadlock as it might access the ScoreSortBuilder (subclass) initializer from the SortBuilder initializer!!!
/**
/**
/**
// optimize if we just sort on score non reversed, we don't really
// need sorting
// verify our nested path
// get our child query, potentially applying a users filter
// this is for back-compat, original single level nested sorting never applied a nested type filter
// apply filters from the previous nested level
// wrap up our parent and child and either process the next level of nesting or return
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** pick the lowest value **/
/** pick the highest value **/
/** Use the sum of all values as sort value. Only applicable for number based array fields. **/
/** Use the average of all values as sort value. Only applicable for number based array fields. **/
/** Use the median of all values as sort value. Only applicable for number based array fields. **/
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// NB: If this changes, make sure to change the default in TermBuilderSuggester
// NB: If this changes, make sure to change the default in TermBuilderSuggester
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Sort should first be based on score, then document frequency and then the term itself. */
/** Sort should first be based on document frequency, then score and then the term itself. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we sort suggestions by their names to ensure iteration over suggestions are consistent
// this is needed as we need to fill in suggestion docs in SearchPhaseController#sortDocs
// in the same order as we enrich the suggestions with fetch results in SearchPhaseController#merge
/**
/**
/**
/**
/**
// The suggested term size specified in request, only used for merging shard responses
/**
/**
/**
/**
/**
/**
// Concatenates the type and the name of the suggestion (ex: completion#foo)
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
// when we parse from RestSuggestAction the current token is null, advance the token
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we only want to output an empty suggestion on empty shards
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// no analyzer name passed in, so try the field's analyzer, or the default analyzer
// if no shard size is set in builder, use size (or at least 5)
//default impl returns the same as writeable name, but we keep the distinction between the two just to make sure
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// collect contexts
// collection was terminated prematurely
// continue with the following leaf
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// combine suggestion entries from participating shards on the coordinating node
// the global top <code>size</code> entries are collected from the shard results
// using a priority queue
// Dedup duplicate suggestions (based on the surface form) if skip duplicates is activated
//options exhausted for this shard
// Completion suggestions are reduced by
// org.elasticsearch.search.suggest.completion.CompletionSuggestion.reduce()
// the option either prints SCORE or inlines the search hit
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Copy the current structure. We will parse, once the mapping is provided
// context is deprecated
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// now we should have field name, check and copy fields over to the suggestion builder we return
// copy over common settings to each suggestion builder
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// Ignore doc values and stored fields
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// No validation is required by default
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we can support more, but max of 255 (1 byte) unique context types per suggest field
// seems reasonable?
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Test if value is a single point in <code>[lon, lat]</code> format
// or a single location
// we write doc values fields differently: one field for all values, so we need to only care about indexed fields
/**
/**
/**
/**
// Ceiling precision: we might return more results
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//TODO public for tests
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//TODO public for tests
// just allocate once
/** Lower scores sorts first; if scores are equal,
// Later (zzz) terms sort before (are weaker than) earlier (aaa) terms:
/*
//www.apache.org/licenses/LICENSE-2.0
/* (non-Javadoc)
/* (non-Javadoc)
/**
/**
// the threshold is the max possible frequency so we can skip the search
// don't override the threshold if the provided min_doc_freq is greater
// than the original term frequency.
// restore the original value back
// We should not use frequency(term) here because it will analyze the term again
// If preFilter and postFilter are the same analyzer it would fail.
// package protected for test
// Merge new candidates into existing ones,
// deduping:
// Sort strongest to weakest:
/** Lower scores sort first; if scores are equal, then later (zzz) terms sort first */
// Later (zzz) terms sort before earlier (aaa) terms:
/** NOTE: this method closes the TokenStream, even on exception, which is awkward
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//en.wikipedia.org/wiki/N-gram#Smoothing_techniques">N-Gram
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//TODO public for tests
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//en.wikipedia.org/wiki/N-gram#Smoothing_techniques">N-Gram
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//TODO public for tests
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// Checks if the template query collateScript yields any documents
// from the index for a correction, collateMatch is updated
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// If the cluster contains both pre 0.90.4 and post 0.90.4 nodes then we'll see Suggestion.Entry
// objects being merged with PhraseSuggestion.Entry objects.  We merge Suggestion.Entry objects
// by assuming they had a low cutoff score rather than a high one as that is the more common scenario
// and the simplest one for us to implement.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// gramSize needs to be optional although there is a default, if unset parser try to detect and use shingle size
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// for now we only have a single type of generators
// now we should have field name, check and copy fields over to the suggestion builder we return
// copy over common settings to each suggestion builder
// try to detect the shingle size
// use a default generator on the same field
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//en.wikipedia.org/wiki/Katz's_back-off_model"> Katz's
//en.wikipedia.org/wiki/N-gram#Smoothing_techniques">N-Gram
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// First see if there are bigrams.  If there aren't then skip looking up the trigram.  This saves lookups
// when the bigrams and trigrams are rare and we need both anyway.
/*
//www.apache.org/licenses/LICENSE-2.0
//TODO public for tests
// terms.size() might be -1 if it's a MultiTerms instance. In that case,
// use reader.maxDoc() as an approximation. This also protects from
// division by zero, by scoreUnigram.
// non recycling for now
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: Extend DirectSpellChecker in 4.1, to get the raw suggested words as BytesRef
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Same behaviour as comparators in suggest module, but for SuggestedWord
// Highest score first, then highest freq first, then lowest term first
// first criteria: the distance
// Same behaviour as comparators in suggest module, but for SuggestedWord
// Highest freq first, then highest score first, then lowest term first
// first criteria: the popularity
// second criteria (if first criteria is equal): the distance
// third criteria: term text
// the "size" parameter and the SortBy for TermSuggestion cannot be parsed from the response, use default values
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// now we should have field name, check and copy fields over to the suggestion builder we return
// copy over common settings to each suggestion builder
// Transfers the builder settings to the target TermSuggestionContext
/** An enum representing the valid suggest modes. */
/** Only suggest terms in the suggest text that aren't in the index. This is the default. */
/** Only suggest terms that occur in more docs then the original suggest text term. */
/** Suggest any matching suggest terms based on tokens in the suggest text. */
/** An enum representing the valid string edit distance algorithms for determining suggestions. */
/** This is the default and is based on <code>damerau_levenshtein</code>, but highly optimized
/** String distance algorithm based on Damerau-Levenshtein algorithm. */
/** String distance algorithm based on Levenshtein edit distance algorithm. */
/** String distance algorithm based on Jaro-Winkler algorithm. */
/** String distance algorithm based on character n-grams. */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/* only consume, don't set */ }, new ParseField(Fields.FAILED));
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// It's OK to change some settings, but we shouldn't allow simply removing them
/**
// Read snapshot info and metadata from the repository
// Make sure that we can restore from this snapshot
// Resolve the indices from the snapshot that need to be restored
// Apply renaming on index names, returning a map of names where
// the key is the renamed index and the value is the original name
// Now we can start the actual restore process by adding shards to be recovered in the cluster state
// and updating cluster metadata (global and index) as needed
// Check if the snapshot to restore is currently being deleted
// Updating cluster state
// We have some indices to restore
// Check that the index is closed or doesn't exist
// Index doesn't exist - create it and start recovery
// Make sure that the index we are about to create has a validate name
// Remove all aliases - they shouldn't be restored
// Index exists and it's closed - open it in metadata and start recovery
// Remove all snapshot aliases
/// Add existing aliases
// Restore global state if needed
// TODO: Should all existing templates be deleted first?
// Don't restore repositories while we are working with them
// TODO: Should we restore them at the end?
// We don't have any indices to restore - we are done
// Make sure that index was fully snapshotted
// Index exist - checking that it's closed
// TODO: Enable restore for open indices
// Index exist - checking if it's partial restore
// Make sure that the number of shards is the same. That's the only thing that we cannot change
/**
// Map of RestoreUUID to a of changes to the shards' restore statuses
// mark snapshot as completed
// mark restore entry for this shard as failed when it's due to a file corruption. There is no need wait on retries
// to restore this shard on another node if the snapshot files are corrupt. In case where a node just left or crashed,
// however, we only want to acknowledge the restore operation once it has been successfully restored on another node.
// if we force an empty primary, we should also fail the restore entry
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Caching hash code
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// write snapshot info to repository snapshot blob format
// write snapshot info for the API and any other situations
/**
// fresh parser? move to the first token
// on a start object move to next token
// It was probably created by newer version - ignoring
// It was probably created by newer version - ignoring
// the old format where there wasn't a UUID
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Workaround for https://github.com/elastic/elasticsearch/issues/25878
// Some old snapshot might still have null in shard failure reasons
/**
// customized to account for discrepancies in shardId/Index toXContent/fromXContent related to uuid
// customized to account for discrepancies in shardId/Index toXContent/fromXContent related to uuid
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// A map of snapshots to the shardIds that we already reported to the master as failed
// this is only useful on the nodes that can hold data
// The constructor of UpdateSnapshotStatusAction will register itself to the TransportService.
// abort any snapshots occurring on the soon-to-be closed shard
/**
// First, remove snapshots that are no longer there
// abort any running snapshots of shards for the removed entry;
// this could happen if for some reason the cluster state update for aborting
// running shards is missed, then the snapshot is removed is a subsequent cluster
// state update, which is being processed here
// For now we will be mostly dealing with a single snapshot at a time but might have multiple simultaneously running
// snapshots in the future
// Now go through all snapshots and update existing or create missing
// Add all new shards to start processing on
// Abort all running shards for this snapshot
// due to CS batching we might have missed the INIT state and straight went into ABORTED
// notify master that abort has completed by moving to FAILED
/**
// do not snapshot when in the process of relocation of primaries so we won't get conflicts
// shard has just been created, or still recovering
// we flush first to make sure we get the latest writes snapshotted
/**
// Master knows about the shard and thinks it has not completed
// but we think the shard is done - we need to make new master know that the shard is done
// but we think the shard failed - we need to make new master know that the shard failed
/**
// By default, we keep trying to post snapshot status messages to avoid snapshot processes getting stuck.
/** Notify the master node that the given shard has been successfully snapshotted **/
/** Notify the master node that the given shard failed to be snapshotted **/
/** Updates the shard snapshot status by sending a {@link UpdateIndexShardSnapshotStatusRequest} to the master node */
/**
// Snapshot is finished - mark it as done
// TODO: Add PARTIAL_SUCCESS status?
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Set of snapshots that are currently being initialized by this node
// Set of snapshots that are currently being ended by this node
// addLowPriorityApplier to make sure that Repository will be created before snapshot
/**
// should only be called once we've validated the repository exists
/**
/**
// first, look at the snapshots in progress
// then, look in the repository
/**
/**
/**
// new UUID for the snapshot
// Store newSnapshot here to be processed in clusterStateProcessed
/**
/**
// check if the snapshot name already exists in the repository
// No indices in this snapshot - we are done
// Replace the snapshot that was just initialized
// We are not longer a master - we shouldn't try to do any cleanup
// The new master will take care of it
// The userCreateSnapshotListener.onResponse() notifies caller that the snapshot was accepted
// for processing. If client wants to wait for the snapshot completion, it can register snapshot
// completion listener in this method. For the snapshot completion to work properly, the snapshot
// should still exist when listener is registered.
// Remove global state from the cluster state
/**
// Most likely scenario - one snapshot is currently running
// Check this snapshot against the query
/**
// If the snapshot failed, but the shard's snapshot does
// not have an exception, it means that partial snapshots
// were disabled and in this case, the shard snapshot will
// *not* have any metadata, so attempting to read the shard
// snapshot status will throw an exception.  Instead, we create
// a status for the shard to indicate that the shard snapshot
// could not be taken due to partial being set to false.
// We don't remove old master when master flips anymore. So, we need to check for change in master
// Cleanup all snapshots that have no more work left:
// 1. Completed snapshots
// 2. Snapshots in state INIT that the previous master failed to start
// 3. Snapshots in any other state that have all their shard tasks completed
/**
/**
// TODO: Restart snapshot on another node?
// Mark the snapshot as aborted as it failed to start from the previous master
// Clean up the snapshot that failed to start from the old master
// Shard that we were waiting for has started on a node, let's process it
// Shard that we were waiting for hasn't started yet or still relocating - will continue to wait
// Shard that we were waiting for went into unassigned state or disappeared - giving up
// If at least one shard was running on a removed node - we need to fail it
/**
/**
// Failure due to not being master any more, don't try to remove snapshot from cluster state the next master
// will try ending this snapshot again
/**
/**
/**
// First, look for the snapshot in the repository
// if nothing found by the same name, then look in the cluster state for current in progress snapshots
// Derive repository generation if a snapshot is in progress because it will increment the generation when it finishes
/**
// don't allow snapshot deletions while a restore is taking place,
// otherwise we could end up deleting a snapshot that is being restored
// and the files the restore depends on would all be gone
// This snapshot is not running - delete
// However other snapshots are running - cannot continue
// add the snapshot deletion to the cluster state
// This snapshot is currently running - stopping shards first
// snapshot is still initializing, mark it as aborted
// snapshot is started - mark every non completed shard as aborted
// Cleanup in case a node gone missing and snapshot wasn't updated for some reason
// Check if we still have shard running on existing nodes
// snapshot is being finalized - wait for shards to complete finalization process
// no shards to wait for but a node is gone - this is the only case
// where we force to finish the snapshot
/**
/**
/**
/**
// The index was deleted before we managed to start the snapshot - mark it as missing.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// if its the first, add empty set
// if its the first, fill it with all the indices...
// add all the previous ones...
// add all the previous ones...
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Safe because we only toString the response
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Implements equals and hashcode for testing
// Totally not efficient, but ok for testing because it ignores order and spacing differences
// Totally not efficient, but ok for testing because consistent with equals
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
// Shortcut the EMPTY_TASK_ID, the only TaskId allowed to have the empty string as its nodeId.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
// This might happen if we are reading an old version of task info
// Note for the future: this has to be backwards and forwards compatible with all changes to the task storage format
// Implements equals and hashCode for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Rest headers that are copied to the task */
/**
// NOTE: ActionListener cannot infer Response, see https://bugs.openjdk.java.net/browse/JDK-8203195
// Check if this task was banned before we start it
// let's clean up the registration
/**
/**
/**
// too early to store anything, shouldn't really be here - just pass the error along
/**
// too early to store anything, shouldn't really be here - just pass the response along
/**
/**
/**
/**
/**
/**
// Set the ban first, so the newly created tasks cannot be registered
// Only set the ban if the node is the part of the cluster
// Now go through already running tasks and cancel them
/**
// Remove all bans that were registered by nodes that are no longer in the cluster state
// Cancel cancellable tasks for the nodes that are gone
/**
/**
// Already cancelled by somebody else
/**
/**
// The task was cancelled, we need to notify the listener
// We need to call the listener outside of the synchronised section to avoid potential bottle necks
// in the listener synchronization
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Implements equals and hashcode for testing
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// we have the index, do it
// The index already exists but doesn't have our mapping
// The mapping was created before meta field was introduced
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//en.wikipedia.org/wiki/Little's_law for more information.
// These temp settings are used to validate the min and max settings below
// TODO: in a subsequent change we hope to extend ThreadPool.Info to be more specific for the thread pool type
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// unwrap other by calling on it.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// last resort
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// always check run here since this may have been cancelled since the last execution and we do not want to run
// if this has not been cancelled reschedule it to run again
/**
// Scheduler only allows Runnable's so we expect no checked exceptions here. If anyone uses submit directly on `this`, we
// accept the wrapped exception in the output.
// only check this if task is done, which it always is except for periodic tasks. Periodic tasks will hang on
// RunnableFuture.get()
/*
//www.apache.org/licenses/LICENSE-2.0
// no queue as this means clients will need to handle rejections on listener queue even if the operation succeeded
// the assumption here is that the listeners should be very lightweight on the listeners side
/**
/**
/**
// no need to have info on "same" thread pool
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Leverage try-with-resources to close the threadpool
// last resort
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// This method is safe to call multiple times as the close context will provide concurrency
// protection and only be completed once. The attached listeners will only be notified once.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// If we are using compression the stream needs to be closed to ensure that EOS marker bytes are written.
// The actual ReleasableBytesStreamOutput will not be closed yet as it is wrapped in flushOnCloseStream when
// passed to the deflater stream.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// wait on previous entry to complete connection attempt
/**
/**
/**
// if we found it and removed it we close
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// if we are not master eligible we don't need a dedicated channel to publish the state
// if we are not a data-node we don't need any dedicated channels for recovery
/**
/**
/** create an empty builder */
/** copy constructor, using another profile as a base */
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Message length of 0 is a ping
// Place the context with the headers from the message
// ignore if its null, the service logs it
// Check the entire message has been read
// calling read() is useful to make sure the message is fully read, even if there is an EOS marker
// in case we throw an exception, i.e. when the limit is hit, we don't want to verify
// calling read() is useful to make sure the message is fully read, even if there some kind of EOS marker
// the circuit breaker tripped
/*
//www.apache.org/licenses/LICENSE-2.0
// Consume the variable header size
// discard features
// for handshakes we are compatible with N-2 since otherwise we can't figure out our initial version
// since we are compatible with N-1 and N+1 so we always send our minCompatVersion as the initial version in the
// handshake. This looks odd but it's required to establish the connection correctly we check for real compatibility
// once the connection is established
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// stack trace is meaningless...
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//No node available for cluster
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// This should not happen as the bytes are already serialized
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// The compressible bytes stream will not close the underlying bytes stream
// we have to call materializeBytes() here before accessing the bytes. A CompressibleBytesOutputStream
// might be implementing compression. And materializeBytes() ensures that some marker bytes (EOS marker)
// are written. Otherwise we barf on the decompressing end when we read past EOF on purpose in the
// #validateRequest method. this might be a problem in deflate after all but it's important to write
// the marker bytes.
// empty features array
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//    @Override public Throwable fillInStackTrace() {
//        return fillStack();
//    }
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// do nothing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// we register the transport service here as a listener to make sure we notify handlers on disconnect etc.
/**
/**
/**
/**
// we stash any context here since this is an internal execution and should not leak any existing context information
// run this on the node that gets the request it's as good as any other
// just in case if we are not connected for some reason we try to connect and if we fail we have to notify the listener
// this will cause some back pressure on the search end and eventually will cause rejections but that's fine
// we can't proceed with a search on a cluster level.
// in the future we might want to just skip the remote nodes in such a case but that can already be implemented on the
// caller end since they provide the listener.
/**
// for testing only
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
//search on _all in the local cluster if neither local indices nor remote indices were specified
/**
/**
// remoteClusters is unmodifiable so its key set will be unmodifiable too
/**
/**
/**
// Wait 10 seconds for a connections. We must use a latch instead of a future because we
// are on the cluster state thread and our custom future implementation will throw an
// assertion.
/**
// this is a new cluster we have to add a new representation
// Changes to connection configuration. Must tear down existing connection
// No changes to connection configuration.
/**
/**
// we need to check if it's true since we could have multiple failures
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: Change to 7.6 after backport
// TODO: Change to 7.6 after backport
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: Evaluate if we actually need PING channels?
// in case we have a IPv6 address ie. [::1]:9300
/**
// try to reconnect and fill up the slot of the disconnected node
// for testing only
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no need for stack trace here, we always have cause
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// why is this?
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// validate seed address
/**
/**
/**
// ISE if we fail the handshake with an version incompatible node
// here we pass on the connection since we can only close it once the sendRequest returns otherwise
// due to the async nature (it will return before it's actually sent) this can cause the request to fail
// due to an already closed connection.
// we stash any context here since this is an internal execution and should not leak any
// existing context information.
/* This class handles the _state response from the remote cluster when sniffing nodes to connect to */
// ISE if we fail the handshake with an version incompatible node
// fair enough we can't connect just move on
// We have to close this connection before we notify listeners - this is mainly needed for test correctness
// since if we do it afterwards we might fail assertions that check if all high level connections are closed.
// from a code correctness perspective we could also close it afterwards.
// once the connection is closed lets try the next node
// Default visibility for tests
// nodes can be tagged with node.attr.remote_gateway: true to allow a node to be a gateway node for cross cluster search
// resolve proxy address lazy here
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// write the size, the size indicates the remaining message size, not including the size int
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// This is the number of bytes necessary to read the message size
// this limit is per-address
// this lock is here to make sure we close this transport and disconnect all the client nodes
// connections while no connect operations is going on
// Call the super method to trigger listeners
// This allows transport implementations to potentially override specific connection profiles. This
// primarily exists for the test implementations.
// ensure we don't open connections while we are closing
// check if v6 is supported, if so, v4 will also work via mapped addresses.
// may get ports appended!
// Bind and start to accept incoming connections.
// No need for locking here since Lifecycle objects can't move from STARTED to INITIALIZED
// package private for tests
// if port not explicitly provided, search for port of address in boundAddresses that matches publishInetAddress
// if no matching boundAddress found, check if there is a unique port for all bound addresses
// this code is a take on guava's HostAndPort, like a HostAndPortRange
// pattern for validating ipv6 bracket addresses.
// not perfect, but PortsRange should take care of any port range validation, not a regex
/**
// Parse a bracketed host, typically an IPv6 literal.
// could be null
// Exactly 1 colon.  Split into host:port.
// 0 or 2+ colons.  Bare hostname or IPv6 literal.
// 2+ colons and not bracketed: exception
// if port isn't specified, fill with the default
// make sure we run it on another thread than a possible IO handler thread
// first stop to accept any incoming connections so nobody can connect to this transport
// close all of the incoming channels. The closeChannels method takes a list so we must convert the set.
// ignore
// just close and ignore - we are already stopped and just need to make sure we release all resources
// close the channel, which will cause a node to be disconnected if relevant
// close the channel as safe measure, which will cause a node to be disconnected if relevant
// close the channel as safe measure, which will cause a node to be disconnected if relevant
// close the channel as safe measure, which will cause a node to be disconnected if relevant
// in case we are able to return data, serialize the exception content and sent it back to the client
// close the channel, which will cause a node to be disconnected if relevant
// Mark the channel init time
/**
/**
/**
/**
/**
/**
/**
// This is a ping
// Actually 'OPTIONS'. But we are only guaranteed to have read six bytes at this point.
/**
/**
/**
/**
// Returns true if all connections have completed successfully
// Mark the channel init time
/*
//www.apache.org/licenses/LICENSE-2.0
// easier to debug if it's already closed
// only fail if we are not sending an error - we might send the error triggered by the previous
// sendResponse call
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no instance
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// for the request we use the minCompatVersion since we don't know what's the version of the node we talk to
// we also have no payload on the request but the response will contain the actual version of the node we talk
// to as the payload.
// The TransportService blocks incoming requests until this has been set.
// Must read the handshake request to exhaust the stream
// During the handshake process, nodes set their stream version to the minimum compatibility
// version they support. When deserializing the response, we use the version the other node
// told us that it actually is in the handshake response (`version`).
// TODO: On backport update to 6.7
// During the handshake process, nodes set their stream version to the minimum compatibility
// version they support. When deciding what response to send, we use the version the other node
// told us that it actually is in the handshake request (`requestVersion`). If it did not tell
// us a `requestVersion`, it is at least a pre-7.6 node.
// TODO: On backport update to 6.7
/*
//www.apache.org/licenses/LICENSE-2.0
/** Whether to add hostname to publish host field when serializing. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// won't happen
/**
// The client-side initiates pings and the server-side responds. So if this is a client channel, this
// method is a no-op.
// In the future it is possible that we may want to kill a channel if we have not read from
// the channel since the last ping. However, this will need to be backwards compatible with
// pre-6.6 nodes that DO NOT respond to pings
/*
//www.apache.org/licenses/LICENSE-2.0
// This is not an Elasticsearch transport message.
// This is a ping
// read and discard headers
// discard features
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/***
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** threads whose name is prefixed by this string will be considered network threads, even though they aren't */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// An LRU (don't really care about concurrency here) that holds the latest timed out requests so if they
// do show up, we can print more descriptive information about them
// tracer log
/** if set will call requests sent to this id to shortcut and executed locally */
/**
/**
// here we start to connect to the remote clusters
// in case the transport is not connected to our local node (thus cleaned on node disconnect)
// make sure to clean any leftover on going handles
// callback that an exception happened, but on a different thread since we don't
// want handlers to worry about stack overflows
// if we get rejected during node shutdown we don't wanna bubble it up
/**
/**
/**
/**
// We don't validate cluster names to allow for CCS connections.
/**
/**
/**
// the caller might not handle this so we invoke the handler
// the caller might not handle this so we invoke the handler
// the caller might not handle this so we invoke the handler
/**
// the caller might not handle this so we invoke the handler
// The parent task is already cancelled - just fail the request
// the caller might not handle this so we invoke the handler
// TODO we can probably fold this entire request ID dance into connection.sendRequest but it will be a bigger refactoring
/*
// local node optimization happens upstream
// usually happen either because we failed to connect to the node
// or because we failed serializing the message
// If holderToNotify == null then handler has already been taken care of.
// callback that an exception happened, but on a different thread since we don't
// want handlers to worry about stack overflows
// if we get rejected during node shutdown we don't wanna bubble it up
//noinspection unchecked
//noinspection unchecked
/**
// TODO we should makes this a hard validation and throw an exception but we need a good way to add backwards layer
// for it. Maybe start with a deprecation layer
/**
/**
/**
/**
/** called by the {@link Transport} implementation once a request has been sent */
/** called by the {@link Transport} implementation once a response was sent to calling node */
/** called by the {@link Transport} implementation after an exception was sent as a response to an incoming request */
// lets see if its in the timeout holder, but sync on mutex to make sure any ongoing timeout handling has finished
// call tracer out of lock
// callback that an exception happened, but on a different thread since we don't
// want handlers to worry about stack overflows
// now that we have the information visible via timeoutInfoHandlers, we try to remove the request id
// response was processed, remove timeout info.
/**
/**
// ignore if its null, the service logs it
// ignore if its null, the service logs it
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// the scheduled internal ping interval setting, defaults to disabled (-1)
// Tcp socket settings
// Connections per node settings
// Tracer settings
/*"),
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// pkg private since it's only used internally
// pkg private since it's only used internally
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// ignore IOException
// TODO we might use the new NIO2 API to get real notification?
// Perform notifications and update children for the current file
// Remained a directory
// File replaced by directory
// Directory replaced by file
// Remained file
// Deleted
// Created
// Same file copy it and update
// This child doesn't appear in the old list - init it
// The child from the old list is missing in the new list
// Delete it
// No files - delete all children
// First delete all children
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// ES change" commentary
/**
/**
//-----------------------------------------------------------------------
/**
// date
// time
// result
// side effect the input collection to indicate the processed fields
// handling unmodifiable collections with no side effect
// ignore, so we can handle unmodifiable collections
//-----------------------------------------------------------------------
/**
// YYYY-MM-DD/YYYYMMDD
// YYYY-MM/YYYY-MM
// YYYY--DD/YYYY--DD (non-iso)
// YYYY/YYYY
// --MM-DD/--MMDD
// --MM/--MM
// ---DD/---DD
//-----------------------------------------------------------------------
/**
// YYYY-DDD/YYYYDDD
// YYYY/YYYY
// -DDD/-DDD
//-----------------------------------------------------------------------
/**
// YYYY-WWW-D/YYYYWWWD
// YYYY-WWW/YYYY-WWW
// YYYY-W-D/YYYYW-D (non-iso)
// YYYY/YYYY
// -WWW-D/-WWWD
// -WWW/-WWW
// -W-D/-W-D
//-----------------------------------------------------------------------
/**
// OK - HMSm/HMS/HM/H - valid in combination with date
// OK - MSm/MS/M/Sm/S - valid ISO formats
//-----------------------------------------------------------------------
/**
/**
//-----------------------------------------------------------------------
/**
/**
/**
/**
/**
/**
/**
/**
/**
//-----------------------------------------------------------------------
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
//-----------------------------------------------------------------------
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
//-----------------------------------------------------------------------
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
//-----------------------------------------------------------------------
// year element (yyyy)
// monthOfYear element (-MM)
// dayOfMonth element (-dd)
// weekyear element (xxxx)
// weekOfWeekyear element (-ww)
// dayOfWeek element (-ee)
// dayOfYear element (-DDD)
// hourOfDay element (HH)
// minuteOfHour element (:mm)
// secondOfMinute element (:ss)
// fractionOfSecond element (.SSSSSSSSS)
// zone offset element
// literal 'T' element
//y,   // year (same as year element)
// year month
// year month day
//w,   // weekyear (same as weekyear element)
// weekyear week
// weekyear week day
//h,    // hour (same as hour element)
// hour minute
// hour minute second
// hour minute second millis
// hour minute second fraction
// date hour
// date hour minute
// date hour minute second
// date hour minute second millis
// date hour minute second fraction
//d,  // date (same as ymd)
// time
// time no millis
// Ttime
// Ttime no millis
// date time
// date time no millis
//wd,  // week date (same as wwd)
// week date time
// week date time no millis
// ordinal date (same as yd)
// ordinal date time
// ordinal date time no millis
// basic date
// basic time
// basic time no millis
// basic Ttime
// basic Ttime no millis
// basic date time
// basic date time no millis
// basic ordinal date
// basic ordinal date time
// basic ordinal date time no millis
// basic week date
// basic week date time
// basic week date time no millis
// date parser element
// time parser element
// date parser
// local date parser
// time parser
// local time parser
// date time parser
// date optional time parser
// local date optional time parser
//-----------------------------------------------------------------------
// Decimal point can be either '.' or ','
// time-element
// minute-element
// second-element
// second fraction
// minute fraction
// hour fraction
// This is different from the general time parser in that the 'T'
// is required.
//-----------------------------------------------------------------------
//-----------------------------------------------------------------------
// ES change, was .appendWeekyear(4, 4)
//-----------------------------------------------------------------------
//-----------------------------------------------------------------------
// ES change, was .appendYear(4, 9)
// ES change, was .appendMonthOfYear(2)
// ES change, was .appendDayOfMonth(2)
// ES change, was .appendWeekyear(4, 9)
// ES change, was .appendWeekOfWeekyear(2)
// ES change, was .appendDayOfYear(3)
// ES change, was .appendHourOfDay(2)
// ES change, was .appendMinuteOfHour(2)
// ES change, was .appendSecondOfMinute(2)
// Support parsing up to nanosecond precision even though
// those extra digits will be dropped.
/*
//www.apache.org/licenses/LICENSE-2.0
// Given we are feeding the same content repeatedly the
// actual memory
// used by bytesDeDuper should not grow
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// collapse field is the last sort
// check merge
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Can't test this how BaseRangeFieldQueryTestCase works now, because we're using BinaryDocValuesField here.
/*
//www.apache.org/licenses/LICENSE-2.0
// intersects (within)
// intersects (crosses)
// intersects (contains, crosses)
// intersects (within)
// intersects (crosses)
// disjoint
// intersects (crosses)
// equal (within, contains, intersects)
// intersects, within
// search
// test includeFrom = false and includeTo = false
// no field in index
// no field in segment
// intersects (within)
/*
//www.apache.org/licenses/LICENSE-2.0
// test with an unknown field
// test with an unknown field and an unknown term
// test with an unknown field and a term that is present in only one field
// if we swap two elements, the resulting query should still be regarded as equal
/*
//www.apache.org/licenses/LICENSE-2.0
// not within:
// not contains:
/*
//www.apache.org/licenses/LICENSE-2.0
// not within:
// not contains:
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// negative values sometimes
// not within:
// not contains:
/*
//www.apache.org/licenses/LICENSE-2.0
// negative values sometimes
// not within:
// not contains:
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Generate a random set of unique terms with ascii character
// Generate a random text made of random terms separated with word-boundaries
// and sentence-boundaries.
// the number of sentences to generate
// the number of terms in the sentence
// capitalize the first letter of the first term in the sentence
// find the passage that contains the current term
// check that the passage is valid
// checks that the start and end of the passage are on word boundaries.
// advance the position to the end of the current passage
// advance the position to the end of the current passage
// randomly advance to the next term to highlight
/*
//www.apache.org/licenses/LICENSE-2.0
//lets include the whitespace at the end to make sure we trim it
//lets include the whitespace at the end to make sure we trim it
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Asking for the jar metadata should not throw exception in tests, no matter how configured */
// throws exception if does not exist, or we cannot access it
// these should never be null
// Note: the cast of the Copy- and MutateFunction is needed for some IDE (specifically Eclipse 4.10.0) to infer the right type
// strict or not should not impact parsing at all here
// strict or not should not impact parsing at all here
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Test the same exception but with the "rest.exception.stacktrace.skip" parameter disabled: the stack_trace must be present
// in the JSON. Since the stack can be large, it only checks the beginning of the JSON.
// just a wrapper which is omitted
// test equivalence
// render header and metadata
/**
// The exception content to parse is built using a XContentBuilder
// because the current Java API does not allow to add metadata/headers
// of other types than list of strings.
// Prints a null failure using generateFailureXContent()
// Failure was null, expecting a "unknown" reason
// Simple elasticsearch exception without cause
// Simple elasticsearch exception with headers (other metadata of type number are not parsed)
// Elasticsearch exception with a cause, headers and parsable metadata
// JDK exception without cause
// JDK exception with cause
// Wrapped exception with cause
// SearchPhaseExecutionException with cause and multiple failures
/**
// Simple elasticsearch exception with headers (other metadata of type number are not parsed)
// Empty or null headers are not printed out by the toXContent method
// Empty or null metadata are not printed out by the toXContent method
/*
//www.apache.org/licenses/LICENSE-2.0
// fair enough
// check
// unknown exception is not directly mapped
// ensure we are carrying over the headers and metadata even if not serialized
// was CreateFailedEngineException
// was DeleteFailedEngineException, deprecated in 6.0 and removed in 7.0
// was DocumentAlreadyExistsException, which is superseded with VersionConflictEngineException
// was EsRejectedExecutionException, which is no longer an instance of ElasticsearchException
// EarlyTerminationException was removed in 6.0
// RoutingValidationException was removed in 5.0
// DeleteByQueryFailedEngineException was removed in 3.0
// was IndexFailedEngineException, deprecated in 6.0 and removed in 7.0
// FlushNotAllowedEngineException was removed in 5.0
// was org.elasticsearch.search.SearchContextException.class
// was org.elasticsearch.index.engine.EngineClosedException.class
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Very simple sanity checks for {@link SpecialPermission} */
/*
//www.apache.org/licenses/LICENSE-2.0
// an actual index has a IndexMetaData.SETTING_INDEX_UUID
// from 7.0 on we are supporting the latest minor of the previous major... this might fail once we add a new version ie. 5.x is
// released since we need to bump the supported minor in Version#minimumCompatibilityVersion()
// TODO: remove this once min compat version is a constant instead of method
// note this is just a silly sanity check, we test it in lucene
// only the latest version for a branch should be a snapshot (ie unreleased)
// Current is weird - it counts as released even though it shouldn't.
// this test ensures we never bump the lucene version in a bugfix release
// should we also assert the lucene bugfix version?
/* tests that if a new version's minCompatVersion is always equal or higher to any older version */
// This exists because 5.1.0 was never released due to a mistake in the release process.
// This verifies that we never declare the version as "released" accidentally.
// It would never pass qa tests later on, but those come very far in the build and this is quick to check now.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// At this point the easiest way to confirm that a handler is loaded is to try to register another one on top of it and to fail
// At this point the easiest way to confirm that a handler is loaded is to try to register another one on top of it and to fail
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// DocWriteResponse is abstract so we have to sneak a subclass in here to test it.
/*
//www.apache.org/licenses/LICENSE-2.0
//makes sure that a reduce is always needed when searching
//makes sure that write operations get sent to the replica as well
//so we are able to intercept those messages and check them
// must set this independently of the plugin so it overrides MockTransportService
// InternalClusterInfoService sends IndicesStatsRequest periodically which messes with this test
// this setting disables it...
//update action goes to the primary, index op gets executed locally, then replicated
//update action goes to the primary, index op gets executed locally, then replicated
//update action goes to the primary, delete op gets executed locally, then replicated
//free context messages are not necessarily sent, but if they are, check their indices
//free context messages are not necessarily sent, but if they are, check their indices
//indices returned by each bulk shard request need to be a subset of the original indices
//every index has an alias
/*
//www.apache.org/licenses/LICENSE-2.0
// set the source, without it, we will have a verification failure
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// validate all responses
/*
//www.apache.org/licenses/LICENSE-2.0
// noinspection OptionalGetWithoutIsPresent
// noinspection OptionalGetWithoutIsPresent
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//[a]sync provide a string
//[a]sync calculate the length of the string
//[a]sync provide a string
//[a]sync calculate the length of the string
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// XContent loses the original exception and wraps it as a message in Elasticsearch exception
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//logger.info(ht.getHotThreads());
// First time, don't ignore idle threads:
// Second time, do ignore idle threads:
// Make sure default is true:
// The filtered stacks should be smaller than unfiltered ones:
/*
//www.apache.org/licenses/LICENSE-2.0
// keystore file should be missing for this test case
// in the missing keystore case no reload should be triggered
// invalid "keystore" file should be present in the config dir
// in the invalid keystore format case no reload should be triggered
// make plugins throw on reload
// "some" keystore should be present
// read seed setting value from the test case (not from the node)
// even if one plugin fails to reload (throws Exception), others should be
// unperturbed
// mock plugin should have been reloaded successfully
// write keystore
// read seed setting value from the test case (not from the node)
// reload call
// shuffle as reload is called in order
// this is expected: the save method is extra diligent and wants to make sure
// the keystore is readable, not relying on umask and whatnot. It's ok, we don't
// care about this in tests.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no-op
// find unassigned primary
// find unassigned replica
// no unassigned shard to explain
// prefer unassigned replicas to started replicas
// prefer started replicas to initializing/relocating replicas
// find shard with given node
// shard is not assigned to given node
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// verify shard info
// verify current node info
// verify unassigned info
// verify cluster info
// verify decision objects
// verify JSON output
// wait till we have passed any pending shard data fetching
// verify shard info
// verify current node info
// verify unassigned info
// verify cluster info
// verify decision objects
// shouldn't be able to allocate to the same node as the primary, the same shard decider should say no
// if we are not including YES decisions, then the node holding the primary should have 1 NO decision,
// the other node should have zero NO decisions
// verify JSON output
// verify shard info
// verify current node info
// verify unassigned info
// verify cluster info
// verify decision objects
// verify JSON output
// verify shard info
// verify current node info
// verify unassigned info
// verify cluster info
// verify decision objects
// verify JSON output
// verify shard info
// verify current node info
// verify unassigned info
// verify cluster info
// verify decision object
// verifying can remain decision object
// verify node decisions
// verify JSON output
// verify shard info
// verify current node info
// verify unassigned info
// verify cluster info
// verify decision object
// verifying cluster rebalance decision object
// verify node decisions
// verify JSON output
// verify shard info
// verify current node info
// verify unassigned info
// verify cluster info
// verify decision object
// verifying cluster rebalance decision object
// verify node decisions
// verify JSON output
// verify shard info
// verify current node info
// verify unassigned info
// verify cluster info
// verify decision object
// verifying cluster rebalance decision object
// verify node decisions
// verify JSON output
// verify shard info
// verify current node info
// verify unassigned info
// verify cluster info
// verify decision objects
// verifying cluster rebalance decision object
// verify node decisions
// verify JSON output
// start replica node first, so it's path will be used first when we start a node after
// stopping all of them at end of test.
// wait for the master to finish processing join.
// wait until the system has fetched shard data and we know there is no valid shard copy
// verify shard info
// verify current node info
// verify unassigned info
// verify decision object
// verify JSON output
// until we reach end of unassigned_info
// we should never display "delayed" from unassigned info
// until we reach end of current_node
// disk info is included, just verify the object is there
// shard data was found on the node, but it is stale
// no shard data was found on the node
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// there are no fields so we're just checking that this doesn't throw anything
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// there are no fields so we're just checking that this doesn't throw anything
/*
//www.apache.org/licenses/LICENSE-2.0
// registers action
// no observer to reconfigure
/*
//www.apache.org/licenses/LICENSE-2.0
// registers action
/*
//www.apache.org/licenses/LICENSE-2.0
// Generate a random cluster health request in version < 7.2.0 and serializes it
// Deserialize and check the cluster health request
// Generate a random cluster health request in current version
// Serialize to node in version < 7.2.0
// Deserialize and check the cluster health request
// null indices in ClusterHealthRequest is deserialized as empty string array
/*
//www.apache.org/licenses/LICENSE-2.0
// Ignore all paths which looks like "indices.RANDOMINDEXNAME.shards"
/*
//www.apache.org/licenses/LICENSE-2.0
// Primary
// Replicas
/*
//www.apache.org/licenses/LICENSE-2.0
//intentionally validating identical order
// add outgoing connection info
// add node calculations
//TODO NodeIndicesStats are not tested here, way too complicated to create, also they need to be migrated to Writeable yet
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// True if the node operation should get stuck until its cancelled
// Simulate a job that takes forever to finish
// Using periodic checks method to identify that the task was cancelled
// Cancel main task
// And send the cancellation request to a random node
// Awaiting for the main task to finish
// We didn't cancel the request and it finished successfully
// That should be rare and can be only in case we didn't block on a single node
// Make sure that the request was successful
// We canceled the request, in this case it should have fail, but we should get partial response
// and we should have at least as many failures as the number of blocked operations
// (we might have cancelled some non-blocked operations before they even started and that's ok)
// We should have the information about the cancelled task in the cancel operation response
// Verify that all cancelled tasks reported that they support cancellation
// Make sure that tasks are no longer running
// Make sure that there are no leftover bans, the ban removal is async, so we might return from the cancellation
// while the ban is still there, but it should disappear shortly
// Cancel all child tasks without cancelling the main task, which should quit on its own
// And send the cancellation request to a random node
// Awaiting for the main task to finish
// Should have cancelled tasks on all nodes
// Make sure that main task is no longer running
// We shouldn't block on the first node since it's leaving the cluster anyway so it doesn't matter
// Make sure that tasks are running
// Simulate the coordinating node leaving the cluster
// Notify only nodes that should remain in the cluster
// Simulate issuing cancel request on the node that is about to leave the cluster
// And send the cancellation request to a random node
// This node still thinks that's part of the cluster, so cancelling should look successful
// Close the first node
// Make sure that tasks are no longer running
// Wait for clean up
// Introduce an additional pseudo random repeatable race conditions
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Run only on data nodes
// First run the health on the master node - should produce only one task on the master node
// counting only registration events
// counting only unregistration events
// Now run the health on a non-master node - should produce one task on master and one task on another node
// counting only registration events
// counting only unregistration events
// Verify that one of these tasks is a parent of another task
// main task
// shard
// level
// tasks
// Make sure all shards are allocated
// the field stats operation should produce one main task
// and then one operation per shard
// the shard level tasks should have the main task as a parent
// main task
// node level tasks
// Make sure all shards are allocated
// the percolate operation should produce one main task
// and then one operation per each node where shards are located
// all node level tasks should have the main task as a parent
// main task
// shard level tasks
// Make sure all shards are allocated
// the validate operation should produce one main task
// and then one operation
// the shard level operation should have the main task as its parent
// main task
// shard level tasks
// primary and replica shard tasks
// Make sure all shards are allocated
// the refresh operation should produce one main task
// Because it's broadcast replication action we will have as many [s] level requests
// as we have primary shards on the coordinating node plus we will have one task per primary outside of the
// coordinating node due to replication.
// If all primaries are on the coordinating node, the number of tasks should be equal to the number of primaries
// If all primaries are not on the coordinating node, the number of tasks should be equal to the number of primaries times 2
// Verify that all [s] events have the proper parent
// This is complicated because if the shard task runs on the same node it has main task as a parent
// but if it runs on non-coordinating node it would have another intermediate [s] task on the coordinating node as a parent
// This shard level task runs on the same node as a parent task - it should have the main task as a direct parent
// This shard level task runs on another node - it should have a corresponding shard level task on the node where main task
// is running
// There should be only one parent task
// we will have as many [s][p] and [s][r] tasks as we have primary and replica shards
// we the [s][p] and [s][r] tasks should have a corresponding [s] task on the same node as a parent
// A [s][p] level task should have a corresponding [s] level task on the same node
// A [s][r] level task should have a corresponding [s] level task on the a different node (where primary is located)
// There should be only one parent task
// main task
// shard task
// shard task on primary
// shard task on replica
// Make sure all shards are allocated to catch replication tasks
// ensures the mapping is available on all nodes so we won't retry the request (in case replicas don't have the right mapping).
// the bulk operation should produce one main task
// we should also get 1 or 2 [s] operation with main operation as a parent
// in case the primary is located on the coordinating node we will have 1 operation, otherwise - 2
// Select the effective shard task
// we have only one task - it's going to be the parent task for all [s][p] and [s][r] tasks
// and it should have the main task as a parent
// task 1 is the parent of task 0, that means that task 0 will control [s][p] and [s][r] tasks
// in turn the parent of the task 1 should be the main task
// otherwise task 1 will control [s][p] and [s][r] tasks
// in turn the parent of the task 0 should be the main task
// we should also get one [s][p] operation with shard operation as a parent
// we should get as many [s][r] operations as we have replica shards
// they all should have the same shard task as a parent
// main task
// shard task
// Make sure all shards are allocated to catch replication tasks
// the search operation should produce one main task
// check that if we have any shard-level requests they all have non-zero length description
// assert that all task descriptions have non-zero length
/**
// First latch waits for the task to start, second on blocks it from finishing.
// Need to run the task in a separate thread because node client's .execute() is blocked by our task listener
// waiting for at least one task to be registered
// Start blocking test task
// Get real client (the plugin is not registered on transport nodes)
// Wait for the task to start on all nodes
// Start blocking test task
// Wait for the task to start on all nodes
//We didn't store the result so it won't come back when we wait
//But the task's details should still be there because we grabbed a reference to the task before waiting for it to complete
// We stored the task so we should get its results
// The task's details should also be there
/**
// Start blocking test task
// Wait for the task to start
// Register listeners so we can be sure the waiting started
// Spin up a request to wait for the test task to finish
/* Wait for the wait to start. This should count down just *before* we wait for completion but after the list/get has got a
// Unblock the request so the wait for completion request can finish
// Now that the task is unblocked the list response will come back
/**
// Start blocking test task
// Wait for the task to start
// Spin up a request that should wait for those tasks to finish
// It will timeout because we haven't unblocked the tasks
// Now we can unblock those requests
/**
// Spin up a request to wait for no matching tasks
// It should finish quickly and without complaint
// Spin up a request to wait for no matching tasks
// It should finish quickly and without complaint
// Spin up a request to wait for all tasks in the cluster to make sure it doesn't cause an infinite loop
// It should finish quickly and without complaint and list the list tasks themselves
// we need this to get task id of the process
// Start non-blocking test task
// run it again to check that the tasks index has been successfully created and can be re-used
// we need this to get task id of the process
// Start non-blocking test task that should fail
// Node isn't found, tasks index doesn't even exist
// Node exists but the task still isn't found
// Save a fake task that looks like it is from a node that isn't part of the cluster
// Now we can find it!
/**
/**
/**
/**
/**
/**
// The test task doesn't have any status
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
/*
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Make sure no tasks are running
// Check task counts using taskManager
// all node tasks + 1 coordinating task
// Check task counts using transport
// pick all test actions
// Coordinating node
// Other nodes node
// There should be a single main task when grouped by tasks
// And as many child tasks as we have nodes
// Check task counts using transport with filtering
// only pick node actions
// Since the main task is not in the list - all tasks should be by themselves
// Check task counts using transport with detailed description
// same request only with detailed description
// Make sure that the main task on coordinating node is the task that was returned to us by execute()
// only pick the main task
// Release all tasks and wait for response
// Make sure that we don't have any lingering tasks
// Get the parent task
// Find tasks with common parent
// Release all tasks and wait for response
// Check task counts using transport with filtering
// only pick node actions
// Check task counts using transport with detailed description
// same request only with detailed description
// Release all tasks and wait for response
// only pick the main action
// Try to cancel main task using action name
// Shouldn't match any tasks since testAction doesn't support cancellation
// Try to cancel main task using id
// Shouldn't match any tasks since testAction doesn't support cancellation
// Make sure that task is still running
// Verify that tasks are marked as non-cancellable
// Release all tasks and wait for response
// Make sure that actions are still registered in the task manager on all nodes
// Twice on the coordinating node and once on all other nodes.
// Simulate task action that fails on one of the tasks on one of the nodes
// Fail in a random way to make sure we can handle all these ways
// Run task action on node tasks that are currently running
// should be successful on all nodes except one
// pick all test actions
// one task failed
// Get successful responses from all nodes except one
// no nodes failed
// Release all node tasks and wait for response
/**
// Start some test nodes action so we could have something to run tasks actions on
// Simulate a task action that works on all nodes except nodes listed in filterNodes.
// We are testing that it works.
// Run task action on node tasks that are currently running
// should be successful on all nodes except nodes that we filtered out
// pick all test actions
// Get successful responses from all nodes except nodes that we filtered out
// no task failed
// no nodes failed
// Make sure that filtered nodes didn't send any responses
// Release all node tasks and wait for response
// Get the parent task
// One element on the top level
// two tasks for the first node
// one tasks for the all other nodes
// Group by parents
// One element on the top level
// Only one top level task
// two tasks for the first node
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// This test checks that the Get Repository operation is never blocked, even if the cluster is read only.
// This test checks that the Get Repository operation is never blocked, even if the cluster is read only.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Commutative
// Changing dryRun makes requests not equal
// Changing explain makes requests not equal
// Changing timeout makes requests not equal
// Changing masterNodeTime makes requests not equal
// Changing commands makes requests not equal
// Can't check hashCode because we can't be sure that changing commands changes the hashCode. It usually does but might not.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// allocation commands have their own tests
// dry-run
// now we allocate
// now fail it N-1 times
// dry-run=false
// without retry_failed we won't allocate that shard
// dry-run=false
// now we manually retry and get the shard back into initializing
// dry-run=false
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we also check that if no settings are changed, deprecation logging still occurs
// we will randomly apply some new dynamic persistent and transient settings
// these are invalid settings that exist as either persistent or transient settings
// these are unknown settings that exist as either persistent or transient settings
// register all the known settings (note that we do not register the unknown settings)
// prepare the dynamic settings update
// force a settings update otherwise our assertions below will fail
// the invalid settings should be archived and not present in non-archived form
// the unknown settings should be archived and not present in non-archived form
// the dynamic settings should be applied
// these are settings that are archived in the cluster state as either persistent or transient settings
// these are invalid settings that exist as either persistent or transient settings
// these are unknown settings that exist as either persistent or transient settings
// register all the known settings (not that we do not register the unknown settings)
// existing archived settings are removed
// the invalid settings should be archived and not present in non-archived form
// the unknown settings should be archived and not present in non-archived form
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// This test checks that the Get Snapshot operation is never blocked, even if the cluster is read only.
// This test checks that the Snapshot Status operation is never blocked, even if the cluster is read only.
/*
//www.apache.org/licenses/LICENSE-2.0
// tests creating XContent and parsing with source(Map) equivalency
/*
//www.apache.org/licenses/LICENSE-2.0
// Don't inject random fields into the custom snapshot metadata, because the metadata map is equality-checked after doing a
// round-trip through xContent serialization/deserialization. Even though the rest of the object ignores unknown fields,
// `metadata` doesn't ignore unknown fields (it just includes them in the parsed object, because the keys are arbitrary), so any
// new fields added to the metadata before it gets deserialized that weren't in the serialized version will cause the equality
// check to fail.
/*
//www.apache.org/licenses/LICENSE-2.0
// We can not subclass AbstractSerializingTestCase because it
// can only be used for instances with equals and hashCode
// GetSnapshotResponse does not override equals and hashCode.
// It does not override equals and hashCode, because it
// contains ElasticsearchException, which does not override equals and hashCode.
// Don't inject random fields into the custom snapshot metadata, because the metadata map is equality-checked after doing a
// round-trip through xContent serialization/deserialization. Even though the rest of the object ignores unknown fields,
// `metadata` doesn't ignore unknown fields (it just includes them in the parsed object, because the keys are arbitrary),
// so any new fields added to the metadata before it gets deserialized that weren't in the serialized version will
// cause the equality check to fail.
// The actual fields are nested in an array, so this regex matches fields with names of the form
// `responses.0.snapshots.3.metadata`
// We set it to false, because GetSnapshotsResponse contains
// ElasticsearchException, whose xContent creation/parsing are not stable.
/*
//www.apache.org/licenses/LICENSE-2.0
// ensure that at least one property is different
// we will only restore properties from the map that are contained in the request body. All other
// properties are restored from the original (in the actual REST action this is restored from the
// REST path and request parameters).
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Do not place random fields in the root object since its fields correspond to shard names.
/*
//www.apache.org/licenses/LICENSE-2.0
// Do not place random fields in the root object or the shards field since their fields correspond to names.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Do not place random fields in the indices field or shards field since their fields correspond to names.
/*
//www.apache.org/licenses/LICENSE-2.0
// Using less than half of Long.MAX_VALUE for random time values to avoid long overflow in tests that add the two time values
/*
//www.apache.org/licenses/LICENSE-2.0
// Do not place random fields in the indices field or shards field since their fields correspond to names.
/*
//www.apache.org/licenses/LICENSE-2.0
// Verify that cluster state api returns after the cluster settings have been updated:
// Pick an arbitrary dynamic cluster setting and change it. Just to get metadata version incremented:
// Verify that the timed out property has been set"
// Fail fast
// Remove transient setting, otherwise test fails with the reason that this test leaves state behind:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: change version to V_6_6_0 after backporting:
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// ok, we hit the disconnected node
// ok, we hit the disconnected node
/*
//www.apache.org/licenses/LICENSE-2.0
// add another node, replicas should get assigned
// make the doc visible
// 1 Jan 2000
// 0 happens when not supported on platform
// these can be -1 if not supported on platform
// start one node with 7 processors.
// wait for the cluster status to settle
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// mutate typesAllowed
// Add language
// Mutate languageContexts
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// This test checks that the Pending Cluster Tasks operation is never blocked, even if an index is read only or whatever.
// restart the cluster but prevent it from performing state recovery
// starting one more node allows the cluster to recover
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Refer to an analyzer by its type so we get its default configuration
// We can refer to a pre-configured token filter by its name to get it
// <-- no config, so use preconfigured filter
// If the preconfigured filter doesn't exist, we use a global filter with no settings
// <-- not preconfigured, but a global one available
// We can build a new char filter to get default values
// <-- basic config, uses defaults
// We can pass a new configuration
// Switch the analyzer out for just a tokenizer
// Now try applying our token filter
// Apply the char filter, checking that the correct configuration gets passed on
// Apply a token filter with parameters
// stop token filter is not prebuilt in AnalysisModule#setupPreConfiguredTokenFilters()
// this should be lowercased and only emit a single token
/**
// create a string with No. words more than maxTokenCount
// request with explain=false to test simpleAnalyze path in TransportAnalyzeAction
// request with explain=true to test detailAnalyze path in TransportAnalyzeAction
/**
// create a string with No. words more than idxMaxTokenCount
// normalizer
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Request is not blocked
// Request is blocked
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Serialising and deserialising an exception seems to remove the "java.base/" part from the stack trace
// in the `reason` property, so we don't compare it directly. Instead, check that the first lines match,
// and that the stack trace has the same number of lines.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// ensure all shards are allocated otherwise the ensure green below might not succeed since we require the merge node
// if we change the setting too quickly we will end up with one replica unassigned which can't be assigned anymore due
// to the require._name below.
// relocate all shards to one node such that we can merge it.
// disable rebalancing to be able to capture the right stats. balancing can move the target primary
// making it hard to pin point the source shards.
// bump replicas
// clean up
/*
//www.apache.org/licenses/LICENSE-2.0
// not necessarily needed here but for completeness we lock here too
// this happens async!!!
// recreate that index
// we sync here since we have to ensure that all indexing operations below for a given ID are done before
// we increment the index version otherwise a doc that is in-flight could make it into an index that it
// was supposed to be deleted for and our assertion fail...
// from here on all docs with index_version == 0|1 must be gone!!!! only 2 are ok;
// fine
// fine we run into a delete index while retrying
// we only really assert that we never reuse segments of old indices or anything like this here and that nothing fails with
// crazy exceptions
/**
// all should fail
// the numeric equivalent of all should also fail
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// As Alias#equals only looks at name, we check the equality of the other Alias parameters here.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//github.com/elastic/elasticsearch/issues/34080", Constants.WINDOWS);
// ensure all shards are allocated otherwise the ensure green below might not succeed since we require the merge node
// if we change the setting too quickly we will end up with one replica unassigned which can't be assigned anymore due
// to the require._name below.
// relocate all shards to one node such that we can merge it.
// now merge source into a 4 shard index
// now update
// relocate all shards to one node such that we can merge it.
// now merge source into a 2 shard index
// let it be allocated anywhere and bump replicas
// now update
// This needs more than the default timeout if a large number of shards were created.
// fail random primary shards to force primary terms to increase
// this can not succeed until the shard is failed and a replica is promoted
// find an ID that routes to the right shard, we will only index to the shard that saw a primary failure
// relocate all shards to one node such that we can merge it.
// needs more than the default to relocate many shards
// now merge source into target
// ensure all shards are allocated otherwise the ensure green below might not succeed since we require the merge node
// if we change the setting too quickly we will end up with one replica unassigned which can't be assigned anymore due
// to the require._name below.
// relocate all shards to one node such that we can merge it.
// disable rebalancing to be able to capture the right stats. balancing can move the target primary
// making it hard to pin point the source shards.
// now merge source into a single shard index
// resolve true merge node - this is not always the node we required as all shards may be on another node
// bump replicas
// clean up
/**
// ensure all shards are allocated otherwise the ensure green below might not succeed since we require the merge node
// if we change the setting too quickly we will end up with one replica unassigned which can't be assigned anymore due
// to the require._name below.
// relocate all shards to one node such that we can merge it.
// now merge source into a single shard index
// we manually exclude the merge node to forcefully fuck it up
// now we move all shards away from the merge node
// erase the forcefully fuckup!
// wait until it fails
// now relocate them all to the right node
// kick off a retry and wait until it's done!
// we support the expected shard size in the allocator to sum up over the source index shards
// ensure all shards are allocated otherwise the ensure green below might not succeed since we require the merge node
// if we change the setting too quickly we will end up with one replica unassigned which can't be assigned anymore due
// to the require._name below.
// relocate all shards to one node such that we can merge it.
// check that index sort cannot be set on the target index
// check that the index sort order of `source` is correctly applied to the `target`
// ... and that the index sort is also applied to updates
// ensure all shards are allocated otherwise the ensure green below might not succeed since we require the merge node
// if we change the setting too quickly we will end up with one replica unassigned which can't be assigned anymore due
// to the require._name below.
// relocate all shards to one node such that we can merge it.
// disable rebalancing to be able to capture the right stats. balancing can move the target primary
// making it hard to pin point the source shards.
// now merge source into a single shard index
// clean up
// demonstrate that the index.routing.allocation.initial_recovery setting from the shrink doesn't carry over into the split index,
// because this would cause the shrink to fail as the initial_recovery node is no longer present.
/*
//www.apache.org/licenses/LICENSE-2.0
//github.com/elastic/elasticsearch/issues/34080", Constants.WINDOWS);
// randomly set the value manually
// let's introduce some updates / deletes on the index
// try to set it if we have a source index with 1 shard
// now update
// now split source into a new index
// let it be allocated anywhere and bump replicas
// now update
// now, do a nested query
// needs more than the default to allocate many shards
// fail random primary shards to force primary terms to increase
// this can not succeed until the shard is failed and a replica is promoted
// find an ID that routes to the right shard, we will only index to the shard that saw a primary failure
// now split source into target
// needs more than the default to relocate many shards
// ensure all shards are allocated otherwise the ensure green below might not succeed since we require the merge node
// if we change the setting too quickly we will end up with one replica unassigned which can't be assigned anymore due
// to the require._name below.
// relocate all shards to one node such that we can merge it.
// disable rebalancing to be able to capture the right stats. balancing can move the target primary
// making it hard to pin point the source shards.
// bump replicas
// clean up
// ensure all shards are allocated otherwise the ensure green below might not succeed since we require the merge node
// if we change the setting too quickly we will end up with one replica unassigned which can't be assigned anymore due
// to the require._name below.
// check that index sort cannot be set on the target index
// check that the index sort order of `source` is correctly applied to the `target`
// ... and that the index sort is also applied to updates
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Request is not blocked
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// +1 for the shards header
// total shard failure
// shard copy failure
/*
//www.apache.org/licenses/LICENSE-2.0
// Request is not blocked
// Request is blocked
// Merging all indices is blocked when the cluster is read-only
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// rarely have no fields
// Not meant to be exhaustive
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Request is not blocked
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// now we modify the provided name such that we can test that the pattern is carried on
// A large max_size
// A small max_size
// An empty index
/*
//www.apache.org/licenses/LICENSE-2.0
//here we compare the string representation as there is some information loss when serializing
//and de-serializing MaxAgeCondition
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// For given alias, verify that condition evaluation fails when the condition doc count is greater than the primaries doc count
// (primaries from only write index is considered)
// For given alias, verify that the condition evaluation is successful when condition doc count is less than the primaries doc count
// (primaries from only write index is considered)
// -1 means there is no primary shard.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Request is not blocked
// Request is blocked
/*
//www.apache.org/licenses/LICENSE-2.0
// don't allow any merges so that the num docs is the expected segments
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//we do not want to add new fields at the root (index-level), or inside settings blocks
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// if the settings are enclose as a "settings" object
// then all other top-level elements will be ignored during the parsing
// here only the settings should be tested, as this test covers explicitly only the XContent parsing
// the rest of the request fields are tested by the SerializingTests
// if enclosedSettings are used, disable the XContentEquivalence check as the
// parsed.toXContent is not equivalent to the test instance
/*
//www.apache.org/licenses/LICENSE-2.0
// no unallocated shards
// all shards
// default with unassigned shards
// ensure index filtering works
// IndicesClusterStateService#failAndRemoveShard() called asynchronously but we need it to have completed here.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// create one that won't fail
// now we start the shard
// now we start the shard
// now we start the shard
// create one that won't fail
// now we start the shard
/*
//www.apache.org/licenses/LICENSE-2.0
// Request is not blocked
// Request is blocked
// Ok, a ClusterBlockException is expected
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// bkd tree is stored off-heap
// bkd tree is stored on heap
// now check multiple segments stats are merged together
// bkd tree is stored off-heap
// bkd tree is stored on heap
// Create an index without automatic refreshes
// Index a document asynchronously so the request will only return when document is refreshed
// Wait for the refresh listener to appear in the stats. Wait a long time because NFS tests can be quite slow!
// There shouldn't be a doc. If there is then we did *something* weird.
// Refresh the index and wait for the request to come back
// The document should appear in the statistics and the refresh listener should be gone
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//invalid json: put index template fails
// Delete all existing templates
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Fetching the iterator doesn't call the callback
// hasNext doesn't trigger the callback
// next does
// next doesn't call the callback when there isn't a backoff available
// The second iterator also calls the callback
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// allowing the auto-generated timestamp to externally be set would allow making the index inconsistent with duplicate docs
// sets the timestamp
// all requests in the json are missing index and type parameters: "_index" : "test", "_type" : "type1",
/** This test ensures that index deletion makes indexing fail quickly, not wait on the index that has disappeared */
/*
//www.apache.org/licenses/LICENSE-2.0
// Shuffle the XContent fields
/*
//www.apache.org/licenses/LICENSE-2.0
// using failures prevents caring about types
/*
//www.apache.org/licenses/LICENSE-2.0
// See issue #8125
/*
//www.apache.org/licenses/LICENSE-2.0
//let's make sure that the bulk action limit trips, one single execution will index all the documents
//let's make sure that this bulk won't be automatically flushed
//we really need an explicit flush as none of the bulk thresholds was reached
//set interval and size to high values
//with concurrent requests > 1 we can't rely on the order of the bulk requests
//we do want to check that we don't get duplicate ids back
//let's make sure that the bulk action limit trips, one single execution will index all the documents
// check if we can call it multiple times
//set interval and size to high values
//with concurrent requests > 1 we can't rely on the order of the bulk requests
//we do want to check that we don't get duplicate ids back
//with concurrent requests > 1 we can't rely on the order of the bulk requests
//we do want to check that we don't get duplicate ids back
/*
//www.apache.org/licenses/LICENSE-2.0
//Have very low pool and queue sizes to overwhelm internal pools easily
// don't mess with this one! It's quite sensitive to a low queue size
// (see also ThreadedActionListener which is happily spawning threads even when we already got rejected)
//.put("thread_pool.listener.queue_size", 1)
// default is 200
// no op
// zero means that we're in the sync case, more means that we're in the async case
// validate all responses
// ignored, we exceeded the write queue size when dispatching the initial bulk request
// we're not expecting any other errors
// we're not expecting that we overwhelmed it even once when we maxed out the number of retries
/**
// this is intentionally *not* static final. We will only ever have one instance of this class per test case and want the
// thread local to be eligible for garbage collection right after the test to avoid leaks.
// Assumption: This method is called from the same thread as the last call to the internal iterator's #hasNext() / #next()
// see also Retry.AbstractRetryHandler#onResponse().
// did we ever retry?
// we should correlate any iterator only once
// update on every invocation as we might get rescheduled on a different thread. Unfortunately, there is a chance that
// we pollute the thread local map with stale values. Due to the implementation of Retry and the life cycle of the
// enclosing class CorrelatingBackoffPolicy this should not pose a major problem though.
// update on every invocation
/*
//www.apache.org/licenses/LICENSE-2.0
// add a single item which won't be over the size or number of items
// wait for flush to execute
//find some randoms that allow this test to take under ~ 10 seconds
//extremely unlikely
//simulate work
//should never happen
//don't start any work until all tasks are submitted
//alternate between ways to add to the bulk processor
//count total docs after processor is closed since there may have been partial batches that are flushed on close.
//don't flush based on size
//simulate work
//should never happen
//don't start any work until all tasks are submitted
//alternate between ways to add to the bulk processor
/*
//www.apache.org/licenses/LICENSE-2.0
// sync global checkpoint quickly so we can verify seq_no_stats aligned between all copies after tests.
// Huge request to keep the write pool busy so that requests waiting on a mapping update in the other bulk request get rejected
// by the write pool
// ignored, one of the two bulk requests was rejected outright due to the write queue being full
/*
//www.apache.org/licenses/LICENSE-2.0
// simulate that we actually executed the modified bulk request:
// So half of the requests have "failed", so only the successful requests are left:
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// issue 7361
// We force here a "id is missing" validation error
// issue 15120
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// this test needs an even num of docs
// issue 4987
// issue 6630
// issue 6410
// issue 9821
// issue 9821
// noop update
// not_found update
// not_found delete
/*
//www.apache.org/licenses/LICENSE-2.0
// no need to wait fof a long time in tests
/**
// Stash some random headers so we can assert that we preserve them
// do everything synchronously, that's fine for a test
// if we have to fail, we need to fail at least once "reliably", the rest can be random
/*
//www.apache.org/licenses/LICENSE-2.0
// Test emulating auto_create_index=false
// Test emulating auto_create_index=true
// Test emulating all indices already created
// Test emulating auto_create_index=true with some indices already created.
// Emulate auto_create_index=-bad,+*
// Emulate auto_create_index=false but the "ok" index already exists
// Use "null" to mean "no indices can be created so don't bother checking"
// If we try to create an index just immediately assume it worked
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Services needed by bulk action */
/** Arguments to callbacks we want to capture, but which require generics, so we must use @Captor */
/** The actual action we want to test, with real indexing mocked */
/** Single item bulk write action that wraps index requests */
/** True if the next call to the index action should act as an ingest node */
/** The nodes that forwarded index requests should be cycled through. */
/** A subclass of the real bulk action to allow skipping real bulk indexing, and marking when it would have happened. */
// set when the "real" bulk execution happens
// pluggable return value for `needToCheck`
// If set to false, will be set to true by call to createIndex
// initialize captors, which must be members to use @Capture because of generics
// setup services that will be called by action
// setup nodes for local and remote
// setup the mocked ingest service for capturing calls
// call on construction of action
// check failure works, and passes through to the listener
// haven't executed yet
// now check success
// have an exception for our one index request
// this is done by the real pipeline execution service when processing
// listener would only be called by real index action, not our mocked one
// check failure works, and passes through to the listener
// haven't executed yet
// now check success
// this is done by the real pipeline execution service when processing
// listener would only be called by real index action, not our mocked one
// should not have executed ingest locally
// but instead should have sent to a remote node with the transport service
// make sure we used one of the nodes
// no local index execution
// listener not called yet
// call the listener for the remote node
// now the listener we passed should have been delegated to by the remote listener
// still no local index execution
// now make sure ingest nodes are rotated through with a subsequent request
// should not have executed ingest locally
// but instead should have sent to a remote node with the transport service
// make sure we used one of the nodes
// no local index execution
// listener not called yet
// call the listener for the remote node
// now the listener we passed should have been delegated to by the remote listener
// still no local index execution
// now make sure ingest nodes are rotated through with a subsequent request
// this test only covers the mechanics that scripted bulk upserts will execute a default pipeline. However, in practice scripted
// bulk upserts with a default pipeline are a bit surprising since the script executes AFTER the pipeline.
// check failure works, and passes through to the listener
// haven't executed yet
// now check success of the transport bulk action
// this is done by the real pipeline execution service when processing
// this is done by the real pipeline execution service when processing
// this is done by the real pipeline execution service when processing
// listener would only be called by real index action, not our mocked one
// check failure works, and passes through to the listener
// haven't executed yet
// no index yet
// still no index yet, the ingest node failed.
// now check success
// this is done by the real pipeline execution service when processing
// now the index is created since we skipped the ingest node path.
// listener would only be called by real index action, not our mocked one
// check failure works, and passes through to the listener
// haven't executed yet
// now check success
// this is done by the real pipeline execution service when processing
// listener would only be called by real index action, not our mocked one
/*
//www.apache.org/licenses/LICENSE-2.0
/** Services needed by bulk action */
// set when the "real" index is created
// index name matches with IDM:
// alias name matches with IDM:
// index name matches with ITMD:
// index name matches with IDM:
// alias name matches with IDM:
// index name matches with ITMD:
// no pipeline:
// request pipeline:
// request pipeline with default pipeline:
// request pipeline with final pipeline:
/*
//www.apache.org/licenses/LICENSE-2.0
// test unit conversion with a controlled clock
// test took advances with System#nanoTime
// translate Windows line endings (\r\n) to standard ones (\n)
/*
//www.apache.org/licenses/LICENSE-2.0
// Translog should change, since there were no problems
// Assert that the document actually made it there
// Should be failed since the document already exists
// Assert that the document count is still 1
// Preemptively abort one of the bulk items, but allow the others to proceed
// since at least 1 item passed, the tran log location should exist,
// and the response should exist and match the item count
// check each response matches the input item, including the rejection
// Check that the non-rejected updates made it to the shard
// Pretend the mappings haven't made it to the node yet
// There should indeed be a mapping update
// Verify that the shard "executed" the operation once
// Verify that the shard "executed" the operation only once (1 for previous invocations plus
// 1 for this execution)
// Return an exception when trying to update the mapping, or when waiting for it to come
// Translog shouldn't be synced, as there were conflicting mappings
// Since this was not a conflict failure, the primary response
// should be filled out with the failure information
// Translog changes, even though the document didn't exist
// Any version can be matched on replica
// Now do the same after indexing the document, it should now find and delete the document
// Translog changes, because the document was deleted
// Any version can be matched on replica
// Basically nothing changes in the request since it's a noop
// check that bulk item was not mutated
// Since this was not a conflict failure, the primary response
// should be filled out with the failure information
// Check that the translog is successfully advanced
// Since this was not a conflict failure, the primary response
// should be filled out with the failure information
// Check that the translog is successfully advanced
// if we sync the location, nothing else is unsynced
// the beating will continue until success has come.
// add a response to the request and thereby check that it is ignored for the primary.
/**
/**
/** Doesn't perform any mapping updates */
/** Always throw the given exception */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// The ShardInfo.Failure's exception is rendered out in a "reason" object. We shouldn't add anything random there
// because exception rendering and parsing are very permissive: any extra object or field would be rendered as
// a exception custom metadata and be parsed back as a custom header, making it impossible to compare the results
// in this test.
// We can't use equals() to compare the original and the parsed delete response
// because the random delete response can contain shard failures with exceptions,
// and those exceptions are not parsed back with the same types.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// merged responses
// Disallow random fields from being inserted under the 'fields' key, as this
// map only contains field names, and also under 'fields.FIELD_NAME', as these
// maps only contain type names.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// "_source" and "fields" just consists of key/value pairs, we shouldn't add anything random there. It is already
// randomized in the randomGetResult() method anyway. Also, we cannot add anything in the root object since this is
// where GetResult's metadata fields are rendered out while            // other fields are rendered out in a "fields" object.
//print the parsed object out and test that the output is the same as the original output
//check that the source stays unchanged, no shuffling of keys nor anything like that
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// force the Object... setter
/*
//www.apache.org/licenses/LICENSE-2.0
// test negative shard count value not allowed
// reindex makes use of index requests without a source so this needs to be handled
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// The ShardInfo.Failure's exception is rendered out in a "reason" object. We shouldn't add anything random there
// because exception rendering and parsing are very permissive: any extra object or field would be rendered as
// a exception custom metadata and be parsed back as a custom header, making it impossible to compare the results
// in this test.
// We can't use equals() to compare the original and the parsed index response
// because the random index response can contain shard failures with exceptions,
// and those exceptions are not parsed back with the same types.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// A pipeline with 2 processors: the test async processor and sync test processor.
// The expected result of async test processor:
// The expected result of sync test processor:
// ignore
/*
//www.apache.org/licenses/LICENSE-2.0
// We only use a single SetProcessor here in each pipeline to test.
// Since the contents are returned as a configMap anyway this does not matter for fromXContent
/*
//www.apache.org/licenses/LICENSE-2.0
//Start first processor
//End first processor
/*
//www.apache.org/licenses/LICENSE-2.0
// We cannot have random fields in the _source field and _ingest field
/**
//exceptions are not of the same type whenever parsed back
/*
//www.apache.org/licenses/LICENSE-2.0
// We cannot have random fields in the _source field and _ingest field
/**
//exceptions are not of the same type whenever parsed back
/*
//www.apache.org/licenses/LICENSE-2.0
// Remove and compare pipeline key. It is always in the verbose result,
// since that is a snapshot of how the ingest doc looks during pipeline execution, but not in the final ingestDocument.
// The key gets added and removed during pipeline execution.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Sometimes we set an id
// Sometimes we explicitly set a boolean (with whatever value)
/*
//www.apache.org/licenses/LICENSE-2.0
// since the pipeline id is not serialized with XContent we set it to null for equality tests.
// we test failures separately since comparing XContent is not possible with failures
// We cannot have random fields in the _source field and _ingest field
/**
//exceptions are not of the same type whenever parsed back
/*
//www.apache.org/licenses/LICENSE-2.0
// we test failures separately since comparing XContent is not possible with failures
// We cannot have random fields in the _source field and _ingest field
/**
//with random fields insertion in the inner exceptions, some random stuff may be parsed back as metadata,
//but that does not bother our assertions, as we only want to test that we don't break.
//exceptions are not of the same type whenever parsed back
/*
//www.apache.org/licenses/LICENSE-2.0
// using a cbor builder here, so that byte arrays do not get converted, so equalTo() below works
// We cannot have random fields in the _source field and _ingest field
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// toggle the snapshot flag of the original Build parameter
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// with a controlled clock, we can assert the exact took time
// with a real clock, the best we can say is that it took as long as we spun for
// skip one to avoid the "all shards failed" failure.
// expect at least 2 shards, so onPhaseDone should report failure.
/*
//www.apache.org/licenses/LICENSE-2.0
// never skip the failure
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// response is unused
/*
//www.apache.org/licenses/LICENSE-2.0
//test that existing values get overridden
//test that existing values get overridden
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// the size of the result set
// the size of the result set
// the size of the result set
// the size of the result set
// phase execution will clean up on the contexts
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// the size of the result set
// the size of the result set
// we use at least 2 hits otherwise this is subject to single shard optimization and we trip an assert...
// also numshards --> 1 hit per shard
// the size of the result set
// all non fetched results will be freed
// the size of the result set
// contexts that are not fetched should be cleaned up
// the size of the result set
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// null is ok here for this test
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// test unit conversion using a controller clock
// test using System#nanoTime
/*
//www.apache.org/licenses/LICENSE-2.0
// The only formats that support stream separator
// scroll is not supported in the current msearch api, so unset it:
// only expand_wildcards, ignore_unavailable and allow_no_indices can be specified from msearch api, so unset other options:
// No need to return a very complex SearchSourceBuilder here, that is tested elsewhere
/*
//www.apache.org/licenses/LICENSE-2.0
// Creating a minimal response is OK, because SearchResponse self
// is tested elsewhere.
// Creating a minimal response is OK, because SearchResponse is tested elsewhere.
/**
//with random fields insertion in the inner exceptions, some random stuff may be parsed back as metadata,
//but that does not bother our assertions, as we only want to test that we don't break.
//exceptions are not of the same type whenever parsed back
/*
//www.apache.org/licenses/LICENSE-2.0
// only count this once per replica
// at least one response otherwise the entire request fails
// for the sake of this test we place the replica on the same node. ie. this is not a mistake since we limit per node now
// only count this once per shard copy
// for the sake of this test we place the replica on the same node. ie. this is not a mistake since we limit per node now
// only count this once per shard copy
// unused yet
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// no source means size = 10
// 4*3 results = 12 we get result 5 to 10 here with from=5 and size=5
/*
//www.apache.org/licenses/LICENSE-2.0
// Failures are grouped (by default)
// SearchPhaseExecutionException has no cause field
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//clusterAlias and absoluteStartMillis do not have public getters/setters hence we randomize them only in this test specifically.
// if scroll isn't set, validate should never add errors
// disabling `track_total_hits` isn't valid in scroll context
// make sure we don't set the request cache for a scroll query
// scroll and `from` isn't valid
// make sure we don't set the request cache for a scroll query
// scroll and `size` is `0`
// Rescore is not allowed on scroll requests
/*
//www.apache.org/licenses/LICENSE-2.0
//set different collapse values for each cluster for simplicity
// the sort fields and the collapse field are not returned when hits are empty
// the collapse field is not returned when hits are empty
//Realistically clusters have the same indices with same names, but different uuid. Yet it can happen that the same cluster
//is registered twice with different aliases and searched multiple times as part of the same search request.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// node2 is not available
// .reason() returns the full stack trace
/*
//www.apache.org/licenses/LICENSE-2.0
//test that existing values get overridden
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// Initialize dependencies of TransportMultiSearchAction
// Initialize dependencies of TransportMultiSearchAction
// Keep track of the number of concurrent searches started by multi search api,
// and if there are more searches than is allowed create an error and remember that.
// randomize whether or not requests are executed asynchronously
// Execute the multi search api and fail if we find an error after executing:
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//simulate scenario where the local cluster is also registered as a remote one
//simulate scenario where the same cluster is registered multiple times with different aliases
//the intention here is not to test that we throw NPE, rather to trigger a situation that makes
//SearchResponseMerger#getMergedResponse fail unexpectedly and verify that the listener is properly notified with the NPE
//setting skip_unavailable to true for all the disconnected clusters will make the request succeed again
//give transport service enough time to realize that the node is down, and to notify the connection listeners
//so that RemoteClusterConnection is left with no connected nodes, hence it will retry connecting next
//setting skip_unavailable to true for all the disconnected clusters will make the request succeed again
//give transport service enough time to realize that the node is down, and to notify the connection listeners
//so that RemoteClusterConnection is left with no connected nodes, hence it will retry connecting next
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// no exception
// no exception
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// magic numbers not exposed through API
// invalid values shouldn't validate
// default is 1
// both values should represent "all"
// enough shards active case
// not enough shards active
// wait for zero shards should always pass
// invalid values
// initial index creation and new routing table info
// want less than half, and primary is already started
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// not enough data nodes, index creation times out
// enough data nodes, all shards are active
// Its possible that the cluster state update task that includes the create index hasn't processed before we timeout,
// and subsequently the test cleanup process does not delete the index in question because it does not see it, and
// only after the test cleanup does the index creation manifest in the cluster state.  To take care of this problem
// and its potential ramifications, we wait here for the index creation cluster state update task to finish
/*
//www.apache.org/licenses/LICENSE-2.0
// test all possible methods that can be interrupted
// we check this here instead of in the catch block to ensure that the catch block executed
/*
//www.apache.org/licenses/LICENSE-2.0
// see #21449
/* When patterns are specified, even if the are all negative, the default is can't create. So a pure negative pattern is the same
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Serialising and deserialising an exception seems to remove the "java.base/" part from the stack trace
// in the `reason` property, so we don't compare it directly. Instead, check that the first lines match,
// and that the stack trace has the same number of lines.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//the filters that don't run will go last in the sorted list
// Safe because its all we test with
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// indexing, by default, will work (waiting for one shard copy only)
// wait for 2 active shard copies
// but really, all is well
// this should work, since we now have two
// but really, all is well
// this should work, since we now have all shards started
/*
//www.apache.org/licenses/LICENSE-2.0
// since static must set to null to be eligible for collection
// check a request was sent to the right number of nodes
// check requests were sent to the right nodes
// check one request was sent to each node
// simulate the master being removed from the cluster but before a new master is elected
// as such, the shards assigned to the master will still show up in the cluster state as assigned to a node but
// that node will not be in the local cluster state on any node that has detected the master as failing
// in this case, such a shard should be treated as unassigned
// the master should not be in the list of nodes that requests were sent to
// check a request was sent to the right number of nodes
// check requests were sent to the right nodes
// check one request was sent to each non-master node
// check the operation was executed only on the expected shards
// check the operation was executed on the correct node
// check the operation results
// simulate removing the master
// simulate node failure
// simulate operation failure
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// still long to induce failures but not too long so test won't time out
// We index data with mapping changes into cluster and have master failover at same time
// index data with mapping changes
// interrupt communication between master and other nodes in cluster
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// remove unneeded threading by wrapping listener with SAME to prevent super.doExecute from wrapping it with LISTENER
// very lightweight operation in memory, no need to fork to a thread
// default implementation, overridden in specific tests
// default implementation, overridden in specific tests
// use a random base version so it can go down when simulating a restart.
// simulate master node removal
// reset the same state to increment a version simulating a join of an existing node
// simulating use being disconnected
// simulate master restart followed by a state recovery - this will reset the cluster state version
// The other node has become master, simulate failures of this node while publishing cluster state through ZenDiscovery
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: pass through task here?
/*
//www.apache.org/licenses/LICENSE-2.0
// check a request was sent to the right number of nodes
// This should be ignored:
// note: I shuffled the overall list, so it's not possible to guarantee that it's in the right order
// check requests were only sent to data nodes
// since static must set to null to be eligible for collection
/*
//www.apache.org/licenses/LICENSE-2.0
// we expect no failures here because UnavailableShardsException does not count as failed
//sometimes add failure (no failure means shard unavailable)
// sometimes fail
// just add a general exception and see if failed shards will be incremented by 2
/*
//www.apache.org/licenses/LICENSE-2.0
// simulate execution of the replication phase on the relocation target node after relocation source was marked as relocated
// add a few in-sync allocation ids that don't have corresponding routing entries
// simulate execution of the replication phase on the relocation target node after relocation source was marked as relocated
// add an in-sync allocation id that doesn't have a corresponding routing entry
// keep things simple
/*
//www.apache.org/licenses/LICENSE-2.0
// Shuffle the XContent fields
// Move to the first start object
/*
//www.apache.org/licenses/LICENSE-2.0
// only activated on primary
// this test only provoked an issue for the primary action, but for completeness, we pick the action randomly
// we pause node after TransportService has moved to stopped, but before closing connections, since if connections are closed
// we would not hit the transport service closed case.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//noinspection unchecked
/* *
// no replicas in oder to skip the replication part
// no permit should be held
/**
// finish relocation
// no replicas in oder to skip the replication part
// try again with a request that is based on a newer cluster state, make sure we waited until that
// cluster state for the index to appear
//TODO I'd have expected this to be true but we fail too early?
// no replicas in oder to skip the replication part
// no replicas in order to skip the replication part
// we always try at least one more time on timeout
// generate a CS change
// whether shard has been marked as relocated already (i.e. relocation completed)
// it should have been freed.
//noinspection unchecked
// simulate execution of the primary phase on the relocation target node
// throws no exception
// check that at unknown node fails
// A replication action doesn't not fail the request
// we use one replica to check the primary term was set on the operation and sent to the replica
//noinspection unchecked
//noinspection unchecked
// no replica, we only want to test on primary
// operation should have finished and counter decreased because no outstanding replica requests
/**
// test wait_for_active_shards index setting used when the default is set on the request
// set to default so index settings are used
// test wait_for_active_shards when default not set on the request (request value should be honored over index setting)
/** test that a primary request is rejected if it arrives at a shard with a wrong allocation id or term */
/** test that a replica request is rejected if it arrives at a shard with a wrong allocation id */
// simulate execution of the node holding the replica
/**
// simulate execution of the node holding the replica
// fail with the exception if there
// no retry yet
// release the waiting
// simulate execution of the node holding the replica
// fail with the exception if there
// release the waiting
// publish a new state (same as the old state with the version incremented)
// Assert that the request was retried, this time successful
/**
/**
// keep things simple
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// node2 doesn't really exist, but we are performing some trickery in mockIndicesService() to pretend that node1 holds both
// the primary and the replica, so redirect the request back to node1.
// An action which acquires all operation permits during execution and set a block
// non delayed operation might fail depending on the order they were executed
/**
// The TransportReplicationAction.getIndexShard() method is overridden for testing purpose but we double check here
// that the permit has been acquired on the primary shard
// The TransportReplicationAction.getIndexShard() method is overridden for testing purpose but we double check here
// that the permit has been acquired on the replica shard
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// The default, but we'll set it anyway just to be explicit
// The default, but we'll set it anyway just to be explicit
// Haven't really responded yet
// Now we can fire the listener manually and we'll get a response
// Haven't responded yet
// Now we can fire the listener manually and we'll get a response
// check that at unknown node fails
// A write replication action proxy should fail the shard
// the shard the request was sent to and the shard to be failed should be the same
// simulate success
// simulate the primary has been demoted
// simulated a node closing exception
/*
//www.apache.org/licenses/LICENSE-2.0
// since static must set to null to be eligible for collection
// this should not trigger retry or anything and the listener should report exception immediately
// result should return immediately
// this should fail because primary not initialized
// this time it should work
// trigger cluster state observer
// wait until the timeout was triggered and we actually tried to send for the second time
// let it fail the second time too
// result should return immediately
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// this methods wants to send one doc to each shard
// used field selection
// this will be ignored.
// 0 length set is not supported.
// always adds a test that fails
// test we expect that...
/*
//www.apache.org/licenses/LICENSE-2.0
// check response is nevertheless serializable to json
// when indexing a field that simply has a question mark, the term vectors will be null
// lets see if the null term vectors are caught...
// when indexing a field that simply has a question mark, the term vectors will be null
// lets see if the null term vectors are caught...
// must be of type string and indexed.
// no tvs
// no tvs
// no tvs
// no tvs
// yes tvs
// yes tvs
// 0the3 4quick9 10brown15 16fox19 20jumps25 26over30
// 31the34 35lazy39 40dog43
// do nothing
// 0the3 4quick9 10brown15 16fox19 20jumps25 26over30
// 31the34 35lazy39 40dog43
// do not test ttf or doc frequency, because here we have
// many shards and do not know how documents are distributed
// docs and pos only returns something if positions or
// payloads or offsets are stored / requestd Otherwise use
// DocsEnum?
// only return something useful if requested and stored
// payloads are never made by the mapping in this test
// only return something useful if requested and stored
//we generate as many docs as many shards we have
// like testSimpleTermVectors but we create fields with no term vectors
// MemoryIndex does not support payloads
// do not test ttf or doc frequency, because here we have many
// shards and do not know how documents are distributed
// We never configure an analyzer with payloads for this test so this is never returned
// setup indices
// index documents with and without term vectors
// request tvs and compare from each index
// compare field value
// compare df and ttf
// compare freq and docs
// compare position, start offsets and end offsets
// setup indices
// index documents existing document
// request tvs from existing document
// request tvs from artificial document
// ensure we get the stats from the same shard as existing doc
// compare existing tvs with artificial
// setup indices
// request tvs from artificial document
// Since the index is empty, all of artificial document's "term_statistics" should be 0/absent
// we're guaranteed to receive terms for that field
// setup mapping and document source
// setup indices with mapping
// index a single document with prepared source
// create random per_field_analyzer and selected fields
// selected fields not specified
// should return all fields that have terms vectors, some with overridden analyzer
// selected fields specified including some not in the mapping
// should return only the specified valid fields, with some with overridden analyzer
// check overridden by keyword analyzer ...
// ensure no other fields are returned
// From translog:
// version 0 means ignore version, which is the default
//all good
// From Lucene index:
// version 0 means ignore version, which is the default
//all good
// From translog:
// version 0 means ignore version, which is the default
//all good
// From Lucene index:
// version 0 means ignore version, which is the default
//all good
// no dfs
// as many terms as there are docs
// setup indices
// index document
// Get search shards
// request termvectors of artificial document from each shard
// setup indices
// index documents with and without term vectors
// request tvs and compare from each index
/*
//www.apache.org/licenses/LICENSE-2.0
// Delimited payload token filter was moved to analysis-common module,
// This test relies heavily on this token filter, even though it is not testing this token filter.
// Solution for now is copy what delimited payload token filter does in this test.
// Unfortunately MockPayloadAnalyzer couldn't be used here as it misses functionality.
// Based on DelimitedPayloadTokenFilter:
// simply set a new length
// we have not seen the delimiter
//create the test document
//create the mapping
/*
//www.apache.org/licenses/LICENSE-2.0
//we generate as many docs as many shards we have
// Version from translog
// [0] version doesn't matter, which is the default
//Version from Lucene index
// [0] version doesn't matter, which is the default
// Version from translog
// [0] version doesn't matter, which is the default
//Version from Lucene index
// [0] version doesn't matter, which is the default
/*
//www.apache.org/licenses/LICENSE-2.0
// write
// read
// see if correct
// write
// read
// write
// read
// write using older version which contains types
// First check the type on the stream was written as "_doc" by manually parsing the stream until the type
// now read the stream as normal to check it is parsed correct if received from an older node
// issue #12311
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// simple script
// simple verbose script
// script with params
// script with params and upsert
// script with doc
// We just upsert one document with now() using a script
// We simulate that the document is not existing yet
// We simulate that the document is not existing yet
// There is no routing and parent because the document doesn't exist
// There is no routing and parent the indexing request
// Doc exists but has no source or fields
// There is no routing and parent on either request
// Doc exists and has the parent and routing fields
// Use the get result parent and routing
// Try again, with detectNoop turned off
// Change the request to be a different doc
// Now where the script changes the op to "delete"
// We treat everything else as a No-op
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// - The ShardInfo.Failure's exception is rendered out in a "reason" object. We shouldn't add anything random there
// because exception rendering and parsing are very permissive: any extra object or field would be rendered as
// a exception custom metadata and be parsed back as a custom header, making it impossible to compare the results
// in this test.
// - The GetResult's "_source" and "fields" just consists of key/value pairs, we shouldn't add anything random there.
// It is already randomized in the randomGetResult() method anyway. Also, we cannot add anything within the "get"
// object since this is where GetResult's metadata fields are rendered out and they would be parsed back as
// extra metadata fields.
// Prints out the parsed UpdateResponse object to verify that it is the same as the expected output.
// If random fields have been inserted, it checks that they have been filtered out and that they do
// not alter the final output of the parsed object.
/**
// We also want small number values (randomNonNegativeLong() tend to generate high numbers)
// in order to catch some conversion error that happen between int/long after parsing.
/*
//www.apache.org/licenses/LICENSE-2.0
//invalid filter, invalid json
// valid json , invalid filter
// For now just making sure that filter was stored with the alias
// alias at work
// alias at work again
//non valid filter, invalid json
//valid json but non valid filter
// Before 2.0 alias filters were parsed at alias creation time, in order
// for filters to work correctly ES required that fields mentioned in those
// filters exist in the mapping.
// From 2.0 and higher alias filters are parsed at request time and therefor
// fields mentioned in filters don't need to exist in the mapping.
/*
//www.apache.org/licenses/LICENSE-2.0
// cluster.read_only = null: write and metadata not blocked
// cluster.read_only = true: block write and metadata
// even if index has index.read_only = false
// cluster.read_only = false: removes the block
// newly created an index has no blocks
// adds index write and metadata block
// other indices not blocked
// blocks can be removed
// all is well
// all is well
/*
//www.apache.org/licenses/LICENSE-2.0
// nothing should happen since we are in non-production mode
// nothing should happen if the initial heap size or the max
// heap size is not available
// simulates OS X versus non-OS X
// nothing should happen if current file descriptor count is
// not available
// nothing should happen
// nothing should happen if current max number of threads is
// not available
// nothing should happen if max size virtual memory is not available
// nothing should happen if max file size is not available
// if system call filter is disabled, nothing should happen
// if system call filter is enabled, but we will not fork, nothing should
// happen
// if system call filter is enabled, and we might fork, the check should be enforced, regardless of bootstrap checks being enabled
// or not
// if not on an early-access build, nothing should happen
// if G1GC is disabled, nothing should happen
// if on or after update 40, nothing should happen independent of whether or not G1GC is enabled
// if not on an Oracle JVM, nothing should happen
// if not Java 8, nothing should happen
// if all permissions are not granted, nothing should happen
// not always enforced
// not enforced for non-zen2 discovery
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** 
/** 
// expected exception
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// initialize as if the max map count is under the limit, tests can override by setting maxMapCount before executing the check
/*
// nothing should happen if current vm.max_map_count is under the limit but mmap is not allowed
// nothing should happen if current vm.max_map_count exceeds the limit
// nothing should happen if current vm.max_map_count is not available
/*
//www.apache.org/licenses/LICENSE-2.0
// directory exists
// directory does not exist: create it
// regular file
/** can't execute processes */
/*
//www.apache.org/licenses/LICENSE-2.0
// check count
// test successful
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// list clusters in the datapath, ignoring anything from extrasfs
/*
//www.apache.org/licenses/LICENSE-2.0
// extra help not printed for usage errors
// extra help not printed for usage errors
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// verify exception is thrown, as well as other non failed sub-commands closed
// properly.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// client actions
// cluster admin actions
// indices admin actions
// TODO this is a really shitty way to test it, we need to figure out a way to test all the client methods
//      without specifying each one (reflection doesn't as each action needs its own special settings, without
//      them, request validation will fail before the test is executed. (one option is to enable disabling the
//      validation in the settings??? - ugly and conceptually wrong)
// choosing arbitrary top level actions to test
// choosing arbitrary cluster admin actions to test
// choosing arbitrary indices admin actions to test
// default header on TPC
// dear god, if we got more than 10 levels down, WTF? just bail
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// All of these should have the origin set
/*
//www.apache.org/licenses/LICENSE-2.0
// This mock will do nothing but verify that parentTaskId is set on all requests sent to it.
// All of these should have the parentTaskId set
// Now lets verify that unwrapped calls don't have the parentTaskId set
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// the initial indices which every cluster state test starts out with
/**
// should not be able to create a ClusterChangedEvent with null values for any of the constructor args
/**
/**
// serves as the base cluster state for the next iteration
/**
// test with all the various tombstone deletion quantities
/**
// test when its not the same IndexMetaData
// make sure the metadata is actually on the cluster state
// test when it doesn't exist
// test when its the same IndexMetaData
/**
// test when nodes have not been added or removed between cluster states
// test when nodes have been removed between cluster states
// test when nodes have been added between cluster states
// test when nodes both added and removed between cluster states
// here we reuse the newState from the previous run which already added extra nodes
/**
// routing tables and index routing tables are same object
// routing tables and index routing tables aren't same object
// index routing tables are different because they don't exist
/**
// no custom metadata present in any state
// next state has new custom metadata
// next state has same custom metadata
// next state has equivalent custom metadata
// next state removes custom metadata
// next state updates custom metadata
// next state adds new custom metadata type
// next state adds two custom metadata type
// next state removes two custom metadata type
// Create a basic cluster state with a given set of indices
// Create a non-initialized cluster state
// Create a modified cluster state from another one, but with some number of indices added and deleted.
// there is some change in metadata cluster state
// Create the discovery nodes for a cluster state.  For our testing purposes, we want
// the first to be master, the second to be master eligible, the third to be a data node,
// and the remainder can be any kinds of nodes (master eligible, data, or both).
// randomly assign the local node if not master
// the master node
// the alternate master node
// we need at least one data node
// remaining nodes can be anything (except for master)
// Create a new DiscoveryNode
// Create the metadata for a cluster state.
// Create the index metadata for a given index.
// Create the index metadata for a given index, with the specified version.
// Create the routing table for a cluster state.
// Create a list of indices to add
// Create a list of indices to delete from a list that already belongs to a particular cluster state.
// execute the indices changes test by generating random index additions and deletions and
// checking the values on the cluster changed event.
// add random # of indices to the next cluster state
// delete a random number of tombstones from cluster state (not zero and not all)
// delete none of the tombstones from cluster state
// delete all tombstones from cluster state
/*
//www.apache.org/licenses/LICENSE-2.0
// master should think it's green now.
// a very high time out, which should never fire due to the local flag
// at this point the original health response should not have returned: there was never a point where the index was green AND
// the master had processed all pending tasks above LANGUID priority.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Get the cluster info service on the master node
// manually control publishing
// get one healthy sample
// drop all outgoing stats requests to force a timeout.
// timeouts shouldn't clear the info
// node info will time out both on the request level on the count down latch. this means
// it is likely to update the node disk usage based on the one response that came be from local
// node.
// indices is guaranteed to time out on the latch, not updating anything.
// now we cause an exception
// check we recover
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// noop
// makes sure that the allocation deciders are setup in the correct order, such that the
// slower allocation deciders come last and we can exit early if there is a NO decision without
// running them. If the order of the deciders is changed for a valid reason, the order should be
// changed in the test too.
/*
//www.apache.org/licenses/LICENSE-2.0
// Update cluster state via full serialization from time to time
// Update cluster states using diffs
// Check non-diffable elements
// Check nodes
// Check routing table
// Check cluster blocks
// Check metadata
// JSON Serialization test - make sure that both states produce similar JSON
// Smoke test - we cannot compare bytes to bytes because some elements might get serialized in different order
// however, serialized size should remain the same
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// there must always be at least an empty graveyard
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// states with no master should never supersede anything
// states should never supersede states from another master
// state from the same master compare by version
/*
//www.apache.org/licenses/LICENSE-2.0
// Test that DiskUsage handles invalid numbers, as reported by some
// filesystems (ZFS & NTFS)
/*
//www.apache.org/licenses/LICENSE-2.0
// verify that we still see the local node in the cluster state
// make sure that all shards recovered before trying to flush
// flush for simpler debugging
// verify that both nodes are still in the cluster state but there is no master
// make sure that all shards recovered before trying to flush
// flush for simpler debugging
// spin here to wait till the state is set
// let major partition to elect new master, to ensure that old master is not elected once partition is restored,
// otherwise persistent setting (which is a part of accepted state on old master) will be propagated to other nodes
/*
//www.apache.org/licenses/LICENSE-2.0
// if the previous iteration was a disrupting one then there could still be some pending disconnections which would
// prevent us from asserting that all nodes are connected in this iteration without this call.
// sometimes do not wait for the disconnections to complete before starting the next connections
// simulate disconnects
// disable exceptions so things can be restored
// connect to one node
// connection attempts to node0 block indefinitely
// can still connect to another node without blocking
// can also disconnect from node0 without blocking
// however, now node0 is considered to be a new node so we will block on a subsequent attempt to connect to it
// once the connection is unblocked we successfully connect to it.
// if we disconnect from a node while blocked trying to connect to it then we do eventually disconnect from it
// assertBusy because the connection completes before disconnecting, so we might briefly observe a connection to node0
// use ensureConnections() to wait until the service is idle
// if we disconnect from a node while blocked trying to connect to it then the listener is notified
// completed even though the connection attempt is still blocked
/*
//www.apache.org/licenses/LICENSE-2.0
// we clean the metadata when loosing a master, therefore all operations on indices will auto create it, if allowed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// each field is about 10 bytes, assuming compression in place
// if the create index is ack'ed, then all nodes have successfully processed the cluster state
// wait for green state, so its both green, and there are no more pending events
// close one index
// expand_wildcards_closed should toggle return only closed index fuu
// ignore_unavailable set to true should not raise exception on fzzbzz
// empty wildcard expansion result should work when allowNoIndices is
// turned on
// empty wildcard expansion throws exception when allowNoIndices is turned off
// ignore_unavailable set to false throws exception when allowNoIndices is turned off
// ensure that the custom is injected into the cluster state
/*
//www.apache.org/licenses/LICENSE-2.0
// all is well
// still no shard should be allocated
// all is well
// now, start a node data, and see that it gets with shards
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// test more than one waiting shard in an index
// test exactly one waiting shard in an index
// test no waiting shards in an index
/*
//www.apache.org/licenses/LICENSE-2.0
// all is well, no master elected
// all is well, no master elected
// all is well, no master elected
// removing the master from the voting configuration immediately triggers the master to step down
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// increase the number of max permits to 2
// release all current permits
// reduce number of max permits to 1
// set back to 2
// take both permits and reduce max permits
// release one permit
// release second permit
// blocked
/*
//www.apache.org/licenses/LICENSE-2.0
// add shards from a non-existent index
// there should be as many task results as tasks
// every task should have a corresponding task result
// the task results are as expected
// the shard was successfully failed and so should not be in the routing table
// check we saw the expected failure
/*
//www.apache.org/licenses/LICENSE-2.0
// Existent shard id but different allocation id
// Non existent shard id
/*
//www.apache.org/licenses/LICENSE-2.0
// the request is a shard failed request
// for the right shard
// sent to the master
// one for master thread, one for the main thread
// assert a retry request was sent
// finish the request
// there failed to be a retry request
// release the driver thread to fail the test
/*
//www.apache.org/licenses/LICENSE-2.0
//no replicas will be allocated as both indices end up on a single node
// On slow machines the initial relocation might be delayed
// check that closed indices are effectively closed
// verify that we have all the primaries on node3
/*
//www.apache.org/licenses/LICENSE-2.0
// This might run slowly on older hardware
// make sure the data is there!
// don't wipe data directories the index needs to be there!
// verify again after cluster was shut down
// wait a bit for the cluster to realize that the shard is not there...
// TODO can we get around this? the cluster is RED, so what do we wait for?
// during a dry run, messages exist but are not logged or exposed
// so we get a NO decision back rather than an exception
// toggle is used to mve the shard from one node to another
// Rerouting shards is not blocked
// Rerouting shards is blocked when the cluster is read only
/*
//www.apache.org/licenses/LICENSE-2.0
// make sure we can recover all the nodes at once otherwise we might run into a state where
// one of the shards has not yet started relocating but we already fired up the request to wait for 0 relocating shards.
// The transient settings still exist in the state
/*
//www.apache.org/licenses/LICENSE-2.0
// create another index
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// should only bootstrap once
// should only bootstrap once
// should only bootstrap once
// should only bootstrap once
/*
//www.apache.org/licenses/LICENSE-2.0
// nodes from last-known cluster state could be in either order
/*
//www.apache.org/licenses/LICENSE-2.0
// Note: the explicit cast of the CopyFunction is needed for some IDE (specifically Eclipse 4.8.0) to infer the right type
// Note: the explicit cast of the CopyFunction is needed for some IDE (specifically Eclipse 4.8.0) to infer the right type
// remove random element
// change random element
// add random element
// Note: the explicit cast of the CopyFunction is needed for some IDE (specifically Eclipse 4.8.0) to infer the right type
/*
//www.apache.org/licenses/LICENSE-2.0
// generated deterministically for repeatable tests
// check that another join does not mess with lastPublishedVersion
// scenario when handling a publish request from a master that we already received a newer state from
// scenario when handling a publish request from a fresh master
// generate cluster UUID deterministically for repeatable tests
// generate cluster state UUID deterministically for repeatable tests
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// restart follower1 and follower2
// we still tolerate the loss of one more node here
// however we do not tolerate the loss of yet another one
// moreover we are still stuck even if two other nodes heal
// we require another node to heal to recover
// allow for a reconfiguration
// would not work if disconnect1 were removed from the configuration
// would not work if disconnect1 were removed from the configuration
// turn leader into candidate, which stabilisation asserts at the end
// disconnect is scheduled
// then wait for a new election
// wait for the removal to be committed
// then wait for the followup reconfiguration
// Each follower may have just sent a leader check, which receives no response
// then wait for the follower to check the leader
// then wait for the exception response
// then wait for a new election
// ALSO the leader may have just sent a follower check, which receives no response
// wait for the leader to check its followers
// then wait for the exception response
// FINALLY:
// wait for the removal to be committed
// then wait for the followup reconfiguration
// This stabilisation time bound is undesirably long. TODO try and reduce it.
// first wait for all the followers to notice the leader has gone
// then wait for a follower to be promoted to leader
// and the first publication times out because of the unresponsive node
// there might be a term bump causing another election
// then wait for both of:
// 1. the term bumping publication to time out
// 2. the new leader to notice that the old leader is unresponsive
// then wait for the new leader to commit a state without the old leader
// then wait for the followup reconfiguration
// ALSO wait for the leader to notice that its followers are unresponsive
// then wait for the leader to try and commit a state removing them, causing it to stand down
// to turn follower into candidate, which stabilisation asserts at the end
// disconnect is scheduled
// then wait for the followup reconfiguration
// the leader may have just sent a follower check, which receives no response
// wait for the leader to check the follower
// then wait for the exception response
// then wait for the removal to be committed
// then wait for the followup reconfiguration
// ALSO the follower may have just sent a leader check, which receives no response
// then wait for the follower to check the leader
// then wait for the exception response, causing the follower to become a candidate
// wait for the leader to notice that the follower is unresponsive
// then wait for the leader to commit a state without the follower
// then wait for the followup reconfiguration
// ALSO wait for the follower to notice the leader is unresponsive
// let followers elect a leader among themselves before healing the leader and running the publication
// disconnect is scheduled
// cluster has two nodes in mode LEADER, in different terms ofc, and the one in the lower term won’t be able to publish anything
// TODO: check if can find a better bound here
// TODO: needs proper term bumping
//        final Cluster cluster = new Cluster(3);
//        cluster.runRandomly();
//        cluster.stabilise();
//        final ClusterNode leader = cluster.getAnyLeader();
//        final ClusterNode follower0 = cluster.getAnyNodeExcept(leader);
//        final ClusterNode follower1 = cluster.getAnyNodeExcept(leader, follower0);
//
//        follower0.coordinator.joinLeaderInTerm(new StartJoinRequest(follower0.localNode, follower0.coordinator.getCurrentTerm() + 1));
//        AckCollector ackCollector = leader.submitValue(randomLong());
//        cluster.stabilise(DEFAULT_CLUSTER_STATE_UPDATE_DELAY);
//        assertTrue("expected ack from " + leader, ackCollector.hasAckedSuccessfully(leader));
//        assertTrue("expected nack from " + follower0, ackCollector.hasAckedUnsuccessfully(follower0));
//        assertTrue("expected ack from " + follower1, ackCollector.hasAckedSuccessfully(follower1));
// the first election should succeed, because only one node knows of the initial configuration and therefore can win a
// pre-voting round and proceed to an election, so there cannot be any collisions
// TODO this wait is unnecessary, we could trigger the election immediately
// Allow two round-trip for pre-voting and voting
// Then a commit of the new leader's first cluster state
// Then allow time for all the other nodes to join, each of which might cause a reconfiguration
// TODO Investigate whether 4 publications is sufficient due to batching? A bound linear in the number of nodes isn't great.
// This is VERY BAD: setting a _different_ initial configuration. Yet it works if the first attempt will never be a quorum.
// initial cluster state send when joining
// possible follow-up reconfiguration was published as a diff
/**
// TODO reboot the leader and verify that the same block is applied when it restarts
// check that if node join validation fails on master, the nodes can't join
// check that if node join validation fails on joining node, the nodes can't join
// fail join validation on a majority of nodes in the initial configuration
// time for the disconnected node to find the master again
// time for joining
// Then a commit of the updated cluster state
// to avoid closing it twice
// to wait for any in-flight check to time out
// to wait for the next check to be sent
// to send the failing check and receive the disconnection response
// drop the publication messages to one node, but then restore connectivity so it remains in the cluster and does not fail
// health checks
// in the 2-node case, auto-shrinking the voting configuration is required to reduce the voting configuration down to just
// the leader, otherwise restarting the other master-eligible node triggers an election
// 1st delay for the setting update, 2nd for the reconfiguration
/*
//www.apache.org/licenses/LICENSE-2.0
// Check grace period
// Check upper bound
// Check upper bound
// Run until we get a delay close to the maximum to show that backing off does work
// do it again to show that the max is reset when the scheduler is restarted
// doesn't throw an IAE
/*
//www.apache.org/licenses/LICENSE-2.0
// passes just enough checks to keep it alive, up to maxRecoveries, and then fails completely
// other nodes are ok
// add another node and see that it schedules checks for this new node but keeps on considering the old one faulty
// remove the faulty node and see that it is removed
// remove the working node and see that everything eventually stops
// add back the faulty node afresh and see that it fails again
// Note: the explicit cast of the CopyFunction is needed for some IDE (specifically Eclipse 4.8.0) to infer the right type
// Does not call into the coordinator in the normal case
// Does not call into the coordinator for a term that's too low, just rejects immediately
// Calls into the coordinator if the term needs bumping
// Calls into the coordinator if not a follower
// If it calls into the coordinator and the coordinator throws an exception then it's passed back to the caller
/*
//www.apache.org/licenses/LICENSE-2.0
// check that sending a join to node1 works
// check that sending a join to node2 works
// check that sending another join to node1 is a noop as the previous join is still in progress
// complete the previous join to node1
// check that sending another join to node1 now works again
// check that sending another join to node2 works if the optionalJoin is different
// complete all the joins and check that isJoinPending is updated
// registers request handler
/*
//www.apache.org/licenses/LICENSE-2.0
// latest V6 released version
// we have to stick with the same major
/*
//www.apache.org/licenses/LICENSE-2.0
// clearing tracked nodes cancels earlier lag detector ...
// ... but later lag detectors still work
// removing a node from the tracked set means it is not tracked
// nodes added after a lag detector was started are also ignored
/*
//www.apache.org/licenses/LICENSE-2.0
// needed because a successful check response might be in flight at the time of failure
// need to connect first for disconnect to have any effect
//noinspection RedundantCast since it is needed for some IDEs (specifically Eclipse 4.8.0) to infer the right type
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// 0: acquire lock
// 0: lock acquisition succeeded
// 1: acquire lock
// 0: lock acquisition failed
// 0: acquire lock
// 1: acquire lock
// 0: lock acquisition failed
// 0: lock acquisition succeeded
// 0: acquire lock
// 1: acquire lock
// 0: lock acquisition succeeded
// 0: lock acquisition failed
// 0: acquire lock
// 0: lock acquisition failed
// 1: acquire lock
// 0: lock acquisition succeeded
/**
// 0: invoke write 42
// 1: invoke read
// 2: invoke read
// 2: read returns 0
// 1: read returns 42
// 0: write returns
// 0: invoke write 42
// 1: invoke read
// 1: read returns 42
// 2: invoke read
// 2: read returns 0, not allowed
// 0: write returns
// 0: invoke write 42
// 1: invoke write 43
// 2: invoke read
// 1: read returns 42
// 3: invoke read
// 3: read returns 43
// 4: invoke read
// 4: read returns 43
// 0: write returns
// 1: write returns
// 0: invoke write 42
// 1: invoke write 43
// 2: invoke read
// 1: read returns 42
// 3: invoke read
// 3: read returns 43
// 4: invoke read
// 4: read returns 42, not allowed
// 0: write returns
// 1: write returns
// 0: invoke write 42 on key x
// 1: invoke read on key x
// 0: invoke write 42 on key y
// 1: invoke read on key y
// 2: invoke read on key x
// 2: invoke read on key y
// 2: read returns 0 on key x
// 2: read returns 0 on key y
// 1: read returns 42 on key y
// 1: read returns 42 on key x
// 0: write returns on key x
// 0: write returns on key y
// 0: invoke write 42 on key x
// 1: invoke read on key x
// 0: invoke write 42 on key y
// 1: invoke read on key y
// 2: invoke read on key x
// 1: read returns 42 on key y
// 2: invoke read on key y
// 2: read returns 0 on key x
// 2: read returns 0 on key y, not allowed
// 1: read returns 42 on key x
// 0: write returns on key x
// 0: write returns on key y
/*
//www.apache.org/licenses/LICENSE-2.0
// Note: the explicit cast of the CopyFunction is needed for some IDE (specifically Eclipse 4.8.0) to infer the right type
// change sourceNode
// change targetNode
// change term
// change last accepted term
// change version
// Note: the explicit cast of the CopyFunction is needed for some IDE (specifically Eclipse 4.8.0) to infer the right type
// change term
// change version
// Note: the explicit cast of the CopyFunction is needed for some IDE (specifically Eclipse 4.8.0) to infer the right type
// change publish response
// change optional join
// Note: the explicit cast of the CopyFunction is needed for some IDE (specifically Eclipse 4.8.0) to infer the right type
// change sourceNode
// change term
// Note: the explicit cast of the CopyFunction is needed for some IDE (specifically Eclipse 4.8.0) to infer the right type
// change sourceNode
// change term
// change version
// Note: the explicit cast of the CopyFunction is needed for some IDE (specifically Eclipse 4.8.0) to infer the right type
// change OptionalJoin
// Note: the explicit cast of the CopyFunction is needed for some IDE (specifically Eclipse 4.8.0) to infer the right type
// Note: the explicit cast of the CopyFunction is needed for some IDE (specifically Eclipse 4.8.0) to infer the right type
/*
//www.apache.org/licenses/LICENSE-2.0
// clone the node before submitting to simulate an incoming join, which is guaranteed to have a new
// disco node object serialized off the network
// we need at least a quorum of voting nodes with a correct term and worse state
// a correct request
// term too low
// better state
// duplicate some requests, which will be unsuccessful
// ignore - these requests are expected to fail
/*
//www.apache.org/licenses/LICENSE-2.0
// to ensure that there is at least one removal
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO need tests that check that the max term seen is updated
// ok to reject some joins.
// This is a _rare_ case where our leader has detected a failure and stepped down, but we are still a follower. It's possible that
// the leader lost its quorum, but while we're still a follower we will not offer joins to any other node so there is no major
// drawback in offering a join to our old leader. The advantage of this is that it makes it slightly more likely that the leader
// won't change, and also that its re-election will happen more quickly than if it had to wait for a quorum of followers to also
// detect its failure.
/*
//www.apache.org/licenses/LICENSE-2.0
// number of publish actions + initial faulty nodes injection
// has no influence
// we must fail node before committing for the node, otherwise failing the node is ignored
// we need to complete publication by failing the node
// has no influence
// check that acking still works after publication completed
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// close to have some unassigned started shards shards..
// inject a node
// open index
// remove the extra node
// Wait for no publication in progress to not accidentally cancel a publication different from the one triggered by the given
// request.
// block none master node.
// due to publish_timeout of 0, wait for data node to have cluster state fully applied
// Here we want to test that things go well if there is a first request
// that adds mappings but before mappings are propagated to all nodes
// another index request introduces the same mapping. The master node
// will reply immediately since it did not change the cluster state
// but the change might not be on the node that performed the indexing
// operation yet
// Don't allocate the shard on the master node
// Check routing tables
// primary must not be on the master node
// only primaries
// Block cluster state processing where our shard is
// Add a new mapping...
// ...and wait for mappings to be available on master
// this request does not change the cluster state, because mapping is already created,
// we don't await and cancel committed publication
// Wait a bit to make sure that the reason why we did not get a response
// is that cluster state processing is blocked and not just that it takes
// time to process the indexing request
// Now make sure the indexing request finishes successfully
// This is essentially the same thing as testDelayedMappingPropagationOnPrimary
// but for replicas
// Here we want to test that everything goes well if the mappings that
// are needed for a document are not available on the replica at the
// time of indexing it
// Force allocation of the primary on the master node by first only allocating on the master
// and then allowing all nodes so that the replica gets allocated on the other node
// Check routing tables
// primary must be on the master
// Block cluster state processing on the replica
// Wait for mappings to be available on master
// index another document, this time using dynamic mappings.
// The ack timeout of 0 on dynamic mapping updates makes it possible for the document to be indexed on the primary, even
// if the dynamic mapping update is not applied on the replica yet.
// this request does not change the cluster state, because the mapping is dynamic,
// we need to await and cancel committed publication
// ...and wait for second mapping to be available on master
// The mappings have not been propagated to the replica yet as a consequence the document count not be indexed
// We wait on purpose to make sure that the document is not indexed because the shard operation is stalled
// and not just because it takes time to replicate the indexing request to the replica
// Now make sure the indexing request finishes successfully
// both shards should have succeeded
/*
//www.apache.org/licenses/LICENSE-2.0
// Retiring a single node shifts the votes elsewhere if possible.
// Retiring a node from a three-node cluster drops down to a one-node configuration
// 7 nodes, one for each combination of live/retired/current. Ideally we want the config to be the non-retired live nodes.
// Since there are 2 non-retired live nodes we round down to 1 and just use the one that's already in the config.
// Only two non-retired nodes in the config, so new config does not shrink below 2
// The config has at least three non-retired nodes so does not shrink below 3
// Three non-retired nodes in the config, so new config does not shrink below 3
// default is "true"
// update to "false"
// no quorum
// explicitly set to "true"
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// to ensure quick node startup
// node ordinal 0
// node ordinal 1
// node ordinals 2 and 3
// node ordinal 0
// node ordinal 1
// give the cluster 2 seconds to elect the master (it should not)
/*
//www.apache.org/licenses/LICENSE-2.0
// a 4-node cluster settles on a 3-node configuration; we then prevent the nodes in the configuration from winning an election
// by failing at the pre-voting stage, so that the extra node must be elected instead when the master shuts down. This extra node
// should then add itself into the voting configuration.
/*
//www.apache.org/licenses/LICENSE-2.0
// ensures that all events are processed (in particular state recovery fully completed)
// see https://github.com/elastic/elasticsearch/issues/24388
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Ignore all paths which looks like "RANDOMINDEXNAME.shards"
/*
//www.apache.org/licenses/LICENSE-2.0
//don't inject random fields at the root, which contains arbitrary shard ids
/*
//www.apache.org/licenses/LICENSE-2.0
// make sure cluster health is always YELLOW, up until the last state where it should be GREEN
// make sure cluster health is YELLOW up until the final cluster state, which contains primary shard
// failed allocations that should make the cluster health RED
// make sure cluster health is YELLOW up until the final cluster state, when it turns GREEN
// make sure cluster health is YELLOW up until the final cluster state, which contains primary shard
// failed allocations that should make the cluster health RED
// because there were previous allocation ids, we should be RED until the primaries are started,
// then move to YELLOW, and the last state should be GREEN when all shards have been started
// if the inactive primaries are due solely to recovery (not failed allocation or previously being allocated),
// then cluster health is YELLOW, otherwise RED
// if the inactive primaries are due solely to recovery (not failed allocation or previously being allocated)
// then cluster health is YELLOW, otherwise RED
// initial index creation and new routing table info
// initial index creation and new routing table info
// generate random node ids
// initialize primaries
// some primaries started
// some primaries failed to allocate
// all primaries started
// initialize replicas
// give the replica a different node id than the primary
// some replicas started
// all replicas started
// all of the replicas may have moved to started in the previous phase already
// returns true if the inactive primaries in the index are only due to cluster recovery
// (not because of allocation of existing shard or previously having allocation ids assigned)
/*
//www.apache.org/licenses/LICENSE-2.0
// do not add elements at the top-level as any element at this level is parsed as a new alias
// do not insert random data into AliasMetaData#filter
/*
//www.apache.org/licenses/LICENSE-2.0
// Doesn't throw an exception because we allow upper case alias names
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// local node is the master
// simulate node removal
// fake an election where conflicting nodes are removed and readded
// local node is the master
// local node is the master
// use allocation filtering
// check that presence of old node means that auto-expansion does not take allocation filtering into account
// remove old node and check that auto-expansion takes allocation filtering into account
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// rounding to next day 00:00
// rounding to today 00:00
/*
//www.apache.org/licenses/LICENSE-2.0
// Occasionally have an empty map
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// check that date properly printed
// the beginning of the parser
// try with max tombstones as some positive integer
// try with max tombstones as the default
/*
//www.apache.org/licenses/LICENSE-2.0
// matching version
// intentionally not using the constant, so upgrading requires you to look at this test
// where you have to update this part and the next one
// no setting configured
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Only closed
// no wildcards, so wildcard expansion don't apply
// Only open
// Open and closed
//ignore unavailable and allow no indices
//ignore unavailable but don't allow no indices
//unavailable indices are ignored but no indices are disallowed
//error on unavailable but allow no indices
//unavailable indices are not ignored, hence the error on the first unavailable indices encountered
//error on both unavailable and no indices
//error on both unavailable and no indices + every alias needs to expand to a single index
// when ignoreAliases option is set, concreteIndexNames resolves the provided expressions
// only against the defined indices
// when ignoreAliases option is not set, concreteIndexNames resolves the provided
// expressions against the defined indices and aliases
/**
// with no indices, asking for all indices should return empty list or exception, depending on indices options
// with existing indices, asking for all indices should return all open/closed indices depending on options
/**
// asking for non existing wildcard pattern should return empty list or exception
//even though it does identify all indices, it's not a pattern but just an explicit list of them
// testing an alias pointing to three indices:
// expected
// concrete index supersedes filtering alias
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// create one that won't fail
// now we start the shard
// now we start the shard
// analysis settings from the request are not overwritten
// similarity settings from the request are not overwritten
// now we start the shard
// used as a value container, not for the concurrency and visibility guarantees
/*
//www.apache.org/licenses/LICENSE-2.0
// Create an unassigned index
// Mock the built reroute
// Remove it
// It is gone
// Make sure we actually attempted to reroute
/*
//www.apache.org/licenses/LICENSE-2.0
// Mock any deletes so we don't need to worry about how MetaDataDeleteIndexService does its job
// We only think about metadata for this test. Not routing or any other fun stuff.
// Create a state with a single index
// Add an alias to it
// Remove the alias from it while adding another one
// Now just remove on its own
// now add some aliases randomly
// add an alias to the index multiple times
// add some aliases to the index
// now perform a remove and add for each alias which is idempotent, the resulting aliases are unchanged
// Create "test" and "test_2"
// Now remove "test" and add an alias to "test" to "test_2" in one go
// Create "test"
// Attempt to add an alias to "test" at the same time as we remove it
// Create "test"
// Try to remove an index twice. This should just remove the index once....
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// double archive?
// the index does not need to be upgraded, but checking that it does should archive any broken settings
// no double upgrade
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO - it will be nice to get a random mapping generator
// the task completed successfully
// the task really was a mapping update
// since we never committed the cluster state update, the in-memory state is unchanged
/*
//www.apache.org/licenses/LICENSE-2.0
// replacing with empty aliases behaves as if aliases were unspecified at request building
// when only one index/alias pair exist
// when alias points to two indices, but valid
// one of the following combinations: [(null, null), (null, true), (null, false), (false, false)]
// when too many write indices
// no alias, no index
// index, no alias
// alias with no index routing
// alias with index routing.
// alias with invalid index routing.
// alias with multiple indices
// no alias, no index
// index, no alias
// alias with no index routing
// alias with index routing.
// alias with invalid index routing.
// alias with no write index
// aliases with multiple indices
// verify that new write index is used
/*
//www.apache.org/licenses/LICENSE-2.0
// This setting is used to simulate cluster state updates
// Change some templates
// Wait for the templates to be updated back to normal
// the updates only happen on cluster state updates, so we need to make sure that the cluster state updates are happening
// so we need to simulate updates to make sure the template upgrade kicks in
// Wipe out all templates
// Make sure all templates are recreated correctly
// the updates only happen on cluster state updates, so we need to make sure that the cluster state updates are happening
// so we need to simulate updates to make sure the template upgrade kicks in
/*
//www.apache.org/licenses/LICENSE-2.0
// +2 to skip tryFinishUpgrade
// tryFinishUpgrade was skipped
// Make sure that update wasn't invoked since we are still running
// 3 upgrades should be completed, in addition to the final calculate
// Make sure that update was called this time since we are no longer running
// Make sure that update wasn't called this time since the index template metadata didn't change
/*
//www.apache.org/licenses/LICENSE-2.0
// templates
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// issue #13334
// when ignoreAliases option is not set, WildcardExpressionResolver resolves the provided
// expressions against the defined indices and aliases
// ignoreAliases option is set, WildcardExpressionResolver throws error when
// ignoreAliases option is set, WildcardExpressionResolver resolves the provided expressions only against the defined indices
// when ignoreAliases option is not set, WildcardExpressionResolver resolves the provided
// expressions against the defined indices and aliases
// ignoreAliases option is set, WildcardExpressionResolver resolves the provided expressions
// only against the defined indices
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// change an attribute
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// initial set up
// initial set up is done
// index more docs to node2 that marks node1 as stale
// create fake corrupted marker on node1
// thanks to master node1 is out of sync
// there is only _stale_ primary
// allocate stale primary
// allocation fails due to corruption marker
// index is red: no any shard is allocated (allocation id is a fake id that does not match to anything)
// index is still red due to mismatch of allocation id
// no any valid shard is there; have to invoke AllocateStalePrimary again
// bring node2 back
// index some docs in several segments
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// notify test that we are blocked
// wait to be unblocked by test
// wait for master thread to be blocked
// only called once
// ensure at least one URGENT priority reroute
// this task might be submitted later
// else this task might be submitted too late to precede the reroute
// may run either before or after reroute
// allow master thread to continue;
// wait for reroute to complete
// see above for assertion that it's only called once
// i.e. it doesn't leak any listeners
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// this will use the same data location as the stopped node
/**
// do a second round with longer delay to make sure it happens
/**
/**
// we want to test both full divergent copies of the shard in terms of segments, and
// a case where they are the same (using sync flush), index Random does all this goodness
// already
/*
//www.apache.org/licenses/LICENSE-2.0
// starting primaries
// starting replicas
// remove node2 and reroute
// starting primaries
// starting replicas
// we need to find the node with the replica otherwise we will not reroute
// remove node that has replica and reroute
// make sure the replica is marked as delayed (i.e. not reallocated)
// mock ClusterService.submitStateUpdateTask() method
// check that delayed reroute task was created and registered with the proper settings
// check that submitStateUpdateTask() was invoked on the cluster service mock
// advance the time on the allocation service to a timestamp that happened after the delayed scheduling
// apply cluster state
// check that shard is not delayed anymore
// check that task is now removed
// simulate calling listener (cluster change event)
// check that no new task is scheduled
// check that no further cluster state update was submitted
/**
// allocate shards
// start primaries
// start replicas
// find replica of short_delay
// find replica of long_delay
// remove node of shortDelayReplica and node of longDelayReplica and reroute
// make sure both replicas are marked as delayed (i.e. not reallocated)
// mock ClusterService.submitStateUpdateTask() method
// check that delayed reroute task was created and registered with the proper settings
// check that submitStateUpdateTask() was invoked on the cluster service mock
// advance the time on the allocation service to a timestamp that happened after the delayed scheduling
// apply cluster state
// check that shard is not delayed anymore
// check that task is now removed
// mock ClusterService.submitStateUpdateTask() method again
// simulate calling listener (cluster change event)
// check that new delayed reroute task was created and registered with the proper settings
// check that submitStateUpdateTask() was invoked on the cluster service mock
// advance the time on the allocation service to a timestamp that happened after the delayed scheduling
// apply cluster state
// check that shard is not delayed anymore
// check that task is now removed
// simulate calling listener (cluster change event)
// check that no new task is scheduled
// check that no further cluster state update was submitted
// starting primaries
// starting replicas
// remove node that has replica and reroute
// make sure the replica is marked as delayed (i.e. not reallocated)
// check that delayed reroute task was created and registered with the proper settings
// update settings with shorter delay
// node leaves with replica shard of index bar that has shorter delay
// remove node that has replica and reroute
// check that delayed reroute task was replaced by shorter reroute task
// existing task was cancelled
// do not check this in the unit tests
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// make sure the same routing value always has each _id fall within the configured partition size
/**
// the preferred shards should be at the front of the list
// verify all the shards are there
// Ensure that a user session is re-routed back to same nodes for
// subsequent searches and that the nodes are selected fairly i.e.
// given identically sorted lists of nodes across all shard IDs
// each shard ID doesn't pick the same node.
// 2 is the bare minimum number of nodes we can reliably expect from
// randomized tests in my experiments over thousands of iterations.
// Ideally we would test for greater levels of machine utilisation
// given a configuration with many nodes but the nature of hash
// collisions means we can't always rely on optimal node usage in
// all cases.
// Regression test for the routing logic - implements same hashing logic
// Test that the shards use a round-robin pattern when there are no stats
// All three shards should have been separate, because there are no stats yet so they're all ranked equally.
// Now let's start adding node metrics, since that will affect which node is chosen
// node 1 should be the lowest ranked node to start
// node 1 starts getting more loaded...
// and more loaded...
// and even more
// finally, node 2 is chosen instead
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// disruption tests need MockTransportService
// testForceStaleReplicaToBePromotedToPrimary replies on the flushing when a shard is no longer assigned.
// returns data paths settings of in-sync shard copy
// kick reroute and wait for all shard states to be fetched
// kick reroute a second time and check that all shards are unassigned
// if true, use stale replica, otherwise a completely empty copy
// force allocation based on node id
// When invoking AllocateEmptyPrimaryAllocationCommand, due to the UnassignedInfo.Reason being changed to INDEX_CREATION,
// its possible that the shard has not completed initialization, even though the cluster health is yellow, so the
// search can throw an "all shards failed" exception.  We will wait until the shard initialization has completed before
// verifying the search hit count.
// allocation id of old primary was cleaned from the in-sync set
// Ensure the stopped primary's data is deleted so that it doesn't get picked up by the next datanode we start
/**
/**
// Make gap in seqno.
// Checks that we fails replicas in one side but not mark them as stale.
/*
//www.apache.org/licenses/LICENSE-2.0
// don't limit recoveries
/**
// the primary failure should increment the primary term;
// now start all replicas too
// relocations shouldn't change much
// primary promotion
// stablize cluster
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// check exhaustiveness
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// don't limit recoveries
/**
// expected
// now start all replicas too
// expected
// Ok the first time
// test no validation errors
// test wrong number of shards causes validation errors
// test wrong number of replicas causes validation errors
// test wrong number of shards and replicas causes validation errors
/** reverse engineer the in sync aid based on the given indexRoutingTable **/
// simulate a primary was initialized based on aid
/*
//www.apache.org/licenses/LICENSE-2.0
// test identity
// test same allocation different state
// test unassigned is false even to itself
// test different shards/nodes/state
// test true scenarios
// test two shards are not mixed
// test two allocations are not mixed
// test different shard states
// change index
// change shard id
// change current node
// change relocating node
// change recovery source (only works for inactive primaries)
// change primary flag
// change state
// change unassigned info
/*
//www.apache.org/licenses/LICENSE-2.0
// starting primaries
/**
/**
// starting primaries
// starting replicas
// remove node2 and reroute
// verify that NODE_LEAVE is the reason for meta
/**
// starting primaries
// starting replicas
// fail shard
// verify the reason and details
/**
// starting primaries
// starting replicas
// remove node2 and reroute
// make sure both replicas are marked as delayed (i.e. not reallocated)
// starting primaries
// starting replicas
// remove node2 and reroute
/*
//www.apache.org/licenses/LICENSE-2.0
// move initializing to started
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// node1 should be sorted first b/c of better weight ranking
// test bad values
// node2 should be sorted first b/c a THROTTLE is higher than a NO decision
// node1 should be sorted first b/c YES decisions are the highest
// yes decisions are not precomputed and cached
// same fields for the ShardAllocationDecision, but should be different instances
// node2 should have the highest sort order
/*
//www.apache.org/licenses/LICENSE-2.0
// shard routing is added as "from recovery" instead of "new index creation" so that we can test below that allocating an empty
// primary with accept_data_loss flag set to false fails
// shard routing is added as "from recovery" instead of "new index creation" so that we can test below that allocating an empty
// primary with accept_data_loss flag set to false fails
// mark all shards as stale
// Since the commands are named writeable we need to register them and wrap the input stream
// Now we can read them!
// move two tokens, parser expected to be "on" `commands` field
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/elastic/elasticsearch/issues/13249 for details
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Unassigned shard is expected.
// Cancel all initializing shards and move started primary to another node.
// +1 for relocating shard.
// Still 1 unassigned.
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO maybe we can randomize these numbers somehow
/* Tests balance over indices only */
/* Tests balance over replicas only */
// move initializing to started
// Ensure that if there any unassigned shards, all of their replicas are unassigned as well
// (i.e. unassigned count is always [replicas] + 1 for each shard unassigned shardId)
/*
// triggered solely by the primary overload on node [1] where a shard
// is rebalanced to node 0
// we have to allocate primaries first
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ridiculously high threshold setting so we won't rebalance
// Create a cluster state with 2 indices, each with 1 started primary shard, and only
// one node initially so that all primary shards get allocated to the same node.  We are only
// using 2 indices (i.e. 2 total primary shards) because if we have any more than 2 started shards
// in the routing table, then we have no guarantees about the order in which the 3 or more shards
// are selected to be rebalanced to the new node, and hence the node to which they are rebalanced
// is not deterministic.  Using only two shards guarantees that only one of those two shards will
// be rebalanced, and so we pick the one that was chosen to be rebalanced and execute the single-shard
// rebalance step on it to make sure it gets assigned to the same node.
// add new nodes so one of the primaries can be rebalanced
// randomly select a subset of the newly added nodes to set filter allocation on (but not all)
// allocate and get the node that is now relocating
// make sure all excluded nodes returned a NO decision
// only one shard, so moving it will not create a better balance anywhere, so all node decisions should
// return the same ranking as the current node
// start off with one node and several shards assigned to that node, then add a few nodes to the cluster,
// each of these new nodes should have a better ranking than the current, given a low enough threshold
// start off with 3 nodes and 7 shards, so that one of the 3 nodes will have 3 shards assigned, the remaining 2
// nodes will have 2 shard each.  then, add another node.  pick a shard on one of the nodes that has only 2 shard
// to rebalance.  the new node should have the best ranking (because it has no shards), followed by the node currently
// holding the shard as well as the other node with only 2 shards (they should have the same ranking), followed by the
// node with 3 shards which will have the lowest ranking.
// should only have one of these
// highest ranked node should not be any of the initial nodes
// worst ranked should be the node with two shards
// add a new node so shards can be rebalanced there
// make sure the previous node id is the same as the current one after rerouting
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we have 10 shards and 4 nodes so 2 nodes have 3 shards and 2 nodes have 2 shards
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// regexp FTW
/*
//www.apache.org/licenses/LICENSE-2.0
//            assertThat(clusterState.routingTable().index("test1").shard(i).primaryShard().state(), equalTo(STARTED));
// we use a second index here (test1) that never gets assigned otherwise allocateUnassigned
// is never called if we don't have unassigned shards.
/*
//www.apache.org/licenses/LICENSE-2.0
// we only allow one relocation at a time
// we only allow one relocation at a time
// we only allow one relocation at a time
// we only allow one relocation at a time
/*
//www.apache.org/licenses/LICENSE-2.0
// starting primaries
// starting replicas
// starting primaries
// starting replicas
// starting primaries
// starting replicas
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if deciders say NO to allocating a primary shard, then the cluster health should be RED
// the only allocation decider that implements this is ShardsLimitAllocationDecider and it always
// returns only YES or NO, never THROTTLE
// if deciders THROTTLE allocating a primary shard, stay in YELLOW state
// if deciders say YES to allocating primary shards, stay in YELLOW state
// make sure primaries are initialized
// we need at least as many nodes as shards for the THROTTLE case, because
// once a shard has been throttled on a node, that node no longer accepts
// any allocations on it
/*
//www.apache.org/licenses/LICENSE-2.0
// now we mark one index as read-only and assert that we don't mark it as such again
// should not reroute when all disks are ok
// should reroute when one disk goes over the watermark
// should not re-route again within the reroute interval
// should reroute again when one disk is still over the watermark
// should not re-route again before reroute has completed
// complete reroute
// should not re-route again within the reroute interval
// should reroute again after the reroute interval
// should not reroute again when it is not required
// Change cluster state so that "test_2" index is blocked (read only)
// When free disk on any of node1 or node2 goes below 5% flood watermark, then apply index block on indices not having the block
// When free disk on node1 and node2 goes above 10% high watermark, then only release index block
// When no usage information is present for node2, we don't release the block
// When disk usage on one node is between the high and flood-stage watermarks, nothing changes
// When disk usage on one node is missing and the other is below the high watermark, nothing changes
// When disk usage on one node is missing and the other is above the flood-stage watermark, affected indices are blocked
// will do one reroute and emit warnings, but subsequent reroutes and associated messages are delayed
// advance time long enough to do another reroute
// will do one reroute and emit warnings, but subsequent reroutes and associated messages are delayed
// only log about dropping below the low disk watermark on a reroute
/*
//www.apache.org/licenses/LICENSE-2.0
// this has the effect of registering the settings updater
// this has the effect of registering the settings updater
// this has the effect of registering the settings updater
// this has the effect of registering the settings updater
// this has the effect of registering the settings updater
// this has the effect of registering the settings updater
// this has the effect of registering the settings updater
/*
//www.apache.org/licenses/LICENSE-2.0
// we might have primary relocating, and the test is only for replicas, so only test in the case of replica allocation
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// randomly add nodes of mixed versions
// always reroute after adding node
// Log the node versions (for debugging if necessary)
// randomly create some indices
// Pick a random subset of primaries to fail
// local node is the master
// at least two nodes that have the data role so that we can allocate shards
/*
//www.apache.org/licenses/LICENSE-2.0
// starting primaries
// starting replicas
// check promotion of replica to primary
// make sure the failedShard is not INITIALIZING again on node3
// add 4 nodes
// start primary shards
// start one replica so it can take over.
// fail the primary shard, check replicas get removed as well...
// the primary gets allocated on another node, replicas are initializing
// add 4 nodes
// start primary shards
// start another replica shard, while keep one initializing
// fail the primary shard, check one replica gets elected to primary, others become INITIALIZING (from it)
// add a single node
// start primary shard
// add another 5.6 node
// start the shards, should have 1 primary and 1 replica available
// start all the replicas
// fail the primary shard again and make sure the correct replica is promoted
// the primary gets allocated on another node
// Skip the node that the primary was on, it doesn't have a replica so doesn't need a version check
// fail the primary shard again, and ensure the same thing happens
// the primary gets allocated on another node
// Skip the node that the primary was on, it doesn't have a replica so doesn't need a version check
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// backup shards are initializing as well, we make sure that they
// recover from primary *started* shards in the
// IndicesClusterStateService
// backup shards are initializing as well, we make sure that they
// recover from primary *started* shards in the
// IndicesClusterStateService
// backup shards are initializing as well, we make sure that they
// recover from primary *started* shards in the
// IndicesClusterStateService
// backup shards are initializing as well, we make sure that they
// recover from primary *started* shards in the
// IndicesClusterStateService
/*
//www.apache.org/licenses/LICENSE-2.0
// add index metadata where we have no routing nodes to check that allocation ids are not removed
// in-sync allocation ids should not be updated
// in-sync allocation ids should not be updated
// force empty primary
// check that in-sync allocation ids are reset by forcing an empty primary
/**
/**
// resend shard failures to check if they are ignored
/**
// in-sync allocation ids should not be updated
// check that inSyncAllocationIds can not grow without bounds
// in-sync allocation set is bounded
// only allocation id of replica was changed
/**
// in-sync allocation ids should not be updated
// in-sync allocation ids should not be updated
// in-sync allocation ids should not be updated
// in-sync allocation ids should not be updated
/**
// in-sync allocation ids should not be updated
// in-sync allocation ids should not be updated
/*
//www.apache.org/licenses/LICENSE-2.0
// now fail it N-1 times
// now we go and check that we are actually stick to unassigned on the next failure
// manual resetting of retry count
// again fail it N-1 times
// now we go and check that we are actually stick to unassigned on the next failure
// now fail it N-1 times
// MaxRetryAllocationDecider#canForceAllocatePrimary should return YES decisions because canAllocate returns YES here
// now we go and check that we are actually stick to unassigned on the next failure
// MaxRetryAllocationDecider#canForceAllocatePrimary should return a NO decision because canAllocate returns NO here
// change the settings and ensure we can do another round of allocation for that index.
// good we are initializing and we are maintaining failure information
// bumped up the max retry count, so canForceAllocatePrimary should return a YES decision
// now we start the shard
// all counters have been reset to 0 ie. no unassigned info
// now fail again and see if it has a new counter
// Counter reset, so MaxRetryAllocationDecider#canForceAllocatePrimary should return a YES decision
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// cached stay decision
// not in explain mode, so should use cached decision
// cached cannot move decision
// final decision is YES, so shouldn't use cached decision
// final decision is NO, but in explain mode, so shouldn't use cached decision
// both nodes have the same decision type but node2 has a higher weight ranking, so node2 comes first
// node2 should have the highest sort order
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// the two indices must stay as is, the replicas cannot move to oldNode2 because versions don't match
// Make sure that primary shards are only allocated on the new node
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// verify where the primary is
// now, fail one node, while the replica is initializing, and it also holds a primary
// 2 replicas and one primary
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/* This test will make random allocation decision on a growing and shrinking
// make sure there is an active replica to prevent from going red
// we stop after 200 iterations if it didn't stabelize by then something is likely to be wrong
/*
//www.apache.org/licenses/LICENSE-2.0
// we only allow one relocation at a time
// we now only relocate 3, since 2 remain where they are!
// make sure we have an even relocation
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we don't handle shrink yet
/*
//www.apache.org/licenses/LICENSE-2.0
// Exhaust all replica allocation attempts with shard failures
// Now allocate replica with retry_failed flag set
/*
//www.apache.org/licenses/LICENSE-2.0
// all shards are unassigned. so no inactive shards or primaries.
// replica got promoted to primary
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// can't force allocate same shard copy to the same node
// can force allocate to a different node
/*
//www.apache.org/licenses/LICENSE-2.0
// Bump the cluster total shards to 2
// the first move will destroy the balance and the balancer will move 2 shards from node2 to node one right after
// moving the nodes to node2 since we consider INITIALIZING nodes during rebalance
// now we are done compared to EvenShardCountAllocator since the Balancer is not soely based on the average
/*
//www.apache.org/licenses/LICENSE-2.0
// we can't use ensureYellow since that one is just as happy with a GREEN status.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// make sure we still have 2 shards initializing per node on the first 25 nodes
// make sure we still have 2 shards initializing per node on the only 25 nodes
// check that we don't have a shard associated with a node with the same index name (we have a single shard)
// make sure we still have 2 shards either relocating or started on the first 25 nodes (still)
//        routingTable = strategy.reroute(new RoutingStrategyInfo(metaData, routingTable), nodes);
/*
//www.apache.org/licenses/LICENSE-2.0
// backup shards are initializing as well, we make sure that they recover
// from primary *started* shards in the IndicesClusterStateService
// backup shards are initializing as well, we make sure that they
// recover from primary *started* shards in the IndicesClusterStateService
// backup shards are initializing as well, we make sure that they
// recover from primary *started* shards in the IndicesClusterStateService
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// backup shards are initializing as well, we make sure that they
// recover from primary *started* shards in the IndicesClusterStateService
/*
//www.apache.org/licenses/LICENSE-2.0
// even though it is throttled, move command still forces allocation
// Some indices are restored from snapshot, the RestoreInProgress must be set accordingly
/*
//www.apache.org/licenses/LICENSE-2.0
// track the failed nodes if shard is not started
// reroute with retryFailed=true should discard the failedNodes
// do not track the failed nodes while shard is started
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// test YES type
// test THROTTLE type
// test NO type
/*
//www.apache.org/licenses/LICENSE-2.0
// 90% used
// 65% used
// 40% used
// 20% used
// 10 bytes
// Primary shard should be initializing, replica should not
// Assert that we're able to start the primary
// Assert that node1 didn't get any shards because its disk usage is too high
// Assert that the replica couldn't be started since node1 doesn't have enough space
// Assert that the replica is initialized now that node3 is available with enough space
// Assert that the replica couldn't be started since node1 doesn't have enough space
// Set the low threshold to 60 instead of 70
// Set the high threshold to 70 instead of 80
// node2 now should not have new shards allocated to it, but shards can remain
// Shards remain started
// Set the low threshold to 50 instead of 60
// Set the high threshold to 60 instead of 70
// node2 now should not have new shards allocated to it, and shards cannot remain
// Shards remain started
// Shard hasn't been moved off of node2 yet because there's nowhere for it to go
// Shards remain started
// Node4 is available now, so the shard is moved off of node2
// 90% used
// 90% used
// 40% used
// 20% used
// 15% used
// 10 bytes
// Primary should initialize, even though both nodes are over the limit initialize
// Make node without the primary now habitable to replicas
// 65% used
// Now the replica should be able to initialize
// Assert that we're able to start the primary and replica, since they were both initializing
// Assert that node1 got a single shard (the primary), even though its disk usage is too high
// Assert that node2 got a single shard (a replica)
// Assert that one replica is still unassigned
//assertThat(clusterState.routingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).size(), equalTo(1));
// Assert that the replica is initialized now that node3 is available with enough space
// Assert that all replicas could be started
// Set the low threshold to 60 instead of 70
// Set the high threshold to 70 instead of 80
// node2 now should not have new shards allocated to it, but shards can remain
// Shards remain started
// Set the low threshold to 50 instead of 60
// Set the high threshold to 60 instead of 70
// node2 now should not have new shards allocated to it, and shards cannot remain
// Shards remain started
// Shard hasn't been moved off of node2 yet because there's nowhere for it to go
// Shards remain started
// One shard is relocating off of node1
// primary shard already has been relocated away
// node with increased space still has its shard
// Shards remain started on node3 and node4
// One shard is relocating off of node2 now
// Initializing on node5
// Node1 still has no shards because it has no space for them
// Node5 is available now, so the shard is moved off of node2
// 69% used
// 99% used
// 10 bytes
// node2 is added because DiskThresholdDecider automatically ignore single-node clusters
// Shard can't be allocated to node1 (or node2) because it would cause too much usage
// No shards are started, no nodes have enough disk for allocation
// 50% used
// 100% used
// 10 bytes
// 10 bytes
// node3 is added because DiskThresholdDecider automatically ignore single-node clusters
// Shard can be allocated to node1, even though it only has 25% free,
// because it's a primary that's never been allocated before
// A single shard is started on node1, even though it normally would not
// be allowed, because it's a primary that hasn't been allocated, and node1
// is still below the high watermark (unlike node3)
// 50% used
// 100% used
// 50% used
// 100% used
// 60% used
// 60% used
// 60% used
// 14 bytes
// 1 bytes
// shards should be initializing
// Assert that we're able to start the primary and replicas
// 60% used
// 60% used
// 100% used
// ensure reroute doesn't fail even though there is negative free space
// We have an index with 2 primary shards each taking 40 bytes. Each node has 100 bytes available
// 80% used
// 0% used
// Two shards consuming each 80% of disk space while 70% is allowed, so shard 0 isn't allowed here
// Two shards consuming each 80% of disk space while 70% is allowed, but one is relocating, so shard 0 can stay
// Creating AllocationService instance and the services it depends on...
// Ensure that the reroute call doesn't alter the routing table, since the first primary is relocating away
// and therefor we will have sufficient disk space on node1.
// 0% used
// 80% used
// 0% used
// We have an index with 1 primary shards each taking 40 bytes. Each node has 100 bytes available
// Two shards consumes 80% of disk space in data node, but we have only one data node, shards should remain.
// Two shards should start happily
// Add another datanode, it should relocate.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// actual test -- after all that bloat :)
// all full
// all full
// 20 - 99 percent since after allocation there must be at least 10% left and shard is 10byte
// this is weird and smells like a bug! it should be up to 20%?
// 10 bytes
// actual test -- after all that bloat :)
// all full
// way bigger than available space
// Intentionally not in the shardRoutingMap. We want to test what happens when we don't know where it is.
// actual test -- after all that bloat :)
// 90% used
// 91% used
// 10% used
// 10% used
// 10 bytes
// not allocated on that node
// not allocated on that node
// check that the DiskThresholdDecider still works even if the source index has been deleted
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// index settings override cluster settings
// index settings override cluster settings
/*
//www.apache.org/licenses/LICENSE-2.0
// we can initially only allocate on node2
// after failing the shard we are unassigned since the node is blacklisted and we can't initialize on the other node
// ok now we are started and can be allocated anywhere!! lets see...
// first create another copy
// now remove the node of the other copy and fail the current
// now bring back node1 and see it's assigned
//put a fake closed source index
/*
//www.apache.org/licenses/LICENSE-2.0
// ensure that indices do not use custom data paths
// ensure that each node has a single data path
// prevent any effects from in-flight recoveries, since we are only simulating a 100-byte disk
// start with all nodes below the watermark
// we have to consistently use bytes or percentage for the disk watermark settings
// Create an index with 10 shards so we can check allocation for it
// move node2 above high watermark
// move all nodes below watermark again
// ensure that each node has a single data path
// prevent any effects from in-flight recoveries, since we are only simulating a 100-byte disk
// start with all nodes below the low watermark
// we have to consistently use bytes or percentage for the disk watermark settings
// Create an index with 6 shards so we can check allocation for it
// Move all nodes above the low watermark so no shard movement can occur, and at least one node above the flood stage watermark so
// the index is blocked
// Cannot add further documents
// Move all nodes below the high watermark so that the index is unblocked
// Attempt to create a new document until DiskUsageMonitor unblocks the index
// ensure that each node has a single data path
// so that a subsequent reroute sees disk usage according to the current state
// shards are 1 byte large
// start with all nodes below the watermark
// disable rebalancing, or else we might move too many shards away and then rebalance them back again
// node2 suddenly has 99 bytes free, less than 10%, but moving one shard is enough to bring it up to 100 bytes free:
// must wait for relocation to start
// ensure that relocations finished without moving any more shards
// ensure that each node has a single data path
// so that a subsequent reroute sees disk usage according to the current state
// shards are 1 byte large
// node 2 only has space for one shard
// start one node with two data paths
// other two nodes have one data path each
// prevent any effects from in-flight recoveries, since we are only simulating a 100-byte disk
// start with all paths below the watermark
// one of the paths on node0 suddenly exceeds the high watermark
// disable rebalancing, or else we might move shards back onto the over-full path since we're not faking that
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// we test with 2 shards since otherwise it's pretty fragile if there are difference in the num or shards such that
// all shards are relocated to the second node which is not what we want here. It's solely a test for the settings to take effect
// prevent via index setting but only on index test
// now enable the index test to relocate since index settings override cluster settings
// flip the cluster wide setting such that we can also balance for index
// test_1 eventually we should have one shard of each index on each node
/**
// same same_host to true, since 2 nodes are started on the same host,
// only primaries should be assigned
// now, update the same_host setting to allow shards to be allocated to multiple nodes on
// the same host - the replica should get assigned
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// serialize with current version
// remove the custom and try serializing again
// Create a new, albeit equal, IndexMetadata object
// Create a new and different IndexMetadata object
// Add the new customs to named writeables
// serialize with current version
// Current version - Both the customs are non null
// serialize with minimum compatibile version
// Old version - TestCustomOne is null and TestCustomTwo is not null
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// make sure keysToAdd does not contain elements in keys
// due to duplicates, set size can be smaller than maxSetSize
/**
/**
/**
/**
/**
/**
/**
/**
// check properties of diffMap
// check properties of appliedDiffMap
/*
//www.apache.org/licenses/LICENSE-2.0
// Additional update task to make sure all previous logging made it to the loggerName
// Additional update task to make sure all previous logging made it to the loggerName
// We don't check logging for this on since there is no guarantee that it will occur before our check
/*
//www.apache.org/licenses/LICENSE-2.0
// there might be other tasks in this node, make sure to only take the ones we add into account in this test
// The tasks can be re-ordered, so we need to check out-of-order
// whenever we test for no tasks, we need to wait since this is a live node
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// randomly assign tasks to executors
// wait for all threads to be ready
// wait for all threads to finish
// wait until all the cluster state updates have been processed
// and until all of the publication callbacks have completed
// assert the number of executed tasks is correct
// assert each executor executed the correct number of tasks
// assert the correct number of clusterStateProcessed events were triggered
// maybe we should notify here?
// Additional update task to make sure all previous logging made it to the loggerName
// We don't check logging for this on since there is no guarantee that it will occur before our check
// check that we don't time out before even committing the cluster state
// check that we timeout if commit took too long
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// do this first, so startedProcessing can be used as a notification that this is done.
// this blocks the cluster state queue, so we can set it up right
// wait to be processed
// these will be the first batch
// release the first 0 task, but not the second
// setup the queue with pending tasks for another executor same priority
// now release the processing
// wait for last task to be processed
// test that for a single thread, tasks are executed in the order
// that they are submitted
// wait for all threads to be ready
// wait for all threads to finish
/*
//www.apache.org/licenses/LICENSE-2.0
// do nothing by default
/**
// can be overridden by TaskBatcherTests
// executed another task to double check that execute on the timed out update task is not called...
/**
// will hold all the tasks in the order in which they were executed
/*
//www.apache.org/licenses/LICENSE-2.0
// now persistent
/*",
/*", "internal:gateway/local*"));
// Cluster settings updates are blocked when the cluster is read only
// But it's possible to update the settings to update the "cluster.blocks.read_only" setting
// Cluster settings updates are blocked when the cluster is read only
// But it's possible to update the settings to update the "cluster.blocks.read_only" setting
// But it's possible to update the settings to update the "cluster.blocks.read_only" setting
// It should work now
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Request is not blocked
// Request is blocked
/*
//www.apache.org/licenses/LICENSE-2.0
// Create an index that will bring us up to the limit
// Create two indexes: One that ends up with fewer shards, and one
// that ends up with more to verify that we check the _total_ number of
// shards the operation would add.
// Since a request with preserve_existing can't change the number of
// replicas, we should never get an error here.
// Test restore after index deletion
// Reduce the shard limit and fill it up
// Fill up the cluster
// Sometimes add some headroom to the limit to check that it works even if you're not already right up against the limit
/*
//www.apache.org/licenses/LICENSE-2.0
//expected exception
//check node preference, first without preference to see they switch
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// delay buffer read..
// delay buffer write..
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// not supported
// not supported
// not supported
// not supported
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// fine - no links on this system
/*
//www.apache.org/licenses/LICENSE-2.0
// #values() guarantees order!
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// date in Feb-3rd, but still in Feb-2nd in -02:00 timezone
// date in Feb-3rd, also in -02:00 timezone
// hour unit
// testing savings to non savings switch
// testing non savings to savings switch
// testing non savings to savings switch (America/Chicago)
// testing savings to non savings switch 2013 (America/Chicago)
// testing savings to non savings switch 2014 (America/Chicago)
/**
// 1970-01-01T00:00:00Z - 2033-05-18T05:33:20.000+02:00
// FIXME this was copy pasted from the other impl and not used. breaks the nasty date actually gets assigned
// check correct unit interval width for units smaller than a day, they should be fixed size except for transitions
// if the interval defined didn't cross timezone offset transition, it should cover unitMillis width
/**
// positive and negative offset possible
/**
// after DST shift
/**
// test DST start
/**
/**
/**
/**
// 1970-01-01T00:00:00Z - 2033-05-18T05:33:20.000+02:00
// check two intervals around date
/**
// first date is the date to be rounded, second the expected result
// here's what this means for interval widths
/**
// the utc date for "2014-10-25T03:00:00+03:00" and "2014-10-25T03:00:00+02:00" is the same, local time turns back 1h here
// Day interval
// DST on
// Day of switching DST on -> off
// Day of switching DST off -> on
// Month interval
// DST on
// Year interval
// Two timestamps in same year and different timezone offset ("Double buckets" issue - #9491)
/**
// standard +/-1 hour DST transition, CET
// 29 Mar 2015 - Daylight Saving Time Started
// at 02:00:00 clocks were turned forward 1 hour to 03:00:00
// 25 Oct 2015 - Daylight Saving Time Ended
// at 03:00:00 clocks were turned backward 1 hour to 02:00:00
// time zone "Asia/Kathmandu"
// 1 Jan 1986 - Time Zone Change (IST → NPT), at 00:00:00 clocks were turned forward 00:15 minutes
//
// hour rounding is stable before 1985-12-31T23:00:00.000 and after 1986-01-01T01:00:00.000+05:45
// the interval between is 105 minutes long because the hour after transition starts at 00:15
// which is not a round value for hourly rounding
// time zone "Australia/Lord_Howe"
// 3 Mar 1991 - Daylight Saving Time Ended
// at 02:00:00 clocks were turned backward 0:30 hours to Sunday, 3 March 1991, 01:30:00
// 27 Oct 1991 - Daylight Saving Time Started
// at 02:00:00 clocks were turned forward 0:30 hours to 02:30:00
// the interval containing the switch time is 90 minutes long
// time zone "Pacific/Chatham"
// 5 Apr 2015 - Daylight Saving Time Ended
// at 03:45:00 clocks were turned backward 1 hour to 02:45:00
// 27 Sep 2015 - Daylight Saving Time Started
// at 02:45:00 clocks were turned forward 1 hour to 03:45:00
// time zone "Europe/Rome", rounding to days. Rome had two midnights on the day the clocks went back in 1978, and
// timeZone.convertLocalToUTC() gives the later of the two because Rome is east of UTC, whereas we want the earlier.
/**
// time zone "America/St_Johns", rounding to days.
// 29 October 2006 - Daylight Saving Time ended, changing the UTC offset from -02:30 to -03:30.
// This happened at 02:31 UTC, 00:01 local time, so the clocks were set back 1 hour to 23:01 on the 28th.
// This means that 2006-10-29 has _two_ midnights, one in the -02:30 offset and one in the -03:30 offset.
// Only the first of these is considered "rounded". Moreover, the extra time between 23:01 and 23:59
// should be considered as part of the 28th even though it comes after midnight on the 29th.
// Times before the first midnight should be rounded up to the first midnight.
// Times between the two midnights which are on the later day should be rounded down to the later day's midnight.
// (this is halfway through the last minute before the clocks changed, in which local time was ambiguous)
// Times between the two midnights which are on the earlier day should be rounded down to the earlier day's midnight.
// (this is halfway through the hour after the clocks changed, in which local time was ambiguous)
// Times after the second midnight should be rounded down to the first midnight.
/**
// First case, dst happens at 1am local time, switching back one hour.
// We want the overlapping hour to count for the next day, making it a 25h interval
// Sunday, 29 October 2000, 01:00:00 clocks were turned backward 1 hour
// to Sunday, 29 October 2000, 00:00:00 local standard time instead
// which means there were two midnights that day.
// Second case, dst happens at 0am local time, switching back one hour to 23pm local time.
// We want the overlapping hour to count for the previous day here
// Sunday, 1 April 1990, 00:00:00 clocks were turned backward 1 hour to
// Saturday, 31 March 1990, 23:00:00 local standard time instead
// make sure the next interval is 24h long again
/**
// Clocks went back at 00:01 between 1987 and 2010, causing overlapping days.
// These timezones are otherwise uninteresting, so just skip this period.
// Clocks went back 3 hours at 02:00 on 2010-03-05, causing overlapping days.
/*
//www.apache.org/licenses/LICENSE-2.0
// The time value's millisecond component must be > 0 so we're limited in the suffixes we can use.
/*
//www.apache.org/licenses/LICENSE-2.0
// Throws out high surrogates
// But will keep the whole character
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// Check headers
// Check rows
// Check getAsMap
// Check getHeaderMap
// Check findHeaderByName
// check row's timestamp
/*
//www.apache.org/licenses/LICENSE-2.0
// Low number so that the test runs quickly, but the results are more interesting with larger numbers
// of indexed documents
// ~12 in practice
// ~13 in practice
// ~20 in practice
// milliseconds
// Avoid randomization which will slow down things without improving
// the quality of this test
// for reproducibility
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we know bytes stream output always creates a paged bytes reference, we use it to create randomized content
/*
//www.apache.org/licenses/LICENSE-2.0
// we know bytes stream output always creates a paged bytes reference, we use it to create randomized content
// sometimes we have a paged ref - pull an iter and walk all pages!
// CompositeBytesReference doesn't share pages
// the assertions in this test only work on no-composite buffers
// CompositeBytesReference shifts offsets
// Slices that cross boundaries are composite too
// But not slices that cover a single sub reference
// strictly within sub
// equal to sub
/*
//www.apache.org/licenses/LICENSE-2.0
// we need a length != (n * pagesize) to avoid page sharing at boundaries
// ensure no single-page optimization
// verify that array() is cheap for small payloads
// verify that toBytes() is cheap for small payloads
// must return true for <= pagesize
// copy contents
// get refs & compare
/*
//www.apache.org/licenses/LICENSE-2.0
// CompositeBytesReference doesn't share pages
// the assertions in this test only work on no-composite buffers
// CompositeBytesReference shifts offsets
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// cache some entries, then randomly lookup keys that do not exist, then check the stats
// track the keys, which will be removed upon eviction (see the RemovalListener)
// cache some entries in batches of size maximumWeight; for each batch, touch the even entries to affect the
// ordering; upon the next caching of entries, the entries from the previous batch will be evicted; we can then
// check that the evicted entries were evicted in LRU order (first the odds in a batch, then the evens in a batch)
// for each batch
// cache entries up to numberOfEntries - maximumWeight; all of these entries will ultimately be evicted in
// batches of size maximumWeight, first the odds in the batch, then the evens in the batch
// finish filling the cache
// assert that the keys were evicted in LRU order
// cache some entries and exceed the maximum weight, then check that the cache has the expected weight and the
// expected evictions occurred
// cache weight should be the largest multiple of weight less than maximumWeight
// the number of evicted entries should be the number of entries that fit in the excess weight
// cache some entries, randomly invalidate some of them, then check that the weight of the cache is correct
// cache some entries, randomly invalidate some of them, then check that the number of cached entries is correct
// cache some entries, step the clock forward, cache some more entries, step the clock forward and then check that
// the first batch of cached entries expired and were removed
// wait for all threads to be ready
// wait for all threads to finish
// randomly promote some entries, step the clock forward, then check that the promoted entries remain and the
// non-promoted entries were removed
// randomly invalidate some cached entries, then check that a lookup for each of those and only those keys is null
// randomly invalidate some cached entries, then check that we receive invalidate notifications for those and only
// those entries
// randomly invalidate some cached entries, then check that a lookup for each of those and only those keys is null
// invalidate with incorrect value
// randomly invalidate some cached entries, then check that we receive invalidate notifications for those and only
// those entries
// invalidate with incorrect value
// invalidate all cached entries, then check that the cache is empty
// invalidate all cached entries, then check that we receive invalidate notifications for all entries
// randomly replace some entries, increasing the weight by 1 for each replacement, then count that the cache size
// is correct
// randomly replace some entries, then check that we received replacement notifications for those and only those
// entries
// wait for all threads to be ready
// wait for all threads to finish
// successfully avoided deadlock, release the main thread
// start a watchdog service
// ensure that we detected deadlock on our threads
// release the main test thread to fail the test
// everything is setup, release the hounds
// wait for either deadlock to be detected or the threads to terminate
// shutdown the watchdog service
// wait for all threads to be ready
// wait for all threads to finish
// wait for all threads to be ready
// wait for all threads to finish
// test that the cache is not corrupted under lots of concurrent modifications, even hitting the same key
// here be dragons: this test did catch one subtle bug during development; do not remove lightly
// wait for all threads to be ready
// wait for all threads to finish
/*
//www.apache.org/licenses/LICENSE-2.0
// we compute the total number of ops based on the bits of the hash
// since the test is much heavier when few bits are used for the hash
// ADD
// REMOVE
// make sure that the old copy has not been modified
// expected
// expected
// expected
// expected
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// hack to make it detected as YAML
// because of the intermediate flush, the two compressed representations
// are different. It can also happen for other reasons like if hash tables
// of different size are being used
// we used the compressed representation directly and did not recompress
// but compressedstring instances are still equal
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// fill block completely with junk
// now we have compressed byte array
// randomize constants again
// fill block completely with junk
/*
//www.apache.org/licenses/LICENSE-2.0
/** Base class for all geo parsing tests */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// test that no exception is thrown. BBOX parsing is not validated
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// make sure that ordinals don't change, because we rely on then in serialization
// sameLongitude and sameLatitude are both 90 degrees away from basePoint along great circles
// GeoDistance.PLANE measures the distance along a straight line in
// (lat, long) space so agrees with GeoDistance.ARC along a line of
// constant longitude but takes a longer route if there is east/west
// movement.
// GeoDistance.ARC calculates the great circle distance (on a sphere) so these should agree as they're both 90 degrees
// These points only differ by a few degrees so the calculation methods
// should match more closely. Check that the deviation is small enough,
// but not too small.
// The biggest deviations are away from the equator and the poles so pick a suitably troublesome latitude.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// multi dimension point
// multi dimension linestring
// test #1: envelope with expected coordinate order (TopLeft, BottomRight)
// test #2: envelope that spans dateline
// test #3: "envelope" (actually a triangle) with invalid number of coordinates (TopRight, BottomLeft, BottomRight)
// test #4: "envelope" with empty coordinates
// test case 1: create an invalid point object with multipoint data format
// test case 2: create an invalid point object with an empty number of coordinates
// test case 1: create an invalid multipoint object with single coordinate
// test case 2: create an invalid multipoint object with null coordinate
// test case 3: create a valid formatted multipoint object with invalid number (0) of coordinates
// test invalid multipolygon (an "accidental" polygon with inner rings outside outer ring)
//first poly (without holes)
//second poly (with hole)
//hole
/*
// test case 1: create an invalid polygon with only 2 points
// test case 2: create an invalid polygon with only 1 point
// test case 3: create an invalid polygon with 0 points
// test case 4: create an invalid polygon with null value points
// test case 5: create an invalid polygon with 1 invalid LinearRing
// test case 6: create an invalid polygon with 0 LinearRings
// test case 7: create an invalid polygon with 0 LinearRings
// two polygons; one without hole, one with hole
//first poly (without holes)
//second poly (with hole)
//hole
// test 1: valid ccw (right handed system) poly not crossing dateline (with 'right' field)
// test 2: valid cw poly
// single dimensions point
// zero dimensions point
// single dimensions point
// coordinates
// geometries
// foo
// start object
// start object
// end of the document
// no more elements afterwards
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// multi dimension point
// multi dimension linestring
// test #1: envelope with expected coordinate order (TopLeft, BottomRight)
// test #2: envelope that spans dateline
// test #3: "envelope" (actually a triangle) with invalid number of coordinates (TopRight, BottomLeft, BottomRight)
// test #4: "envelope" with empty coordinates
// test case 1: create an invalid point object with multipoint data format
// test case 2: create an invalid point object with an empty number of coordinates
// test case 1: create an invalid multipoint object with single coordinate
// test case 2: create an invalid multipoint object with null coordinate
// test case 3: create a valid formatted multipoint object with invalid number (0) of coordinates
// test invalid multipolygon (an "accidental" polygon with inner rings outside outer ring)
//one poly (with two holes)
// first hole
//second hole
// test invalid multipolygon (an "accidental" polygon with inner rings outside outer ring)
//first poly (without holes)
//second poly (with hole)
//hole
// test 1: ccw poly not crossing dateline
// test 2: ccw poly crossing dateline
// test 3: cw poly not crossing dateline
// test 4: cw poly crossing dateline
// test 1: ccw poly not crossing dateline
// test 2: ccw poly crossing dateline
// test 3: cw poly not crossing dateline
// test 4: cw poly crossing dateline
/**
// test case 1: create an invalid polygon with only 2 points
// test case 2: create an invalid polygon with only 1 point
// test case 3: create an invalid polygon with 0 points
// test case 4: create an invalid polygon with null value points
// test case 5: create an invalid polygon with 1 invalid LinearRing
// test case 6: create an invalid polygon with 0 LinearRings
// test case 7: create an invalid polygon with 0 LinearRings
// add 3d point to test ISSUE #10501
// test self crossing ccw poly not crossing dateline
// test #1: two polygons; one without hole, one with hole
//first poly (without holes)
//second poly (with hole)
//hole
// test #2: multipolygon; one polygon with one hole
// this test converting the multipolygon from a ShapeCollection type
// to a simple polygon (jtsGeom)
// hole
//equals returns true only if geometries are in the same order
// test 1: valid ccw (right handed system) poly not crossing dateline (with 'right' field)
// test 2: valid ccw (right handed system) poly not crossing dateline (with 'ccw' field)
// test 3: valid ccw (right handed system) poly not crossing dateline (with 'counterclockwise' field)
// test 4: valid cw (left handed system) poly crossing dateline (with 'left' field)
// test 5: valid cw multipoly (left handed system) poly crossing dateline (with 'cw' field)
// test 6: valid cw multipoly (left handed system) poly crossing dateline (with 'clockwise' field)
// single dimensions point
// zero dimensions point
// single dimensions point
// coordinates
// geometries
// foo
// start object
// start object
// end of the document
// no more elements afterwards
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Generate a random line that goes over poles and stretches beyond -180 and +180
// from time to time go over poles
// Check that the length of original and decomposed lines is the same
// Check that normalized linestring generates the same points as the normalized multipoint based on the same set of points
// current algorithm shifts edges to left
// In WKT the orientation is ignored
/*
//www.apache.org/licenses/LICENSE-2.0
// Shape builder conversion doesn't support altitude
// Test ShapeBuilder -> Geometry Serialization
// Test Geometry -> ShapeBuilder Serialization
// Test Geometry -> Geometry
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Coerce should automatically close the polygon
// No coerce - the polygon parsing should fail
// Start object
// Field Name
// Field Value
// Make sure we can parse values outside the normal lat lon boundaries
// Start object
// Field Name
// Field Value
// Start object
// Field Name
// Field Value
// if we serialize non-null value - it should be serialized as geojson
// Start object
// Field Name
// Field Value
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// {
// field name
// field value
// }
// no more tokens
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// malformed - extra paren
// TODO generate more malformed WKT
// test comments
// add 3d point to test ISSUE #10501
// add 3d point to test ISSUE #10501
// test store z disabled
// add 3d point to test ISSUE #10501
// test store z disabled
// test self crossing ccw poly not crossing dateline
// malformed points in a polygon is a common typo
// assert empty shape collection
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Building a simple LineString
// Building a linestring that needs to be wrapped
// Building a lineString on the dateline
// Building a lineString on the dateline
// LineString that needs to be wrapped
/** note: only supported by S4J at the moment */
// tests that the following shape (defined in counterclockwise OGC order)
// https://gist.github.com/anonymous/7f1bb6d7e9cd72f5977c crosses the dateline
// expected results: 3 polygons, 1 with a hole
// a giant c shape
// 3/4 of an embedded 'c', crossing dateline once
// embedded hole right of the dateline
// tests that the following shape (defined in clockwise non-OGC order)
// https://gist.github.com/anonymous/7f1bb6d7e9cd72f5977c crosses the dateline
// expected results: 3 polygons, 1 with a hole
// a giant c shape
// 3/4 of an embedded 'c', crossing dateline once
// embedded hole right of the dateline
// test case 1: test the positive side of the dateline
// test case 2: test the negative side of the dateline
// test case 1: test the positive side of the dateline
// test case 2: test the negative side of the dateline
// test a shape with one tangential (shared) vertex (should pass)
// test a shape with one invalid tangential (shared) vertex (should throw exception)
// test a shape with one tangential (shared) vertex for each hole (should pass)
// test shape with two tangential (shared) vertices (should throw exception)
/**
// cw: should produce a multi polygon spanning hemispheres
// cw: geo core will convert to ccw across the dateline
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// original center was 0.0, 0.0
/*
//www.apache.org/licenses/LICENSE-2.0
// move one corner to the middle of original
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// one point is minimum
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// change either point in shell or in random hole
/**
// A valid polygon that is oriented correctly (anticlockwise) but which
// confounds a naive algorithm for determining its orientation leading
// ES to believe that it crosses the dateline and "fixing" it in a way
// that self-intersects.
// Should not throw an exception
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// tries to move away open file handles
//www.google.com");
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// a convenience method for testing the write of a writeable enum
// a convenience method for testing the read of a writeable enum
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// test empty stream to array
// write single byte
// write byte-by-byte
// bulk-write with wrong args
// first bulk-write empty array: should not change anything
// bulk-write again with actual bytes
// write in bulk
// first create initial offset
// now write the rest - more than fits into the remaining first page
// now write the rest - more than fits into the remaining page + a full page after
// that,
// ie. we cross over into a third
// write byte-by-byte
// write byte-by-byte
// write byte-by-byte
// create & fill byte[] with randomized data
/*
// Read works for positive and negative numbers
// Use NoCheck variant so we can write negative numbers
// Write doesn't work for negative numbers
/*
//www.apache.org/licenses/LICENSE-2.0
// does not throw exception
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// randomize access order
// touch the entries to set the access order
// read some bytes
// read some bytes
/*
//www.apache.org/licenses/LICENSE-2.0
/** this testcase won't work in joda. See comment in {@link #testPartialTimeParsing()}
/*
//without timezone
// date_optional part of a parser names "strict_date_optional_time" or "date_optional"time
// means that date part can be partially parsed.
//in all these examples the second pattern will be used
//both parsing failures should contain pattern and input text in exception
//both patterns fail parsing the input text due to only 2 digits of millis. Hence full text was not parsed.
//7 (ok joda) vs 1 (java by default) but 7 with customized org.elasticsearch.common.time.IsoLocale.ISO8601
//Sunday
//2019-21 (ok joda) vs 2019-22 (java by default) but 2019-21 with customized org.elasticsearch.common.time.IsoLocale.ISO8601
//these parsers should allow both ',' and '.' as a decimal point
// only java.time has nanos parsing, but the results for 3digits should be the same
//This should fail, but java is ok with this because the field has the same value
//        assertJavaTimeParseException("2001-01-01T00:00:00.123,123Z", "strict_date_optional_time_nanos");
// the following fail under java 8 but work under java 10, needs investigation
// different timezone parsing styles require a different number of letters
// ... and can be combined, note that this is not an XOR, so one could append both timezones with this example
// also ensure that locale based dates are the same
// joda comes up with a different exception message here, so we have to adapt
//in all these examples the second pattern will be used
// joda comes up with a different exception message here, so we have to adapt
// the iso 8601 parser is available via Joda.forPattern(), so we have to test this slightly differently
//with strict resolving, YearOfEra expect an era, otherwise it won't resolve to a date
// java 8 has a bug in DateTimeFormatter usage when printing dates that rely on isSupportedBy for fields, which is
// what we use for epoch time. This change accounts for that bug. It should be removed when java 8 support is removed
/*
//www.apache.org/licenses/LICENSE-2.0
//the pattern has to be composite and the match should not be on the first one
// timezone works within date format
// test alternative ways of writing zero offsets, according to ISO 8601 +00:00, +00, +0000 should work.
// joda also seems to allow for -00:00, -00, -0000
// but also externally
// and timezone in the date has priority
// timezone does not affect now
// If a user only specifies times, then the date needs to always be 1970-01-01 regardless of rounding
// Implicit rounding happening when parts of the date are not specified
// implicit rounding with explicit timezone in the date format
// Explicit rounding using the || separator
// rounding should also take into account time zone
// with DST
// datemath still works on timestamps
// also check other time units
// a timestamp before 10000 is a year
// 10000 is also a year, breaking bwc, used to be a timestamp
// but 10000 with T is still a date format
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//this is a low level test for the deprecation logger, setup and checks are done manually
// convert UTF-16 to UTF-8 by hand to show the hard-coded constant below is correct
// noinspection ForLoopReplaceableByForEach
// cleanup after ourselves
// test that characters other than '\' and '"' are left unchanged
// test that valid characters are left unchanged
// when no encoding is needed, the original string is returned (optimization)
// Test that the number of warning headers don't exceed 'http.max_warning_header_count'
// try to log three warning messages
// Test that the size of warning headers don't exceed 'http.max_warning_header_size'
// try to log three warning messages
// assert that the size of all warning headers is less or equal to 1Kb
// mocking the logger used inside DeprecationLogger requires heavy hacking...
// trigger file permission, like rolling logs would
/*
//www.apache.org/licenses/LICENSE-2.0
//confirms exception is correctly parsed
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Create a shadow Engine, which will freak out because there is no
// index yet
// ignore interruptions
// count down latch
// now shadow engine should try to be created
// delete a random file
// match_all does not match anything on an empty index
// we need more than 8 documents because doc values are artificially penalized by IndexOrDocValuesQuery
// Random access by default
// Moves to sequential access if Bits#get is called more than the number of matches
/**
//bugs.openjdk.java.net/browse/JDK-4724038
//openjdk.java.net/jeps/260
// add assume's here if needed for certain platforms, but we should know if it does not work.
// mark the indexing hit non-aborting error
/*
//www.apache.org/licenses/LICENSE-2.0
// ok
// What we wanted
// Adding them back is a no-op
// same for reader2, but with a force merge to trigger evictions
/*
//www.apache.org/licenses/LICENSE-2.0
/** Simple tests for this filterreader */
/** Test that core cache key (needed for NRT) is working */
// add two docs, id:0 and id:1
// open reader
// delete id:0 and reopen
// we should have the same cache key as before
/*
//www.apache.org/licenses/LICENSE-2.0
// use keyword analyzer we rely on the stored field holding the exact term.
// we don't want to do any merges, so we won't expunge deletes
// add all docs
// now go over each doc, build the relevant references and filter
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// This is a custom query that extends AutomatonQuery and want to make sure the equals method works
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// add series of docs with terms of decreasing df
// setup MLT query
// perform MLT query
// check best terms are topN of highest idf
// clean up
/*
//www.apache.org/licenses/LICENSE-2.0
// read using int size
// Read by one byte at a time
// Read several bytes into target
// Read several bytes into 0-offset target
// Read using slice
// assert that position in the original input didn't change
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// to have deleted docs
// found doc
// not found doc
// deleted doc
/**
// return the last doc when there are duplicates
// delete the first doc only
// delete both docs
/*
//www.apache.org/licenses/LICENSE-2.0
// test reuse of uid field
// Nested
// Root
/** Test that version map cache works, is evicted on close, etc */
// should increase cache size by 1
// should be cache hit
// core should be evicted from the map
/** Test that version map cache behaves properly with a filtered reader */
// now wrap the reader
// same size map: core cache key is shared
// core should be evicted from the map
// between two known versions, should use the lucene version of the previous version
// too old version, major should be the oldest supported lucene version minus 1
// future version, should be the same version as today
/*
//www.apache.org/licenses/LICENSE-2.0
// edge case
// edge case
// edge case
// edge case
// first octet out of range
// second octet out of range
// third octet out of range
// fourth octet out of range
// octet that can not be parsed
// first octet out of range
// second octet out of range
// third octet out of range
// fourth octet out of range
// network mask out of range
// network mask out of range
// network mask that can not be parsed
// invalid because fourth octet is not zero
// invalid because third octet is not zero
// invalid because second octet is not zero
// invalid because first octet is not zero
// create cases that have a bit set outside of the network mask
// random number of strings with valid octets and valid network masks
// assert the resulting block has the right size
/*
//www.apache.org/licenses/LICENSE-2.0
// should end with ":0"
// should begin with "0:"
// too many parts
// too many parts
// too many parts
// :: must remove at least one 0.
// too many parts (9 instead of 8)
// hextet exceeds 16 bits
// expected behavior
// expected behavior
// Shouldn't hit DNS, because it's an IP string literal.
// Shouldn't hit DNS, because it's an IP string literal.
// Shouldn't hit DNS, because it's an IP string literal.
// Shouldn't hit DNS, because it's an IP string literal.
// Don't need to test IPv4 much; it just calls getHostAddress().
// Unfortunately the InetAddress.toString() method for IPv6 addresses
// does not collapse contiguous shorts of zeroes with the :: abbreviation.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Test that ipv4 address formatting round trips */
/** Test that ipv6 address formatting round trips */
/**
/** creates address without any lookups. hostname can be null, for missing */
/** creates scoped ipv6 address without any lookups. hostname can be null, for missing */
/*
//www.apache.org/licenses/LICENSE-2.0
// content doesn't matter we check reference equality
// content doesn't matter we check reference equality
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// test that selecting by name is possible
/*
//www.apache.org/licenses/LICENSE-2.0
/*", "one");
/*/x", "three");
// https://github.com/elastic/elasticsearch/pull/17916
// https://github.com/elastic/elasticsearch/pull/17916
/*/{test}/_endpoint", "test5");
/*", params), equalTo("test1"));
/*", params), equalTo("test3"));
/*/_endpoint", params), equalTo("test4"));
/*/_endpoint", params), equalTo("test5"));
//https://github.com/elastic/elasticsearch/issues/14177
//https://github.com/elastic/elasticsearch/issues/13665
/*
//www.apache.org/licenses/LICENSE-2.0
// marker states for data
// "fresh" is intentionally not 0 to ensure we covered this code path
// we cannot really free the internals of a byte[], so mark it for verification
// impl has protection against double release: ok
// otherwise ensure that the impl may not be returned twice
// get & keep reference to new/recycled data
// now exhaust the recycler
// Recycler size increases on release, not on obtain!
// release first ref, verify for destruction
// get & keep reference to pooled data
// randomize & return to pool
// verify that recycle() ran
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// will never match
// will never match
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// accepts the flags?
// construct a pattern that matches this string by repeatedly replacing random substrings with '*' characters
// construct a pattern that does not match this string by inserting a non-matching character (a digit)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Some timezones have transitions that do not change the offset, so we have to
// repeatedly call previousTransition until a nontrivial transition is found.
// There are no earlier transitions
// Progress was made
// field.roundFloor() works as long as the offset doesn't change.  It is worth getting this case out of the way first, as
// the calculations for fixing things near to offset changes are a little expensive and are unnecessary in the common case
// of working in UTC.
// When rounding to hours we consider any local time of the form 'xx:00:00' as rounded, even though this gives duplicate
// bucket names for the times when the clocks go back. Shorter units behave similarly. However, longer units round down to
// midnight, and on the days where there are two midnights we would rather pick the earlier one, so that buckets are
// uniquely identified by the date.
// `anyLocalStartOfDay` is _supposed_ to be the Unix timestamp for the start of the day in question in the current time
// zone.  Mostly this just means "midnight", which is fine, and on days with no local midnight it's the first time that
// does occur on that day which is also ok. However, on days with >1 local midnight this is _one_ of the midnights, but
// may not be the first. Check whether this is happening, and fix it if so.
// No previous transitions, so there can't be another earlier local midnight.
// NB we only assume interference from one previous transition. It's theoretically possible to have two transitions in
// quick succession, both of which have a midnight in them, but this doesn't appear to happen in the TZDB so (a) it's
// pointless to implement and (b) it won't be tested. I recognise that this comment is tempting fate and will likely
// cause this very situation to occur in the near future, and eagerly look forward to fixing this using a loop over
// previous transitions when it happens.
// `alsoLocalStartOfDay` is the Unix timestamp for the start of the day in question if the previous offset were in
// effect.
// Therefore the previous offset _is_ in effect at `alsoLocalStartOfDay`, and it's earlier than anyLocalStartOfDay,
// so this is the answer to use.
// The previous offset is not in effect at `alsoLocalStartOfDay`, so the current offset must be.
// field.roundFloor() mostly works as long as the offset hasn't changed in [rounded, utcMillis], so look at where
// the offset most recently changed.
// The offset did not change in [rounded, utcMillis], so roundFloor() worked as expected.
// The offset _did_ change in [rounded, utcMillis]. Put differently, this means that none of the times in
// [previousTransition+1, utcMillis] were rounded, so the rounded time must be <= previousTransition.  This means
// it's sufficient to try and round previousTransition down.
// add one unit and round to get to next rounded value
// in rare case we need to add more than one unit
// check if we crossed DST transition, in this case we want the
// last rounded value before the transition
/*
/**
// get the offset at instantLocal (first estimate)
// adjust instantLocal using the estimate and recalc the offset
// if the offsets differ, we must be near a DST boundary
// determine if we are in the DST gap
// we are in the DST gap
/*
//www.apache.org/licenses/LICENSE-2.0
// dont include nano/micro seconds as rounding would become zero then and throw an exception
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// date in Feb-3rd, but still in Feb-2nd in -02:00 timezone
// date in Feb-3rd, also in -02:00 timezone
// hour unit
// testing savings to non savings switch
// testing non savings to savings switch
// testing non savings to savings switch (America/Chicago)
// testing savings to non savings switch 2013 (America/Chicago)
// testing savings to non savings switch 2014 (America/Chicago)
/**
// 1970-01-01T00:00:00Z - 2033-05-18T05:33:20.000+02:00
// check correct unit interval width for units smaller than a day, they should be fixed size except for transitions
// if the interval defined didn't cross timezone offset transition, it should cover unitMillis width
/**
// positive and negative offset possible
/**
// after DST shift
/**
// test DST start
/**
/**
/**
/**
// 1970-01-01T00:00:00Z - 2033-05-18T05:33:20.000+02:00
// check two intervals around date
/**
// first date is the date to be rounded, second the expected result
// here's what this means for interval widths
/**
// the utc date for "2014-10-25T03:00:00+03:00" and "2014-10-25T03:00:00+02:00" is the same, local time turns back 1h here
// Day interval
// DST on
// Day of switching DST on -> off
// Day of switching DST off -> on
// Month interval
// DST on
// Year interval
// Two timestamps in same year and different timezone offset ("Double buckets" issue - #9491)
/**
// standard +/-1 hour DST transition, CET
// 29 Mar 2015 - Daylight Saving Time Started
// at 02:00:00 clocks were turned forward 1 hour to 03:00:00
// 25 Oct 2015 - Daylight Saving Time Ended
// at 03:00:00 clocks were turned backward 1 hour to 02:00:00
// time zone "Asia/Kathmandu"
// 1 Jan 1986 - Time Zone Change (IST → NPT), at 00:00:00 clocks were turned forward 00:15 minutes
//
// hour rounding is stable before 1985-12-31T23:00:00.000 and after 1986-01-01T01:00:00.000+05:45
// the interval between is 105 minutes long because the hour after transition starts at 00:15
// which is not a round value for hourly rounding
// time zone "Australia/Lord_Howe"
// 3 Mar 1991 - Daylight Saving Time Ended
// at 02:00:00 clocks were turned backward 0:30 hours to Sunday, 3 March 1991, 01:30:00
// 27 Oct 1991 - Daylight Saving Time Started
// at 02:00:00 clocks were turned forward 0:30 hours to 02:30:00
// the interval containing the switch time is 90 minutes long
// time zone "Pacific/Chatham"
// 5 Apr 2015 - Daylight Saving Time Ended
// at 03:45:00 clocks were turned backward 1 hour to 02:45:00
// 27 Sep 2015 - Daylight Saving Time Started
// at 02:45:00 clocks were turned forward 1 hour to 03:45:00
// time zone "Europe/Rome", rounding to days. Rome had two midnights on the day the clocks went back in 1978, and
// timeZone.convertLocalToUTC() gives the later of the two because Rome is east of UTC, whereas we want the earlier.
/**
// time zone "America/St_Johns", rounding to days.
// 29 October 2006 - Daylight Saving Time ended, changing the UTC offset from -02:30 to -03:30.
// This happened at 02:31 UTC, 00:01 local time, so the clocks were set back 1 hour to 23:01 on the 28th.
// This means that 2006-10-29 has _two_ midnights, one in the -02:30 offset and one in the -03:30 offset.
// Only the first of these is considered "rounded". Moreover, the extra time between 23:01 and 23:59
// should be considered as part of the 28th even though it comes after midnight on the 29th.
// Times before the first midnight should be rounded up to the first midnight.
// Times between the two midnights which are on the later day should be rounded down to the later day's midnight.
// (this is halfway through the last minute before the clocks changed, in which local time was ambiguous)
// Times between the two midnights which are on the earlier day should be rounded down to the earlier day's midnight.
// (this is halfway through the hour after the clocks changed, in which local time was ambiguous)
// Times after the second midnight should be rounded down to the first midnight.
/**
// First case, dst happens at 1am local time, switching back one hour.
// We want the overlapping hour to count for the next day, making it a 25h interval
// Sunday, 29 October 2000, 01:00:00 clocks were turned backward 1 hour
// to Sunday, 29 October 2000, 00:00:00 local standard time instead
// which means there were two midnights that day.
// Second case, dst happens at 0am local time, switching back one hour to 23pm local time.
// We want the overlapping hour to count for the previous day here
// Sunday, 1 April 1990, 00:00:00 clocks were turned backward 1 hour to
// Saturday, 31 March 1990, 23:00:00 local standard time instead
// make sure the next interval is 24h long again
/**
//github.com/JodaOrg/joda-time/issues/373)
// Formatter used to print and parse the sample date.
// Printing the date works but parsing it back fails
// with Joda 2.9.4
/**
// Clocks went back at 00:01 between 1987 and 2010, causing overlapping days.
// These timezones are otherwise uninteresting, so just skip this period.
// Clocks went back 3 hours at 02:00 on 2010-03-05, causing overlapping days.
/*
//www.apache.org/licenses/LICENSE-2.0
// different value
// missing value
// secureSettings.setString("dummy.consistent.secure.string.setting", "string_value");
// missing value
// "dummy.consistent.secure.string.affix.setting.affix2.suffix"
// missing values
// dummy.consistent.secure.string.affix.setting.affix1.suffix
// dummy.consistent.secure.string.affix.setting.affix2.suffix
/*
//www.apache.org/licenses/LICENSE-2.0
// hashes not yet published
// publish
// change value
// publish change
// add two affix settings to the keystore
// hashes not yet published
// publish
// change value
// publish change
// add value
// publish
// remove value
// missing value test.affix.first.bar
// hashes not yet published
// publish only the simple string setting
// publish only the affix string setting
// publish both settings
/*
//www.apache.org/licenses/LICENSE-2.0
// default is chosen based on actual heap size
/*
//www.apache.org/licenses/LICENSE-2.0
// Test various prefix/suffix lengths
/*
//www.apache.org/licenses/LICENSE-2.0
// removed
// added
// modified
// modified to trip validator
// modified to trip validator
// removed
// added
// modified
// reset to default
// reset to default
// affix setting - complex matcher
// array settings - complex matcher
// array settings - complex matcher - only accepts numbers
// affix settings don't know their concrete keys
// affix settings don't know their concrete keys
// affix settings don't know their concrete keys
/*", "internal:gateway/local*").build());
/*"));
// test fallback to node settings
// here we fall back to 'logger.level' which is our default.
/* validateInternalOrPrivateIndex */ true);
/* validateInternalOrPrivateIndex */ true);
// nothing should happen, validation should not throw an exception
/* validateInternalOrPrivateIndex */ false);
// nothing should happen, validation should not throw an exception
/* validateInternalOrPrivateIndex */ false);
/*
//www.apache.org/licenses/LICENSE-2.0
// close another time and no exception is thrown
// close another time and no exception is thrown
/*
//www.apache.org/licenses/LICENSE-2.0
// Test using direct filtering
// Test using toXContent filtering
/*
//www.apache.org/licenses/LICENSE-2.0
// Those should fail
// Some settings have both scopes - that's fine too if they have per-node defaults
/*
//www.apache.org/licenses/LICENSE-2.0
// overriding a single value with an array
// overriding an array with a shorter array
// overriding an array with a longer array
// overriding an array with a single value
// test that other arrays are not overridden
// overriding a deeper structure with an array
// overriding an array with a deeper structure
// check array
// this is just terrible but it's the existing behavior!
// check array
/*
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// try update bogus value
// try update bogus value
//github.com/elastic/elasticsearch/issues/33135")
// the purpose of this test is merely to ensure that a validator is invoked with the appropriate values
// It gets more complicated when there are two settings objects....
// change from default
// change back to default...
// now update and check that we got it
// now update and check that we got it
// reset to default
// reset to default
// try to parse this really annoying format
// try to parse this really annoying format
/**
// Those should pass
// We accept settings with no scope but they will be rejected when we register with SettingsModule.registerSetting
// We accept settings with multiple scopes but they will be rejected when we register with SettingsModule.registerSetting
/**
// GIVEN an affix setting changed from "prefix._foo"="bar" to "prefix._foo"=null
// WHEN creating an affix updater
// THEN affix updater is always expected to have changed (even when defaults are omitted)
// THEN changes are expected when defaults aren't omitted
// THEN changes are reported when defaults aren't omitted
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// as per WeekFields.ISO first week starts on Monday and has minimum 4 days
// first week of 2016 starts on Monday 2016-01-04 as previous week in 2016 has only 3 days
// first week of 2015 starts on Monday 2014-12-29 because 4days belong to 2019
// as per WeekFields.ISO first week starts on Monday and has minimum 4 days
// first week of 2016 starts on Monday 2016-01-04 as previous week in 2016 has only 3 days
// first week of 2015 starts on Monday 2014-12-29 because 4days belong to 2019
// this is not in the duelling tests, because the epoch millis parser in joda time drops the milliseconds after the comma
// but is able to parse the rest
// as this feature is supported it also makes sense to make it exact
// this is not in the duelling tests, because the epoch second parser in joda time drops the milliseconds after the comma
// but is able to parse the rest
// as this feature is supported it also makes sense to make it exact
// zone is null by default due to different behaviours between java8 and above
// different timezone, thus not equals
// different locale, thus not equals
// different pattern, thus not equals
// named formats too
// named formats too
// from 1970 epoch till around 2100
// timezone not allowed with just date
// milliseconds can be separated using comma or decimal point
// microseconds can be separated using comma or decimal point
// nanoseconds can be separated using comma or decimal point
// also check nanos of the epoch_millis formatter if it is rounded up to the nano second
// also check nanos of the epoch_millis formatter if it is rounded up to the nano second
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// dublin timezone in joda does not account for DST
// this has been added in joda 2.10.2 but is not part of the JDK 12.0.1 tzdata yet
// does not throw
// roundtrip does not throw either
// test with some leapyear
/*
//www.apache.org/licenses/LICENSE-2.0
//the pattern has to be composite and the match should not be on the first one
// timezone works within date format
// test alternative ways of writing zero offsets, according to ISO 8601 +00:00, +00, +0000 should work.
// joda also seems to allow for -00:00, -00, -0000
// but also externally
// and timezone in the date has priority
// timezone does not affect now
// If a user only specifies times, then the date needs to always be 1970-01-01 regardless of rounding
// due to rounding up, we have to add the number of milliseconds here manually
// Implicit rounding happening when parts of the date are not specified
// implicit rounding with explicit timezone in the date format
// Explicit rounding using the || separator
// rounding should also take into account time zone
// with DST
// datemath still works on timestamps
// also check other time units
// a timestamp before 100000 is a year
// but 10000 with T is still a date format
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// serialize
//expected
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Make sure a value of > Long.MAX_VALUE bytes throws an exception
// Make sure for units other than BYTES a size of -1 throws an exception
// Make sure for any unit a size < -1 throws an exception
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// cases that should throw exceptions
// custom AUTO
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// success
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// sometimes we'll look for NaN
// sometimes we'll have NaN in the array
// pick up all the indices that fall within the range of [lookForValue - tolerance, lookForValue + tolerance]
// we need to do this, since we choose the values randomly and we might end up having multiple values in the
// array that will match the looked for value with the random tolerance. In such cases, the binary search will
// return the first one that will match.
/*
//www.apache.org/licenses/LICENSE-2.0
// single page
// likely multiple pages
// single page
// likely multiple pages
// single page
// likely multiple pages
// single page
// likely multiple pages
// identity = equality
// equality: both empty
// not equal: contents differ
// not equal: contents differ
// not equal: contents differ
// null arg has hashCode 0
// empty array should have equal hash
// FUN FACT: Arrays.hashCode() and BytesReference.bytesHashCode() are inconsistent for empty byte[]
// final int emptyHash3 = new BytesArray(BytesRef.EMPTY_BYTES).hashCode();
// assertEquals(emptyHash1, emptyHash3); -> fail (1 vs. 0)
// large arrays should be different
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Test high load factors to make sure that collision resolution works fine
// START - tests borrowed from LUCENE
/**
/**
/**
//hash.add(ref);
// string found in hash
// add again to check duplicates
// END - tests borrowed from LUCENE
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we have to mark we're ready now (actually done).
// in all other cases, we expect a cancellation exception.
/*
//www.apache.org/licenses/LICENSE-2.0
// check content is the same
// check stability
// reverse
/*
//www.apache.org/licenses/LICENSE-2.0
// Was sized sufficiently large that all of these values should be retained
// Unclear when it will saturate exactly, but should be before 100 given the configuration
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Test high load factors to make sure that collision resolution works fine
// found in hash
// add again to check duplicates
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Setup the first filter
// Setup the second filter
// now merge and verify the combined set
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//test.local/").match(new URI("http://test.local/")));
//test.local/somepath").match(new URI("http://test.local/")));
//test.local/somepath").match(new URI("http://test.local/somepath")));
//test.local/somepath").match(new URI("http://test.local/somepath/more")));
//test.local/somepath/*").match(new URI("http://test.local/somepath/more")));
/*").match(new URI("http://test.local/somepath/more")));
//test.local/somepath/*").match(new URI("http://test.local/somepath/more/andmore/../bitmore")));
/*").match(new URI("http://test.local/somepath/more/andmore/../bitmore")));
//test.local/somepath/*").match(new URI("http://test.local/")));
/*").match(new URI("http://test.local/")));
//test.local:1234/somepath/*").match(new URI("http://test.local/somepath/more")));
/*").match(new URI("http://test.local/somepath/more")));
//test.local:1234/somepath/*").match(new URI("http://test.local:1234/somepath/more")));
/*").match(new URI("http://test.local:1234/somepath/more")));
//*.local:1234/somepath/*").match(new URI("http://foobar.local:2345/somepath/more")));
/*.local:1234/somepath/*").match(new URI("http://foobar.local:2345/somepath/more")));
//*.local:*/somepath/*").match(new URI("http://foobar.local:2345/somepath/more?par=val")));
/*.local:*/somepath/*").match(new URI("http://foobar.local:2345/somepath/more?par=val")));
//*.local:*/somepath/*?*").match(new URI("http://foobar.local:2345/somepath/more?par=val")));
/*.local:*/somepath/*?*").match(new URI("http://foobar.local:2345/somepath/more?par=val")));
//*.local:*/somepath/*?*").match(new URI("http://foobar.local:2345/somepath/more?par=val#frag")));
/*.local:*/somepath/*?*").match(new URI("http://foobar.local:2345/somepath/more?par=val#frag")));
//*.local:*/somepath/*?*#*").match(new URI("http://foobar.local:2345/somepath/more?par=val#frag")));
/*.local:*/somepath/*?*#*").match(new URI("http://foobar.local:2345/somepath/more?par=val#frag")));
//*.local/somepath/*?*#*").match(new URI("http://foobar.local/somepath/more")));
/*.local/somepath/*?*#*").match(new URI("http://foobar.local/somepath/more")));
/*
//www.apache.org/licenses/LICENSE-2.0
// 1 for runInternal plus 1 for the test sequence
// 1 for runInternal plus 1 for the test sequence
// 1 for runInternal plus 1 for the test sequence
// This should only take 2 milliseconds in ideal conditions, but allow 10 seconds in case of VM stalls
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// it's "not stopped or closed"
// onAfter uses it too, but we're not testing it here
// it's stopped or closed
// onAfter uses it too, but we're not testing it here
// it's "not stopped or closed"
// it's stopped or closed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: check why we need a loop, can't we just use received.addAndGet(candidates.size())
// first thread blocks, the rest should be non blocking.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// all is well
//wait until thread executes this task
//otherwise, a task might be queued
//wait until thread executes this task
//otherwise, a task might be queued
// Doesn't matter is going to be rejected
/*
// Doesn't matter is going to be rejected
/*
//www.apache.org/licenses/LICENSE-2.0
// we test a fixed and an auto-queue executor but not scaling since it does not reject
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// lock again
// just acquire this and make sure we can :)
/*
//www.apache.org/licenses/LICENSE-2.0
// we need to do more than just call onResponse as this often results in synchronous
// execution of the listeners instead of actually going async
/*
//www.apache.org/licenses/LICENSE-2.0
// will execute after the first LOW (fifo)
// will execute after the first HIGH (fifo)
// will execute after the first LOW (fifo)
// will execute after the first HIGH (fifo)
// will execute after the first LOW (fifo)
// will execute after the first HIGH (fifo)
// will execute after the first LOW (fifo)
// will execute after the first HIGH (fifo)
/* enough timeout to catch them in the pending list... */, new Runnable() {
// sleep a bit to double check that execute on the timed out update task is not called...
// We should never get here
// the timeout handler is added post execution (and quickly cancelled). We have allow for this
// and use assert busy
/*
//www.apache.org/licenses/LICENSE-2.0
// test unit conversion with a controlled clock
// test age advances with System#nanoTime
// creation happened before start, so age will be at least as
// large as elapsed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Execute exactly 3 (measureWindow) times
// The queue capacity should have increased by 50 since they were very fast tasks
// Execute a task multiple times that takes 1ms
// Execute a task multiple times that takes 1m
// Execute a task multiple times that takes 1m
// The queue capacity should decrease, but no lower than the minimum
// Execute a task multiple times that takes 1ms
// The queue capacity should increase, but no higher than the maximum
/** Use a runnable wrapper that simulates a task with unknown failures. */
/**
/** Execute a blank task {@code times} times for the executor */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// previously there was a bug here and this would return false
/*
//www.apache.org/licenses/LICENSE-2.0
// Queue size already equal to desired capacity
// Not worth adjusting
// Not worth adjusting
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// fill the queue to capacity
// this thread will try to offer items to the queue while the queue size thread is polling the size
// synchronize each iteration of checking the size with each iteration of offering, each iteration is a race
// this thread will repeatedly poll the size of the queue keeping track of the maximum size that it sees
// synchronize each iteration of checking the size with each iteration of offering, each iteration is a race
// wait for the threads to finish
// the maximum size of the queue should be equal to the capacity
/*
//www.apache.org/licenses/LICENSE-2.0
// pretend that another thread created the same response
// pretend that another thread created the same response at a different time
// Create a runnable that should run with some header
// We don't see the header outside of the runnable
// But we do inside of it
// but not after
// Create a runnable that should run with some header
// Now attempt to rewrap it
// We get the original context inside the runnable
// In fact the second wrapping didn't even change it
// create a abstract runnable, add headers and transient objects and verify in the methods
// We don't see the header outside of the runnable
// But we do inside of it
// verify not seen after
// repeat with regular runnable
// a runnable that throws from onFailure
// We don't see the header outside of the runnable
// But we do inside of it
// but not after
// a runnable that throws from onAfter
// We don't see the header outside of the runnable
// But we do inside of it
// but not after
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// changes to the outer list are not seen since flatten pre-caches outer list on init:
// but changes to the original inner lists are seen:
/*
//www.apache.org/licenses/LICENSE-2.0
// assert the resulting difference us unmodifiable
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// 1970-01-01T00:00:00Z - 2033-05-18T05:33:20.000+02:00
// 2016-01-01T00:00:00.000Z
// 2016-12-25T07:59:42.213Z
// ZonedDateTime
// Instant
// LocalDateTime (no time zone)
// LocalDate (no time, no time zone)
// LocalTime (no date, no time zone)
// OffsetDateTime
// also test with a date that has a real offset
// OffsetTime
// also test with a date that has a real offset
// DayOfWeek enum, not a real time value, but might be used in scripts
// Month
// MonthDay
// Year
// Duration
// Period
// Big integers cannot be handled explicitly, but if some values happen to be big ints,
// we can still call parser.map() and get the bigint value so that eg. source filtering
// keeps working
/**
// map 0 -> map 1
// map 1 -> map 0 loop
// map 0 -> map 1
// map 1 -> map 2
// map 2 -> map 0 loop
// map 0 -> it1
// it 1 -> map 1, map 2
// map 2 -> map 0 loop
// Build the XContentBuilder, convert its bytes to JSON and check it matches
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// CBOR is binary, cannot use String
// for this {"f "=> 5} perl encoder for example generates:
//assertThat(((Number) XContentHelper.convertToMap(bytes, true).v2().get("foo")).intValue(), equalTo(5));
// this if for {"foo" : 5} in python CBOR
// also make sure major type check doesn't collide with SMILE and JSON, just in case
/*
//www.apache.org/licenses/LICENSE-2.0
// Parser current token is null
//binary values will be parsed back and returned as base64 strings when reading from json and yaml
//binary values will be parsed back and returned as BytesArray when reading from cbor and smile
// because of the missing type to identify the parser, we expect no return value, but also no exception
/*
//www.apache.org/licenses/LICENSE-2.0
/*";
/*";
/*
//www.apache.org/licenses/LICENSE-2.0
// double close, and check there is no error...
// try again...
// we get a space at the start here since it thinks we are not in the root object (fine, we will ignore it in the real code we use)
// up to 20k random terms
/*
//www.apache.org/licenses/LICENSE-2.0
// Running this part twice triggers the issue.
// See https://github.com/elastic/elasticsearch/issues/8629
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Sample test case **/
// Test method: rawField(String fieldName, BytesReference content)
// Test method: rawField(String fieldName, InputStream content)
// Test: Array of values (no filtering)
// Test: Array of values (with filtering)
// Test: Array of objects (no filtering)
// Test: Array of objects (with filtering)
// Test: Array of objects (with partial filtering)
// more complex object...
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// lists
// fields with . in them
// only objects
// implicit include
// explicit include
// wild card include
// dots in field names in includes
// dots in field names in excludes
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// foo.**.bar.*
// test.dot\.ted
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// {
//    "index" : "test",
//    "source" : {
//         value : "something"
//    }
// }
// "index"
// "source"
//        JsonLocation location1 = parser.getCurrentLocation();
//        parser.skipChildren();
//        JsonLocation location2 = parser.getCurrentLocation();
//
//        byte[] sourceData = new byte[(int) (location2.getByteOffset() - location1.getByteOffset())];
//        System.arraycopy(data, (int) location1.getByteOffset(), sourceData, 0, sourceData.length);
//
//        JsonParser sourceParser = new JsonFactory().createJsonParser(new FastByteArrayInputStream(sourceData));
//        assertThat(sourceParser.nextToken(), equalTo(JsonToken.START_OBJECT));
//        assertThat(sourceParser.nextToken(), equalTo(JsonToken.FIELD_NAME)); // "value"
//        assertThat(sourceParser.nextToken(), equalTo(JsonToken.VALUE_STRING));
//        assertThat(sourceParser.nextToken(), equalTo(JsonToken.END_OBJECT));
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// now check with the custom field query
/*
//www.apache.org/licenses/LICENSE-2.0
// we use 30s as timeout in many places.
// sync global checkpoint quickly so we can verify seq_no_stats aligned between all copies after tests.
// the network unresponsive disruption may leave operations in flight
// this is because this disruption scheme swallows requests by design
// as such, these operations will never be marked as finished
// for hitting simulated network failures quickly
// for hitting simulated network failures quickly
// for hitting simulated network failures quickly
// for hitting simulated network failures quickly
// still long to induce failures but to long so test won't time out
// <-- for hitting simulated network failures quickly
// Network delay disruption waits for the min between this
// value and the time of disruption and does not recover immediately
// when disruption is stop. We should make sure we recover faster
// then the default of 30s, causing ensureGreen and friends to time out
// TODO: add partial partitions
/*
//www.apache.org/licenses/LICENSE-2.0
// wait on the threads to finish
// verify that the publisher times out
// wait on the threads to finish
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Don't use AbstractDisruptionTestCase.DEFAULT_SETTINGS as settings
// (which can cause node disconnects on a slow CI machine)
// now search for the documents and see if we get a reply
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
//github.com/elastic/elasticsearch/issues/41068")
// id -> node sent.
// fine - semaphore interrupt
// in case of a bridge partition, shard allocation can fail "index.allocation.max_retries" times if the master
// is the super-connected node and recovery source and target are on opposite sides of the bridge
/**
// simulate handling of sending shard failure during an isolation
// fail a random shard
// we cannot use the NetworkUnresponsive disruption type here as it will swallow the "shard failed" request, calling neither
// onSuccess nor onFailure on the provided listener.
// heal the partition
// the cluster should stabilize
// the listener should be notified
// the failed shard should be gone
/*
// otherwise we will fail during clean-up
/**
//github.com/elastic/elasticsearch/issues/11665
// We know this will time out due to the partition, we check manually below to not proceed until
// the delete has been applied to the master node and the master eligible node.
// Don't restart the master node until we know the index deletion has taken effect on master and the master eligible node.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// shutting down the nodes, to avoid the leakage check tripping
// on the states associated with the commit requests we may have dropped
// don't wait for initial state, we want to add the disruption while the cluster is forming
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//", inner);
// prevents checkpoint file to be updated
/**
// ignore
/*
//www.apache.org/licenses/LICENSE-2.0
// minus 1 because we are ignoring the first line that's a comment
// only one of the two is valid and will be used
// sets up the config dir, writes to the unicast hosts file in the config dir,
// and then runs the file-based unicast host provider to get the list of discovery nodes
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// a very long GC, but it's OK as we remove the disruption when it has had an effect
// restore GC
// make sure all nodes agree on master
/**
// make sure cluster reforms
// make sure isolated need picks up on things.
// restore isolation
// trigger a reroute now, instead of waiting for the background reroute of RerouteService
// and wait for it to finish and for the cluster to stabilize
// verify all cluster states are the same
// use assert busy to wait for cluster states to be applied (as publish_timeout has low value)
// assert nodes are identical
/**
// Makes sure that the get request can be executed on each node locally:
// Everything is stable now, it is now time to simulate evil...
// but first make sure we have no initializing shards and all is green
// (waiting for green here, because indexing / search in a yellow index is fine as long as no other nodes go down)
// Simulate a network issue between the unlucky node and the rest of the cluster.
// The unlucky node must report *no* master node, since it can't connect to master and in fact it should
// continuously ping until network failures have been resolved. However
// It may a take a bit before the node detects it has been cut off from the elected master
// Wait until the master node sees al 3 nodes again.
// The unlucky node must report *no* master node, since it can't connect to master and in fact it should
// continuously ping until network failures have been resolved. However
// It may a take a bit before the node detects it has been cut off from the elected master
// make sure we have stable cluster & cross partition recoveries are canceled by the removal of the missing node
// the unresponsive partition causes recoveries to only time out after 15m (default) and these will cause
// the test to fail due to unfreed resources
// create one field
/*
//www.apache.org/licenses/LICENSE-2.0
// Note: the explicit cast of the CopyFunction is needed for some IDE (specifically Eclipse 4.8.0) to infer the right type
// Note: the explicit cast of the CopyFunction is needed for some IDE (specifically Eclipse 4.8.0) to infer the right type
/*
//www.apache.org/licenses/LICENSE-2.0
// -1 means address resolution fails
// sourceNode is not yet known
// sourceNode is not yet known
// MockTransportAddressConnector verifies no multiple connection attempts
// need to wait for the connection to timeout, then for another wakeup, before discovering the peer
// MockTransportAddressConnector verifies no multiple connection attempts
// need to wait for the connection to timeout, then for another wakeup, before discovering the peer
/*
//www.apache.org/licenses/LICENSE-2.0
// close in reverse order as opened
// JDK stack is broken, it does not iterate in the expected order (http://bugs.java.com/bugdatabase/view_bug.do?bug_id=4475301)
// only one of the two is valid and will be used
/*
//www.apache.org/licenses/LICENSE-2.0
// super.nodeSettings enables file-based discovery, but here we disable it again so we can test the static list:
// super.nodeSettings sets this to an empty list, which disables any search for other nodes, but here we want this to happen:
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Writing incompatible snapshot can cause this test to fail due to a race condition in repo initialization
// by the current master and the former master. It is not causing any issues in real life scenario, but
// might make this test to fail. We are going to complete initialization of the snapshot to prevent this failures.
// The snapshot started, we can start disruption so the INIT state will arrive to another master node
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Figure out what is the elected master node
// Pick a node that isn't the elected master.
// Simulate a network issue between the unlucky node and elected master node in both directions.
// Wait until elected master has removed that the unlucky node...
// The unlucky node must report *no* master node, since it can't connect to master and in fact it should
// continuously ping until network failures have been resolved. However
// It may a take a bit before the node detects it has been cut off from the elected master
// Wait until the master node sees all 3 nodes again.
// The elected master shouldn't have changed, since the unlucky node never could have elected itself as master
/**
/**
/**
// Save the current master node as old master node, because that node will get frozen
// Simulating a painful gc by suspending all threads for a long time on the current elected master node.
// Save the majority side
// Keeps track of the previous and current master when a master node transition took place on each node on the majority side:
// Wait for majority side to elect a new master
// The old master node is frozen, but here we submit a cluster state update task that doesn't get executed, but will be queued and
// once the old master node un-freezes it gets executed.  The old master node will send this update + the cluster state where it is
// flagged as master to the other nodes that follow the new master. These nodes should ignore this update.
// Save the new elected master node
// Stop disruption
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// ignore
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// check count
// test successful
// count with no query is a match all one
// failure
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
///test/repos/repo1")), notNullValue());
//test/repos/repo1")), nullValue());
///test/repos/../repo1")), nullValue());
//localhost/test/")), nullValue());
///test/repos/repo1!/repo/")), notNullValue());
///test/repos/repo1!/repo/")).toString(), endsWith("repo1!/repo/"));
///test/repos/../repo1!/repo/")), nullValue());
//localhost/test/../repo1?blah!/repo/")), nullValue());
// test that environment paths are absolute and normalized
// the above paths will be treated as relative to the working directory
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// simulate older data path layout by moving data under "nodes/0" folder
// create extra file/folder, and check that upgrade fails
// check that upgrade works
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: fix test to allow extras
// Reuse the same location and attempt to lock again
// Close the environment that holds the lock and make sure we can get the lock after release
// Defaults to not hooking up std out
// False means don't hook up std out
// But true means hook std out up statically
// Clean up after ourselves
// expected
// expected
// can lock again?
// expected
// expected
// expected
// ok
// fire the threads up
// simulate some previous left over temp files
// check we clean up
// build settings using same path.data as original but with node.data=false and node.master=false
// test that we can create data=false and master=false with no meta information
// build settings using same path.data as original but with node.data=false
// test that we can create data=false env with only meta information. Also create shard data for following asserts
// assert that we get the stricter message on meta-data when both conditions fail
// build settings using same path.data as original but with node.master=false
// test that we can create master=false env regardless of data.
// test that we can create data=true, master=true env. Also remove state dir to leave only shard data for following asserts
// assert that we fail on shard data even without the metadata dir.
/** Converts an array of Strings to an array of Paths, adding an additional child if specified */
/*
//www.apache.org/licenses/LICENSE-2.0
// VersionUtils.randomVersion() only returns known versions, which are necessarily no later than Version.CURRENT; however we want
// also to consider our behaviour with all versions, so occasionally pick up a truly random version.
// the behaviour tested here is only appropriate if the current version is compatible with versions 7 and earlier
// when the current version is incompatible with version 7, the behaviour should change to reject files like the given resource
// which do not have the version field
/*
//www.apache.org/licenses/LICENSE-2.0
// verify test setup
// by restarting as master and data node, we can check that the index definition was really deleted and also that the tool
// does not mess things up so much that the nodes cannot boot as master or data node any longer.
// index is gone.
/*
//www.apache.org/licenses/LICENSE-2.0
// verify test setup
//verify cleaned.
// verify test setup
//verify clean.
// use a commutative digest to avoid dependency on file system order.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// not a match b/c not realtime
// not a match b/c not realtime
// write
// read
// write complex
// read complex
/*
//www.apache.org/licenses/LICENSE-2.0
// first fetch, no data, still on going
// fire a response, wait on reroute incrementing
// verify we get back the data node
// all is well
// first fetch, no data, still on going
// fire a response, wait on reroute incrementing
// verify we get back the data node
// add a failed response for node1
// first fetch, no data, still on going
// fire a response, wait on reroute incrementing
// failure, fetched data exists, but has no data
// on failure, we reset the failure on a successive call to fetchData, and try again afterwards
// 2 reroutes, cause we have a failure that we clear
// first fetch, no data, still on going
// handle a response with incorrect round id, wait on reroute incrementing
// fire a response (with correct round id), wait on reroute incrementing
// verify we get back the data node
// add a failed response for node1
// first fetch, no data, still on going
// handle a failure with incorrect round id, wait on reroute incrementing
// fire a response, wait on reroute incrementing
// failure, fetched data exists, but has no data
// no fetched data, 2 requests still on going
// fire the first response, it should trigger a reroute
// there is still another on going request, so no data
// fire the second simulation, this should allow us to get the data
// no more ongoing requests, we should fetch the data
// no fetched data, 2 requests still on going
// fire the first response, it should trigger a reroute
// fire the second simulation, this should allow us to get the data
// since one of those failed, we should only have one entry
// no fetched data, 2 requests still on going
// fire the first response, it should trigger a reroute
// now, add a second node to the nodes, it should add it to the ongoing requests
// no fetch data, has a new node introduced
// fire the second simulation, this should allow us to get the data
// since one of those failed, we should only have one entry
// must work also with no data
// no fetched data, request still on going
// verify we get back right data from node
// second fetch gets same data
// prepare next request
// no fetched data, new request on going
// verify we get new data back
// no fetched data, request still on going
// clear cache while request is still on going, before it is processed
// prepare next request
// verify still no fetched data, request still on going
// verify we get new data back
// we are simulating a master node switch, wait for it to not be null
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// The setting AUTO_IMPORT_DANGLING_INDICES_SETTING is deprecated, so we must disable
// warning checks or all the tests will fail.
// check that several runs when not in the metadata still keep the dangled index around
// simulate allocation to the metadata
// check that several runs when in the metadata, but not cleaned yet, still keeps dangled
/*
//www.apache.org/licenses/LICENSE-2.0
// testRecoverBrokenIndexMetadata replies on the flushing on shutdown behavior which can be randomly disabled in MockInternalEngine.
// all is well
// all is well
/**
/**
// this is invalid but should be archived
// this one is not validated ahead of time and breaks allocation
// check that the cluster does not keep reallocating shards
// try to open it with the broken setting - fail again!
/**
// check that the cluster does not keep reallocating shards
// try to open it with the broken setting - fail again!
// wait for state recovery
// delete these settings
//github.com/elastic/elasticsearch/issues/48701")
// This test relates to loading a broken state that was written by a 6.x node, but for now we do not load state from old nodes.
// It's possible for a 6.x node to add a tombstone for an index but not actually delete the index metadata from disk since that
// deletion is slightly deferred and may race against the node being shut down; if you upgrade to 7.x when in this state then the
// node won't start.
//        writeBrokenMeta(metaStateService -> {
//            metaStateService.writeGlobalState("test", MetaData.builder(metaData)
//                // we remove the manifest file, resetting the term and making this look like an upgrade from 6.x, so must also reset the
//                // term in the coordination metadata
//                .coordinationMetaData(CoordinationMetaData.builder(metaData.coordinationMetaData()).term(0L).build())
//                // add a tombstone but do not delete the index metadata from disk
//               .putCustom(IndexGraveyard.TYPE, IndexGraveyard.builder().addTombstone(metaData.index("test").getIndex()).build()).build());
//            for (final Path path : paths) {
//                try (Stream<Path> stateFiles = Files.list(path.resolve(MetaDataStateFormat.STATE_DIR_NAME))) {
//                    for (final Path manifestPath : stateFiles
//                        .filter(p -> p.getFileName().toString().startsWith(Manifest.FORMAT.getPrefix())).collect(Collectors.toList())) {
//                        IOUtils.rm(manifestPath);
//                    }
//                }
//            }
//        });
/*
//www.apache.org/licenses/LICENSE-2.0
// generate random coordinationMetaData with different lastAcceptedConfiguration and lastCommittedConfiguration
// open LucenePersistedState to make sure that cluster state is written out to each data path
// verify that the freshest state was rewritten to each data path
//generate random coordinationMetaData with different lastAcceptedConfiguration and lastCommittedConfiguration
// generate a series of updates and check if batching works
// bump term
// update cluster state
// verify that the freshest state was rewritten to each data path
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// check that the default is not set
// ensure default is set when setting expected_nodes
// ensure default is set when setting expected_data_nodes
// ensure default is set when setting expected_master_nodes
// ensure settings override default
// ensure default is set when setting expected_nodes
/*
//www.apache.org/licenses/LICENSE-2.0
// We only guarantee atomicity of writes, if there is initial Manifest file
/*
// ensure no overflow
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: fix test to work with ExtrasFS
/**
// indices are empty since they are serialized separately
// now corrupt it
// expected
// one long is the checksum... 8 bytes
// collision
// checksum corrupted
// corrupt a file that we do not necessarily
// need here....
// make sure the index tombstones are the same too
// check that we reserialize unknown metadata correctly again
// now corrupt all the latest ones and make sure we fail to load the state
//we call loadLatestState not on full path set, but only on random paths from this set. This is to emulate disk failures.
//if there was a WriteStateException we need to override current state before we continue
/**
// start object
/*
//www.apache.org/licenses/LICENSE-2.0
// this test checks that index state is written on data only nodes if they have a shard allocated
// this test checks that the index data is removed from a data only node once all shards have been allocated away from it
// close the index
// update the mapping. this should cause the new meta data to be written although index is closed
// make sure it was also written on red node although index is closed
/* Try the same and see if this also works if node was just restarted.
// make sure it was also written on red node although index is closed
// finally check that meta data is also written of index opened again
// make sure index is fully initialized and nothing is changed anymore
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// verify that the freshest state is chosen
// verify that loadBestOnDiskState has same check
// first establish consistent node IDs and write initial metadata
// check if we can open writer again
// check if we can open writer again
// if someone attempted surgery on the metadata index by hand, e.g. deleting broken segments, then maybe the global metadata
// isn't there any more
// if someone attempted surgery on the metadata index by hand, e.g. deleting broken segments, then maybe the global metadata
// is duplicated
// if someone attempted surgery on the metadata index by hand, e.g. deleting broken segments, then maybe some index metadata
// is duplicated
// do not duplicate global metadata
// -1 because it's incremented in .put()
// ensure we do not wastefully persist the same index metadata version by making a bad update with the same version
// ensure that we do persist the same index metadata version by making an update with a higher version
// ensure that we also persist the index metadata when the term changes
// -1 because it's incremented in .put()
// -1 because it's incremented in .put()
// -1 because it's incremented in .put()
// ensure no overflow
/*
//www.apache.org/licenses/LICENSE-2.0
// with old version, we can't know if a shard was allocated before or not
/**
/**
/**
/**
/**
// check that allocation id is reused
/**
// check that allocation id is reused
/**
// check that allocation id is reused
/**
// since the deciders return a NO decision for allocating a shard (due to the guaranteed NO decision from the second decider),
// the allocator will see if it can force assign the primary, where the decision will be YES
/**
// since both deciders here return a NO decision for allocating a shard,
// the allocator will see if it can force assign the primary, where the decision will be either NO or THROTTLE,
// so the shard will remain un-initialized
/**
// since we have a NO decision for allocating a shard (because the second decider returns a NO decision),
// the allocator will see if it can force assign the primary, and in this case,
// the TestAllocateDecision's decision for force allocating is to THROTTLE (using
// the default behavior) so despite the other decider's decision to return YES for
// force allocating the shard, we still THROTTLE due to the decision from TestAllocateDecision
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// sometimes just use defaults
// default
/*
//www.apache.org/licenses/LICENSE-2.0
// we are shutting down nodes - make sure we don't have 2 clusters if we test network
//We don't check for failures in the flush response: if we do we might get the following:
// FlushNotAllowedEngineException[[test][1] recovery is in progress, flush [COMMIT_TRANSLOG] is not allowed]
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// wait for primary allocations here otherwise if we have a lot of shards we might have a
// shard that is still in post recovery when we restart and the ensureYellow() below will timeout
// note: default replica settings are tied to #data nodes-1 which is 0 here. We can do with 1 in this test.
// insert enough docs so all shards will have a doc
// insert a two docs, some shards will not have anything
// we have to verify primaries are started for them to be restored
// wait for primary allocations here otherwise if we have a lot of shards we might have a
// shard that is still in post recovery when we restart and the ensureYellow() below will timeout
// disable bootstrapping
// clean two nodes
// TODO: remove once refresh doesn't fail immediately if there a master block:
// https://github.com/elastic/elasticsearch/issues/9997
// client().admin().cluster().prepareHealth("test").setWaitForYellowStatus().get();
// create the index with our mapping
// disable merges to keep segments the same
// expire retention leases quickly
// start the replica node; we do this after indexing so a file-based recovery is triggered to ensure the files are identical
// index some more documents; we expect to reuse the files that already exist on the replica
// prevent a sequence-number-based recovery from being possible
// We need an extra flush to advance the min_retained_seqno of the SoftDeletesPolicy
// we have to recover the segments file since we commit the translog ID on engine startup
// nodes may need to report the shards they processed the initial recovered cluster state from the master
// make sure state is not recovered
// multi data path might only have one path in use
// start another node so cluster consistency checks won't time out due to the lack of state
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// AllocationService only calls GatewayAllocator if there're unassigned shards
/**
// Index more documents and flush to destroy sync_id and remove the retention lease (as file_based_recovery_threshold reached).
// AllocationService only calls GatewayAllocator if there are unassigned shards
// need to wait for events to ensure the reroute has happened since we perform it async when a new node joins.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
// Rarely use a seqNo above retainingSeqNoOnPrimary, which could in theory happen when primary fails and comes back quickly.
// has retention lease but store is empty
/**
/**
/**
/**
/**
// we sometime return empty list of files, make sure we test this as well
// mark shard as delayed if reason is NODE_LEFT
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// multi get with specific field
// Now test values being fetched from stored fields.
// From translog:
//all good
// From Lucene index:
//all good
// From translog:
//all good
// From Lucene index:
//all good
// Version from translog
// [0] version doesn't matter, which is the default
//Version from Lucene index
// [0] version doesn't matter, which is the default
// Version from translog
// [0] version doesn't matter, which is the default
//Version from Lucene index
// [0] version doesn't matter, which is the default
// multi types in 5.6
// Flush fails if shard has ongoing recoveries, make sure the cluster is settled down
// before refresh - document is only in translog
//after refresh - document is in translog and also indexed
//after flush - document is in not anymore translog - only indexed
// before refresh - document is only in translog
//after refresh - document is in translog and also indexed
//after flush - document is in not anymore translog - only indexed
// before refresh - document is only in translog
//after refresh - document is in translog and also indexed
//after flush - document is in not anymore translog - only indexed
// before refresh - document is only in translog
//after refresh - document is in translog and also indexed
//after flush - document is in not anymore translog - only indexed
//after flush - document is in not anymore translog - only indexed
//for get
//same for multi get
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// sometimes have a leading whitespace between comma delimited elements
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: Enable these Cors tests when the Cors logic lives in :server
//    public void testCorsEnabledWithoutAllowOrigins() {
//        // Set up an HTTP transport with only the CORS enabled setting
//        Settings settings = Settings.builder()
//            .put(HttpTransportSettings.SETTING_CORS_ENABLED.getKey(), true)
//            .build();
//        HttpResponse response = executeRequest(settings, "remote-host", "request-host");
//        // inspect response and validate
//        assertThat(response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN), nullValue());
//    }
//
//    public void testCorsEnabledWithAllowOrigins() {
//        final String originValue = "remote-host";
//        // create an HTTP transport with CORS enabled and allow origin configured
//        Settings settings = Settings.builder()
//            .put(SETTING_CORS_ENABLED.getKey(), true)
//            .put(SETTING_CORS_ALLOW_ORIGIN.getKey(), originValue)
//            .build();
//        HttpResponse response = executeRequest(settings, originValue, "request-host");
//        // inspect response and validate
//        assertThat(response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN), notNullValue());
//        String allowedOrigins = response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN);
//        assertThat(allowedOrigins, is(originValue));
//    }
//
//    public void testCorsAllowOriginWithSameHost() {
//        String originValue = "remote-host";
//        String host = "remote-host";
//        // create an HTTP transport with CORS enabled
//        Settings settings = Settings.builder()
//            .put(SETTING_CORS_ENABLED.getKey(), true)
//            .build();
//        HttpResponse response = executeRequest(settings, originValue, host);
//        // inspect response and validate
//        assertThat(response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN), notNullValue());
//        String allowedOrigins = response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN);
//        assertThat(allowedOrigins, is(originValue));
//
//        originValue = "http://" + originValue;
//        response = executeRequest(settings, originValue, host);
//        assertThat(response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN), notNullValue());
//        allowedOrigins = response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN);
//        assertThat(allowedOrigins, is(originValue));
//
//        originValue = originValue + ":5555";
//        host = host + ":5555";
//        response = executeRequest(settings, originValue, host);
//        assertThat(response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN), notNullValue());
//        allowedOrigins = response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN);
//        assertThat(allowedOrigins, is(originValue));
//
//        originValue = originValue.replace("http", "https");
//        response = executeRequest(settings, originValue, host);
//        assertThat(response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN), notNullValue());
//        allowedOrigins = response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN);
//        assertThat(allowedOrigins, is(originValue));
//    }
//
//    public void testThatStringLiteralWorksOnMatch() {
//        final String originValue = "remote-host";
//        Settings settings = Settings.builder()
//            .put(SETTING_CORS_ENABLED.getKey(), true)
//            .put(SETTING_CORS_ALLOW_ORIGIN.getKey(), originValue)
//            .put(SETTING_CORS_ALLOW_METHODS.getKey(), "get, options, post")
//            .put(SETTING_CORS_ALLOW_CREDENTIALS.getKey(), true)
//            .build();
//        HttpResponse response = executeRequest(settings, originValue, "request-host");
//        // inspect response and validate
//        assertThat(response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN), notNullValue());
//        String allowedOrigins = response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN);
//        assertThat(allowedOrigins, is(originValue));
//        assertThat(response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_CREDENTIALS), equalTo("true"));
//    }
//
//    public void testThatAnyOriginWorks() {
//        final String originValue = NioCorsHandler.ANY_ORIGIN;
//        Settings settings = Settings.builder()
//            .put(SETTING_CORS_ENABLED.getKey(), true)
//            .put(SETTING_CORS_ALLOW_ORIGIN.getKey(), originValue)
//            .build();
//        HttpResponse response = executeRequest(settings, originValue, "request-host");
//        // inspect response and validate
//        assertThat(response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN), notNullValue());
//        String allowedOrigins = response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_ORIGIN);
//        assertThat(allowedOrigins, is(originValue));
//        assertThat(response.headers().get(HttpHeaderNames.ACCESS_CONTROL_ALLOW_CREDENTIALS), nullValue());
//    }
// send a response
// inspect what was written
// send a response
// inspect what was written
// ensure we have reserved bytes
// do something builder
// ESTestCase#after will invoke ensureAllArraysAreReleased which will fail if the response content was not released
// ESTestCase#after will invoke ensureAllArraysAreReleased which will fail if the response content was not released
// ESTestCase#after will invoke ensureAllArraysAreReleased which will fail if the response content was not released
// TODO: These exist for the Cors tests
//        if (originValue != null) {
//            httpRequest.headers().add(HttpHeaderNames.ORIGIN, originValue);
//        }
//        httpRequest.headers().add(HttpHeaderNames.HOST, host);
// get the response
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// this asserts that the final_pipeline was used, without us having to actually create the pipeline etc.
// this asserts that the high_order_final_pipeline was selected, without us having to actually create the pipeline etc.
// this asserts that this pipeline is the final pipeline executed
/*
//www.apache.org/licenses/LICENSE-2.0
// Turning off document logging doesn't log source[]
// Turning on document logging logs the whole thing
// Turning off document logging doesn't log source[]
// Turning on document logging logs the whole thing
// And you can truncate the source
// And you can truncate the source
// Throwing a error if source cannot be converted
/*
//www.apache.org/licenses/LICENSE-2.0
// causes index service creation to fail
// causes index service creation to fail
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Unit test(s) for IndexService */
// task can throw exceptions!!
// here we need to swap first before we let it go otherwise threads might be very fast and run that task twice due to
// random exception and the schedule interval is 1ms
// now close the index
// now reopen the index
// now disable
// set it to 100ms
// set it to 200ms
// set it to 200ms again
// now close the index
// now reopen the index
// now close the index
// now reopen the index
// now disable the refresh
// when we update we reschedule the existing task AND fire off an async refresh to make sure we make everything visible
// before that this is why we need to wait for the refresh task to be unscheduled and the first doc to be visible
// this one either becomes visible due to a concurrently running scheduled refresh OR due to the force refresh
// we are running on updateMetaData if the interval changes
// refresh every millisecond
// this one becomes visible due to the force refresh we are running on updateMetaData if the interval changes
// this one becomes visible due to the scheduled refresh
// very often :)
// very often :)
// disable
// very often :)
/*
//www.apache.org/licenses/LICENSE-2.0
// use version number that is unknown
// test default
// validation should fail since we are not ignoring private settings
// validation should fail since we are not ignoring private settings
// nothing should happen since we are ignoring private settings
// validation should fail since we are not ignoring archived settings
// validation should fail since we are not ignoring archived settings
// nothing should happen since we are ignoring archived settings
// no double archive
// private setting
// enabled by default on 7.0+ or later
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// the beginning of the parser
/*
//www.apache.org/licenses/LICENSE-2.0
// see if defaults are restored
/*
//www.apache.org/licenses/LICENSE-2.0
// Make sure we log the change:
// #6882: make sure we can change index.merge.scheduler.max_thread_count live
// Make sure we log the change:
/*
//www.apache.org/licenses/LICENSE-2.0
// Makes sure that output doesn't contain any new lines
/*
//www.apache.org/licenses/LICENSE-2.0
// only for the test index
/*
//www.apache.org/licenses/LICENSE-2.0
// if we didn't find a version (but the index does support it), we don't like it unless MATCH_ANY
// deletes
// and the stupid usual case
// Old indexing code, dictating behavior
//        if (expectedVersion != Versions.MATCH_ANY && currentVersion != Versions.NOT_SET) {
//            // an explicit version is provided, see if there is a conflict
//            // if we did not find anything, and a version is provided, so we do expect to find a doc under that version
//            // this is important, since we don't allow to preset a version in order to handle deletes
//            if (currentVersion == Versions.NOT_FOUND) {
//                throw new VersionConflictEngineException(shardId, index.type(), index.id(), Versions.NOT_FOUND, expectedVersion);
//            } else if (expectedVersion != currentVersion) {
//                throw new VersionConflictEngineException(shardId, index.type(), index.id(), currentVersion, expectedVersion);
//            }
//        }
//        updatedVersion = (currentVersion == Versions.NOT_SET || currentVersion == Versions.NOT_FOUND) ? 1 : currentVersion + 1;
// MATCH_ANY must throw an exception in the case of external version, as the version must be set! it used as the new value
// if we didn't find a version (but the index does support it), we always accept
// and the standard behavior
// Old indexing code, dictating behavior
//        // an external version is provided, just check, if a local version exists, that its higher than it
//        // the actual version checking is one in an external system, and we just want to not index older versions
//        if (currentVersion >= 0) { // we can check!, its there
//            if (currentVersion >= index.version()) {
//                throw new VersionConflictEngineException(shardId, index.type(), index.id(), currentVersion, index.version());
//            }
//        }
//        updatedVersion = index.version();
// MATCH_ANY must throw an exception in the case of external version, as the version must be set! it used as the new value
// if we didn't find a version (but the index does support it), we always accept
// and the standard behavior
// Old indexing code
//        if (index.versionType() == VersionType.INTERNAL) { // internal version type
//            updatedVersion = (currentVersion == Versions.NOT_SET || currentVersion == Versions.NOT_FOUND) ? 1 : currentVersion + 1;
//        } else { // external version type
//            updatedVersion = expectedVersion;
//        }
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Use a shorter refresh interval to speed up the tests. We'll be waiting on this interval several times.
// Index normally
// Now delete with blockUntilRefresh
// Index normally
// Update with RefreshPolicy.WAIT_UNTIL
// Upsert with RefreshPolicy.WAIT_UNTIL
// Update-becomes-delete with RefreshPolicy.WAIT_UNTIL
// Index by bulk with RefreshPolicy.WAIT_UNTIL
// Update by bulk with RefreshPolicy.WAIT_UNTIL
// Delete by bulk with RefreshPolicy.WAIT_UNTIL
// Update makes a noop
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/* The snake_case version of the name should not filter out any stopwords while the
// This shouldn't contain English stopwords
// This *should* contain English stopwords
// We should only get a warning from the token filter that is referenced in settings
// exception will not throw because we're not on Version.CURRENT
// We should only get a warning from the normalizer, because we're on a version where 'deprecated'
// works fine
// exception will not throw because we're not on Version.LATEST
/*
//www.apache.org/licenses/LICENSE-2.0
/* Comma separated list */
/* Array */
// some invalid UTF-8
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Use an empty plugin that doesn't define anything so the test doesn't need a ton of null checks.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// if only "default" is set in the map, all getters should return the same analyzer
/*
//www.apache.org/licenses/LICENSE-2.0
// sometimes also return reloadable custom analyzer
/*
//www.apache.org/licenses/LICENSE-2.0
// special case, these two are the same instance
// same es version should be cached
// Same Lucene version should be cached:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// check that when using regular non-search time filters only, we get an exception
/**
// wait until all running threads have seen the unaltered upper case analysis at least once
// wait until all running threads have seen the new lower case analysis at least once
/*
//www.apache.org/licenses/LICENSE-2.0
// This config uses different size of shingles so graph analysis is disabled
// This config uses a single size of shingles so graph analysis is enabled
/*`
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// don't specify
/*
//www.apache.org/licenses/LICENSE-2.0
// now cached
// There are 3 segments
// now cached
// Only one segment now, so the size must be 1
// There is no reference from readers and writer to any segment in the test index, so the size in the fbs cache must be 0
// all is well
/*
//www.apache.org/licenses/LICENSE-2.0
// we test against default codec so never get a random one here!
// write some docs with it, inspect .si to see this was the used compression
/*
//www.apache.org/licenses/LICENSE-2.0
// Advance the global checkpoint to between [safeIndex, safeIndex + 1)
// Captures and releases some commits
// Snapshotting commits must not be deleted.
// We don't need to retain translog for snapshotting commits.
// We should never keep invalid commits regardless of the value of the global checkpoint.
// Safe commit is the last commit - no need to clean up
// Advanced but not enough for any commit after the safe commit becomes safe
// Advanced enough for some index commit becomes safe
// Advanced enough for the last commit becomes safe
// Safe commit is the last commit - no need to clean up
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// some settings to keep num segments low
/*
//www.apache.org/licenses/LICENSE-2.0
// Tricky: TimeValue.parseTimeValue casts this long to a double, which steals 11 of the 64 bits for exponent, so we can't use
// the full long range here else the assert below fails:
/*
//www.apache.org/licenses/LICENSE-2.0
// now lets make this document visible
// random empty refresh
// now we add this to the map
// randomly refresh here again
// now, optimize and wait for merges, see that we have no merge flag
// we could have multiple underlying merges, so the generation may increase more than once
// we should have had just 1 merge, so last generation should be exact
// delete tombstones
// recovered already
// ready
// create a document
// its not there...
// but, not there non realtime
// but, we can still get it (in realtime)
// but not real time is not yet visible
// refresh and it should be there
// now its there...
// also in non realtime
// now do an update
// its not updated yet...
// but, we can still get it (in realtime)
// refresh and it should be updated
// now delete
// its not deleted yet
// but, get should not see it (in realtime)
// refresh and it should be deleted
// add it back
// its not there...
// refresh and it should be there
// now its there...
// now flush
// and, verify get (in real time)
// make sure we can still work with the engine
// now do an update
// its not updated yet...
// refresh and it should be updated
// create a document
// its not there...
// refresh and it should be there
// now its there...
// don't release the search result yet...
// delete, refresh and do a new search, it should not be there
// the non release search result should not see the deleted yet...
// to advance persisted local checkpoint
// run this a couple of times to get some coverage
// we might hit a concurrent flush from a finishing merge here - just wait if ongoing...
/**
/*
// If we already merged down to 1 segment, then the next force-merge will be a noop. We need to add an extra segment to make
// merges happen so we can verify that _recovery_source are pruned. See: https://github.com/elastic/elasticsearch/issues/41628.
// fine
// delete
// delete
// randomly interleave
// insert some duplicates
// use 5 to go above 0 for magic numbers
// generate a conflict
// TODO: add support for non-existing docs
// generate a conflict
// refresh and take the chance to check everything is ok so far
// even if doc is not not deleted, lastFieldValue can still be null if this is the
// first op and it failed.
// simulate GC deletes
// delete
// other version types don't support out of order processing.
// last op wasn't delete
// delete
// #5891: make sure IndexWriter's infoStream output is
// sent to lucene.iw with log level TRACE:
// First, with DEBUG, which should NOT log IndexWriter output:
// Again, with TRACE, which should log IndexWriter output:
// mostly index, sometimes delete
// we have some docs indexed, so delete one of them
// index a document
// to advance persisted local checkpoint
// only update rarely as we do it every doc
// to guarantee the global checkpoint is written to the translog checkpoint
// after recovering from translog, all docs have been flushed to Lucene segments, so here we will assert
// that the committed max seq no is equivalent to what the current primary seq no is, as all data
// we have assigned sequence numbers to should be in the commit
// this test writes documents to the engine while concurrently flushing/commit
// and ensuring that the commit points contain the correct sequence number data
// create N indexing threads to index documents simultaneously
// wait for all threads to start at the same time
// index random number of docs
// start the indexing threads
// wait for indexing threads to all be ready to start
// create random commit points
// don't keep on piling up too many commits
// we increase the wait time to make sure we eventually if things are slow wait for threads to finish.
// this will reduce pressure on disks and will allow threads to make progress without piling up too many commits
// now, verify all the commits have the correct docs according to the user commit data
// local checkpoint and max seq no shouldn't go backwards
// make sure localCheckpoint <= highest seq no found <= maxSeqNo
// make sure all sequence numbers up to and including the local checkpoint are in the index
// _seq_no are stored as doc values for the time being, so this is how we get them
// (as opposed to using an IndexSearcher or IndexReader)
// #8603: make sure we can separately log IFD's messages
// First, with DEBUG, which should NOT log IndexWriter output:
// Again, with TRACE, which should only log IndexWriter IFD output:
// Add document
// Delete document we just added:
// Get should not find the document
// Give the gc pruning logic a chance to kick in
// Delete non-existent document
// Get should not find the document (we never indexed uid=2):
// Try to index uid=1 with a too-old version, should fail:
// Get should still not find the document
// Try to index uid=2 with a too-old version, should fail:
// Get should not find the document
/**
// this test fails if any reader, searcher or directory is not closed - MDW FTW
// all is fine
// create
// to advance persisted local checkpoint
// open and recover tlog
// creating an empty index will create the first translog gen and commit it
// opening the empty index will make the second translog file but not commit it
// opening the engine again (i=0) will make the third translog file, which then be committed
// open index with new tlog
// open and recover tlog with empty tlog
// test that we can force start the engine , even if the translog is missing.
// fake a new translog, causing the engine to point to a missing one.
// when a new translog is created it should be ok
// since we rollback the IW we are writing the same segment files again after starting IW but MDW prevents
// this so we have to disable the check explicitly
// to advance local checkpoint
// we need to reuse the engine config unless the parser.mappingModified won't work
/* create a TranslogConfig that has been created with a different UUID */
// and recover again!
/**
// try to acquire the writer lock - i.e., everything is closed, we need to synchronize
// to avoid races between closing threads
// all good.
// one shot
// test document failure while indexing
// test index with document failure
// test non document level failure is thrown
// simulate close by corruption
// this is a hack to add a failure during store document which triggers a tragic event
// and in turn fails the engine
// normal close
// now the engine is closed check we respond correctly
// this is a hack to add a failure during store document which triggers a tragic event
// and in turn fails the engine
// operations with a seq# equal or lower to the local checkpoint are not indexed to lucene
// and the version lookup is skipped
// promote to primary: first do refresh
// if the replica operation wasn't a retry, the operation arriving on the newly promoted primary must be a retry
// make sure flush cleans up commits for later.
// for other tests to access this
// for other tests to access this
// fine
// Non-existent doc returns no seqnum and no primary term
// create a document
// Index the same document again
// Index the same document for the third time, this time changing the primary term
// we can query by the _seq_no
// id -> latest seq_no
/**
/** java docs */
// we can only advance as far as the number of operations that did not conflict
// each time the version increments as we walk the list, that counts as a successful operation
// sequence numbers start at zero, so the expected local checkpoint is the number of successful operations minus one
/**
/*
// skip to the op that we added to the translog
// fills n gap and 2 manual noop.
/**
/**
/*
/*
/**
// to advance local checkpoint
// to advance local checkpoint
// now snapshot the tlog and ensure the primary term is updated
// now do it again to make sure we preserve values etc.
// all is well
// disable merges to make sure that the reader doesn't change unexpectedly during the test
// now ensure external refreshes are reflected on the internal reader
// Advance the global checkpoint during the flush to create a lag between a persisted global checkpoint in the translog
// (this value is visible to the deletion policy) and an in memory global checkpoint in the SequenceNumbersService.
// Keep only one safe commit as the oldest commit.
// If the global checkpoint is still unassigned, we keep an empty(eg. initial) commit as a safe commit.
// Make sure we keep all translog operations after the local checkpoint of the safe commit.
// check that we can still read the commit that we captured
// check it's clean up
// Global checkpoint advanced enough - only the last commit is kept.
// taking snapshots from the safe commit.
// pending snapshots - should not release any commit.
// release the last snapshot - delete all except the last commit
// A new engine may have more than one empty translog files - the test should account this extra.
// Stale operations skipped by Lucene but added to translog - still able to flush
// If the new index commit still points to the same translog generation as the current index commit,
// we should not enable the periodically flush condition; otherwise we can get into an infinite loop of flushes.
// create a gap here
// the merge listner runs concurrently after the force merge returned
// this is a reproduction of https://github.com/elastic/elasticsearch/issues/28714
// first index an append only document and then delete it. such that we have it in the tombstones
// now index more append only docs and refresh so we re-enabel the optimization for unsafe version map
// once we are here the version map is unsafe again and we need to do a refresh inside the get calls to ensure we
// de-optimize. We also enabled GCDeletes which now causes pruning tombstones inside that refresh that is done internally
// to ensure we de-optimize. One get call will purne and the other will try to lock the version map concurrently while
// holding the lock that pruneTombstones needs and we have a deadlock
// Prune tombstones whose seqno < gap_seqno and timestamp < clock-gcInterval.
// Prune tombstones whose seqno at most the local checkpoint (eg. seqno < gap_seqno).
// Need a margin for gcInterval/4.
// Fill the seqno gap - should prune all tombstones.
// Need a margin for gcInterval/4.
// get the last one.
// advance persisted local checkpoint
// do not recover from translog
// We need to remove min_retained_seq_no commit tag as the actual hard-deletes engine does not have it.
// do not recover from translog
// make adding operations to translog slower
// now we have 2 segments since we now added a tombstone plus the old segment with the delete
// lets force merge the tombstone and the original segment and make sure the doc is still there but the ID term is gone
// everything is pruned away
/**
// trim everything in translog
/*
//www.apache.org/licenses/LICENSE-2.0
// less than 50% off
// now refresh
// With Java 9, RamUsageTester computes the memory usage of maps as
// the memory usage of an array that would contain exactly all keys
// and values. This is an under-estimation of the actual memory
// usage since it ignores the impact of the load factor and of the
// linked list/tree that is used to resolve collisions. So we use a
// bigger tolerance.
// less than 50% off
// Java 8 is more accurate by doing reflection into the actual JDK classes
// so we give it a lower error bound.
// less than 25% off
// length of the array must be the same as the len of the ref... there is an assertion in LiveVersionMap#putUnderLock
// make sure we track the latest timestamp and seqno we pruned the deletes
// deletes can be the same version
// here we keep track of the deletes and ensure that all deletes that are not visible anymore ie. not in the map
// have a timestamp that is smaller or equal to the maximum timestamp that we pruned on
// if we don't do anything ie. no adds etc we will stay with the safe access required
// refresh otherwise we won't prune since it's tracked by the current map
/*
//www.apache.org/licenses/LICENSE-2.0
// always enable soft-deletes
// Empty engine
// Get snapshot via engine will auto refresh
/**
// have to verify without source since we are randomly testing without _source
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Ensure that we can't open two noop engines for the same store
// advance persisted local checkpoint
// advance persisted local checkpoint
// don't compare memory in bytes since we load the index with term-dict off-heap
/*
//www.apache.org/licenses/LICENSE-2.0
// make sure we have at least 2 segments to ensure we do an actual merge to kick out all postings for
// soft deletes
// prune away a single ID
// delete it
// drop all ids
// first add a doc such that we can force merge
/*
//www.apache.org/licenses/LICENSE-2.0
// the locked down engine should still point to the previous commit
// Close and reopen the main engine
// the locked down engine should still point to the previous commit
// advance persisted local checkpoint
// we don't want the assertion to trip in this test
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// don't wrap if there is nothing to do
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Advances the global checkpoint and the local checkpoint of a safe commit
// Release some locks
// getting the query has side effects, updating the internal state of the policy
// we only expose the minimum sequence number to the merge policy if the retention lock is not held
// setup leases where the minimum retained sequence number is more than the policy dictated by the global checkpoint
// set the local checkpoint of the safe commit to more than the policy dicated by the global checkpoint
// leases are for more than the local checkpoint
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// minimum number of bytes that this fielddata instance is expected to require
// the segments are force merged to a single segment so that the sorted binary doc values can be asserted within a single segment.
// Previously we used the SlowCompositeReaderWrapper but this is an unideal solution so force merging is a better idea.
// Some impls (FST) return size 0 and some (PagedBytes) do take size in the case no actual data is loaded
/*
//www.apache.org/licenses/LICENSE-2.0
// LogByteSizeMP to preserve doc ID order
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//d.add(new StringField("value", one(), Field.Store.NO)); // MISSING....
// TODO: Have tests with more docs for sorting
//d.add(new StringField("value", one(), Field.Store.NO)); // MISSING
// missing value is set to an actual value
// First segment
// Second segment
// Third segment
// 3 b/c 1 segment level caches and 1 top level cache
// in case of doc values, we don't cache atomic FD, so only the top-level cache is there
// Now only 3 segment level entries, only the toplevel reader has been closed, but the segment readers are still used by IW
/*
//www.apache.org/licenses/LICENSE-2.0
// test remove duplicate value
// Test SortedBinaryDocValues's decoding:
// Test whether ScriptDocValues.BytesRefs makes a deepcopy
/*
//www.apache.org/licenses/LICENSE-2.0
// Testing SortedSetDVOrdinalsIndexFieldData:
// Testing PagedBytesIndexFieldData
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// test # docs with value
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// missing
//d.add(new StringField("value", one(), Field.Store.NO)); // MISSING....
/*
//www.apache.org/licenses/LICENSE-2.0
// copy the ifdService since we can set the listener only once.
// copy the ifdService since we can set the listener only once.
// set it the first time...
// now set it again and make sure we fail
// all well
// no exception
/*
//www.apache.org/licenses/LICENSE-2.0
/** Returns an implementation based on paged bytes which doesn't implement WithOrdinals in order to visit different paths in the code,
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// existing values
// non-existing values
/*
//www.apache.org/licenses/LICENSE-2.0
// remap the ordinals in case we have gaps?
// 0
// 1
// 3
// 3
// 4
// 5
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we need the default codec to check for singletons
/*
//www.apache.org/licenses/LICENSE-2.0
// Index some documents
// check request cache stats are clean
// Search for a range and check that it missed the cache (since its the
// first time it has run)
// Search again and check it hits the cache
// Index some more documents in the query range and refresh
// Search again and check the request cache for another miss since request cache should be invalidated by refresh
/*
//www.apache.org/licenses/LICENSE-2.0
//test that we can parse what we print out
//we need to move to the next token, the start object one that we manually added is not expected
// we are unlucky and our random mutation is equal to our mutation, try the other candidate
//meta fields are single value only, besides _ignored
/*
//www.apache.org/licenses/LICENSE-2.0
//test that we can parse what we print out
//print the parsed object out and test that the output is the same as the original output
//check that the source stays unchanged, no shuffling of keys nor anything like that
// We don't expect to retrieve the index/type/id of the GetResult because they are not rendered
// by the toXContentEmbedded method.
// Test that we can parse the result of toXContentEmbedded()
//print the parsed object out and test that the output is the same as the original output
//check that the source stays unchanged, no shuffling of keys nor anything like that
// serializes and deserializes with streamable, then prints back to xcontent
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// case 1: a simple binary value
// case 2: a value that looks compressed: this used to fail in 1.x
// after 5.x
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// for some reason, ESTestCase doesn't provide randomFloatBetween
/*
//www.apache.org/licenses/LICENSE-2.0
// boolean fields are indexed and have doc values by default
// now change some parameters
// omit "false"/"true" here as they should still be parsed correctly
// omit "false"/"true" here as they should still be parsed correctly
// after 5.x
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//unable to assert about context, covered in a REST test
//unable to assert about context, covered in a REST test
//"41.12,-71.34"
//unable to assert about geofield content, covered in a REST test
//unable to assert about weight, covered in a REST test
//unable to assert about weight, covered in a REST test
// empty inputs are ignored
// null inputs are ignored
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Check json serialization
// Check data parsing
// new field has doc values
// should go to the root doc
// should go to the parent doc
// should go to the current doc
// no exception
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Fields with no value indexed.
// Create an index with some docValues
// Create the doc values reader
// Read index and check the doc values
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// stage1 mapping should not have been modified
// but merged should
// ok that's expected
// not in the mapping yet, try again
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: make this a real unit test
// no update!
// Verify in the case where only a single type is allowed that the _id field is added to nested documents:
// Nested document:
// Root document:
// creates an object mapper, which is about 100x harder than it should be....
// parses without error
// reparse it
// in this case, we analyze the type object as the actual document, and ignore the other same level fields
// when the type is not the first one, we don't confuse it...
// Even though we matched the dynamic format, we do not match on numbers,
// which are too likely to be false positives
// We should have generated a date field
// no update since we reused the existing type
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we don't use indexRandom because the order of requests is important here
// general case, the parsing code complains that it can't parse "bar" as a "long"
// rare case: the node that processes the index request doesn't have the mappings
// yet and sends a mapping update to the master node to map "bar" as "text". This
// fails as it had been already mapped as a long by the previous index request.
// we don't use indexRandom because the order of requests is important here
/*
//www.apache.org/licenses/LICENSE-2.0
// foo is already defined in the mappings
// original mapping not modified
// but we have an update
// Make sure that mapping updates are incremental, this is important for performance otherwise
// every new field introduction runs in linear time with the total number of fields
// original mapping not modified
// but we have an update
// foo is NOT in the update
// original mapping not modified
// but we have an update
// original mapping not modified
// but we have an update
// original mapping not modified
// but we have an update
// original mapping not modified
// but we have an update
// Even if the dynamic type of our new field is long, we already have a mapping for the same field
// of type string so it should be mapped as a string
// float
// double
// double that can be accurately represented as a float
// float detected through numeric detection
// inherited from dynamic date format
// inherited from dynamic date format since the mapping in the template did not specify a format
// not inherited from the dynamic date format since the template defined an explicit format
// https://github.com/elastic/elasticsearch/issues/18625
// elasticsearch used to apply templates that do not have a match_mapping_type first
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// if a wrong match type is specified, we ignore the template
// type-based template
// name-based template
// path-based template
// regex matching
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Let's add a Dummy Point
// Let's add a Dummy Shape
// Let's add a Original String
// ignore this for now
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// handled in post parse
/*
//www.apache.org/licenses/LICENSE-2.0
// test if the highlighting is excluded when we use wildcards
// make sure it is not excluded when we explicitly provide the fieldname
// make sure it is not excluded when we explicitly provide the fieldname and a wildcard
/*
//www.apache.org/licenses/LICENSE-2.0
// Like a String mapper but with very few options. We just use it to test if highlighting on a custom string mapped field works as expected.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//double check that submitting the filtered mappings to an unfiltered index leads to the same get field mappings output
//as the one coming from a filtered index with same mappings
//double check that submitting the filtered mappings to an unfiltered index leads to the same field_caps output
//as the one coming from a filtered index with same mappings
// properties.value is an object field in the new mapping
//check that the returned filtered mappings are still valid mappings by submitting them and retrieving them back
//the mappings are returned unfiltered for this index, yet they are the same as the previous ones that were returned filtered
/*
//www.apache.org/licenses/LICENSE-2.0
// and now corner cases
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Add an alias 'alias' to the concrete field 'foo'.
// Check that the alias refers to 'foo'.
// Update the alias to refer to a new concrete field 'bar'.
// Check that the alias now refers to 'bar'.
// Add an alias 'alias' to the concrete field 'foo'.
// Check that the alias maps to this field type.
// Update the boost for field 'foo'.
// Check that the alias maps to the new field type.
// expected
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// doc values are enabled by default, but in this test we disable them; we should only have 2 points
// doc values are enabled by default, but in this test we disable them; we should only have 2 points
/**
// explicit false accept_z_value test
// test geohash as keyword
// test geohash as string
// create index and add random test points
// TODO these tests are bogus and need to be Fix
// query by geohash subfield
// query by latlon subfield
// after 5.x
// Shouldn't matter if we specify the value explicitly or use null value
// Shouldn't matter if we specify the value explicitly or use null value
/**
//github.com/elastic/elasticsearch/pull/49645
// Set ignore_z_value = false and ignore_malformed = true and test that a malformed point for null_value is normalized.
// geo_point [91, 181] should have been normalized to [89, 1]
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// explicit right orientation test
/**
// explicit false coerce test
/**
// explicit false accept_z_value test
/**
// explicit false ignore_malformed test
// verify nothing changed
// change mapping; orientation
// after 5.x
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// unset cluster setting
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// it would be nice to check the entire serialized default mapper, but there are
// a whole lot of bogus settings right now it picks up from calling super.doXContentBody...
/*
//www.apache.org/licenses/LICENSE-2.0
// if the list includes a prefix query we fallback to a bool query
// same lo/hi values but inclusive=false so this won't match anything
// Upper bound is the min IP and is not inclusive
// Lower bound is the max IP and is not inclusive
// same lo/hi values but inclusive=false so this won't match anything
// same lo/hi values but inclusive=false so this won't match anything
// lower bound is ipv4, upper bound is ipv6
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// There are conflicts, so the `name.not_indexed3` has not been added
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Empty name not allowed in index created after 5.0
/*
//www.apache.org/licenses/LICENSE-2.0
// current impl ignores args and shourd always return INTERSECTS
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// explicit right orientation test
/**
// explicit false coerce test
/**
// explicit false accept_z_value test
/**
// explicit false ignore_malformed test
// 70m is more precise so it wins
// distance_error_pct was not specified so we expect the mapper to take the highest precision between "precision" and
// "tree_levels" setting distErrPct to 0 to guarantee desired precision
// 70m is less precise so it loses
// 70m is more precise so it wins
/* 50m is default */
/* 50m is default */
// verify nothing changed
// correct mapping
// after 5.x
// term strategy changes the default for points_only, check that we handle it correctly
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Boost is updateable, so no exception should be thrown.
// Store is not updateable, so we expect an exception.
/*
//www.apache.org/licenses/LICENSE-2.0
// no exception
/**
// adding one more field should trigger exception
// no exception
// no exception
// partitioned index must have routing
// valid partitioned index
// Set the total fields limit to the number of non-alias fields, to verify that adding
// a field alias pushes the mapping over the limit.
// now reload, this should change the tokenfilterFactory inside the analyzer
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// first check that for newer versions we throw exception if copy_to is found within multi field
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// reparse it
// The underlying order of the fields in multi fields in the mapping source should always be consistent, if not this
// can to unnecessary re-syncing of the mappings between the local instance and cluster state
// underlying map is LinkedHashMap, so this ok:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// default limit allows at least two nested fields
// explicitly setting limit to 0 prevents nested fields
// setting limit to 1 with 2 nested fields fails
// do not check nested fields limit if mapping is not updated
// parsing a doc with No. nested objects > defaultMaxNoNestedDocs fails
// setting limit to allow only two nested objects in the whole doc
// parsing a doc with 2 nested objects succeeds
// parsing a doc with 3 nested objects fails
// setting limit to allow only two nested objects in the whole doc
// parsing a doc with 2 nested objects succeeds
// parsing a doc with 3 nested objects fails
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// not supported as of 5.0
/**
// after version 5
// the following two strings are in-range for a long after coercion
/*
//www.apache.org/licenses/LICENSE-2.0
// current impl ignores args and should always return INTERSECTS
// these will lose precision if they get treated as a double
// make sure the accuracy loss of half floats only occurs at index time
// this test checks that searching half floats yields the same results as
// searching floats that are rounded to the closest half float
// Make sure we construct the IndexOrDocValuesQuery objects with queries that match
// the same ranges
/*
//www.apache.org/licenses/LICENSE-2.0
// GIVEN an enriched mapping with "baz" new field
// WHEN merging mappings
// THEN "baz" new field is added to merged mapping
// GIVEN a mapping with "foo" field disabled
// WHEN merging mappings
// THEN a MapperException is thrown with an excepted message
// GIVEN a mapping with "disabled" field enabled
// WHEN merging mappings
// THEN a MapperException is thrown with an excepted message
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// test full name
/*
//www.apache.org/licenses/LICENSE-2.0
// verify more complex path_match expressions
/*
//www.apache.org/licenses/LICENSE-2.0
// date_range ignores the coerce parameter and epoch_millis date format truncates floats (see issue: #14641)
// test null value for min and max
// test null max value
// test null range
// test no bounds specified
// if type is date_range we check that the mapper contains the default format and locale
// otherwise it should not contain a locale or format
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// need to increase once more, otherwise interval is empty because edge values are exclusive
/**
/**
// quit test for other range types
// dates will break the default format, month/day of month is turned around in the format
// setting mapping format which is compatible with those dates
// See https://github.com/elastic/elasticsearch/issues/25950
/*
//www.apache.org/licenses/LICENSE-2.0
// update with a different explicit value
// update with an implicit value: no change
// update with a different explicit value
// update with an implicit value: no change
// no update if formatters are not set explicitly
// no update if templates are not set explicitly
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// using default of true
// not changing is ok
// not changing is ok
// not changing is ok
// extra end object (invalid JSON)
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Stop filter remains in server as it is part of lucene-core
// special case: default index analyzer
// special case: default search analyzer
// special case: default index/search analyzer
// no exception this time
// Empty name not allowed in index created after 5.0
// will be replaced with MockSynonymAnalyzer
// https://github.com/elastic/elasticsearch/issues/43976
// will be replaced with MockSynonymAnalyzer
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// check AnalysisMode.ALL works
// check that "analyzer" set to something that only supports AnalysisMode.SEARCH_TIME or AnalysisMode.INDEX_TIME is blocked
// check AnalysisMode.ALL and AnalysisMode.SEARCH_TIME works
// check that "analyzer" set to AnalysisMode.INDEX_TIME is blocked
// check that "analyzer" set to AnalysisMode.INDEX_TIME is blocked if there is no search analyzer
// check AnalysisMode.INDEX_TIME is okay if search analyzer is also set
// For indices created prior to 8.0, we should only emit a warning and not fail parsing.
// For indices created in 8.0 or later, we should throw an error.
/*
//www.apache.org/licenses/LICENSE-2.0
// 0x23 is equal to '#'
// prepend a zero to make sure leading zeros are not ignored
/*
//www.apache.org/licenses/LICENSE-2.0
//test store, ... all the parameters that are not to be changed just like in other fields
// simulate like in MetaDataMappingService#putMapping
// expected
// make sure simulate flag actually worked - no mappings applied
// expected
// the version should be unchanged after putting the same mapping again
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// don't start with START_OBJECT to provoke exception
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// https://github.com/elastic/elasticsearch/issues/7240
//we didn't set minimum should match initially, there are no should clauses so it should be 0
/**
// if it's empty we rewrite to match all
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// since age is automatically registered in data, we encode it as numeric
/*
//www.apache.org/licenses/LICENSE-2.0
// DATE_NANOS_FIELD_NAME
// nano_dates long accept milliseconds since epoch
// if (fieldName.equals(DATE_FIELD_NAME))
// NANOSECONDS
// origin as string
// origin as long
// origin as string
// origin as long
// origin as string
// origin as array
// origin as object
/*
//www.apache.org/licenses/LICENSE-2.0
// also sometimes test wildcard patterns
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// should be all good
/*
//www.apache.org/licenses/LICENSE-2.0
/** Randomly generate either NaN or one of the two infinity values. */
// check the top-left/bottom-right combination of setters
// check the bottom-left/ top-right combination of setters
// TODO: remove the if statement once we always use LatLonPoint
// just check if we can parse the query
// toXContent generates the query in geojson only; for now we need to test against the expected
// geojson generated content
// parse with wkt
// check the builder's generated geojson content against the expected json output
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: remove the if statement once we always use LatLonPoint
// The parsedQuery contains IndexOrDocValuesQuery, which wraps LatLonPointDistanceQuery which in turn has default visibility,
// so we cannot access its fields directly to check and have to use toString() here instead.
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: Test case when there are no docValues
// This is a temporary fix because sometimes the RandomShapeGenerator
// returns null. This is if there is an error generating the polygon. So
// in this case keep trying until we successfully generate one
/*
//www.apache.org/licenses/LICENSE-2.0
// force the new shape impl
// LatLonShape does not support MultiPoint queries
// LatLonShape does not support CONTAINS:
// Logic for doToQuery is complex and is hard to test here. Need to rely
// on Integration tests to determine if created query is correct
// TODO improve GeoShapeQueryBuilder.doToQuery() method to make it
// easier to test here
// see #3878
/*
//www.apache.org/licenses/LICENSE-2.0
// see #7686.
// check that type that is not an array and also ids that are numbers are parsed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//fields is printed out as an object but parsed into a List where order matters, we disable shuffling
// not supported by nested queries
// Random script fields deduped on their field name.
/*
//www.apache.org/licenses/LICENSE-2.0
// term1 [] term2/term3/term4 term5
// term1 term2:2/term3 term4 term5
// term1 [] term2:4/term3 [] [] term4 term5
// term1 term2:2/term3 term4 [] term5
/*
//www.apache.org/licenses/LICENSE-2.0
// just change name/boost
// This looks weird, but it's fine, because the innermost fixField wins
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// all queries except the last should be TermQuery or SynonymQuery
// the last query should be PrefixQuery
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// calculate expected minimumShouldMatch value
// depending on analyzer being set or not we can have term lowercased along the way, so to simplify test we just
// compare lowercased terms here
// Booleans become t/f when querying a boolean field
// TODO
// triggers a different code path
// no exception
// no exception
// no exception
// field boost is ignored on a single term query
// field boost is ignored on phrase query
/**
/**
/**
// if we hit a date field with "now", this should diable cachability
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// add repeat token
/*
//www.apache.org/licenses/LICENSE-2.0
// MLT only supports string fields, unsupported fields are tested below
// we also preset the item requests
// and for the unlike items too
// indexed item or artificial document
// if no field is specified MLT uses all mapped fields for this item
// per field analyzer
// like field is required
// for the default field
// fix the analyzer?
/**
//doc contains arbitrary content, anything can be added to it and no exception will be thrown
/**
// we rely on integration tests for a deeper check here
// Reset the default value
/**
// items are always fetched
// specifically trigger case where query is cacheable
// specifically trigger case where query is not cacheable
/*
//www.apache.org/licenses/LICENSE-2.0
// field with random boost
// sets other parameters of the multi match query
// test with fields with boost and patterns delegated to the tests further below
// we rely on integration tests for deeper checks here
// STRING_FIELD_NAME_2 is a keyword field
/**
// triggers a different code path
// no exception
// no exception
// default value `*` sets leniency to true
// `*` is in the list of the default_field => leniency set to true
// should fail because lenient defaults to false
// explicitly sets lenient
// Reset to the default value
/**
// if we hit a date field with "now", this should diable cachability
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO how to assert this?
// have to rewrite again because the provided queryBuilder hasn't been rewritten (directly returned from
// doCreateTestQueryBuilder)
/**
// WrapperQueryBuilder makes sure we always rewrite
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// min length 4 makes sure that the text is not an operator (AND/OR) so toQuery won't break
// also avoid "now" since we might hit dqte fields later and this complicates caching checks
//we only use string fields (either mapped or unmapped)
// nothing yet, put additional assertions here.
// Tests fix for https://github.com/elastic/elasticsearch/issues/29403
// simple multi-term
// simple multi-term with phrase query
// simple with additional tokens
// complex
// no parent should cause guinea and pig to be treated as separate tokens
// span query
// span query with slop
/**
// no exception
// no exception
// no exception
//  type=phrase
// non-prefix queries do not work with range queries simple syntax
// throws an exception when lenient is set to false
// Prefix
// Wildcard
// Fuzzy
// Range
// Term
// prefix
// Fuzzy
// Default unmapped field
// Unmapped prefix field
// Unmapped fields
// Multi block
// default value `*` sets leniency to true
// `*` is in the list of the default_field => leniency set to true
// Reset the default value
/**
// Prefix
// Now check what happens if the quote field does not exist
// Reset the default value
/**
// if we hit all fields, this should contain a date field and should diable cachability
// if we hit a date field with "now", this should diable cachability
// everything else is fine on all fields
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// We make sure this query has no types to avoid deprecation warnings in the
// tests that use this method.
/**
// for now, only use String Rangequeries for MultiTerm test, numeric and date makes little sense
// see issue #12123 for discussion
/*
//www.apache.org/licenses/LICENSE-2.0
// switch between numeric and date ranges
// use mapped integer field for numeric range queries
// use mapped date field, using date string representation
// Create timestamp option only then we have a date mapper,
// otherwise we could trigger exception.
// we have to normalize the incoming value into milliseconds since it could be literally anything
// todo can't check RangeFieldQuery because its currently package private (this will change)
/**
// since age is automatically registered in data, we encode it as numeric
// We test 01/01/2012 from gte and 2030 for lt
// Test Invalid format
// TODO what else can we assert
// no exception
// Range query with open bounds rewrite to an exists query
/**
// queries on date fields using "now" should not be cached
/*
//www.apache.org/licenses/LICENSE-2.0
// The purpose of this test case is to test RangeQueryBuilder.getRelation()
// Whether it should return INTERSECT/DISJOINT/WITHIN is already tested in
// RangeQueryBuilderTests
// can't make assumptions on a missing reader, so it must return INTERSECT
// no values -> DISJOINT
/*
//www.apache.org/licenses/LICENSE-2.0
// mapped or unmapped fields
/*
//www.apache.org/licenses/LICENSE-2.0
// we expect the underlying exception here
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//script_score.script.params can contain arbitrary parameters. no error is expected when
//adding additional objects within the params object.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we avoid strings with "now" since those can have different caching policies that are checked elsewhere
// Check operator handling, and default field handling.
// to avoid occasional cases
// in setup where we didn't
// add types but strict field
// resolution
/**
/*
// check special case: one term & one field should get simplified to a TermQuery
// Prefix
// Fuzzy
// non-phrase won't detect multi-word synonym because of whitespace splitting
// phrase will pick it up
// phrase with slop
// Now check what happens if the quote field does not exist
// default value `*` sets leniency to true
// `*` is in the list of the default_field => leniency set to true
// Reset to the default value
/**
/**
// if we hit all fields, this should contain a date field and should diable cachability
// if we hit a date field with "now", this should diable cachability
// everything else is fine on all fields
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//verify that the result is still a span query, despite the boost that might get set (SpanBoostQuery rather than BoostQuery)
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// also test negative values, they should implicitly be changed to 0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// generate unicode string in 10% of cases
/**
// we need same field name in all clauses, so we only randomize value
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// generate unicode string in 10% of cases
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// terms query or lookup query
// make between 0 and 5 different values of the same type
// TODO: needs testing for date_nanos type
// right now the mock service returns us a list of strings
// we only do the check below for string fields (otherwise we'd have to decode the values)
// expected returned terms depending on whether we have a terms query or a terms lookup query
// terms lookup removes null values
// At least one name is good
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// specifically trigger the two cases where query is cacheable
// also test one case where query is not cacheable
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// WrapperQueryBuilder adds some optimization if the wrapper and query builder have boosts / query names that wraps
// the actual QueryBuilder that comes from the binary blob into a BooleanQueryBuilder to give it an outer boost / name
// this causes some queries to be not exactly equal but equivalent such that we need to rewrite them before comparing.
// null == *:*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// do not shuffle fields that may contain arbitrary content
//script_score.script.params can contain arbitrary parameters. no error is expected when adding additional objects
//within the params object. Score functions get parsed in the data nodes, so they are not validated in the coord node.
/**
// sometimes provide no seed
// guaranteed to exist
/**
/**
/*
/*
//without a functions array, we support only a single function, weight can't be associated with the function either.
// don't permit an array of factors
// this should be equivalent to the same with a match_all query
/**
//verify that an error is thrown rather than setting the query twice (https://github.com/elastic/elasticsearch/issues/16583)
/**
/**
/**
// check the two non-cacheable cases explicitly
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// now test all together
// otherwise we could get a cached entry that does not have approximations
// check for document that has no matching function
// check for document that has a matching function
// test that field_value_factor function throws an exception on negative scores
// test that field_value_factor function using modifier ln throws an exception on negative scores
// test that field_value_factor function using modifier log throws an exception on negative scores
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// only the superclass has state
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// it's not important how many slices there are, we just need a number for forSlice
/*
//www.apache.org/licenses/LICENSE-2.0
// XContentEquivalence fails in the exception case, due to how exceptions are serialized.
/*
//www.apache.org/licenses/LICENSE-2.0
// failures are tested separately, so we can test XContent equivalence at least when we have no failures
// we test includeCreated params in the Status tests
// If one of them is null both of them should be null
/**
//with random fields insertion in the inner exceptions, some random stuff may be parsed back as metadata,
//but that does not bother our assertions, as we only want to test that we don't break.
//exceptions are not of the same type whenever parsed back
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Just check the message because we're not testing exception serialization in general here.
// These all should be believably small because we sum them if we have multiple workers
// smallest unit of time during toXContent is Milliseconds
// failures are tested separately, so we can test xcontent equivalence at least when we have no failures
/**
//with random fields insertion in the inner exceptions, some random stuff may be parsed back as metadata,
//but that does not bother our assertions, as we only want to test that we don't break.
//exceptions are not of the same type whenever parsed back
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Nothing else to randomize
// No extra assertions needed
// TODO: Implement standard to/from x-content parsing tests
/*
//www.apache.org/licenses/LICENSE-2.0
// The whole thing succeeded so we should have got the success
// Rethrow any failures just so we get a nice exception if there were any. We don't expect any though.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// There sometimes will be a warning about specifying types in reindex requests being deprecated.
// Unsupported place to put query
//example.com:9200");
//:9200"));
//example.com"));
//example.com:9200");
// Didn't set the timeout so we should get the default
// Didn't set the timeout so we should get the default
//other.example.com:9201");
//[::1]:9201");
//other.example.com:9201/");
//other.example.com:9201/proxy-path/");
//[host]:[port](/[pathPrefix])? but was [https]",
//localhost:9200");
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: Implement standard to/from x-content parsing tests
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
// Rethrottle on a random number of threads, one of which is this thread.
// Other threads should finish up quickly as they are checking the same AtomicBoolean.
// Thread pool that returns a ScheduledFuture that claims to have a negative delay
// Have the task use the thread pool to delay a task that does nothing
// Even though the future returns a negative delay we just return 0 because the time is up.
/*
//www.apache.org/licenses/LICENSE-2.0
// just append one to the translog so we can assert below
// original append-only arrives after recovery completed
// prevent the indexing on the primary from returning (it was added to Lucene and translog already)
// to make sure that this operation is replicated to the replica via recovery, then via replication.
// force an update of the timestamp
// lets check if that also happens if no translog record is replicated
/*
// simulate a background global checkpoint sync at which point we expect the global checkpoint to advance on the replicas
/**
// allow to add Noop
// start with the primary only so two first failures are replicated to replicas via recovery from the translog of the primary.
// we flush at the end of peer recovery
// the failure replicated directly from the replication channel.
/**
// add some replicas
// Make sure that replica2 receives translog ops (eg. op2) from replica1
// and does not overwrite its stale operation (op1) as it is trimmed.
// wait until resync completed.
// Make sure that peer-recovery transfers all but non-overridden operations.
/**
// I think we can just set this to something very small (10ms?) and also set ThreadPool#ESTIMATED_TIME_INTERVAL_SETTING to 0?
// delete arrives on replica first.
// index arrives on replica lately.
/**
// Append-only request - without id
/*
//www.apache.org/licenses/LICENSE-2.0
// simulate a background global checkpoint sync at which point we expect the global checkpoint to advance on the replicas
// replica has something to catch up with, but since we trimmed the primary translog, we should fall back to full recovery
/*
// slip the extra document into the replica
// randomly introduce a conflicting document
// simulate docs that were inflight when primary failed, these will be rolled back
// check that local checkpoint of new primary is properly tracked after primary promotion
// index some more
// As a replica keeps a safe commit, the file-based recovery only happens if the required translog
// for the sequence based recovery are not fully retained and extra documents were added to the primary.
// Make sure the global checkpoint on the new primary is persisted properly,
// otherwise the deletion policy won't trim translog
// We need an extra flush to advance the min_retained_seqno on the new primary so ops-based won't happen.
// The min_retained_seqno only advances when a merge asks for the retention query.
// We also need to make sure that there is no retention lease holding on to any history. The lease for the old primary
// expires since there are no unassigned shards in this replication group).
//noinspection OptionalGetWithoutIsPresent since there must be at least one lease
// Make sure that flushing on a recovering shard is ok.
// simulate docs that were inflight when primary failed, these will be rolled back
// Recover a replica should rollback the stale documents
// Index more docs - move the global checkpoint >= seqno of the stale operations.
// Recover a replica again should also rollback the stale documents.
// TODO: check translog trimming functionality once rollback is implemented in Lucene (ES trimming is done)
// simulate docs that were inflight when primary failed
// have to replicate to another replica != newPrimary one - the subject to trim
// check translog on replica is trimmed
// simulate a background global checkpoint sync at which point we expect the global checkpoint to advance on the replicas
// wait for the pending ops to "hang"
// index some more
// index some more
// now recovery can finish
// make sure we release indexers before closing
// index a doc which is not part of the snapshot, but also does not complete on replica
// the pending doc is latched in the engine
// unblock indexing for the next doc
// recovery has not completed, therefore the global checkpoint can have advanced on the primary
// the pending document is not done, the checkpoints can not have advanced on the replica
// wait for recovery to enter the translog phase
// wait for the translog phase to complete and the recovery to block global checkpoint advancement
// recovery is now in the process of being completed, therefore the global checkpoint can not have advanced on the primary
// the global checkpoint advances can only advance here if a background global checkpoint sync fires
// deliver via normal replication
// we flush quite often
// we flush quite often
// replica swaps engine during rollback
// it maybe that not ops have been transferred, block now
/*
//www.apache.org/licenses/LICENSE-2.0
// we need to make changes to retention leases to sync it to replicas
// since we don't sync retention leases when promoting a new primary.
// we turn off the translog retention policy using the generic threadPool
/*
//www.apache.org/licenses/LICENSE-2.0
// rewrites to a TermQuery
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// check that synonym query is used for a single field
// check that blended term query is used for multiple fields
/*
//www.apache.org/licenses/LICENSE-2.0
// we automatically add a filter since the inner query might match non-nested docs
// this time we do not add a filter since the inner query only matches inner docs
// we need to add the filter again because of include_in_parent
// we need to add the filter again because of include_in_root
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// GeoPoint doesn't care about coordinate system bounds, this simply validates equality and hashCode.
// Based on #5390
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// This doc will not be included, because it doesn't have nested docs
// Some garbage docs, just to check if the NestedFieldComparator can deal with this.
// Moved to method, because floating point based XFieldComparatorSource have different outcome for SortMode avg,
// than integral number based implementations...
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we need to have a bit more segments than what RandomIndexWriter would do by default
// This doc will not be included, because it doesn't have nested docs
// Some garbage docs, just to check if the NestedFieldComparator can deal with this.
// Specific genre
// reverse sort order
// Specific genre and reverse sort order
// Nested filter + query
// Multiple Nested filters + query
// Nested filter + Specific genre
/*
//www.apache.org/licenses/LICENSE-2.0
// https://github.com/elastic/elasticsearch/issues/7644
// let's create two dummy search stats with groups
// adding these two search stats and checking group stats are correct
// another call, adding again ...
// making sure stats2 was not affected (this would previously return 2!)
// adding again would then return wrong search stats (would return 4! instead of 3)
/*
//www.apache.org/licenses/LICENSE-2.0
// The missing piece to fill all bits.
// Tests with released internal bitset.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// set the sync interval high so it does not execute during this test. This only allows the global checkpoint to catch up
// on a post-operation background sync if translog durability is set to sync. Async durability relies on a scheduled global
// checkpoint sync to allow the information about persisted local checkpoints to be transferred to the primary.
/*
// prevent global checkpoint syncs between all nodes
// restore global checkpoint syncs between all nodes
// start concurrent indexing threads
// synchronize the start of the threads
// wait for the threads to finish
// the shard is initializing
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// make sure we always index the last seqNo to simplify maxSeq checks
// make sure we always index the last seqNo to simplify maxSeq checks
// sychronize starting with the test thread
// synchronize with the test thread checking if we are no longer waiting
// synchronize starting with the waiting thread
// synchronize with the waiting thread to mark that it is complete
/*
//www.apache.org/licenses/LICENSE-2.0
//github.com/elastic/elasticsearch/issues/48701")
/*
// Change the node ID so that the persisted retention lease no longer applies.
/*
//www.apache.org/licenses/LICENSE-2.0
// must be set in each test
// not NO_OPS_PERFORMED since this always results in file-based recovery
// simulate a safe commit that is behind the given global checkpoint, so that no files need to be transferrred
// simulate a later safe commit containing no documents, which is always better to transfer than any ops
/*
//www.apache.org/licenses/LICENSE-2.0
// we do not want to hold a lock on the replication tracker in the callback!
// assert that the new retention lease callback was invoked
// reset the invocation marker so that we can assert the callback was not invoked when renewing the lease
/*
// we do not want to hold a lock on the replication tracker in the callback!
// assert that the new retention lease callback was invoked
// reset the invocation marker so that we can assert the callback was not invoked when removing the lease
// renew the lease
// now force the lease to expire
// leases do not expire on replicas until synced from the primary
/**
// synchronize the threads invoking ReplicationTracker#persistRetentionLeases(Path path)
// wait for all the threads to finish
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// note: allocations can never be empty in practice as we always have at least one primary shard active/in sync
// it is however nice not to assume this on this level and check we do the right thing.
// increment checkpoints
// now insert an unknown active/insync id , the checkpoint shouldn't change but a refresh should be requested.
// first check that adding it without the master blessing doesn't change anything.
// now notify for the new id
// now it should be incremented
// there is a shard copy pending in sync, the global checkpoint can not advance
// we are implicitly marking the pending in sync copy as in sync with the current global checkpoint, no advancement should occur
// now we expect that the global checkpoint would advance
// now update all knowledge of all shards
// update again
// now remove shards
// we added 10 to make sure it's advanced in the second time
// synchronize starting with the test thread
// synchronize with the test thread checking if we are no longer waiting
// synchronize starting with the waiting thread
// normal path, shard catches up
// synchronize with the waiting thread to mark that it is complete
// master changes its mind and cancels the allocation
// synchronize starting with the test thread
// synchronize with the test thread checking if we are interrupted
// synchronize starting with the waiting thread
// synchronize with the waiting thread to mark that it is complete
// first we assert that the in-sync and tracking sets are set up correctly
// now we will remove some allocation IDs from these and ensure that they propagate through
/*
// the tracking allocation IDs should play no role in determining the global checkpoint
// now we are going to add a new allocation ID and bring it in sync which should move it to the in-sync allocation IDs
// using a different length than we have been using above ensures that we can not collide with a previous allocation ID
/*
/**
// simulate transferring the global checkpoint to the new primary after finalizing recovery before the handoff
// cluster state update after primary context handoff
// abort handoff, check that we can continue updates and retry handoff
// do another handoff
// send primary context through the wire
// apply cluster state update on old primary while primary context is being transferred
// activate new primary
// apply cluster state update on new primary so that the states on old and new primary are comparable
// apply cluster state update on new primary while primary context is being transferred
// activate new primary
// apply cluster state update on old primary so that the states on old and new primary are comparable
// apply cluster state update on both copies while primary context is being transferred
// no cluster state update
/*
// we can however assert that shared knowledge of the local checkpoint and in-sync status is equal
// ensure we do not duplicate an allocation ID
// Leases for assigned replicas do not expire
// Leases that don't correspond to assigned replicas, however, are expired by this time.
// Leases still don't expire
// Also leases are renewed before reaching half the expiry time
//noinspection OptionalGetWithoutIsPresent
// Leases still don't expire
// ... and any extra peer recovery retention leases are expired immediately since the shard is fully active
//noinspection OptionalGetWithoutIsPresent
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
/*
/*
/*
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// the retention leases on the shard should be persisted
// we should forward the request containing the current retention leases to the replica
// the retention leases on the shard should be updated
// the retention leases on the shard should be persisted
// the result should indicate success
/*
//www.apache.org/licenses/LICENSE-2.0
// we will add multiple retention leases and expect to see them synced to all replicas
// simulate a peer recovery which locks the soft deletes policy on the primary
// check retention leases have been written on the primary
// check current retention leases have been synced to all replicas
// check retention leases have been written on the replica
// simulate a peer recovery which locks the soft deletes policy on the primary
// simulate a peer recovery which locks the soft deletes policy on the primary
// check retention leases have been written on the primary
// check current retention leases have been synced to all replicas
// check retention leases have been written on the replica
// we will add multiple retention leases, wait for some to expire, and assert a consistent view between the primary and the replicas
// update the index for retention leases to live a long time
// check current retention leases have been synced to all replicas
// update the index for retention leases to short a long time, to force expiration
// sleep long enough that the current retention lease has expired
// now that all retention leases are expired should have been synced to all replicas
// we will add multiple retention leases and expect to see them synced to all replicas
// put a new lease
// now renew all existing leases; we expect to see these synced to the replicas
// check all retention leases have been synced to all replicas
/*
// when we increase the number of replicas below we want to exclude the replicas from being allocated so that they do not recover
// cause some recoveries to fail to ensure that retention leases are handled properly when retrying a recovery
// return a ConnectTransportException to the START_RECOVERY action
// return an exception to the FINALIZE action
// now allow the replicas to be allocated and wait for recovery to finalize
// check current retention leases have been synced to all replicas
// check retention leases have been written on the replica; see RecoveryTarget#finalizeRecovery
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// the retention leases on the shard should be persisted
// we should forward the request containing the current retention leases to the replica
// we should start with an empty replication response
// the retention leases on the shard should be updated
// the retention leases on the shard should be persisted
// the result should indicate success
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// only listeners waiting on a lower global checkpoint will have been notified
// test the listeners are not invoked twice
// these listeners will have been notified by the second global checkpoint update, and all the other listeners should not be
// closing should also not notify the listeners
// these listeners should have been notified that we closed, and all the other listeners should not be
// the listener should be notified immediately
// test the listeners are not invoked twice
// closing should also not notify the listeners
// the listener should be notified immediately
// test the listeners are not invoked twice
// we are going to synchronize the actions of three threads: the updating thread, the listener thread, and the main test thread
// synchronize starting with the listener thread and the main test thread
// synchronize ending with the listener thread and the main test thread
// synchronize starting with the updating thread and the main test thread
// sometimes this will notify the listener immediately
// synchronize ending with the updating thread and the main test thread
// synchronize starting with the updating thread and the listener thread
// synchronize ending with the updating thread and the listener thread
// one last update to ensure all listeners are notified
// wait for all the listeners to be notified
// now shutdown
// ensure the listener notification occurred on the executor
/*
//www.apache.org/licenses/LICENSE-2.0
// this test also tests if calls are correct if one or more listeners throw exceptions
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Should not be able to acquire the lock because it's already open
// Test without the regular shard lock to assume we can acquire it
// (worst case, meaning that the shard lock could be acquired and
// we're green to delete the shard's directory)
// we can't use tlog.needsSync() here since it also takes the global checkpoint into account
// we explicitly want to check here if our durability checks are taken into account so we only
// check if we are synced upto the current write location
// the lastWriteLocaltion has a Integer.MAX_VALUE size so we have to create a new one
// Now, try closing and changing the settings
/* size of the operation + two generations header&footer*/, ByteSizeUnit.BYTES)).build()).get();
// this is async
// this is async
// wait until the roll completes
// size of the operation plus two generations of overhead.
// size of the operation plus header and footer
// A flush stats may include the new total count but the old period count - assert eventually.
// this is all IMC needs to do - check current memory and refresh
// this is all IMC needs to do - check current memory and refresh
/** Check that the accounting breaker correctly matches the segments API for memory usage */
// Generate a couple of segments
// Use routing so 2 documents are guaranteed to be on the same shard
// Test that force merging causes the breaker to be correctly adjusted
// Test that we're now above the parent limit due to the segments
// Test that deleting the index causes the breaker to correctly be decremented
// Accounting breaker should now be 0
// adding a listener expecting a lower global checkpoint should fire immediately
// do not flush
/**
/**
// Skip files added by Lucene's ExtraFS
/*
//www.apache.org/licenses/LICENSE-2.0
// the write thread pool uses a bounded size and can get rejections, see setupThreadPool
// ok, notify future
/**
// we preserve the thread context here so that we have a different context in the call to acquire than the context present
// when the releasable is closed
// test both with and without a executor name
// an operation that is submitted while there is a delay in place should be delayed
// now we will delay operations while the first operation is still executing (because it is latched)
// if we submit another operation, it should be delayed
// we racily submit operations and a delay, and then ensure that all operations were actually completed
// check that all operations completed
/*
// check idempotence
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// current time is mocked through the thread pool
// renew the lease
// now force the lease to expire
// force the retention leases to persist
// the written retention leases should equal our current retention leases
// when we recover, we should recover the retention leases
// we should not recover retention leases when force-allocating a stale primary
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// fail shard
// check state file still exists
// but index can't be opened for a failed shard
// just a sanity check that we impl hashcode
// we could race and assert on the count before the permit is returned
/**
// most of the time this is large enough that most of the time there will be at least one gap
// promote the replica
/*
// promote the replica
/*
// relocation target
// simulate promotion
// started replica
// initializing replica / primary
// relocation source
// advance local checkpoint
// advance local checkpoint
// but you can not increment with a new primary term until the operations on the older primary term complete
// our operation should be blocked until the previous operations complete
// our operation should still be blocked
// now lock acquisition should have succeeded
// if rollback happens we roll translog twice: one when we flush a commit before opening a read-only engine
// and one after replaying translog (upto the global checkpoint); otherwise we roll translog once.
// skip asserting translog and Lucene as we rolled back Lucene but did not execute resync
// create the primary shard with a callback that sets a boolean when the global checkpoint sync is invoked
// add a replica
// set up local checkpoints on the shard copies
// initialize the local knowledge on the primary of the persisted global checkpoint on the replica shard
// initialize the local knowledge on the primary of the persisted global checkpoint on the primary
// simulate a background maybe sync; it should only run if the knowledge on the replica of the global checkpoint lags the primary
// simulate that the background sync advanced the global checkpoint on the replica
// reset our boolean so that we can assert after another simulated maybe sync
// this time there should not be a sync since all the replica copies are caught up with the primary
// most of the time this is large enough that most of the time there will be at least one gap
// ensure that after the local checkpoint throw back and indexing again, the local checkpoint advances
// the two threads synchronize attempting to acquire an operation permit
// we wait for both operations to complete
/*
/***
// try to serialize it to ensure values survive the serialization
// Some path weirdness on windows
// Override two Directory methods to make them fail at our will
// We use AtomicReference here to inject failure in the middle of the test not immediately
// We use Supplier<IOException> instead of IOException to produce meaningful stacktrace
// (remember stack trace is filled when exception is instantiated)
//fileLength method is called during storeStats try block
//it's not called when store is marked as corrupted
//listAll method is called when marking store as corrupted
// refresh on: finalize and end of recovery
// finalizing a replica involves two refreshes with soft deletes because of estimateNumberOfHistoryOperations()
// check time advances
// refresh on: finalize and end of recovery
// check time advances
// It will generate exception
// start finalization of recovery
// recovery can only be finalized after we release the current primaryOperationLock
// recovery can be now finalized
// ensure we wait for all primary operation locks to be acquired
// start recovery thread
// ensure we only transition after pending operations completed
// complete pending operations
// complete recovery/relocation
// ensure relocated successfully once pending operations are done
/*
// manually advance msu for this delete
// isolate the delete in it's own generation
// Flushing a new commit with local checkpoint=1 allows to skip the translog gen #1 in recovery.
// advance local checkpoint
// Advance the global checkpoint to remove the 1st commit; this shard will recover the 2nd commit.
// delete #1 won't be replayed.
// check that local checkpoint of new primary is properly tracked after recovery
// check that local checkpoint of new primary is properly tracked after primary relocation
/* This test just verifies that we fill up local checkpoint up to max seen seqID on primary recovery */
// start a replica shard and index the second doc
// OK!
// we can't issue this request through a client because of the inconsistencies we created with the cluster state
// doing it directly instead
// Index #0, index #1
// stick the global checkpoint here.
// Recovering from store should discard doc #1
// create a gap in the history
// only flush source
// make sure get uses the wrapped reader
// test global ordinals are evicted
// If a field doesn't exist an empty IndexFieldData is returned and that isn't cached:
// ensure we are not influencing the indexing stats
//
// corrupt entry
// translog recovery on the next line would otherwise fail as we are in POST_RECOVERY
// Shard is still inactive since we haven't started recovering yet
// Shard is still inactive since we haven't started recovering yet
// Shard should now be active since we did recover:
// Shard is still inactive since we haven't started recovering yet
// Shard should now be active since we did recover:
// we're only checking that listeners are called when the engine is open, before there is no point
// check that local checkpoint of new primary is properly tracked after recovery
// now check that it's persistent ie. that the added shards are committed
// at least two documents so we have docs to delete
// Need to update and sync the global checkpoint and the retention leases for the soft-deletes retention MergePolicy.
// flush the buffered deletes
// merge them away
// A doc should be more than 100 bytes.
// Do some updates and deletes, then recheck the correlation again.
/**
// no engine
// start shard and perform index check on startup. It enforce shard to fail due to corrupted index files
// check that corrupt marker is there
// Ignored because corrupted shard can throw various exceptions on close
// create corrupted marker
// try to start shard on corrupted files
// try to start another time shard on corrupted files
// check that corrupt marker is there
/**
/**
// last operation can't be a gap as it's not a gap anymore
// this is an update
// advance local checkpoint
// now loop until we are fast enough... shouldn't take long
// now loop until we are fast enough... shouldn't take long
// wait until the thread-pool has moved the timestamp otherwise we can't assert on this below
// make sure we refresh once the shard is inactive
// Forces a refresh with the INTERNAL scope
// Deleting a doc causes its memory to be freed from the breaker
// Here we are testing that a fully deleted segment should be dropped and its memory usage is freed.
// In order to instruct the merge policy not to keep a fully deleted segment,
// we need to flush and make that commit safe so that the SoftDeletesPolicy can drop everything.
// Acquire a new searcher, adding it to the list
// Close one of the readers at random
// re-check because it could have decremented after the check
// We need to wait for all ongoing merges to complete. The reason is that during a merge the
// IndexWriter holds the core cache key open and causes the memory to be registered in the breaker
// Close remaining searchers
// Close shard
// Check that the breaker was successfully reset to 0, meaning that all the accounting was correctly applied
// produce segments
// flush happens in the background using the flush threadpool
// produce segments
// check stats on closed and on opened shard
// engine reference was switched twice: current read/write engine -> ready-only engine -> new read/write engine
// we might have trimmed some operations if the translog retention policy is ignored (when soft-deletes enabled).
/**
// in integration tests, this is done as a listener on IndexService.
// close store.
/**
// entries with corrupted source
/**
// create gaps in sequence numbers
// just like a following shard, we need to skip this check for now.
/*
//www.apache.org/licenses/LICENSE-2.0
/** Separate test class from ShardPathTests because we need static (BeforeClass) setup to install mock filesystems... */
// Sneakiness to install mock file stores so we can pretend how much free space we have on each path.data:
/** Mock file system that fakes usable space for each FileStore */
//", inner);
// Use 2 data paths:
// Make sure all our mocking above actually worked:
// Path a has lots of free space, but b has little, so new shard should go to a:
// Test the reverse: b has lots of free space, but a has little, so new shard should go to b:
// Now a and be have equal usable space; we allocate two shards to the node, and each should go to different paths:
// #11122: this was the original failure: on a node with 2 disks that have nearly equal
// free space, we would always allocate all N incoming shards to the one path that
// had the most free space, never using the other drive unless new shards arrive
// after the first shards started using storage:
// Use 2 data paths:
// Make sure all our mocking above actually worked:
// Path a has lots of free space, but b has little, so new shard should go to a:
// First shard should go to a
// Second shard should go to b
// 2 shards go to 'a' and 1 to 'b'
// Use 2 data paths:
// Use 2 data paths:
// Make sure all our mocking above actually worked:
// Path a has lots of free space, but b has little, so new shard should go to a:
// First shard should go to a
// Second shard should go to b
// Shard for new index should go to a
// 2 shards go to 'b' and 1 to 'a'
/*
//www.apache.org/licenses/LICENSE-2.0
// Index doc but not advance local checkpoint.
// everything up to global checkpoint included
// every document is sent off separately
// Index doc but not advance local checkpoint.
// ignore
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Setup dependencies of the listeners
// Now setup the InternalEngine which is much more complicated because we aren't mocking anything
// Immediately run listeners rather than adding them to the listener thread pool like IndexShard does to simplify the test.
// we don't need to notify anybody in this test
/* same document */, "2" /* different document */));
// Fill the listener slots
// We shouldn't have called any of them
// Add one more listener which should cause a refresh.
// That forces all the listeners through. It would be on the listener ThreadPool but we've made all of those execute immediately.
/* Closing flushed pending listeners as though they were refreshed. Since this can only happen when the index is closed and no
// If you add a listener for an already refreshed location then it'll just fire even if closed
// But adding a listener to a non-refreshed location will fail
/**
/**
// This thread just refreshes every once in a while to cause trouble.
// These threads add and block until the refresh makes the change visible and then do a non-realtime get.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// index some docs in several segments
// Try running it before the node is stopped (and shard is closed)
// Try running it before the shard is corrupted, it should flip out because there is no corruption file marker
// shard should be failed due to a corrupted index
// there is only _stale_ primary (due to new allocation id)
// never flush - always recover from translog
// Index some documents
// having no extra docs is an interesting case for seq no based recoveries - test it more often
// flush the replica, so it will have more docs than what the primary will have
// shut down the replica node to be tested later
// Restart the single node
// all shards should be failed due to a corrupted translog
// have to shut down primary node - otherwise node lock is present
//noinspection EmptyTryBlock since we're just trying to obtain the lock
// there is only _stale_ primary (due to new allocation id)
// Run a search and make sure it succeeds
// Ensure that the global checkpoint and local checkpoint are restored from the max seqno of the last commit.
// never flush - always recover from translog
// Index some documents
// having no extra docs is an interesting case for seq no based recoveries - test it more often
// sample the replica node translog dirs
// stop data nodes
// Corrupt the translog file(s) on the replica
// Start the node with the non-corrupted data path
// Run a search and make sure it succeeds
// check replica corruption
// the replica translog was disabled so it doesn't know what hte global checkpoint is and thus can't do ops based recovery
// Ensure that the global checkpoint and local checkpoint are restored from the max seqno of the last commit.
/** Disables translog flushing for the specified index */
/*
//www.apache.org/licenses/LICENSE-2.0
// create same directory structure as prod does
// Adding rollover info to IndexMetaData to check that NamedXContentRegistry is properly configured
// Try running it before the shard is closed, it should flip out because it can't acquire the lock
// close shard
// Try running it before the shard is corrupted
// close shard
// test corrupted shard and add corruption marker
// run command with dry-run
// mean dry run
// run command without dry-run
// reopen shard
// close shard
// test corrupted shard
// it has to fail on start up due to index.shard.check_on_startup = checksum
// translog is corrupted already - do not check consistency
// run command with dry-run
// mean dry run
// run command without dry-run
// reopen shard
// index some docs in several segments
// close shard
// test corrupted shard and add corruption marker
// run command with dry-run
// mean dry run
// mean dry run
// run command without dry-run
// reopen shard
// index a single doc to have files on a disk
// close shard
// `--index index_name --shard-id 0` has to be resolved to indexPath
// index some docs in several segments
// run command with dry-run
// mean dry run
// mean dry run
// run command without dry-run
// open shard with the same location
// index is corrupted - don't even try to check index on close - it fails
// index some docs in several segments
/*
//www.apache.org/licenses/LICENSE-2.0
// one doc is indexed above blocking
// with ZERO we are guaranteed to see the doc since we will wait for a refresh in the background
// we can't assert on hasRefreshed since it might have been refreshed in the background on the shard concurrently.
// and if the background refresh wins the refresh race (both call maybeRefresh), the document might not be visible
// until the background refresh is done.
// async on purpose to make sure it happens concurrently
// now disable background refresh and make sure the refresh happens
// wait for both to ensure we don't have in-flight operations
// We need to ensure a `scheduledRefresh` triggered by the internal refresh setting update is executed before we index a new doc;
// otherwise, it will compete to call `Engine#maybeRefresh` with the `scheduledRefresh` that we are going to verify.
// We can make sure that all scheduled refresh tasks are done by submitting *maximumPoolSize* blocking tasks,
// then wait until all of them completed. Note that using ThreadPoolStats is not watertight as both queue and
// active count can be 0 when ThreadPoolExecutor just takes a task out the queue but before marking it active.
/*
//www.apache.org/licenses/LICENSE-2.0
// this test also tests if calls are correct if one or more listeners throw exceptions
/*
//www.apache.org/licenses/LICENSE-2.0
// we refreshed
// we read from the translog
// now again from the reader
// we refreshed
// we read from the translog
// we read from the translog
/*
//www.apache.org/licenses/LICENSE-2.0
// missing separator
// missing opening bracket
// missing closing bracket
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// all docs belong in this shard
// check the last docs in the segment and make sure they all have the right shard id
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// check that we didn't merge
// check that we didn't merge
// we won't get here - no permission ;)
// if we run into that situation we know it's supported.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// this might happen when computing max scores
// this might happen when computing max scores
/*
//www.apache.org/licenses/LICENSE-2.0
// Tests #16594
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// random corruption
// No failures should read as usual
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// +2 because there are 3 files
// An index output is open so no caching
// An index output is open so no caching
// +3 because there are 3 files
// +2 because there are 2 files now
/*
//www.apache.org/licenses/LICENSE-2.0
// we really need local GW here since this also checks for corruption etc.
// and we need to make sure primaries are not just trashed if we don't have replicas
// speed up recoveries
/**
// have enough space for 3 copies
// no checkindex - we corrupt shards on purpose
// no translog based flush - it might change the .liv / segments.N files
// double flush to create safe commit in case of async durability
// we have to flush at least once here since we don't corrupt the translog
/*
// sometimes due to cluster rebalacing and random settings default timeout is just not enough.
/*
// primary + 2 replicas
/**
// no checkindex - we corrupt shards on purpose
// no translog based flush - it might change the .liv / segments.N files
// double flush to create safe commit in case of async durability
// we have to flush at least once here since we don't corrupt the translog
/*
// sometimes on slow nodes the replication / recovery is just dead slow
/**
// keep on retrying
// allocated with empty commit
// flip one byte in the content
/**
// don't go crazy here it must recovery fast
// This does corrupt files on the replica, so we can't check:
// we have to flush at least once here since we don't corrupt the translog
// flip one byte in the content
// we are green so primaries got not corrupted.
// ensure that no shard is actually allocated on the unlucky node
/**
// no replicas for this test
// no checkindex - we corrupt shards on purpose
// no translog based flush - it might change the .liv / segments.N files
// we have to flush at least once here since we don't corrupt the translog
// we don't corrupt segments.gen since S/R doesn't snapshot this file
// the other problem here why we can't corrupt segments.X files is that the snapshot flushes again before
// it snapshots and that will write a new segments.X+1 file
/**
// no checkindex - we corrupt shards on purpose
// no translog based flush - it might change the .liv / segments.N files
// we have to flush at least once here since we don't corrupt the translog
// disable allocations of replicas post restart (the restart will change replicas to primaries, so we have
// to capture replicas post restart)
// enable allocation
// multi data path might only have one path in use
// treeset makes sure iteration order is deterministic
// multi data path might only have one path in use
// don't corrupt fully deleted segments - they might be removed on snapshot
// .liv and segments_N are per commit files and might change after corruption
// multi data path might only have one path in use
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// never flush - always recover from translog
// Index some documents
/*
//www.apache.org/licenses/LICENSE-2.0
// a write operation might still be in flight when the test has finished
// so we should not check the operation counter here
/**
//github.com/elastic/elasticsearch/issues/8788
//create a transport service that throws a ConnectTransportException for one bulk request and therefore triggers a retry.
/*
//www.apache.org/licenses/LICENSE-2.0
// prevent warnings
// default
// explicit directory impls
/*
//www.apache.org/licenses/LICENSE-2.0
// ok
// ok
// ok
// write a wrong checksum to the file
// we write the checksum in the try / catch block below
// ok
// ok
// set default codec - all segments need checksums
// flush
// check before we committed
// expected
// Check file
// Corrupt file and check again
// ok
// Flip one byte
// bump the time
// si files are different - containing timestamps etc
// in lucene 5 nothing is identical - we use random ids in file headers
// check the self diff
// lets add some deletes
// segments_N + del file
// an entire segment must be missing (single doc segment got dropped)
// the commit file is different
// check the self diff
// add a new commit
// force CFS - easier to test here since we know it will add 3 files
// segments_N, del file, cfs, cfe, si for the new segment
// the del file must be different
// segments_N,cfs, cfe, si for the new segment
// segments_N, cfs, cfe, si for the new segment
// an entire segment must be missing (single doc segment got dropped)  plus the commit is different
// this time random codec....
// we keep all commits and that allows us clean based on multiple snapshots
// ignore
// do not check for correct files, we have enough tests for that above
// empty file
// expected
// we have to remove the index since it's corrupted and might fail the MocKDirWrapper checkindex call
// I use ram dir to prevent that virusscanner being a PITA
// I use ram dir to prevent that virusscanner being a PITA
// we only need the header to trigger the exception
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// clear all stats first
// we make sure each node gets at least a single shard...
// check current
// check suggest count
// check suggest time
// the upperbound is num shards * total time since we do searches in parallel
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// to account for the op we already read
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// if we crashed while rolling a generation then we might have copied `translog.ckp` to its numbered generation file but
// have not yet written a new `translog.ckp`. During recovery we must also verify that this file is intact, so it's ok to
// corrupt this file too (either by writing the wrong information, correctly formatted, or by properly corrupting it)
// else checkpoint copy has the correct content so it's now a candidate for the usual kinds of corruption
// missing or corrupt checkpoint already, find something else to break...
// TreeSet makes sure iteration order is deterministic
// deleting the unnecessary checkpoint file doesn't count as a corruption
/**
// read
// corrupt
// rewrite
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// make a new policy as committed gen can't go backwards (for now)
// disable age
// disable size
// disable age and zie
// disable total files
/*
//www.apache.org/licenses/LICENSE-2.0
// corruption corrupted the version byte making this look like a v2, v1 or v0 translog
/*
//www.apache.org/licenses/LICENSE-2.0
// A default primary term is used by translog instances created in this test.
// delete all the locations
// if a previous test failed we clean up things here
// only randomize between nog age retention and a long one, so failures will have a chance of reproducing
// all good
// all good
// all good
// all good
// some strings (like '/' , '..') do not refer to a file, which we this method should return
// some FS don't like our random file names -- let's just skip these random choices
// force flushing and updating of stats
// self control cleaning for test
// out range
/**
/**
// a map of all written ops and their returned location.
// a signal for all threads to stop
// any errors on threads
// we need not do this concurrently as we need to make sure that the generation
// we're committing - is still present when we're committing
// expose the new checkpoint (simulating a commit), before we trim the translog
// captures the last committed checkpoint, while holding the view, simulating
// recovery logic which captures a view and gets a lucene commit
// captures al views that are written since the view was created (with a small caveat see bellow)
// these are what we expect the snapshot to return (and potentially some more).
// slow down things a bit and spread out testing..
// force stopping, if all writers crashed
// we are the last location so everything should be synced
// not syncing now
// not syncing now
// do this first so that there is at least one pending tlog entry
// we are the last location so everything should be synced
// not syncing now
// expected
// live reader!
// we intentionally don't close the tlog that is in the prepareCommit stage since we try to recovery the uncommitted
// translog here as well.
// recover twice
// we intentionally don't close the tlog that is in the prepareCommit stage since we try to recovery the uncommitted
// translog here as well.
// recover twice
// we intentionally don't close the tlog that is in the prepareCommit stage since we try to recovery the uncommitted
// translog here as well.
// increment primaryTerm to avoid potential negative numbers
// add a single operation to current with seq# > trimmed seq# but higher primary term
// it is possible to trim after generation rollover
// have to use exactly the same source for same seq# if primaryTerm is not changed
// use the latest source of op with the same seq# - therefore no break
// use ongoing primaryTerms - or the same as it was
/**
// at least one roll + inc of primary term has to be there - otherwise trim would not take place at all
// last attempt we have to make roll as well - otherwise could skip trimming as it has been trimmed already
// if we are so happy to reach the max attempts - fail it always`
// check that despite of IO exception translog is not corrupted
// all is well
// lets verify we can concurrently read this
// all is well
// we are closed
// writes pretty big docs so we cross buffer borders regularly
// writes pretty big docs so we cross buffer boarders regularly
//TODO once we have a mock FS that can simulate we can also fail on plain sync
// w00t
// this holds a reference to the current tlog channel such that it's not closed
// if we hit a tragic event. this is important to ensure that asserts inside the Translog#add doesn't trip
// otherwise our assertions here are off by one sometimes.
// boom now we failed
// drop all that haven't been synced
/**
// engine blows up, after committing the above generation
// no trimming done yet, just recovered
/**
// disable retention so we trim things
// expected...
// we don't know when things broke exactly
// don't do partial writes for checkpoints we rely on the fact that the bytes are written as an atomic operation
// simulate going OOM and dying just at the wrong moment.
// see https://github.com/elastic/elasticsearch/issues/15754
// if we have a LeakFS here we fail if not all resources are closed
// all is well
// don't copy the new file
// we add N+1 and N+2 to ensure we only delete the N+1 file and never jump ahead and wipe without the right condition
/**
//writes pretty big docs so we cross buffer boarders regularly
// we have to sync here first otherwise we don't know if the sync succeeded if the commit fails
// we survived all the randomness!!!
// lets close the translog and if it succeeds we are all synced again. If we don't do this we will close
// it in the finally block but miss to copy over unsynced docs to syncedDocs and fail the assertion down the road...
// failed in fsync but got fully written
// we were committing and blew up in one of the syncs, but they made it through
// failed - that's ok, we didn't even create it
// now randomly open this failing tlog again just to make sure we can also recover from failing during recovery
// failed - that's ok, we didn't even create it
// we don't wanna fail here but we might since we write a new checkpoint and create a new tlog file
// we never managed to successfully create a translog, make it
// don't fail in the ctor
//fine
/**
// make sure we keep some files around
// we control retention via time, disable size based calculations for simplicity
// We always create an empty translog.
// immediate cleanup
// one extra roll to make sure that all ops so far are available via a reader and a translog-{gen}.ckp
// file in a consistent way, in order to simplify checking code.
/** Make sure that it's ok to close a translog snapshot multiple times */
// close method should never be called directly from Translog (the only exception is closeOnTragicEvent)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Catch chars that are more than a single byte
// we can create an index of max length
/*
//www.apache.org/licenses/LICENSE-2.0
// start one server
// start another server
// first wait for 2 nodes in the cluster
// explicitly call reroute, so shards will get relocated to the new node (we delay it in ES in case other nodes join)
// start another server
// first wait for 3 nodes in the cluster
// explicitly call reroute, so shards will get relocated to the new node (we delay it in ES in case other nodes join)
// kill the first server
// verify health
// last, lets delete the index
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// index resolver throws this if it does not find the exact index due to day changes
/*
//www.apache.org/licenses/LICENSE-2.0
// small indexing buffer so that we can trigger refresh after buffering 100 deletes
// We need to set a larger buffer for the IndexWriter; otherwise, it will flush before the IndexingMemoryController.
// #10312
// Force merge so we know all merges are done before we start deleting:
// need to assert busily as IndexingMemoryController refreshes in background
/*
//www.apache.org/licenses/LICENSE-2.0
// Size of each shard's indexing buffer
// How many bytes this shard is currently moving to disk
// Shards that are currently throttled
// disable it
// First time we are seeing this shard:
// Each doc we index takes up a megabyte!
// add another shard
// remove first shard
// remove second shard
// add a new one
// index into one shard only, crosses the 5mb limit, so shard1 is refreshed
// shard1 crossed 5 mb and is now cleared:
// We are now using 5 MB, so we should be writing shard0 since it's using the most heap:
// Now we are still writing 3 MB (shard0), and using 5 MB index buffers, so we should now 1) be writing shard1,
// and 2) be throttling shard1:
// More indexing to shard0
// Now we are using 5 MB again, so shard0 should also be writing and now also be throttled:
// Both shards finally finish writing, and throttling should stop:
// block refresh
// disable it
// allow refresh
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//start with a single node
//add a listener that keeps track of the shard state changes
//create an index that should fail
//create an index
//new shards got started
//add a node: 3 out of the 6 shards will be relocated to it
//disable allocation before starting a new node, as we need to register the listener first
//add a listener that keeps track of the shard state changes
//re-enable allocation
//the 3 relocated shards get closed on the first node
//the 3 relocated shards get created on the second node
//increase replicas from 0 to 1
//3 replicas are allocated to the first node
//3 replicas are allocated to the second node
//close the index
//we keep track of all the states (ordered) a shard goes through
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Verify defaults
// we need to wait until all shards are allocated since recovery from
// gateway will fail unless the majority of the replicas was allocated
// pre-closing. with lots of replicas this will fail.
//TODO: temporary work-around for #5531
// Verify defaults for wildcards, when specifying no indices (*, _all, /)
// Now force allow_no_indices=true
// Verify defaults for wildcards, with one wildcard expression and one existing index
// Verify defaults for wildcards, with two wildcard expression and one existing index
// Now force allow_no_indices=true
//TODO: temporary work-around for #5531
//you should still be able to run empty searches without things blowing up
//you should still be able to run empty searches without things blowing up
// For now don't handle closed indices
// if there are no indices to open/close and allow_no_indices=true (default), the open/close is a no-op
// if there are no indices to open/close throw an exception
/*
//www.apache.org/licenses/LICENSE-2.0
// got emptied, but no changes to other metrics
// forgot everything
// this triggers some assertions
// evicted
// no changes
// forgot everything about shard1
// forgot everything about shard2
// this triggers some assertions
// Make sure the cache behaves correctly when a segment that is associated
// with an empty cache gets closed. In that particular case, the eviction
// callback is called with a number of evicted entries equal to 0
// see https://github.com/elastic/elasticsearch/issues/15043
// this used to fail because we were evicting an empty cache on
// the segment from r1
// this triggers some assertions
// never cache
/*
//www.apache.org/licenses/LICENSE-2.0
// One of the primary purposes of the query cache is to cache aggs results
// This is not a random example: serialization with time zones writes shared strings
// which used to not work well with the query cache because of the handles stream output
// see #9500
// The cached is actually used
// Force merge the index to ensure there can be no background merges during the subsequent searches that would invalidate the cache
// Force merge the index to ensure there can be no background merges during the subsequent searches that would invalidate the cache
// Force merge the index to ensure there can be no background merges during the subsequent searches that would invalidate the cache
// Force merge the index to ensure there can be no background merges during the subsequent searches that would invalidate the cache
// Because the query will INTERSECT with the 3rd index it will not be
// rewritten and will still contain `now` so won't be recorded as a
// cache miss or cache hit since queries containing now can't be cached
// Force merge the index to ensure there can be no background merges during the subsequent searches that would invalidate the cache
// If size > 0 we should no cache by default
// If search type is DFS_QUERY_THEN_FETCH we should not cache
// If search type is DFS_QUERY_THEN_FETCH we should not cache even if
// the cache flag is explicitly set on the request
// If the request has an non-filter aggregation containing now we should not cache
// If size > 1 and cache flag is set on the request we should cache
// If the request has a filter aggregation containing now we should cache since it gets rewritten
// Force merge the index to ensure there can be no background merges during the subsequent searches that would invalidate the cache
// Check the hit count and miss count together so if they are not
// correct we can see both values
/*
//www.apache.org/licenses/LICENSE-2.0
// initial cache
// cache hit
// Closing the cache doesn't modify an already returned CacheEntity
// closed shard but reader is still open
// initial cache
// cache the second
// Closing the cache doesn't change returned entities
// release
// closed shard but reader is still open
// clear all for the indexShard Idendity even though is't still open
// third has not been validated since it's a different identity
// initial cache
// cache hit
// load again after invalidate
// release
// closed shard but reader is still open
/*
//www.apache.org/licenses/LICENSE-2.0
// limit the number of threads created
// default the watermarks low values to prevent tests from failing on nodes without enough disk space
// turning on the real memory circuit breaker leads to spurious test failures. As have no full control over heap usage, we
// turn it off for these tests.
// empty list disables a port scan for other nodes
/*
//www.apache.org/licenses/LICENSE-2.0
// "bogus" index has not been removed
// create the alias for the index
// try to import a dangling index with the same name as the alias, it should fail
// remove the alias
// now try importing a dangling index with the same name as the alias, it should succeed.
//import an index with minor version incremented by one over cluster master version, it should be ignored
/**
//github.com/elastic/elasticsearch/issues/18054
// if all goes well, this won't throw an exception, otherwise, it will throw an IllegalStateException
/**
// the shard that is going to fail
// this allows us to control the indices that exist
// generate fake shards and their responses
// real one, which has a logger defined
// Calculate the counts for a cluster 1 node smaller than we have to ensure we have headroom
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// registry always has the current version
// analysis service has the expected version
// verify position increment gap
// check custom class name (my)
//        MatcherAssert.assertThat(wordList, hasItems(words));
/**
// Need mock keyword tokenizer here, because alpha / beta versions are broken up by the dash.
/**
/**
// Simple tokenizer that always spits out a single token with some preconfigured characters
// These are current broken by https://github.com/elastic/elasticsearch/issues/24752
//        assertEquals("test" + (noVersionSupportsMultiTerm ? "no_version" : ""),
//                analyzers.get("no_version").normalize("", "test").utf8ToString());
//        assertEquals("test" + (luceneVersionSupportsMultiTerm ? version.luceneVersion.toString() : ""),
//                analyzers.get("lucene_version").normalize("", "test").utf8ToString());
//        assertEquals("test" + (elasticsearchVersionSupportsMultiTerm ? version.toString() : ""),
//                analyzers.get("elasticsearch_version").normalize("", "test").utf8ToString());
// Simple char filter that appends text to the term
// Simple token filter that appends text to the term
/*
//www.apache.org/licenses/LICENSE-2.0
// index some amount of data
// close some of the indices
// check that all above configured analyzers have been loaded
// check that all of the prebuilt analyzers are still open
// if it is not null in the cache, it has been loaded
// ensure analyzers are still open by checking there is no ACE
/*
//www.apache.org/licenses/LICENSE-2.0
// issue #5974
//analyzer only
//analyzer only
//custom analyzer
//tokenizer
//tokenfilters
//check other attributes
//tokenizer
// tokenfilter(lowercase)
// tokenfilter({"type": "stop", "stopwords": ["foo", "buzz"]})
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// testing that dictionary specific settings override node level settings
/*
//www.apache.org/licenses/LICENSE-2.0
// never trip
// Parent will trip right before regular breaker would trip
/**
// anything below 100 bytes should work (overhead) - current memory usage is zero
// assume memory usage has increased to 150 bytes
// a reservation that bumps memory usage to less than 200 (150 bytes used  + reservation < 200)
// anything >= 20 bytes (10 bytes * 2 overhead) reservation breaks the parent but it must be low enough to avoid
// breaking the child breaker.
// it was the parent that rejected the reservation
// lower memory usage again - the same reservation should succeed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// initializing a shard should succeed when enableRandomFailures is disabled
// active shards can be failed if state persistence was disabled in an earlier CS update
// check that all shards in local routing nodes have been allocated
// shard must either be there or there must be a failure
// initializing a shard should succeed when enableRandomFailures is disabled
// index metadata has been updated
// shard has been created
// shard has latest shard routing
// all other shards / indices have been cleaned up
// check if we have shards of that index in failedShardsCache
// if yes, we might not have cleaned the index as failedShardsCache can be populated by another thread
/**
/**
/**
// note: it's ok for a replica in post recovery to be started and promoted at once
// this can happen when the primary failed after we sent the start shard message
/*
//www.apache.org/licenses/LICENSE-2.0
// transport actions
// it's not used
// mocks
// MetaDataCreateIndexService creates indices using its IndicesService instance to check mappings -> fake it here
// services
// metaData upgrader should do nothing
/*
//www.apache.org/licenses/LICENSE-2.0
// we have an IndicesClusterStateService per node in the cluster
// each of the following iterations represents a new cluster state update processed on all nodes
// calculate new cluster state
// multiple iterations to simulate batching of cluster states
// apply cluster state to nodes (incl. master)
// check that cluster state has been properly applied to node
// TODO: check if we can go to green by starting all shards and finishing all iterations
/**
// a cluster state derived from the initial state that includes a created index
// the initial state which is derived from the newly created cluster state but doesn't contain the index
// pick a data node to simulate the adding an index cluster state change event on, that has shards assigned to it
// simulate the cluster state change on the node
// create a new empty cluster state with a brand new cluster UUID
// simulate the cluster state change on the node
// check that in memory data structures have been removed once the new cluster state is applied,
// but the persistent data is still there
/**
// the initial state which is derived from the newly created cluster state but doesn't contain the index
// pick a data node to simulate the adding an index cluster state change event on, that has shards assigned to it
// simulate the cluster state change on the node
// start the replica
// close the index and open it up again (this will sometimes swap roles between primary and replica)
// the initial state which is derived from the newly created cluster state but doesn't contain the index
// pick a data node to simulate the adding an index cluster state change event on, that has shards assigned to it
// simulate the cluster state change on the node
// check that failing unrelated allocation does not remove shard
// local node is the master
// at least two nodes that have the data role so that we can allocate shards
// add nodes to clusterStateServiceMap
// randomly remove no_master blocks
// randomly add no_master blocks
// if no_master block is in place, make no other cluster state changes
// randomly create new indices (until we have 200 max)
// randomly delete indices
// randomly close indices
// randomly open indices
// randomly update settings
// randomly reroute
// randomly start and fail allocated shards
// randomly add and remove nodes (except current master)
// add node
// remove node
// and add it back
// TODO: go masterless?
/*
//www.apache.org/licenses/LICENSE-2.0
// don't use assertAllSuccessful it uses a randomized context that belongs to a different thread
// now, start new node and relocate a shard there and see if sync id still there
//  create an index but disallow allocation
// this should not hang but instead immediately return with empty result set
// just to make sure the test actually tests the right thing
// Index extra documents to one replica - synced-flush should fail on that replica.
// Index extra documents to all shards - synced-flush should be ok.
// Do reindex documents to the out of sync replica to avoid trigger merges
// Do not renew synced-flush
// Shards were updated, renew synced flush.
// Manually remove or change sync-id, renew synced flush.
// Change the existing sync-id of a single shard.
// Flush will create a new commit without sync-id
/*
//www.apache.org/licenses/LICENSE-2.0
// pull another commit and make sure we can't sync-flush with the old one
// wait for the GCP sync spawned from the index request above to complete to avoid that request disturbing the check below
// wipe it...
/*
//www.apache.org/licenses/LICENSE-2.0
/** Utils for SyncedFlush */
/**
/*
// cause the assert busy to retry
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// see #3544
// The 'fieldNames' array is used to help with retrieval of index terms
// after testing
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Get mappings by full name
// Get mappings by name
// get mappings by name across multiple indices
//fix #6552
/*
//www.apache.org/licenses/LICENSE-2.0
// Get all mappings
// Get all mappings, via wildcard support
// Get mappings in indexa
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//no changes, we return
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Tests for the noop breakers, which are non-dynamic settings */
// This is set low, because if the "noop" is not a noop, it will break
// This is set low, because if the "noop" is not a noop, it will break
// index some different terms so we have some field data for loading
// A cardinality aggregation uses BigArrays and thus the REQUEST breaker
// no exception because the breaker is a noop
// index some different terms so we have some field data for loading
// Sorting using fielddata and thus the FIELDDATA breaker
// no exception because the breaker is a noop
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Reset all breaker settings back to their defaults */
// clear all caches, we could be very close (or even above) the limit and then we will not be able to reset the breaker settings
/** Returns true if any of the nodes used a noop breaker */
// index some different terms so we have some field data for loading
// clear field data cache (thus setting the loaded field data back to 0)
// Update circuit breaker settings
// execute a search that loads field data (sorting on the "test" field)
// again, this time it should trip the breaker
// Create an index where the mappings have a field data filter
// index some different terms so we have some field data for loading
// execute a search that loads field data (sorting on the "test" field)
// clear field data cache (thus setting the loaded field data back to 0)
// Update circuit breaker settings
// execute a search that loads field data (sorting on the "test" field)
// again, this time it should trip the breaker
// Make request breaker limited to a small amount
// index some different terms so we have some field data for loading
// A cardinality aggregation uses BigArrays and thus the REQUEST breaker
// Make request breaker limited to a small amount
// index some different terms so we have some field data for loading
// A terms aggregation on the "test" field should trip the bucket circuit breaker
/** Issues a cache clear and waits 30 seconds for the field data breaker to be cleared */
// ignore, we forced a circuit break
// calls updates settings to reset everything to default, checking that the request
// is not blocked by the above inflight circuit breaker
// send bulk request from source node to target node later. The sole shard is bound to the target node.
// we use the limit size as a (very) rough indication on how many requests we should sent to hit the limit
// can either fail directly with an exception or the response contains exceptions (depending on client)
// each item must have failed with CircuitBreakingException
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// parent: {:limit 70}, fd: {:limit 50}, request: {:limit 20}
// parent: {:limit 70}, fd: {:limit 40}, request: {:limit 30}
// parent: {:limit 70}, fd: {:limit 50}, request: {:limit 20}
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// {}
// test-str
// I don't use randomNumericType() here because I don't want "byte", and I want "float" and "double"
// test-num
// properties
// rarely no exception
/* some seeds just won't let you create the index at all and we enter a ping-pong mode
// don't assert on failures here
// Sort by the string and numeric fields, to load them into field data
// Now, clear the cache and check that the circuit breaker has been
// successfully set back to zero. If there is a bug in the circuit
// breaker adjustment code, it should show up here by the breaker
// estimate being either positive or negative.
// make sure all shards are there - there could be shards that are still starting up.
// Since .cleanUp() is no longer called on cache clear, we need to call it on each node manually
// Clean up the cache, ensuring that entries' listeners have been called
// TODO: Generalize this class and add it as a utility
/*
//www.apache.org/licenses/LICENSE-2.0
// Don't keep any indices in the graveyard, so that when we delete an index,
// it's definitely considered to be dangling.
/**
// Restart node, deleting the index in its absence, so that there is a dangling index to recover
/**
// Restart node, deleting the index in its absence, so that there is a dangling index to recover
// Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable
// amount of time
/**
// Restart node, deleting the index in its absence, so that there is a dangling index to recover
// Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable
// amount of time
/*
//www.apache.org/licenses/LICENSE-2.0
// indexing process aborted early, no need for more relocations as test has already failed
// extra paranoia ;)
/*
//www.apache.org/licenses/LICENSE-2.0
// one chunk per sec..
// small chunks
// Should not expect any responses back
// force a shard recovery from nodeA to nodeB
// we should now have two total shards, one primary and one replica
// validate node A recovery
// validate node B recovery
// force a shard recovery from nodeA to nodeB
// do sync flush to gen sync id
// hold peer recovery on phase 2 after nodeB down
// nodeB stopped, peer recovery from nodeA to nodeC, it will be cancelled after nodeB get started.
// wait for peer recovery from nodeA to nodeB which is a no-op recovery so it skips the CLEAN_FILES stage and hence is not blocked
// make sure nodeA has primary and nodeB has replica
// wait for it to be finished
// we have to use assertBusy as recovery counters are decremented only when the last reference to the RecoveryTarget
// is decremented, which may happen after the recovery was done.
// relocations of replicas are marked as REPLICA and the source node is the node holding the primary (B)
// shutdown node with relocation source of replica shard and check if recovery continues
// relocations of replicas are marked as REPLICA and the source node is the node holding the primary (B)
// start a master node
//RecoveryTarget.Actions.TRANSLOG_OPS, <-- may not be sent if already flushed
/**
// start a master node
// ensures that it's considered as valid recovery attempt by source
// also waits for relocation / recovery to complete
// if a primary relocation fails after the source shard has been marked as relocated, both source and target are failed. If the
// source shard is moved back to started because the target fails first, it's possible that there is a cluster state where the
// shard is marked as started again (and ensureGreen returns), but while applying the cluster state the primary is failed and
// will be reallocated. The cluster will thus become green, then red, then green again. Triggering a refresh here before
// searching helps, as in contrast to search actions, refresh waits for the closed shard to be reallocated.
// Perform some replicated operations so the replica isn't simply empty, because ops-based recovery isn't better in that case
//noinspection StatementWithEmptyBody
// time passes
// Flush twice to update the safe commit's local checkpoint
/** Makes sure the new master does not repeatedly fetch index metadata from recovering replicas */
// Recovery should keep syncId if no indexing activity on the primary after synced-flush.
// do not reallocate the lost shard
// expire leases quickly
// sync frequently
// disable global checkpoint background sync so we can verify the start recovery request
// avoid refresh when we are failing a shard
//noinspection OptionalGetWithoutIsPresent because it fails the test if absent
// We do not guarantee that the replica can recover locally all the way to its own global checkpoint before starting
// to recover from the primary, so we must be careful not to perform an operations-based recovery if this would require
// some operations that are not being retained. Emulate this by advancing the lease ahead of the replica's GCP:
//noinspection OptionalGetWithoutIsPresent because it fails the test if absent
// wait for all history to be discarded
// ensure that all operations are in the safe commit
/*
//noinspection OptionalGetWithoutIsPresent because it fails the test if absent
// makes a safe commit
//noinspection OptionalGetWithoutIsPresent because it fails the test if absent
// Ensures that you can remove a replica and then add it back again without any ill effects, even if it's allocated back to the
// node that held it previously, in case that node hasn't completely cleared it up.
// prevent the primary from marking the replica as stale so the replica can get promoted.
// cancel recoveries
/*
//www.apache.org/licenses/LICENSE-2.0
// re-adding after removing previous attempt works
/*
//www.apache.org/licenses/LICENSE-2.0
// empty copy
// good copy
// corrupted copy
// copy with truncated translog
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// verify that both sending and receiving files can be completed with a single thread
// ensure all chunk requests have been completed; otherwise some files on the target are left open.
// expected.
// we have to use assert busy as we may be interrupted while acquiring the permit, if so we want to check
// that the permit is released.
// limited by the remaining chunks
// limited by the buffering chunks
// don't run checkindex we might corrupt the index in these tests
/*
//www.apache.org/licenses/LICENSE-2.0
// all well = it's already registered
// we have to close it here otherwise rename fails since the write.lock is held by the engine
/*
//www.apache.org/licenses/LICENSE-2.0
// initialize with some data and then reset
// before we start we must report 0
// cool
// but reset should be always possible.
// we don't need to test the time aspect, it's done in the timer test
// force one
// we don't need to test the time aspect, it's done in the timer test
// force one
/*
//www.apache.org/licenses/LICENSE-2.0
// force a roll and flush
// rolling/flushing is async
/*
// create out of order delete and index op on replica
// delete #1
// manually advance msu for this delete
// isolate the delete in it's own generation
// index #0
// index #3
// Flushing a new commit with local checkpoint=1 allows to delete the translog gen #1.
// index #2
// advance local checkpoint
// index #5 -> force NoOp #4.
// 4 ops + seqno gaps (delete #1 is removed but index #0 will be replayed).
// 5 ops + seqno gaps
// 5 ops + seqno gaps
// wait for primary/replica sync to make sure seq# gap is closed.
// If soft-deletes is enabled, delete#1 will be reclaimed because its segment (segment_1) is fully deleted
// index#0 will be retained if merge is disabled; otherwise it will be reclaimed because gcp=3 and retained_ops=0
// create out of order delete and index op on replica
// delete #1
// manually advance msu for this delete
// isolate delete#1 in its own translog generation and lucene segment
// index #0
// index #3
// Flushing a new commit with local checkpoint=1 allows to delete the translog gen #1.
// index #2
// advance local checkpoint
// index #5 -> force NoOp #4.
// wait for primary/replica sync to make sure seq# gap is closed.
// index some shared docs
// we don't want merges to happen here - we call maybe merge on the engine
// later once we stared it up otherwise we would need to wait for it here
// we also don't specify a codec here and merges should use the engines for this index
// create a new translog
// file based recovery should be made
// history uuid was restored
/**
// Make sure the flushing will eventually be completed (eg. `shouldPeriodicallyFlush` is false)
/*
//www.apache.org/licenses/LICENSE-2.0
// sometimes test with a closed index
// pick up a data node that contains a random primary shard
// stop the random data node, all remaining shards are promoted to primaries
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we can not update the setting via the update settings API
/*
//www.apache.org/licenses/LICENSE-2.0
// we can not update the setting via the update settings API
/*
//www.apache.org/licenses/LICENSE-2.0
//only 2 copies allocated (1 replica) across 2 nodes
//all 3 copies allocated across 3 nodes
//a single copy is allocated (replica set to 0)
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// now do it on a closed index
// user has no dependency
// now we are consistent
// now try to remove it and make sure it fails
// now we are consistent
// this one can change
// this one can't
// this one can change
// this one can't
// Now verify via dedicated get settings api:
// this one can change
// Now verify via dedicated get settings api:
// now close the index, change the non dynamic setting, and see that it applies
// Wait for the index to turn green before attempting to close it
// this one can change
// this one can't
// this one can change
// this one really can't
// Now verify via dedicated get settings api:
// delete is still in cache this should fail
// Make sure the time has advanced for InternalEngine#resolveDocVersion()
// delete should not be in cache
// Closing an index is blocked
// if the read-only block is present, remove it
// now put the same block again
// if the read-only block is present, remove it
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// disable closing
/*
//www.apache.org/licenses/LICENSE-2.0
// First close should be fully acked
// Second close should be acked too
// no replicas to avoid recoveries that could fail the index closing
// Closing an index should execute noop peer recovery
// Open a closed index should execute noop recovery
/**
// index more documents while one shard copy is offline
/**
// allocate shard to first data node
// move single shard to second node
/*
//www.apache.org/licenses/LICENSE-2.0
// wait for the master to finish processing join.
// relocate one shard for every index to be closed
// Build the list of shards for which recoveries will be blocked
// Create a SendRequestBehavior that will block outgoing start recovery request
// start index closing threads
// Closing is not always acknowledged when shards are relocating: this is the case when the target shard is initializing
// or is catching up operations. In these cases the TransportVerifyShardBeforeCloseAction will detect that the global
// and max sequence number don't match and will not ack the close.
// If a shard recovery has been interrupted, we expect its index to be closed
/*
//www.apache.org/licenses/LICENSE-2.0
//no problem if we try to open an index that's already in open state
// check the index still contains the records that we indexed
// Closing an index is not blocked
// Opening an index is not blocked
// Closing an index is blocked
// Opening an index is blocked
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// all is well
// Ignore
// Expected
/*
//www.apache.org/licenses/LICENSE-2.0
// requires custom completion format
//Filter/Query cache is cleaned periodically, default is 60s, so make sure it runs often. Thread.sleep for 60s is bad
// sort to load it to field data...
// sort to load it to field data...
// now check the per field stats
// sort to load it to field data and filter to load filter cache
// Make sure the filter cache entries have been removed...
// index docs until we have at least one doc on each shard, otherwise, our tests will not work
// since refresh will not refresh anything on a shard that has 0 docs and its search response get cached
// index the data again...
// clean the cache
// test explicit request parameter
// set the index level setting to false, and see that the reverse works
// clean the cache
// Provoke slowish merging by making many unique terms:
//nodesStats = client().admin().cluster().prepareNodesStats().setIndices(true).get();
// make sure we see throttling kicking in:
// Provoke slowish merging by making many unique terms:
//nodesStats = client().admin().cluster().prepareNodesStats().setIndices(true).get();
//Wait 5 minutes for throttling to kick in
// Optimize & flush and wait; else we sometimes get a "Delete Index failed - not acked"
// when ESIntegTestCase.after tries to remove indices created by the test:
// make sure that number of requests in progress is 0
// check flags
// check get
// missing get
// clear all
// reset defaults
// index failed
// clear all
// reset defaults
// rely on 1 replica for this tests
// clear all
// set the flags
// check the flags
// check the complement
// Need to persist the global checkpoint for the soft-deletes retention MP.
// the query cache has an optimization that disables it automatically if there is contention,
// so we run it in an assertBusy block which should eventually succeed
// Here we are testing that a fully deleted segment should be dropped and its cached is evicted.
// In order to instruct the merge policy not to keep a fully deleted segment,
// we need to flush and make that commit safe so that the SoftDeletesPolicy can drop everything.
/**
// increasing the number of shards increases the number of chances any one stats request will hit a race
// start threads that will index concurrently with stats requests
// start threads that will get stats concurrently with indexing
// release the hounds
// wait for a failure, or for fifteen seconds to elapse
// stop all threads and wait for them to complete
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// simplify this and only use a single data path
// by default this value is 1 sec in tests (30 sec in practice) but we adding disruption here
// which is between 1 and 2 sec can cause each of the shard deletion requests to timeout.
// to prevent this we are setting the timeout here to something highish ie. the default in practice
// testShardActiveElseWhere might change the state of a non-master node
// so we cannot check state consistency of this cluster
// sometimes add cluster-state delay to trigger observers in IndicesStore.ShardActiveRequestHandler
// wait a little so that cluster state observer is registered
/**
// use a tracer on the target node to track relocation start and end
// Whenever a node deletes a shard because it was relocated somewhere else, it first
// checks if enough other copies are started somewhere else. The node sends a ShardActiveRequest
// to the other nodes that should have a copy according to cluster state.
/* Test that shard is deleted in case ShardActiveRequest after relocation and next incoming cluster state is an index delete. */
// add a transport delegate that will prevent the shard active request to succeed the first time after relocation has finished.
// node_1 will then wait for the next cluster state change before it tries a next attempt to delete the shard.
// delete the index. node_1 that still waits for the next cluster state update will then get the delete index next.
// it must still delete the shard, even if it cannot find it anymore in indicesservice
// we will use this later on, handy to start now to make sure it has a different data folder that node 1,2 &3
// disable allocation to control the situation more easily
// we have to do this in two steps as we now do async shard fetching before assigning, so the change to the
// allocation filtering may not have immediate effect
// TODO: we should add an easier to do this. It's too much of a song and dance..
// wait for 4 active shards - we should have lost one shard
// disable allocation again to control concurrency a bit and allow shard active to kick in before allocation
// disable relocations when we do this, to make sure the shards are not relocated from node2
// due to rebalancing, and delete its content
/*
//www.apache.org/licenses/LICENSE-2.0
// Shard exists locally, can't delete shard
/*
//www.apache.org/licenses/LICENSE-2.0
// creates a simple index template
/*
//www.apache.org/licenses/LICENSE-2.0
// clean all templates setup by the framework.
// check get all templates on an empty index.
// test create param
// index something into test_index, will match on both templates
// field2 is not stored.
// now only match on one template (template_1)
// clean all templates setup by the framework.
// check get all templates on an empty index.
// clean all templates setup by the framework.
// check get all templates on an empty index.
// Search the complex filter alias
//invalid filter but valid json: put index template works fine, fails during index creation
//invalid json: put index template fails
// Indexing into a should succeed, because the field mapping for field 'field' is defined in the test mapping.
// Indexing into b index should fail, since there is field with name 'field' in the mapping
// Before 2.0 alias filters were parsed at alias creation time, in order
// for filters to work correctly ES required that fields mentioned in those
// filters exist in the mapping.
// From 2.0 and higher alias filters are parsed at request time and therefor
// fields mentioned in filters don't need to exist in the mapping.
// So the aliases defined in the index template for this index will not fail
// even though the fields in the alias fields don't exist yet and indexing into
// an index that doesn't exist yet will succeed
// clean all templates setup by the framework.
// check get all templates on an empty index.
//Now, a complete mapping with two separated templates is error
// base template
// put template using custom_1 analyzer
// ax -> matches template
// bx -> matches template
// clean all templates setup by the framework.
// check get all templates on an empty index.
// provide more partitions than shards
// provide an invalid mapping for a partitioned index
// no templates yet
// a valid configuration that only provides the partition size
// create an index with too few shards
// finally, create a valid index
/*
//www.apache.org/licenses/LICENSE-2.0
//ugly hack to assert current count = 1
//Simulates the drop processor
/*
//www.apache.org/licenses/LICENSE-2.0
//false, never call processor never increments metrics
//true, always call processor and increments metrics
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// cleanup
// cleanup
/*
//www.apache.org/licenses/LICENSE-2.0
//test that we don't strip out the _source prefix when _ingest is used
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// If there is pipeline defined and a node joins that doesn't have the processor installed then
// that pipeline can't be used on this node.
/*
//www.apache.org/licenses/LICENSE-2.0
// Perform an update with no changes:
// Delete pipeline:
// Delete existing pipeline:
// Start empty
// Start empty
// Start empty
// add a new pipeline:
// overwrite existing pipeline:
// Delete pipeline matching wildcard
// Exception if we used name which does not exist
// match all wildcard works on last remaining pipeline
// match all wildcard does not throw exception if none match
// get all variants: (no IDs or '*')
// Start empty
// Start empty
// Start empty
// Start empty
// Start empty
// Start empty
// Start empty
// Start empty
// Start empty
// Test to make sure that ingest respects content types other than the default index content type
//avoid returning null and dropping the document
// Start empty
//total
//pipeline
//processor
//total
//pipeline
//processor
//update cluster state and ensure that new stats are added to old stats
//total
//pipeline
//The number of processors for the "id1" pipeline changed, so the per-processor metrics are not carried forward. This is
//due to the parallel array's used to identify which metrics to carry forward. With out unique ids or semantic equals for each
//processor, parallel arrays are the best option for of carrying forward metrics between pipeline changes. However, in some cases,
//like this one it may not readily obvious why the metrics were not carried forward.
//test a failure, and that the processor stats are added from the old stats
//total
//pipeline
//processor
//not carried forward since type changed
//carried forward and added from old stats
// Start empty
// Ingest cluster state listener state should be invoked first:
// Processor factory should be invoked secondly after ingest cluster state listener:
// Create ingest service:
// Create pipeline and apply the resulting cluster state, which should update the counter in the right order:
// Start empty
// Sanity check that counter has been updated twice:
//ingest metadata will not be the same (timestamp differs every time)
/*
//www.apache.org/licenses/LICENSE-2.0
//legacy output logic
//pipeline1 -> processor1,processor2; pipeline2 -> processor3
//intentionally enforcing the identical ordering
//pre 6.5 did not serialize any processor stats
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//start the chain
//check the stats
//current
//count
//time
//failure
/*
//www.apache.org/licenses/LICENSE-2.0
//the step for key 2 is never executed due to conditional and thus not part of the result set
//failed processor
//each invocation updates key1 with a random int
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// even after overflowing these should not be negative
// add a path to increase the total bytes until it overflows
// this overflows
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// drop a random setting or two
// we should get an exception that a setting is missing
// or the test will fail for the wrong reason
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// if we are running on HotSpot, and the test JVM was started
// with UseG1GC, then JvmInfo should successfully report that
// G1GC is enabled
// for JDK 9 the default collector when no collector is specified is G1 GC
/*
//www.apache.org/licenses/LICENSE-2.0
// fake debug threshold, info will be double this and warn will
// be triple
// we are faking that youngCollections collections occurred
// this number is chosen so that we squeak over the
// random threshold when computing the average collection
// time: note that average collection time will just be
// youngMultiplier * youngDebugThreshold + 1 which ensures
// that we are over the right threshold but below the next
// threshold
// fake that we did not exceed the threshold
// fake debug threshold, info will be double this and warn will
// be triple
// we are faking that oldCollections collections occurred
// this number is chosen so that we squeak over the
// random threshold when computing the average collection
// time: note that average collection time will just be
// oldMultiplier * oldDebugThreshold + 1 which ensures
// that we are over the right threshold but below the next
// threshold
// fake that we did not exceed the threshold
/*
//www.apache.org/licenses/LICENSE-2.0
// Mem
// Threads
// GC
// Buffer Pools
// Classes
/*
//www.apache.org/licenses/LICENSE-2.0
// load average is unavailable on Windows
// we should be able to get the load average
// one minute load average is available, but 10-minute and 15-minute load averages are not
// unknown system, but the best case is that we have the one-minute load average
// On platforms with no swap
// These could be null if transported from a node running an older version, but shouldn't be null on the current node
// avoid silliness with representing doubles
// This cgroup data is missing a line about cpuacct
// This cgroup data is missing a line about cpu
// This cgroup data is missing a line about memory
/**
// This is the highest value that can be stored in an unsigned 64 bit number, hence too big for long
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Open/Max files descriptors are not supported on Windows platforms
// CPU percent can be negative if the system recent cpu usage is not available
// CPU time can return -1 if the platform does not support this operation, let's see which platforms fail
// Committed total virtual memory can return -1 if not supported, let's see which platforms fail
/*
//www.apache.org/licenses/LICENSE-2.0
// a cluster name was set
// a cluster name was set
/* path.home is in the base settings */, settings.names().size());
/*
//www.apache.org/licenses/LICENSE-2.0
// leading whitespace not allowed
// trailing whitespace not allowed
// non-LDH hostname not allowed
// close should not interrupt ongoing tasks
// but awaitClose should
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// checks all properties that are expected to be unchanged.
// Once we start changing them between versions this method has to be changed as well
// pick a random long that sometimes exceeds an int:
/*
//www.apache.org/licenses/LICENSE-2.0
// again, using only the indices flag
/*
//www.apache.org/licenses/LICENSE-2.0
// Should succeed, since no wildcards
// Should succeed, since no wildcards
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Needed by {@link ClusterService} **/
/** Needed by {@link PersistentTasksClusterService} **/
// add an unassigned task that should get assigned because it's assigned to a non-existing node or unassigned
// add a task assigned to non-existing node that should not get assigned
// explanation should correspond to the action name
// add a new unassigned task
// we don't have any unassigned tasks - add some
// add a node if there are unassigned tasks
// change routing table to simulate a change
// Remove all unassigned tasks that cause changing assignments they might trigger a significant change
// we don't have any unassigned tasks - adding a node or changing a routing table shouldn't affect anything
// remove a node that doesn't have any tasks assigned to it and it's not the master node
// clear the task
// Just add a random index - that shouldn't change anything
/** Creates a PersistentTasksClusterService with a single PersistentTasksExecutor implemented by a BiFunction **/
/*
//www.apache.org/licenses/LICENSE-2.0
// From time to time update status
// Things that should be serialized
// Things that shouldn't be serialized
/*
//www.apache.org/licenses/LICENSE-2.0
/** Needed by {@link ClusterService} **/
/** Needed by {@link PersistentTasksClusterService} **/
/** Asserts that the given cluster state contains nbTasks tasks that are assigned **/
/** Asserts that the given cluster state contains nbTasks tasks that are NOT assigned **/
/** Asserts that the cluster state contains nbTasks tasks that verify the given predicate **/
/*
//www.apache.org/licenses/LICENSE-2.0
// Make sure that at least one of the tasks is running
// Wait for the task to start
// Restart cluster
// Check that cluster state is correct
// Wait for all tasks to start
// Complete the running task and make sure it finishes properly
// Make sure the task is removed from the cluster state
/*
//www.apache.org/licenses/LICENSE-2.0
// Wait for the task to start
// Verifying parent
// Fail the running task and make sure it restarts properly
// Wait for the task to disappear completely
// Wait for the task to start
// Verifying parent and description
//try sending completion request with incorrect allocation id
// Make sure that the task is still running
// Verifying the task runs on the new node
// Wait for the task to disappear completely
// Remove the persistent task
// Speed up rechecks to a rate that is quicker than what settings would allow
// Verifying the task can now be assigned
// Remove the persistent task
// Complete the running task and make sure it finishes properly
// Wait for the task to disappear
// Complete the running task and make sure it finishes properly
// Wait for the task to start
// Fail the running task and make sure it restarts properly
// Wait for the task to disappear completely
// Speed up rechecks to a rate that is quicker than what settings would allow
// Disallow re-assignment after it is unallocated to verify master and node state
// Verify that the task is NOT running on the node
// Verify that the task is STILL in internal cluster state
// Allow it to be reassigned again to the same node
// Verify it starts again
// Complete or cancel the running task
// Complete the running task and make sure it finishes properly
// Cancel the running task and make sure it finishes properly
// Wait for the task to start
// Wait for the task to finish
// Make sure the task is removed from the cluster state
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// need to account for 5 original tasks on each node and their relocations
// Action for this node was added, let's make sure it was invoked
// Add task on some other node
// Make sure action wasn't called again
// Start another task on this node
// Make sure action was called this time
// Finish both tasks
// Add task on some other node
// Make sure action wasn't called again
// Simulate reallocation of the failed task on the same node
// Simulate removal of the finished task
// Make sure action was only allocated on this node once
// Allocate first task
// Check the task is know to the task manager
// Make sure it returns correct status
// Relocate the task to some other node or remove it completely
// Make sure it returns correct status
// That should trigger cancellation request
// Notify successful cancellation
// finish or fail task
// Check the task is now removed from task manager
// Failed to start the task, make sure it wasn't invoked further
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Implements equals and hashcode for testing
// wait for something to happen
// speedup finishing on closed nodes
// This can take a while during large cluster restart
// Cancellation make cause different ways for the task to finish
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// The directory structure on macOS must match Apple's .app
// structure or Gatekeeper may refuse to run the program
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// previous settings still exist
// added setting exists
// does not override pre existing settings
// force a "Not a directory" exception to be thrown so that we can extract the locale-dependent message
// locale-dependent translation of "Not a directory"
// control iteration order, so we get know the beginning of the cycle
// control iteration order
// control iteration order
// control iteration order
// static inner class of this test
// copy from jar, exactly as is
// copy from dir, and use a different canonical path to not conflict with test classpath
// Note: testing dup codebase with core is difficult because it requires a symlink, but we have mock filesystems and security manager
// need a jar file of core dep, use log4j here
// This test opens a child classloader, reading a jar under the test temp
// dir (a dummy plugin). Classloaders are closed by GC, so when test teardown
// occurs the jar is deleted while the classloader is still open. However, on
// windows, files cannot be deleted when they are still open by a process.
// This test opens a child classloader, reading a jar under the test temp
// dir (a dummy plugin). Classloaders are closed by GC, so when test teardown
// occurs the jar is deleted while the classloader is still open. However, on
// windows, files cannot be deleted when they are still open by a process.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// make sure the cluster state is green, and all has been recovered
// make sure the cluster state is green, and all has been recovered
// now start shutting nodes down
// make sure the cluster state is green, and all has been recovered
// make sure the cluster state is green, and all has been recovered
// closing the 3rd node
// make sure the cluster state is green, and all has been recovered
// make sure the cluster state is yellow, and all has been recovered
// see https://github.com/elastic/elasticsearch/issues/14387
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// now flush, just to make sure we have some data in the index, not just translog
// now start another node, while we index
// make sure the cluster state is green, and all has been recovered
// now flush, just to make sure we have some data in the index, not just translog
// now flush, just to make sure we have some data in the index, not just translog
// now start more nodes, while we index
// now, shutdown nodes
//Printing out shards and their doc count
//if there was an error we try to wait and see if at some point it'll get fixed
//lets now make the test fail if it was supposed to fail
/*
//www.apache.org/licenses/LICENSE-2.0
// sync global checkpoint quickly so we can verify seq_no_stats aligned between all copies after tests.
// if we have replicas shift those
// we want to control refreshes
// if we have replicas shift those
// wait for shard to reach post recovery
// verify cluster was finished.
// the default of the `index.allocation.max_retries` is 5.
// index while relocating
// move all shards to the new nodes (it waits on relocation)
// we want to control refreshes
// we want to control refreshes
// corrupting the segments_N files in order to make sure future recovery re-send files
// flip one byte in the content
/*
//www.apache.org/licenses/LICENSE-2.0
// now start another one so we move some primaries
/*
//www.apache.org/licenses/LICENSE-2.0
// test relies on exact file extensions
/**
// we use 2 nodes a lucky and unlucky one
// the lucky one holds the primary
// the unlucky one gets the replica and the truncated leftovers
// create the index and prevent allocation on any other nodes than the lucky one
// we have no replicas so far and make sure that we allocate the primary on the lucky node
// only allocate on the lucky node
// index some docs and check if they are coming back
// ensure we have flushed segments and make them a big one via optimize
// double flush to create safe commit in case of async durability
//
// now allow allocation on all nodes
// at this point we got some truncated left overs on the replica on the unlucky node
// now we are allowing the recovery to allocate again and finish to see if we wipe the truncated files
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// assert equals and hashcode
// assert equals when using index name for id
//assert not equals when name or id differ
/*
//www.apache.org/licenses/LICENSE-2.0
// Would throw
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// test that adding the same snapshot id to the repository data throws an exception
// test that adding a snapshot and its indices works
// verify that the new repository data has the new snapshot and its indices
// if it was a new index, only the new snapshot should be in its set
// test that initializing indices works
// make sure the repository data's indices no longer contain the removed snapshot
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// index documents in the shards
// snapshot the shard
// capture current store files
// close the shard
// delete some random files in the store
// build a new shard using the same store directory as the closed shard
// restore the shard
// check that the shard is not corrupted
// check that all files have been restored
// index documents in the shards
// snapshot the shard
/** Create a {@link Repository} with a random name **/
// eliminate thread name check as we create repo manually
// Apply state once to initialize repo properly like RepositoriesService would
/** Create a {@link Environment} with random path.home and path.repo **/
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// the reason for this plug-in is to drop any assertSnapshotOrGenericThread as mostly all access in this test goes from test threads
// eliminate thread name check as we access blobStore on test/main threads
// write to and read from a index file with no entries
// write to and read from an index file with snapshots but no indices
// write to and read from a index file with random repository data
// write to index generational file
// adding more and writing to a new index generational file
// removing a snapshot and writing to a new index generational file
// write to index generational file
// write repo data again to index generational file, errors because we already wrote to the
// N+1 generation from which this repository data instance was created
/*
//www.apache.org/licenses/LICENSE-2.0
//noinspection OptionalGetWithoutIsPresent because we know there's a subdirectory
/*
//www.apache.org/licenses/LICENSE-2.0
// roll back to the first snap and then incrementally restore
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// if we try to decode the path, this will throw an IllegalArgumentException again
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// test it
// log, then forward
// empty text, not a valid header
/* ESC */, (char)31 /* unit separator*/, (char)127 /* DEL */) +
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// We want to have reproducible results in this test, hence we disable real memory usage accounting
// we can do this here only because we know that we don't adjust breaker settings dynamically in the test
// the rest controller relies on the caller to stash the context, so we should expect these values here as we didn't stash the
// context in this test
// don't want to test everything -- just that it actually wraps the handler
// don't want to test everything -- just that it actually wraps the handlers
/**
//no op
// we will produce an error in the rest handler and one more when sending the error response
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//tools.ietf.org/html/rfc2616#section-10.4.6">HTTP/1.1 -
/*
/*
/*
// Remove OPTIONS, or else we'll get a 200 instead of 405
// Initialize test candidate RestController
// A basic RestHandler handles requests to the endpoint
// Register valid test handlers with test RestController
// Generate a test request with an invalid HTTP method
// Send the request and verify the response status code
/*
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//localhost:9200", "http://localhost:9215",
//localhost:9200", "https://localhost");
//localhost:9200", "http://localhost:9215/foo",
//");
// This is a valid URL
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// don't check the actual builder being closed so we can test auto close
/*
//www.apache.org/licenses/LICENSE-2.0
// the empty responses are handled in the HTTP layer so we do
// not assert on them here
// do this to mimic what the rest layer does
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//    # Assumes the following setup
//    curl -X PUT "localhost:9200/index" -H "Content-Type: application/json" -d'
//    {
//      "aliases": {
//        "foo": {},
//        "foobar": {}
//      }
//    }'
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// GIVEN a valid query
// WHEN
// THEN query is valid (i.e. not marked as invalid)
// GIVEN an empty (i.e. invalid) query wrapped into a valid JSON
// WHEN
// THEN query is marked as invalid
// GIVEN an invalid query due to a malformed JSON
// WHEN
// THEN query is marked as invalid
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// now, verify the table is correct
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// should be matched as well due to the aliases
// no match
// invalid alias
// timestamp
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//no channels get closed in this test, hence we expect as many channels as we created in the map
/**
/**
//no need to wait here, there will be no close listener registered, nothing to wait for.
//here the channel will be first registered, then straight-away removed from the map as the close listener is invoked
//test that cancel tasks is best effort, failure received are not propagated
//make sure that search is sometimes also called from the same thread before the task is returned
//if the channel is already closed, the listener gets notified immediately, from the same thread.
/*
//www.apache.org/licenses/LICENSE-2.0
// see https://github.com/elastic/elasticsearch/issues/13278
// all is well, we can't have two mappings, one provided, and one in the alias
// Expected
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// wait for events again to make sure we got the aliases on all nodes
/*
/*
//Let's make sure that, even though 2 docs are available, only one is returned according to the size we set in the request
//Therefore the reduce phase has taken place, which proves that the QUERY_AND_FETCH search type wasn't erroneously forced.
/*
//www.apache.org/licenses/LICENSE-2.0
//github.com/elastic/elasticsearch/issues/33857", Constants.WINDOWS);
// creates random routing groups and repeatedly halves the index until it is down to 1 shard
// verifying that the count is correct for each shrunken index
// we need the floor and ceiling of the routing_partition_size / factor since the partition size of the shrunken
// index will be one of those, depending on the routing value
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Very simple sanity checks for {@link ClassPermission} */
/** not recommended but we test anyway */
/*
//www.apache.org/licenses/LICENSE-2.0
// each call to get or getValue will be run with limited permissions, just as they are in scripts
/* Create a temporary directory to prove we are running with the
// the assertions are run with the same reduced privileges scripts run with
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Simple tests for {@link ScriptException} */
/** ensure we can round trip in serialization */
/** Test that our elements are present in the json output */
/** ensure the script stack is immutable */
/** ensure no parameters can be null */
/*
//www.apache.org/licenses/LICENSE-2.0
// check that allowing more than available doesn't pollute the returned contexts
/*
//www.apache.org/licenses/LICENSE-2.0
// failure to load to old namespace scripts with the same id but different langs
// failure to load a new namespace script and old namespace script with the same id but different langs
// failure to load a new namespace script and old namespace script with the same id but different langs with additional scripts
// okay to load the same script from the new and old namespace if the lang is the same
// ScriptMetaData doesn't allow us to see the scripts inside it so
// the best we can do here is create a new random instance and rely
// on the fact that the new instance is very unlikely to be equal to
// the old one
/*
//www.apache.org/licenses/LICENSE-2.0
// only care about compilation, not execution
//prevent duplicates using map
//mock the script that gets retrieved from an index
// even though circuit breaking is allowed to be configured per minute, we actually weigh this over five minutes
// simply by multiplying by five, so even setting it to one, requires five compilations to break
// should pass
// should pass
// should pass
// should pass
// pass
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// simple script value string
// complex template using script as the field name
// complex script with script object
// complex script using "code" backcompat
// complex script with script object and empty options
// complex script with embedded template
// check for missing lang parameter when parsing a script
// check for missing source parameter when parsing a script
// check for illegal options parameter when parsing a script
// check for unsupported template context
/*
//www.apache.org/licenses/LICENSE-2.0
//Would be nice to have a single builder that gets its name as a parameter, but the name wouldn't get a value when the object
//is created reading from the stream (constructor that takes a StreamInput) which is a problem as we check that after reading
//a named writeable its name is the expected one. That's why we go for the following less dynamic approach.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// resultWindow greater than maxResultWindow and scrollContext is null
// resultWindow greater than maxResultWindow and scrollContext isn't null
// resultWindow not greater than maxResultWindow and both rescore and sort are not null
// rescore is null but sort is not null and rescoreContext.getWindowSize() exceeds maxResultWindow
// rescore is null but sliceBuilder is not null
// No exceptions should be thrown
/*
//www.apache.org/licenses/LICENSE-2.0
// not checking exception messages as they could depend on the JVM
// not checking exception messages as they could depend on the JVM
/*
//www.apache.org/licenses/LICENSE-2.0
// the last doc must be a root doc
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Make sure we have a few segments
// The response might not have failed on all shards - we need to clean scroll
// Disable block so the first request would pass
// Enable block so the second request would block
// The response didn't fail completely - update scroll id
/*
//www.apache.org/licenses/LICENSE-2.0
// we need at least 2 segments - so no merges should be allowed
/*
//www.apache.org/licenses/LICENSE-2.0
//sort fields are simplified before serialization, we write directly the simplified version
//otherwise equality comparisons become complicated
// This instance is used to test the transport serialization so it's fine
// to produce shard targets (withShardTarget is true) since they are serialized
// in this layer.
// We don't set SearchHit#shard (withShardTarget is false) in this test
// because the rest serialization does not render this information so the
// deserialized hit cannot be equal to the original instance.
// There is another test (#testFromXContentWithShards) that checks the
// rest serialization with shard targets.
// The index uuid is not serialized in the rest layer
/*
//www.apache.org/licenses/LICENSE-2.0
// jump to first START_OBJECT
/**
// jump to first START_OBJECT
/**
// jump to first START_OBJECT
/*
//www.apache.org/licenses/LICENSE-2.0
//add here deprecated queries to make sure we log a deprecation warnings when they are used
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// this kills the keep-alive reaper we have to reset the node after this test
// if refresh happens while checking the exception, the subsequent reference count might not match, so we switch it off
// here we trigger some refreshes to ensure the IR go out of scope such that we hit ACE if we access a search
// context in a non-sane way.
/* not a scroll */);
// that's fine
// the search context should inherit the default timeout
// the search context should inherit the query timeout
/**
// adding the maximum allowed number of docvalue_fields to retrieve
/**
// adding the maximum allowed number of script_fields to retrieve
/**
// Open all possible scrolls, clear some of them, then open more until the limit is reached
// make sure that the wrapper is called when the context is actually created
// we add a search action listener in a plugin above to assert that this is actually used
// we still make sure can match is executed on the network thread
/**
// induce an artificial NPE
/*
//www.apache.org/licenses/LICENSE-2.0
//make sure that for BytesRef, we provide a specific doc value format that overrides format(BytesRef)
//xcontent serialization doesn't write/parse the raw sort values, only the formatted ones
//make sure that BytesRef are not provided as formatted values
//to simplify things, we directly serialize what we expect we would parse back when testing xcontent serialization
// skip to the elements start array token, fromXContent advances from there if called
/*
//www.apache.org/licenses/LICENSE-2.0
// this line causes timeouts to report failures
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// very frequent checks
// see issue #5165 - this test fails each time without the fix in pull #5170
/*
//www.apache.org/licenses/LICENSE-2.0
// simple field aggregation, no scores needed
// agg on a script => scores are needed
// TODO: can we use a mock script service here?
// String scriptAgg = "{ \"my_terms\": {\"terms\": {\"script\": \"doc['f'].value\"}}}";
// assertTrue(needsScores(index, scriptAgg));
//
// String subScriptAgg = "{ \"my_outer_terms\": { \"terms\": { \"field\": \"f\" }, \"aggs\": " + scriptAgg + "}}";
// assertTrue(needsScores(index, subScriptAgg));
// make sure the information is propagated to sub aggregations
// top_hits is a particular example of an aggregation that needs scores
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Lower down the number of buckets generated by multi bucket aggregation tests in
// order to avoid too many aggregations to be created.
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Equivalent to:
//
// List values = doc['values'];
// double[] res = new double[values.size()];
// for (int i = 0; i < res.length; i++) {
//      res[i] = values.get(i) - dec;
// };
// return res;
/*
//www.apache.org/licenses/LICENSE-2.0
// register aggregations as NamedWriteable
// parseAggregators expects to be already inside the xcontent object
// ensure that the unlikely does not happen: 2 aggs share the same name
// just a couple of aggregations, sufficient for the purpose of this test
// never reached
// just 1 type of pipeline agg, sufficient for the purpose of this test
/*
//www.apache.org/licenses/LICENSE-2.0
// we have to prefer CURRENT since with the range of versions we support
// it's rather unlikely to get the current actually.
// create some random type with some default field, those types will
// stick around for all of the subclasses
// the aggregation name is missing
//.startObject("tag_count")
//.endObject()
// the aggregation type is missing
//.startObject("cardinality")
//.endObject()
// Check the filter was rewritten from a wrapper query to a terms query
// Check that a further rewrite returns the same aggregation factories builder
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//create some random type with some default field, those types will stick around for all of the subclasses
/**
/**
// TODO we only change name and boost, we should extend by any sub-test supplying a "mutate" method that randomly changes one
// aspect of the object under test
// we use the streaming infra to create a copy of the query provided as
// argument
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// disables the max bucket limit for this test
// Make sure that unordered, reversed, disjoint and/or overlapping ranges are supported
// Duel with filters
// test long/double/string terms aggs with high number of buckets that require array growth
// Duel between histograms and scripted terms
// test high numbers of percentile buckets to make sure paging and release work correctly
// https://github.com/elastic/elasticsearch/issues/6435
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//serialize this enough times to make sure that we are able to write again what we read
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// identical behavior to AbstractSerializingTestCase, except assertNotSame is only called for
// compound and aggregation order because _key and _count orders are static instances.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// setScorer should not be invoked as it has not been set
// Only collect should be invoked:
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// randomly index the document twice so that we have deleted
// docs that match the filter
// See NullPointer issue when filters are empty:
// https://github.com/elastic/elasticsearch/issues/8438
// both, both&tag1, both&tag2, tag1&tag2
// Check intersection buckets are computed correctly by comparing with
// ANDed query bucket results
// Empty intersections are not returned.
// Create more filters than is permitted by Lucene Bool clause settings.
// Helper methods for building maps of QueryBuilders
/*
//www.apache.org/licenses/LICENSE-2.0
//TODO[PCS]: add builder pattern here
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// You may use the code below to evaluate the impact of the BucketUtils.suggestShardSideQueueSize
/*// You may use the code below to evaluate the impact of the BucketUtils.suggestShardSideQueueSize
// heuristic
// parameter of the zipf distribution
// shuffle terms
// distribute into shards like routing would
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: would be nice to have more random data here
// date: Jan 2, dates: Jan 2, Feb 3
// date: Feb 2, dates: Feb 2, Mar 3
// date: Feb 15, dates: Feb 15, Mar 16
// date: Mar 2, dates: Mar 2, Apr 3
// date: Mar 15, dates: Mar 15, Apr 16
// date: Mar 23, dates: Mar 23, Apr 24
/*
/**
/**
/*
// we're testing on days, so the base must be rounded to a day
// in days
// should be in the middle
// randomizing the number of buckets on the min bound
// (can sometimes fall within the data range, but more frequently will fall before the data range)
// randomizing the number of buckets on the max bound
// (can sometimes fall within the data range, but more frequently will fall after the data range)
// it could be that the random bounds.min we chose ended up greater than
// bounds.max - this should
// trigger an error
// constructing the newly expected bucket list
// when explicitly specifying a format, the extended bounds should be defined by the same format
// expected
/**
//github.com/elastic/elasticsearch/issues/12278)
// we pick a random timezone offset of +12/-12 hours and insert two documents
// one at 00:00 in that time zone and one at 12:00
// retrieve those docs with the same time zone and extended bounds
/**
//github.com/elastic/elasticsearch/issues/23776)
// retrieve those docs with the same time zone and extended bounds
/**
/**
/**
/**
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/**
//Search interval 24 hours
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Jan 2
// Feb 2
// Feb 15
// Mar 2
// Mar 15
// Mar 23
// dummy docs
//there is a daylight saving time change on 11th March so suffix will be different
/*
/*
/*
/*
/*
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/**
// using no format should work when to/from is compatible with format in
// mapping
// using different format should work when to/from is compatible with
// format in aggregation
// providing numeric input with format should work, but bucket keys are
// different now
// providing numeric input without format should throw an exception
/**
// using no format should work when to/from is compatible with format in
// mapping
// using no format should also work when and to/from are string values
// also e-notation should work, fractional parts should be truncated
// using different format should work when to/from is compatible with
// format in aggregation
// providing different numeric input with format should work, but bucket
// keys are different now
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// idx_unmapped_author is same as main index but missing author field
// "id,cat,name,price,inStock,author_t,series_t,sequence_i,genre_s",
// Tests that we can refer to nested elements under a sample in a path
// statement
// For this test to be useful we need >1 genre bucket to compare
// Test multiple samples gathered under buckets made by a parent agg
// Test samples nested under samples
// One of the indexes is missing the "author" field used for
// diversifying results
//All of the indices are missing the "author" field used for diversifying results
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: randomize the size?
// used to test order by single-bucket sub agg
// the main purpose of this test is to make sure we're not allocating 2GB of memory per shard
// Find total number of unique terms
// Gather terms using partitioned aggregations
/*
// the max for "1" is 2
// the max for "0" is 4
// expected
// expected
// expected
// expected
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
// randomly index the document twice so that we have deleted docs that match the filter
// See NullPointer issue when filters are empty:
// https://github.com/elastic/elasticsearch/issues/8438
/*
//www.apache.org/licenses/LICENSE-2.0
// randomly index the document twice so that we have deleted docs that match the filter
// See NullPointer issue when filters are empty:
// https://github.com/elastic/elasticsearch/issues/8438
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// original should be unchanged
// The other bucket is disabled by default
// but setting a key enables it automatically
// unless the other bucket is explicitly disabled
// test non-keyed filter that doesn't rewrite
// test non-keyed filter that does rewrite
// test keyed filter that doesn't rewrite
// test non-keyed filter that does rewrite
// test sub-agg filter that does rewrite
// unkeyed array
// keyed object
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// below 500km
// above 500km, below 1000km
// above 1000km
// random cities with no location
// first point is within the ~17.5km, the second is ~710km
// first point is ~576km, the second is within the ~35km
// above 1000km
// random cities with no location
// TODO: use diamond once JI-9019884 is fixed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
//generate random point
//Index at the highest resolution
//Update expected doc counts for all resolutions..
// Update expected doc counts for all resolutions..
//Check we only have one bucket with the best match for that resolution
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Intentionally not writing any docs
// Note that `global`'s fancy support for ignoring the query comes from special code in AggregationPhase. We don't test that here.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// from setup we have between 6 and 20 documents, each with value 1 in test field
// first bucket should start at -5, contain 4 documents
// last bucket should have (numDocs % interval + 1) docs
/**
// shifting by offset>2 creates new extra bucket [0,offset-1]
// if offset is >= number of values in original last bucket, that effect is canceled
// first bucket
// randomizing the number of buckets on the min bound
// (can sometimes fall within the data range, but more frequently will fall before the data range)
// randomizing the number of buckets on the max bound
// (can sometimes fall within the data range, but more frequently will fall after the data range)
// it could be that the random bounds.min we chose ended up greater than bounds.max - this should cause an
// error
// constructing the newly expected bucket list
// expected
// randomizing the number of buckets on the min bound
// (can sometimes fall within the data range, but more frequently will fall before the data range)
// randomizing the number of buckets on the max bound
// (can sometimes fall within the data range, but more frequently will fall after the data range)
// it could be that the random bounds.min we chose ended up greater than bounds.max - this should cause an
// error
// constructing the newly expected bucket list
// expected
/**
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO randomize the size?
// used to test order by single-bucket sub agg
// TODO randomize the size?
// the main purpose of this test is to make sure we're not allocating 2GB of memory per shard
// Find total number of unique terms
// Gather terms using partitioned aggregations
/*
// the max for "1" is 2
// the max for "0" is 4
// expected
// expected
// expected
// expected
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
// check that terms2 is a subset of terms1
// all terms
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we need at least one child overall
// Test based on: https://github.com/elastic/elasticsearch/issues/9280
// This must be 0
// and this must be empty
// This must be 0
// and this must be empty
/*
//www.apache.org/licenses/LICENSE-2.0
// shift sequence by 1, to ensure we have negative values, and value 3 on the edge of the tested ranges
// Create two indices and add the field 'route_length_miles' as an alias in
// one, and a concrete field in the other.
// 1 + 2
// 3 + 4 + 5
// 2
// 3, 4, 5
/*
/*
/*
/*
// TODO: use diamond once JI-9019884 is fixed
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// nested.field2: 1
// nested.field2: 4
// nested.field2: 7
// nested.field2: 2
// nested.field2: 3
// nested.field2: 5
// nested.field2: 6
// nested.field2: 8
// nested.field2: 9
// Test that parsing the reverse_nested agg doesn't fail, because the parent nested agg is unmapped:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// idx_unmapped_author is same as main index but missing author field
// "id,cat,name,price,inStock,author_t,series_t,sequence_i,genre_s",
// Tests that we can refer to nested elements under a sample in a path
// statement
// For this test to be useful we need >1 genre bucket to compare
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we still only return 3 entries (based on the 'size' param)
// <-- count is now fixed
// we still only return 3 entries (based on the 'size' param)
// <-- count is now fixed
// we still only return 3 entries (based on the 'size' param)
// <-- count is now fixed
// we still only return 3 entries (based on the 'size' param)
// <-- count is now fixed
/*
//www.apache.org/licenses/LICENSE-2.0
// we need at least 2
// routing key to shard 1
// routing key to shard 2
/*
// total docs in shard "1" = 15
// total docs in shard "2"  = 12
/*
//www.apache.org/licenses/LICENSE-2.0
// Use significant_text on text fields but occasionally run with alternative of
// significant_terms on legacy fieldData=true too.
// Now create some holes in the index with selective deletes caused by updates.
// This is the scenario that caused this issue https://github.com/elastic/elasticsearch/issues/7951
// Scoring algorithms throw exceptions if term docFreqs exceed the reported size of the index
// from which they are taken so need to make sure this doesn't happen.
// compute significance score by
// 1. terms agg on class and significant terms
// 2. filter buckets and set the background to the other class and set is_background false
// both should yield exact same result
//check that results for both classes are the same with exclude negatives = false and classes are routing ids
/**
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// see https://github.com/elastic/elasticsearch/issues/5998
//high score but low doc freq
//low score but high doc freq
// make sure the terms all get score > 0 except for this one
// first, check that indeed when not setting the shardMinDocCount parameter 0 terms are returned
// see https://github.com/elastic/elasticsearch/issues/5998
//low doc freq but high score
//low score but high doc freq
// first, check that indeed when not setting the shardMinDocCount parameter 0 terms are returned
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// filter size grater than max size should thrown a exception
// return builder itself to skip rewrite
// filter size not grater than max size should return an instance of AdjacencyMatrixAggregatorFactory
/*
//www.apache.org/licenses/LICENSE-2.0
// InternalAdjacencyMatrix represents the upper triangular matrix:
// 2 filters (matrix of 2x2) generates 3 buckets
// 3 filters generates 6 buckets
// 4 filters generates 10 buckets
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// sort ascending, null bucket is first
// sort descending, null bucket is last
/*
// test with no bucket
// source field and index sorting config have different order
// reverse source order
/*
//www.apache.org/licenses/LICENSE-2.0
// we don't create global ordinals but we test this mode when the reader has a single segment
// since ordinals are global in this case.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we use specific format only for date histogram on a long/date field
// and the raw format for the other types
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ensure we add at least one date histo
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// make sure we have more than one segment to test the merge
/*
//www.apache.org/licenses/LICENSE-2.0
// make sure we have more than one segment to test the merge
// filter name already present so it should be merge with the previous one ?
// make sure we have more than one segment to test the merge
// Always true because we include 'other' in the agg
/*
//www.apache.org/licenses/LICENSE-2.0
// this is what the FiltersAggregationBuilder ctor does when not providing KeyedFilter
/*
//www.apache.org/licenses/LICENSE-2.0
// Nothing extra to assert
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// Intentionally not writing any docs
// Precision-adjust longitude/latitude to avoid wrong bucket placement
// Internally, lat/lng get converted to 32 bit integers, loosing some precision.
// This does not affect geohashing because geohash uses the same algorithm,
// but it does affect other bucketing algos, thus we need to do the same steps here.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// can create a factory
// can create a factory
// 5.6cm is approx. smallest distance represented by precision 12
// can create a factory
// can create a factory
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// can create a factory
// can create a factory
// can create a factory
/*
//www.apache.org/licenses/LICENSE-2.0
// precision values below 8 can lead to parsing errors
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
//noinspection ConstantConditions
/**
/**
// z=30
// z=0,x=0,y=1
// z=0,x=1,y=0
/**
// GeoPoint would be in the center of the bucket, thus must produce the same hash
// Same point should be generated from the string key
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Nothing extra to assert
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// since an exception is thrown, this assertion won't be invoked.
/*
//www.apache.org/licenses/LICENSE-2.0
// 5 sec interval with minDocCount = 0
// 5 sec interval with minDocCount = 3
// 5 sec interval with minDocCount = 0
// 5 sec interval with minDocCount = 3
/*
//www.apache.org/licenses/LICENSE-2.0
// no timeZone => no rewrite
// fixed timeZone => no rewrite
// daylight-saving-times => rewrite if doesn't cross
// Rounded values are no longer all within the same transitions => no rewrite
// ~ 1 day
// Because the interval is large, rounded values are not
// within the same transitions as the values => no rewrite
// ~ 1 month
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// Calendar interval case - three months, three bucketLong.MIN_VALUE;s
/*
// Fixed interval case - 4 periods of 30 days
// No offset, just to make sure the ranges line up as expected
// 10 minute offset should shift all data into one bucket
// No offset, just to make sure the ranges line up as expected
// 10 minute offset should shift all data into one bucket
/*
/*
// Guard case, make sure the agg buckets as expected without min doc count
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// end of year 9999
// beginning of year -9999
// Construct with one missing bound
/**
// It'd probably be better to randomize the formatter
// parsed won't *equal* expected because equal includes the String parts
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// Since we pass 0 as the starting index to getAppropriateRounding, we'll also use
// an innerInterval that is quite large, such that targetBuckets * roundings[i].getMaximumInnerInterval()
// will be larger than the estimate.
// We want to pass a roundingIdx of zero, because in order to reproduce this bug, we need the function
// to increment the rounding (because the bug was that the function would not use the innerIntervals
// from the new rounding.
// First, try to calculate the correct innerInterval using the normalizedDuration.
// This handles cases where highest and lowest are further apart than the interval being used.
//Next, if our bucketCount is still above what we need, we'll go back and determine the interval
// based on a size calculation.
// Iterate through the input buckets, and for each bucket, determine if it's inside
// the range of the bucket in the outer loop. if it is, add the doc count to the total
// for that bucket.
// If there is only a single bucket, and we haven't added it above, add a bucket with no documents.
// this step is necessary because of the roundedBucketKey < keyForBucket + intervalInMillis above.
// pick out the actual reduced values to the make the assertion more readable
/*
//www.apache.org/licenses/LICENSE-2.0
//in order for reduction to work properly (and be realistic) we need to use the same interval, minDocCount, emptyBucketInfo
//and base in all randomly created aggs as part of the same test run. This is particularly important when minDocCount is
//set to 0 as empty buckets need to be added to fill the holes.
//it's ok if min and max are outside the range of the generated buckets, that will just mean that
//empty buckets won't be added before the first bucket and/or after the last one
//avoid having different random instance start from exactly the same base
//rarely leave some holes to be filled up with empty buckets in case minDocCount is set to 0
/*
//www.apache.org/licenses/LICENSE-2.0
//in order for reduction to work properly (and be realistic) we need to use the same interval, minDocCount, emptyBucketInfo
//and offset in all randomly created aggs as part of the same test run. This is particularly important when minDocCount is
//set to 0 as empty buckets need to be added to fill the holes.
//it's ok if minBound and maxBound are outside the range of the generated buckets, that will just mean that
//empty buckets won't be added before the first bucket and/or after the last one
//rarely leave some holes to be filled up with empty buckets in case minDocCount is set to 0
// issue 26787
// Set the key of one bucket to NaN. Must be the last bucket because NaN is greater than everything else.
/*
//www.apache.org/licenses/LICENSE-2.0
// This throws a number format exception (which is a subclass of IllegalArgumentException) and might be ok?
// Note, these values are carefully chosen to ensure that no matter what offset we pick, no two can end up in the same bucket
/*
//www.apache.org/licenses/LICENSE-2.0
// bucket 0 5
// bucket -5, 0
// bucket 0, 5, 10
// bucket 40, 45
// bucket 0 5
// bucket -5, 0
// bucket 0, 5, 10
// bucket 40, 45
// bucket 0 5
// bucket -5, 0
// bucket 0, 5, 10
// bucket 40, 45
// bucket 0
// bucket 0
// bucket 0, 5, 10
// bucket 0, 5
// bucket 0 5
// bucket -5, 0
// bucket 0, 5, 10
// bucket -15
// bucket 0, 5
// bucket  5, 10
// bucket 10
// bucket -1, 4
// bucket -6 -1 4
// bucket 4, 9
// bucket 39, 44, 49
//assertEquals(7, histogram.getBuckets().size());
// bucket 0 5
// bucket -5, 0
// bucket 0, 5, 10
// bucket 40, 45
// I'd like to randomize the offset here, like I did in the test for the numeric side, but there's no way I can think of to
// construct the intervals such that they wouldn't "slosh" between buckets.
/*
//www.apache.org/licenses/LICENSE-2.0
// Nothing extra to assert
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Nothing extra to assert
/*
//www.apache.org/licenses/LICENSE-2.0
// Nothing extra to assert
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// intentionally not writing any docs
//add some random nested docs that don't belong
// 1 segment with, 1 root document, with 3 nested sub docs
// 1 segment with:
// 1 document, with 1 nested subdoc
// and 1 document, with 1 nested subdoc
// The bug manifests if 6 docs are returned, because currentRootDoc isn't reset the previous child docs from the first
// segment are emitted as hits.
// reverse order:
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// intentionally not writing any docs
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// simulate aggregation
// now do it the naive way
// simulate aggregation
// now do it the naive way
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Add some overlapping range
/*
//www.apache.org/licenses/LICENSE-2.0
// Add some overlapping range
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Add some overlapping range
/*
//www.apache.org/licenses/LICENSE-2.0
// Apparently we expect a string here
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// "id,cat,name,price,inStock,author_t,series_t,sequence_i,genre_s,genre_id",
// wrong field:
// "id,cat,name,price,inStock,author_t,series_t,sequence_i,genre_s,genre_id",
// huge shard_size
// huge maxDocsPerValue
/*
//www.apache.org/licenses/LICENSE-2.0
// Nothing extra to assert
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// flush on open to have a single segment with predictable docIds
// flush on open to have a single segment with predictable docIds
// Test with an outrageously large size to ensure that the maxDoc protection works
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// test that stream output can actually be read - does not replace bwc test
// write
// read
// populates the registry through side effects
// Create aggregations as they might come from three different shards and return as list.
// test that
// 1. The output of the builders can actually be parsed
// 2. The parser does not swallow parameters after a significance heuristic was defined
// test jlh with string
// test gnd with string
// test mutual information with string
// test with builders
// test exceptions
//term is only in the subset, not at all in the other set but that is because the other set is empty.
// this should actually not happen because only terms that are in the subset are considered now,
// however, in this case the score should be 0 because a term that does not exist cannot be relevant...
// the terms do not co-occur at all - should be 0
// comparison between two terms that do not exist - probably not relevant
// terms co-occur perfectly - should be 1
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// flush on open to have a single segment
// Use a background filter which just happens to be same scope as whole-index.
// Search "odd"
// Search even
// Search odd with regex includeexcludes
// Search with string-based includeexcludes
/**
// flush on open to have a single segment
// Search "odd"
/**
// flush on open to have a single segment
// Attempt aggregation on unmapped field
// Search "odd"
/**
// flush on open to have a single segment
// Attempt aggregation on range field
// flush on open to have a single segment
// Use a background filter which just happens to be same scope as whole-index.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// flush on open to have a single segment
// Search "odd" which should have no duplication
// Search "even" which will have duplication
// flush on open to have a single segment
/**
// flush on open to have a single segment
// No significant results to be found in this test - only checking we don't end up
// with the internal exception discovered in issue https://github.com/elastic/elasticsearch/issues/25029
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// The one rare term
// bump to 2 since we're only including "2"
// bump to 2 since we're only including "2"
// Note: the search and reduce test will generate no segments (due to no docs)
// and so will return a null agg because the aggs aren't run/reduced
// bucket 0 5
// bucket -5, 0
// bucket 0, 5, 10
// bucket 40, 45
// match root document only
// match root document only
// match root document only
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// the main purpose of this test is to make sure we're not allocating 2GB of memory per shard
// Find total number of unique terms
// Gather terms using partitioned aggregations
/*
// no execution hint so that the logic that decides whether or not to use ordinals is executed
// the max for "more" is 2
// the max for "less" is 4
// the max for "more" is 2
// the max for "less" is 4
// the max for "more" is 2
// the max for "less" is 4
// expected
// expected
// expected
// expected
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// We do not use LuceneTestCase.newSearcher because we need a DirectoryReader
// if order by count then we need to use compound so that we can also sort by key as tie breaker:
// Note - other places we throw IllegalArgumentException
// Note - other places we throw IllegalArgumentException
// Note - other places we throw IllegalArgumentException
// force the breadth_first mode
// match root document only
// match root document only
/*
//www.apache.org/licenses/LICENSE-2.0
// Added to debug a test failure where the terms aggregation seems to be reporting two documents with the same
// value for NUMBER_FIELD_NAME. This will check that after random indexing each document only has 1 value for
// NUMBER_FIELD_NAME and it is the correct value. Following this initial change its seems that this call was getting
// more that 2000 hits (actual value was 2059) so now it will also check to ensure all hits have the correct index and type.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Script to take a field name in params and sum the values of the field. */
/** Script to sum the values of a field named {@code values}. */
/** Script to return the value of a field named {@code value}. */
/** Script to return the {@code _value} provided by aggs framework. */
/** Script to return a random double */
// Intentionally not writing any docs
// Summing up a normal array and expect an accurate value
// Summing up an array which contains NaN and infinities and expect a result same as naive summation
// Summing up some big double values and expect infinity result
/**
// Test that an aggregation not using a script does get cached
/**
// Test that an aggregation using a deterministic script gets cached
// Test that an aggregation using a nondeterministic script does not get cached
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Intentionally not writing any docs
/*
//www.apache.org/licenses/LICENSE-2.0
// linear counting should be picked, and should be accurate
// error is not bound, so let's just make sure it is > 0
// assertThat(global.getDocCount(), equalTo(numDocs));
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Kahan summation gave the same result no matter what order we added
// naive addition gave a small floating point error
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//since the value(49.95) is a constant, variance should be 0
// Summing up an array which contains NaN and infinities and expect a result same as naive summation
// Summing up some big double values and expect infinity result
/*
//www.apache.org/licenses/LICENSE-2.0
// Same as previous test, but uses a default value for sigma
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
// Intentionally not writing any docs
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
// random hashes
// special cases
//System.out.println(e.cardinality(bucket) + " <> " + set.size());
// use a gaussian so that all instances don't collect as many hashes
// hashes with lots of leading zeros trigger different paths in the code that we try to go through here
// all hashes felt into the same bucket so hll would expect a count of 1
/*
//www.apache.org/licenses/LICENSE-2.0
// Summing up some big double values and expect infinity result
// we don't print out VALUE_AS_STRING for avg.getCount() == 0, so we cannot get the exact same value back
/*
//www.apache.org/licenses/LICENSE-2.0
//we force @After to have it run before ESTestCase#after otherwise it fails
/*
//www.apache.org/licenses/LICENSE-2.0
// The order in which you add double values in java can give different results. The difference can
// be larger for large sum values, so we make the delta in the assertion depend on the values magnitude
// summing squared values, see reason for delta above
// for count == 0, fields are rendered as `null`, so  we test that we parse to default values used also in the reduce phase
// also as_string values are only rendered for count != 0
// Summing up some big double values and expect infinity result
/*
//www.apache.org/licenses/LICENSE-2.0
// we occasionally want to test top = Double.NEGATIVE_INFINITY since this triggers empty xContent object
/*
//www.apache.org/licenses/LICENSE-2.0
// Re-encode lat/longs to avoid rounding issue when testing InternalGeoCentroid#hashCode() and
// InternalGeoCentroid#equals()
// if the new count is > 0 then we need to make sure there is a
// centroid or the constructor will throw an exception
/*
//www.apache.org/licenses/LICENSE-2.0
// it is hard to check the values due to the inaccuracy of the algorithm
/*
//www.apache.org/licenses/LICENSE-2.0
// it is hard to check the values due to the inaccuracy of the algorithm
/*
//www.apache.org/licenses/LICENSE-2.0
// we write Double.NEGATIVE_INFINITY and Double.POSITIVE_INFINITY to xContent as 'null', so we
// cannot differentiate between them. Also we cannot recreate the exact String representation
/*
//www.apache.org/licenses/LICENSE-2.0
// Double.compare handles NaN, which we use for no result
/*
//www.apache.org/licenses/LICENSE-2.0
// we write Double.NEGATIVE_INFINITY and Double.POSITIVE_INFINITY to xContent as 'null', so we
// cannot differentiate between them. Also we cannot recreate the exact String representation
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we want the same value types (also for nested lists, maps) for all random aggregations
// the last one needs to be a leaf value, not map or list
/**
// mock script always returns the size of the input aggs list as result
// longs that fit into the integer range are parsed back as integer
// based on the xContent type, floats are sometimes parsed back as doubles
// the last one needs to be a leaf value, not map or
// list
/*
//www.apache.org/licenses/LICENSE-2.0
// no test since reduce operation is unsupported
/*
//www.apache.org/licenses/LICENSE-2.0
// Summing up some big double values and expect infinity result
// for count == 0, fields are rendered as `null`, so  we test that we parse to default values used also in the reduce phase
// also as_string values are only rendered for count != 0
// count is greater than zero
// count is zero
/*
//www.apache.org/licenses/LICENSE-2.0
// Summing up a normal array and expect an accurate value
// Summing up an array which contains NaN and infinities and expect a result same as naive summation
// Summing up some big double values and expect infinity result
/*
//www.apache.org/licenses/LICENSE-2.0
// it is hard to check the values due to the inaccuracy of the algorithm
// the min/max values should be accurate due to the way the algo works so we can at least test those
// quantiles would return NaN
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Lucene's TopDocs initializes the maxScore to Float.NaN, if there is no maxScore
// Lucene's TopDocs initializes the maxScore to Float.NaN, if there is no maxScore
/**
// Values passed to getComparator shouldn't matter
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we don't print out VALUE_AS_STRING for avg.getCount() == 0, so we cannot get the exact same value back
/*
//www.apache.org/licenses/LICENSE-2.0
/** Script to take a field name in params and sum the values of the field. */
/** Script to sum the values of a field named {@code values}. */
/** Script to return the value of a field named {@code value}. */
/** Script to return the {@code _value} provided by aggs framework. */
/** Script to return a random double */
// Intentionally not writing any docs
// Note this is the script value (19L), not the doc values above
// insert some documents without a value for the metric field.
// checks that documents inside the max leaves are all deleted
// Do not add any documents
/**
// Test that an aggregation not using a script does get cached
/**
// Test that an aggregation using a script does not get cached
// Test that an aggregation using a nondeterministic script does not get cached
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// intentionally not writing any docs
/**
// even
// odd
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The name of the script engine type this plugin provides. */
/** Script to take a field name in params and sum the values of the field. */
/** Script to sum the values of a field named {@code values}. */
/** Script to return the value of a field named {@code value}. */
/** Script to return the {@code _value} provided by aggs framework. */
/** Script to return a random double */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Script to take a field name in params and sum the values of the field. */
/** Script to sum the values of a field named {@code values}. */
/** Script to return the value of a field named {@code value}. */
/** Script to return the {@code _value} provided by aggs framework. */
// Note: this comes straight from missing, and is not inverted from script
// insert some documents without a value for the metric field.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// just add 1 for each doc the script is run on
// intentionally not writing any docs
/**
/**
// all documents have score of 1.0
// The result value depends on the script params.
// The result value depends on the script params.
// No need to add docs for this test
// No need to add docs for this test
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Lazily populate state.list for tests without an init script
// Lazily populate state.list for tests without an init script
// Equivalent to:
//
// newaggregation = [];
// sum = 0;
//
// for (s in state.list) {
//      sum += s
// };
//
// newaggregation.add(sum);
// return newaggregation"
//
// Equivalent to:
//
// newaggregation = [];
// sum = 0;
//
// for (state in states) {
//      for (s in state) {
//          sum += s
//      }
// };
//
// newaggregation.add(sum);
// return newaggregation"
//
// Equivalent to:
//
// newaggregation = [];
// sum = 0;
//
// for (state in states) {
//      for (s in state) {
//          sum += s
//      }
// };
//
// newaggregation.add(sum * multiplier);
// return newaggregation"
//
// creating an index to test the empty buckets functionality. The way it
// works is by indexing
// two docs {value: 0} and {value : 2}, then building a histogram agg
// with interval 1 and with empty
// buckets computed.. the empty bucket is the one associated with key
// "1". then each test will have
// to check that this bucket exists with the appropriate sub
// aggregations.
// When using the MockScriptPlugin we can map Stored scripts to inline scripts:
// the id of the stored script is used in test method while the source of the stored script
// must match a predefined script from CustomScriptPlugin.pluginScripts() method
// When using the MockScriptPlugin we can map File scripts to inline scripts:
// the name of the file script is used in test method while the source of the file script
// must match a predefined script from CustomScriptPlugin.pluginScripts() method
// We don't know how many shards will have documents but we need to make
// sure that at least one shard ran the map script
// Split the params up between the script and the aggregation.
// A particular shard may not have any documents stored on it so
// we have to assume the lower bound may be 0. The check at the
// bottom of the test method will make sure the count is correct
// A particular shard may not have any documents stored on it so
// we have to assume the lower bound may be 0. The check at the
// bottom of the test method will make sure the count is correct
// We'll just get a ClassCastException a couple lines down if we're wrong, its ok.
/**
// Make sure we are starting with a clear cache
// Test that a non-deterministic init script causes the result to not be cached
// Test that a non-deterministic map script causes the result to not be cached
// Test that a non-deterministic combine script causes the result to not be cached
// NOTE: random reduce scripts don't hit the query shard context (they are done on the coordinator) and so can be cached.
// Test that all deterministic scripts cause the request to be cached
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Summing up a normal array and expect an accurate value
// Summing up an array which contains NaN and infinities and expect a result same as naive summation
// Summing up some big double values and expect infinity result
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Intentionally not writing any docs
// Summing up a normal array and expect an accurate value
// Summing up an array which contains NaN and infinities and expect a result same as naive summation
// Summing up some big double values and expect infinity result
/*
//www.apache.org/licenses/LICENSE-2.0
// Create two indices and add the field 'route_length_miles' as an alias in
// one, and a concrete field in the other.
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: Fix T-Digest: this assertion should pass but we currently get ~15
// https://github.com/elastic/elasticsearch/issues/14851
// assertThat(rank.getPercent(), Matchers.equalTo(0d));
// TODO: Fix T-Digest: this assertion should pass but we currently get ~59
// https://github.com/elastic/elasticsearch/issues/14851
// assertThat(rank.getPercent(), Matchers.equalTo(100d));
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
// Intentionally not writing any docs
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
// Regression test for #19528
// See https://github.com/tdunning/t-digest/pull/70/files#diff-4487072cee29b939694825647928f742R439
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// The "a" bucket
// The "b" bucket
// The "c" bucket
// The "d" bucket
// We do not use LuceneTestCase.newSearcher because we need a DirectoryReader for "testInsideTerms"
// only merge adjacent segments
// first window (see BooleanScorer) has matches on one clause only
// any doc in 0..2048
// second window has matches in two clauses
// any doc in 0..2048
// we need all docs to be in the same segment
// just check that it does not fail with exceptions
/*
//www.apache.org/licenses/LICENSE-2.0
// Test that top_hits aggregation is fed scores if query results size=0
// Also check that min_score setting works when size=0
// (technically not a test of top_hits but implementation details are
// tied up with the need to feed scores into the agg tree even when
// users don't want ranked set of query results.)
// Also need to sort on _doc because there are two reviewers with the same name
// randomly from stored field or _source
// Can't explain nested hit with the main query, since both are in a different scopes, also the nested doc may not
// even have matched with the main query.
// If top_hits would have a query option then we can explain that query
// Returns the version of the root document. Nested docs don't have a separate version
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script field does not get cached
// Test that a request using a nondeterministic script sort does not get cached
// Test that a request using a deterministic script field does not get cached
// Test that a request using a deterministic script sort does not get cached
// Ensure that non-scripted requests are cached as normal
// delete this - if we use tests.iters it would fail
// Rescore with default sort on relevancy (score)
// Rescore should not be applied if the sort order is not relevancy
/*
//www.apache.org/licenses/LICENSE-2.0
// parent test shuffles xContent, we need to make sure highlight fields are ordered
/*
//www.apache.org/licenses/LICENSE-2.0
// Intentionally not writing any docs
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Make sure we are starting with a clear cache
// Test that a request using a nondeterministic script does not get cached
// Test that a request using a deterministic script gets cached
// Ensure that non-scripted requests are cached as normal
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Intentionally not writing any docs
// (7*2 + 2*3 + 3*3) / (2+3+3) == 3.625
// Summing up a normal array and expect an accurate value
// Summing up an array which contains NaN and infinities and expect a result same as naive summation
// Summing up some big double values and expect infinity result
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Finally, reduce the pipeline agg
// Histo has to go first to exercise the bug
/*
//www.apache.org/licenses/LICENSE-2.0
// + (fieldValue < 0 ? -1 : 0) - (minRandomValue / interval - 1);
/*
//www.apache.org/licenses/LICENSE-2.0
// First try to point to a non-existent agg
// Now try to point to a single bucket agg
// Now try to point to a valid multi-bucket agg (no exception should be thrown)
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Script source is not interpreted but it references a pre-defined script from CustomScriptPlugin
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Source is not interpreted but my_script is defined in CustomScriptPlugin
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// These become our baseline
// Now let's test using size
// Finally, let's test using size + from
// Since all max values are equal, we expect the order of keyBucketSort to have been preserved
/*
//www.apache.org/licenses/LICENSE-2.0
// Check if the combination ended up being invalid
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// some index names used during these tests
// TODO: would be nice to have more random data here
// date: Jan 2, dates: Jan 2, Feb 3
// date: Feb 2, dates: Feb 2, Mar 3
// date: Feb 15, dates: Feb 15, Mar 16
// date: Mar 2, dates: Mar 2, Apr 3
// date: Mar 15, dates: Mar 15, Apr 16
// date: Mar 23, dates: Mar 23, Apr 24
/**
// epoch millis: 1332547200000
// day with dst shift, only 23h long
// the following is normalized using a 23h bucket width
/**
// day with dst shift -1h, 25h long
// the following is normalized using a 25h bucket width
/**
// the shift happens during the next bucket, which includes the 45min that do not start on the full hour
// the following is normalized using a 105min bucket width
/*
//www.apache.org/licenses/LICENSE-2.0
// expected bucket values for random setup with gaps
// setup for index with empty buckets
// randomized setup for index with empty buckets
/**
/**
// start value, gets
// overwritten
// start value, gets
// overwritten
// make approximately half of the buckets empty
// make approximately half of the buckets empty
/*
//www.apache.org/licenses/LICENSE-2.0
// expected bucket values for random setup with gaps
// setup for index with empty buckets
// randomized setup for index with empty buckets
// make approximately half of the buckets empty
/**
/**
// start value, gets
// overwritten
// start value, gets
// overwritten
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// + (fieldValue < 0 ? -1 : 0) - (minRandomValue / interval - 1);
// creates 6 documents where the value of the field is 0, 1, 2, 3,
// 3, 5
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// First try to point to a non-existent agg
// Now try to point to a single bucket agg
// Now try to point to a valid multi-bucket agg (no exception should be
// thrown)
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// no test since reduce operation is unsupported
// we write Double.NEGATIVE_INFINITY and Double.POSITIVE_INFINITY to xContent as 'null', so we
// cannot differentiate between them. Also we cannot recreate the exact String representation
/*
//www.apache.org/licenses/LICENSE-2.0
// no test since reduce operation is unsupported
// we write Double.NEGATIVE_INFINITY, Double.POSITIVE amd Double.NAN to xContent as 'null', so we
// cannot differentiate between them. Also we cannot recreate the exact String representation
/*
//www.apache.org/licenses/LICENSE-2.0
// no test since reduce operation is unsupported
/*
//www.apache.org/licenses/LICENSE-2.0
// no test since reduce operation is unsupported
// we cannot ensure we get the same as_string output for Double.NaN values since they are rendered as
// null and we don't have a formatted string representation in the rest output
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// no test since reduce operation is unsupported
// we write Double.NEGATIVE_INFINITY, Double.POSITIVE amd Double.NAN to xContent as 'null', so we
// cannot differentiate between them. Also we cannot recreate the exact String representation
/*
//www.apache.org/licenses/LICENSE-2.0
// + (fieldValue < 0 ? -1 : 0) - (minRandomValue / interval - 1);
/**
// you need to add an additional index with no fields in order to trigger this (or potentially a shard)
// so that there is an UnmappedTerms in the list to reduce.
/*
//www.apache.org/licenses/LICENSE-2.0
// First try to point to a non-existent agg
// Now try to point to a single bucket agg
// Now try to point to a valid multi-bucket agg (no exception should be
// thrown)
/*
//www.apache.org/licenses/LICENSE-2.0
// + (fieldValue < 0 ? -1 : 0) - (minRandomValue / interval - 1);
/*
//www.apache.org/licenses/LICENSE-2.0
// First try to point to a non-existent agg
// Now try to point to a single bucket agg
// Now try to point to a valid multi-bucket agg (no exception should be
// thrown)
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Trend value
// HW requires at least two periods of data
// Smoothed value
// Trend value
// Seasonal value
// Initial level value is average of first season
// Calculate the slopes between first and second season for each period
// Calculate first seasonal
// HW requires at least two periods of data
// HW requires at least two periods of data
// Smoothed value
// Trend value
// Seasonal value
// Initial level value is average of first season
// Calculate the slopes between first and second season for each period
// Calculate first seasonal
/*
//www.apache.org/licenses/LICENSE-2.0
// + (fieldValue < 0 ? -1 : 0) - (minRandomValue / interval - 1);
// All good
/*
//www.apache.org/licenses/LICENSE-2.0
// First try to point to a non-existent agg
// Now try to point to a single bucket agg
// Now try to point to a valid multi-bucket agg (no exception should be
// thrown)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// start a gap
// add to the existing gap
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// This is a gap bucket
// This is a gap bucket
/**
// Gaps only apply to metric values, not doc _counts
// If there was a gap in doc counts and we are ignoring, just skip this bucket
// otherwise insert a zero instead of the true value
// If this isn't a gap, or is a _count, just insert the value
// Still under the initial lag period, add nothing and move on
// Peek here, because we rely on add'ing to always move the window
// Normalize null's to NaN
// Both have values, calculate diff and replace the "empty" bucket
// The tests need null, even though the agg doesn't
/*
//www.apache.org/licenses/LICENSE-2.0
// + (fieldValue < 0 ? -1 : 0) - (minRandomValue / interval - 1);
/*
//www.apache.org/licenses/LICENSE-2.0
// First try to point to a non-existent agg
// Now try to point to a single bucket agg
// Now try to point to a valid multi-bucket agg (no exception should be
// thrown)
/*
//www.apache.org/licenses/LICENSE-2.0
// + (fieldValue < 0 ? -1 : 0) - (minRandomValue / interval - 1);
/*
//www.apache.org/licenses/LICENSE-2.0
// First try to point to a non-existent agg
// Now try to point to a single bucket agg
// Now try to point to a valid multi-bucket agg (no exception should be
// thrown)
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// means everything included
// Serializes/deserializes an IncludeExclude statement with a single clause
// Serializes/deserializes the IncludeExclude statement with include AND
// exclude clauses
// Include and Exclude clauses are parsed independently and then merged
/*
//www.apache.org/licenses/LICENSE-2.0
// exists in the current segment
// missing in all segments
// exists in other segments only
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// expected
/*
//www.apache.org/licenses/LICENSE-2.0
// Script values are supposed to support null, single values, arrays and collections
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// 1 replica in our default...
// TODO: randomize the wait for active shards value on index creation and ensure the appropriate
// number of data nodes are started for the randomized active shard count value
// we will go the primary or the replica, but in a
// randomized re-creatable manner
// at least one shard should be successful when refreshing
// we want to make sure that while recovery happens, and a replica gets recovered, its properly refreshed
// first, verify that search normal search works
/*
//www.apache.org/licenses/LICENSE-2.0
// if we did not search all shards but had no failures that is potentially fine
// if only the hit-count is wrong. this can happen if the cluster-state is behind when the
// request comes in. It's a small window but a known limitation.
// this is the more critical but that we hit the actual hit array has a different size than the
// actual number of hits.
// it's possible that all shards fail if we have a small number of shards.
// with replicas this should not happen
// this might time out on some machines if they are really busy and you hit lots of throttling
// if we hit only non-critical exceptions we make sure that the post search works
/*
//www.apache.org/licenses/LICENSE-2.0
// rarely no exception
// don't assert on failures here
// we don't check anything here really just making sure we don't leave any open files or a broken index behind.
// check match all
/*
//www.apache.org/licenses/LICENSE-2.0
// rarely no exception
// we cannot expect that the index will be valid
// it's OK to timeout here
/* some seeds just won't let you create the index at all and we enter a ping-pong mode
// don't assert on failures here
// we don't check anything here really just making sure we don't leave any open files or a broken index behind.
// check match all
// if a scheduled refresh or flush fails all shards we see all shards failed here
// check the index still contains the records that we indexed without errors
/*
//www.apache.org/licenses/LICENSE-2.0
// all is well
// all is well
/*
//www.apache.org/licenses/LICENSE-2.0
// we need to have age (ie number of repeats of "test" term) high enough
// to produce the same 8-bit norm for all docs here, so that
// the tf is basically the entire score (assuming idf is fixed, which
// it should be if dfs is working correctly)
// With the current way of encoding norms, every length between 1048 and 1176
// are encoded into the same byte
// finished
// finished
// finished
// finished
// all is well
// Add custom score query with bogus script
/*
//www.apache.org/licenses/LICENSE-2.0
// sometimes we move it on the START_OBJECT to
// test the embedded case
// TODO add test checking that changing any member of this class produces an object that is not equal to the original
//we use the streaming infra to create a copy of the builder provided as argument
/**
//verify that only what is set gets printed out through toXContent
// invalid format
// invalid format
// invalid format
// invalid format
/*
//www.apache.org/licenses/LICENSE-2.0
//disable xcontent shuffling on the highlight builder
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Directly refer to the second level:
// Issue #9723
// number_of_shards = 1, because then we catch the expected exception in the same way.
// (See expectThrows(...) below)
// index the message in an object form instead of an array
// the field name (comments.message) used for source filtering should be the same as when using that field for
// other features (like in the query dsl or aggs) in order for consistency:
// Source filter on a field that does not exist inside the nested document and just check that we do not fail and
// return an empty _source:
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Execute search at least two times to load it in cache
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
//this way `fields` is printed out as a json array
/**
/**
/**
/**
// shard context will only need indicesQueriesRegistry for building Query objects nested in highlighter
/**
/**
/**
/**
/**
// need to set this together, otherwise parsing will complain
// also test the string setter
// also test the string setter
/**
/**
// change settings that only exists on top level
// add another field
// change existing fields
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO as we move analyzers out of the core we need to move some of these into HighlighterWithAnalyzersTests
// test the kibana case with * as fieldname that will try highlight all fields including meta fields
// see #3486
// we don't store title and don't use term vector, now lets see if it works...
// we don't store title, now lets see if it works...
// we don't store title, now lets see if it works...
//asking for the whole field to be highlighted
//sentences will be generated out of each value
// Issue #5175
//postings hl doesn't support require_field_match, its field needs to be queried directly
//works using stored field
/**
// Index one megabyte of "t   " over and over and over again
/*
// First check highlighting without any matched fields set
// And that matching a subfield doesn't automatically highlight it
// Add the subfield to the list of matched fields but don't match it.  Everything should still work
// like before we added it.
// Now make half the matches come from the stored field and half from just a matched field.
// Now remove the stored field from the matched field list.  That should work too.
// Now make sure boosted fields don't blow up when matched fields is both the subfield and stored field.
// Now just all matches are against the matched field.  This still returns highlighting.
// And all matched field via the queryString's field parameter, just in case
// Finding the same string two ways is ok too
// But we use the best found score when sorting fragments
// which can also be written by searching on the subfield
// Speaking of two fields, you can have two fields, only one of which has matchedFields enabled
// And you can enable matchedField highlighting on both
// Setting a matchedField that isn't searched/doesn't exist is simply ignored.
// If the stored field doesn't have a value it doesn't matter what you match, you get nothing.
// If the stored field is found but the matched field isn't then you don't get a result either.
// But if you add the stored field to the list of matched fields then you'll get a result again
// You _can_ highlight fields that aren't subfields of one another.
// LUCENE 3.1 UPGRADE: Caused adding the space at the end...
// LUCENE 3.1 UPGRADE: Caused adding the space at the end...
// simple search on body with standard analyzer with a simple field query
// search on title.key and highlight on title
// simple search on body with standard analyzer with a simple field query
// search on title.key and highlight on title.key
// simple search on body with standard analyzer with a simple field query
// search on title.key and highlight on title
// simple search on body with standard analyzer with a simple field query
// search on title.key and highlight on title.key
//should not fail if there is a wildcard
// Because of SOLR-3724 nothing is highlighted when FVH is used
// Using plain highlighter instead of FVH
// Using plain highlighter instead of FVH on the field level
// This query used to fail when the field to highlight was absent
// Issue #3211
// Highlighting of numeric fields is not supported, but it should not raise errors
// (this behavior is consistent with version 0.20)
// Issue #3200
// Mock tokenizer will throw an exception if it is resetted twice
// Make sure the highlightQuery is taken into account when it is set on the highlight context instead of the field
// When you don't set noMatchSize you don't get any results if there isn't anything to highlight.
// When noMatchSize is set to 0 you also shouldn't get any
// When noMatchSize is between 0 and the size of the string
// The FVH also works but the fragment is longer than the plain highlighter because of boundary_max_scan
// Unified hl also works but the fragment is longer than the plain highlighter because of the boundary is the word
// We can also ask for a fragment longer than the input string and get the whole string
// We can also ask for a fragment exactly the size of the input field and get the whole field
// unified hl returns the first sentence as the noMatchSize does not cross sentence boundary.
// You can set noMatchSize globally in the highlighter as well
// We don't break if noMatchSize is less than zero though
// The no match fragment should come from the first value of a multi-valued field
// And noMatchSize returns nothing when the first entry is empty string!
// except for the unified highlighter which starts from the first string with actual content
// But if the field was actually empty then you should get no highlighting field
// Same for if the field doesn't even exist on the document
// Again same if the field isn't mapped
// The no match fragment should come from the first value of a multi-valued field
//if there's a match we only return the values with matches (whole value as number_of_fragments == 0)
//lets fall back to the standard highlighter then, what people would do to highlight query matches
// simple search on body with standard analyzer with a simple field query
//lets make sure we analyze the query and we highlight the resulting terms
//stopwords are not highlighted since not indexed
// search on title.key and highlight on title
//stopwords are now highlighted since we used only whitespace analyzer here
// simple search on body with standard analyzer with a simple field query
// search on title.key and highlight on title.key
//generating text with word to highlight in a different position
//(https://github.com/elastic/elasticsearch/issues/4103)
/**
// Match queries
// Query string with a single field
// Query string with a single field without dismax
// Query string with more than one field
// Query string boosting the field
// Try with a bool query
// Try with a boosting query
// Try with a boosting query using a negative boost
// check that we do not get an exception for geo_point fields in case someone tries to highlight
// it accidentially with a wildcard
// see https://github.com/elastic/elasticsearch/issues/17537
// same as above but in this example the query gets rewritten during highlighting
// see https://github.com/elastic/elasticsearch/issues/17537#issuecomment-244939633
// check that keyword highlighting works
// For unified and fvh highlighters we just check that the nested query is correctly extracted
// but we highlight the root text field since nested documents cannot be highlighted with postings nor term vectors
// directly.
/** Sole constructor. */
/*
//www.apache.org/licenses/LICENSE-2.0
// we need to wrap xContent output in proper object to create a parser for it
// skip to the opening object token, fromXContent advances from here and starts with the field name
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Ensure the response has entries for both requested fields.
// Check the capabilities for the 'distance' field.
// Check the capabilities for the 'route_length_miles' alias.
/*
//www.apache.org/licenses/LICENSE-2.0
// field2 is not stored, check that it is not extracted from source.
// see #8203
// off by default on binary fields
/*
//www.apache.org/licenses/LICENSE-2.0
// Test Gauss
// Test Exp
// Test Lin
// add tw docs within offset
// add docs outside offset
// Test Gauss
// Test Exp
// Test Lin
// force no dummy docs
// Test Gauss
// Test Exp
// this is equivalent to new GeoPoint(20, 11); just flipped so scores must be same
// decay score should return 0.5 for this function and baseQuery should return 2.0f as it's score
// so, we indexed a string field, but now we try to score a num field
// so, we indexed a string field, but now we try to score a num field
// Index for testing MIN and MAX
// Now test AVG and SUM
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// document 2 scores higher because 17 > 5
// try again, but this time explicitly use the do-nothing modifier
// document 1 scores higher because 1/5 > 1/17
// doc 3 doesn't have a "test" field, so an exception will be thrown
// We are expecting an exception, because 3 has no field
// doc 3 doesn't have a "test" field but we're defaulting it to 100 so it should be last
// field is not mapped but we're defaulting it to 100 so all documents should have the same score
// -1 divided by 0 is infinity, which should provoke an exception.
// This is fine, the query will throw an exception if executed
// locally, instead of just having failures
/*
//www.apache.org/licenses/LICENSE-2.0
// Pass the random score as a document field so that it can be extracted in the script
//github.com/elastic/elasticsearch/issues/10253 */
/** make sure min_score works if functions is empty, see https://github.com/elastic/elasticsearch/issues/10253 */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// this
// we cannot assert that they are equal since some shards might not have docs at all
// Make sure non-zero from works:
// Tests a rescore window smaller than number of hits:
// Now, rescore only top 2 hits w/ proximity:
// Only top 2 hits were re-ordered:
// Now, rescore only top 3 hits w/ proximity:
// Only top 3 hits were re-ordered:
// Tests a rescorer that penalizes the scores:
// Now, penalizing rescore (nothing matches the rescore query):
// 6 and 1 got worse, and then the hit (2) outside the rescore window were sorted ahead:
// Comparator that sorts hits and rescored hits in the same way.
// The rescore uses the docId as tie, while regular search uses the slot the hit is in as a tie if score
// and shard id are equal during merging shard results.
// This comparator uses a custom tie in case the scores are equal, so that both regular hits and rescored hits
// are sorted equally. This is fine since tests only care about the fact the scores should be equal, not ordering.
// we need to cut off here since this is the tail of the queue and we might not have fetched enough docs
// forces QUERY_THEN_FETCH because of https://github.com/elastic/elasticsearch/issues/4829
// no dummy docs since merges can change scores while we run queries.
// ensure we hit the same shards for tie-breaking
// no weight - so we basically use the same score as the actual query
// ensure we hit the same shards for tie-breaking
// check equivalence
// ensure we hit the same shards for tie-breaking
// check equivalence
// ensure we hit the same shards for tie-breaking
// First set the rescore window large enough that both rescores take effect
// Now squash the second rescore window so it never gets to see a seven
// We have no idea what the second hit will be because we didn't get a chance to look for seven
// Now use one rescore to drag the number we're looking for into the window of another
// Not sure which one it is but it is ninety something
// #11277
/*
//www.apache.org/licenses/LICENSE-2.0
// make sure we are done otherwise preference could change?
// at least one char!!
// randomPreference should not start with '_' (reserved for known preference types (e.g. _shards)
// get all docs otherwise we are prone to tie-breaking
// for tie-breaking we have to resort here since if the score is
// identical we rely on collection order which might change.
// randomly change some docs to get them in different segments
// watch out this is inclusive the max values!
// we add 1 to the index field to make sure that the scripts below never compute log(0)
// Test for accessing _score
// Test for accessing _score.intValue()
// Test for accessing _score.longValue()
// Test for accessing _score.floatValue()
// Test for accessing _score.doubleValue()
// all random scores should be in range [0.0, 1.0]
// get all docs otherwise we are prone to tie-breaking
// get all docs otherwise we are prone to tie-breaking
// get all docs otherwise we are prone to tie-breaking
/*
//www.apache.org/licenses/LICENSE-2.0
// to NY: 5.286 km
// to NY: 0.4621 km
// to NY: 1.055 km
// to NY: 1.258 km
// to NY: 2.029 km
// to NY: 8.572 km
// from NY
// from NY
/*
//www.apache.org/licenses/LICENSE-2.0
// Test doc['location'].arcDistance(lat, lon)
// Test doc['location'].planeDistance(lat, lon)
// Test doc['location'].geohashDistance(lat, lon)
// Test doc['location'].arcDistance(lat, lon + 360)/1000d
// Test doc['location'].arcDistance(lat + 360, lon)/1000d
// no hits please
/*
//www.apache.org/licenses/LICENSE-2.0
// self intersection polygon
// polygon with hole
// polygon with overlapping hole
// polygon with intersection holes
// Common line in polygon
// Multipolygon: polygon with hole and polygon within the whole
// Create a multipolygon with two polygons. The first is an rectangle of size 10x10
// with a hole of size 5x5 equidistant from all sides. This hole in turn contains
// the second polygon of size 4x4 equidistant from all sites
// Point in polygon
// Point in polygon hole
// by definition the border of a polygon belongs to the inner
// so the border of a polygons hole also belongs to the inner
// of the polygon NOT the hole
// Point on polygon border
// Point on hole border
// Point not in polygon
// Point in polygon hole
// Create a polygon that fills the empty area of the polygon defined above
// re-check point on polygon hole
// Create Polygon with hole and common edge
// Polygon WithIn Polygon
// Create a polygon crossing longitude 180.
// Create a polygon crossing longitude 180 with hole.
// Simple root case
// Root cases (Outer cells)
// Root crossing dateline
// level1: simple case
// Level1: crossing cells
// disjoint works in terms of intersection
/*
//www.apache.org/licenses/LICENSE-2.0
// to NY: 5.286 km
// to NY: 0.4621 km
// to NY: 1.055 km
// to NY: 1.258 km
// to NY: 2.029 km
// to NY: 8.572 km
// from NY
// from NY
// from NY
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// create index
// left orientation test
// right orientation test
/**
// create index
// test self crossing ccw poly not crossing dateline
// create index
/**
// create index
// create index
/*
//www.apache.org/licenses/LICENSE-2.0
// close the polygon
// This search would fail if both geoshape indexing and geoshape filtering
// used the bottom-level optimization in SpatialPrefixTree#recursiveGetNodes.
// now test the query variant
// Create a random geometry collection.
//issues.apache.org/jira/browse/LUCENE-8634 is fixed",
// Create a random geometry collection to index.
// vector strategy does not yet support multipoint queries
// Create a random geometry collection to query
/** tests querying a random geometry collection with a point */
// Create a random geometry collection to index.
// Create a random geometry collection.
// vector strategy does not yet support multipoint queries
// index the mbr of the collection
// Create a random geometry collection.
// no shape
// RandomShapeGenerator created something other than a POINT type, verify the correct exception is thrown
// test that point was inserted
// MULTIPOINT
// POINT
// test that point was inserted
// Test for issue #34418
// A geometry collection that is fully within the indexed shape
// A geometry collection that is partially within the indexed shape
// A geometry collection that is disjoint with the indexed shape
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// create index
// left orientation test
// right orientation test
/**
// create index
// test self crossing ccw poly not crossing dateline
/**
// create index
/**
// create index
// test self crossing of circles
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// add a document that matches foo:bar but will be deleted
// Searching a non-existing term will trigger a null scorer
// make sure scorers are created only once, see #1725
/**
// Not delegated since we change the live docs
// apply deletes when needed:
// slow
// very slow, but necessary in order to be correct
// we don't use the cost
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Non-filtering alias should turn off all filters because filters are ORed
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Add 10 when valueForDisplay is called so it is easy to be sure it *was* called
/*
//www.apache.org/licenses/LICENSE-2.0
//Issue #30148
// Issue #14944
// Issue #2489
// Issue #3039
// Issue #3252
// Implicit list of fields -> ignore numeric fields
// Explicit list of fields including numeric fields -> fail
// mlt query with no field -> exception because _all is not enabled)
// mlt query with string fields
// mlt query with at least a numeric field -> fail by default
// mlt query with at least a numeric field -> fail by command
// mlt query with at least a numeric field but fail_on_unsupported_field set to false
// mlt field query on a numeric field -> failure by default
// mlt field query on a numeric field -> failure by command
// mlt field query on a numeric field but fail_on_unsupported_field set to false
// index one document with all the values
// index each document with only one of the values
// make sure they are not all empty
// routing to ensure we hit the shard with the doc
// strict all terms must match!
// should be properly parsed but ignored ...
// strict all terms must match but date is ignored
//Issue #29678
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// check on no data, see it works
// check the numDocs
// search for something that matches the nested doc, and see that we don't find the nested doc
// now, do a nested query
// add another doc, one that would match if it was not nested...
// filter
// check with type prefix
// check delete, so all is gone...
// check the numDocs
// do some multi nested queries
// When IncludeNestedDocsQuery is wrapped in a FilteredQuery then a in-finite loop occurs b/c of a bug in
// IncludeNestedDocsQuery#advance()
// This IncludeNestedDocsQuery also needs to be aware of the filter from alias
// Doc with missing nested docs if nested filter is used
// access id = 1, read, max value, asc, should use matt and shay
// access id = 1, read, min value, asc, should now use adrien and luca
// execute, by matt or luca, by user id, sort missing first
// missing first
// execute, by matt or luca, by username, sort missing last (default)
// missing last
// https://github.com/elastic/elasticsearch/issues/31554
// sum: 11
// sum: 7
// sum: 2
// Without nested filter
// With nested filter
// Nested path should be automatically detected, expect same results as above search request
// Check if closest nested type is resolved
// Sort mode: sum
// Sort mode: sum with filter
// Sort mode: avg
// Sort mode: avg with filter
// Issue #9305
// No nested mapping yet, there shouldn't be anything in the fixed bit set cache
// Now add nested mapping
// index simple data
// only when querying with nested the fixed bitsets are loaded
/*
//www.apache.org/licenses/LICENSE-2.0
// see #2896
//_only_local is a stricter preference, we need to send the request to a data node
//this test needs at least a replica to make sure two consecutive searches go to two different copies of the same data
//this test needs at least a replica to make sure two consecutive searches go to two different copies of the same data
// multiple wildchar to cover multi-param usecase
// Mix of valid and invalid nodes
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// also often use "small" values in tests
// don't overflow Long.MAX_VALUE;
/**
// "breakdown" just consists of key/value pairs, we shouldn't add anything random there
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// The ProfileResults "breakdown" section just consists of key/value pairs, we shouldn't add anything random there
// also we don't want to insert into the root object here, its just the PROFILE_FIELD itself
/*
//www.apache.org/licenses/LICENSE-2.0
// for the first 256 calls, nanoTime() is called
// once for `start` and once for `stop`
// only called nanoTime() 3356 times, which is significantly less than 100000
// Make sure the cumulated timing is 42 times the number of calls as expected
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// also often use relatively "small" values, otherwise we will mostly test huge longs
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// Check the children
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// scores are not needed
// will use index stats
// exception when getting the scorer
// no exception, means scorerSupplier is delegated
/*
//www.apache.org/licenses/LICENSE-2.0
// make sure to often test this with small values too
/*
//www.apache.org/licenses/LICENSE-2.0
// If depth is exhausted, or 50% of the time return a terminal
// Helps limit ridiculously large compound queries
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: move this to a unit test somewhere...
// simple property
// object fields
// empty doc
// We do NOT index dummy documents, otherwise the type for these dummy documents
// would have _field_names indexed while the current type might not which might
// confuse the exists/missing parser at query time
// exists
/*
//www.apache.org/licenses/LICENSE-2.0
// very likely that we hit a random doc that has the same score so orders are random since
// the doc id is the tie-breaker
// check if it's equivalent to a match query.
// id sort field is a tie, in case hits have the same score,
// the hits will be sorted the same consistently
// id tie sort
// test group based on analyzer -- all fields are grouped into a cross field search
// counter example
// counter example
// test if boosts work
// has ultimate in the last_name and that is boosted
// since we try to treat the matching fields as one field scores are very similar but we have a small bias towards the
// more frequent field that acts as a tie-breaker internally
// Test group based on numeric fields
// Two numeric fields together caused trouble at one point!
/*
// Lenient wasn't always properly lenient with two numeric fields
// Check that cross fields works with date fields
/**
//github.com/elastic/elasticsearch/issues/18710 where a
// we need to cut off here since this is the tail of the queue and we might not have fetched enough docs
/*
//www.apache.org/licenses/LICENSE-2.0
// field with doc-values but not indexed will need to collect
// search sort is a prefix of the index sort
// 1. Test a sort on long field
// 2. Test a sort on long field + date field
// 3. Test a sort on date field
// 4. Test a sort on date field + long field
//github.com/elastic/elasticsearch/issues/49703")
// assert score docs are in order and their number is as expected
// test that docs are properly sorted on the first sort
// test that docs are properly sorted on the secondary sort
// used to check that numeric long or date sort optimization was run
// should not be there, expected to search with CollectorManager
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// nested doesn't match because it's hidden
// bool doesn't match
// binary doesn't match
// suggest doesn't match
// geo_point doesn't match
// geo_shape doesn't match
// properties
// type1
// The wildcard field matches aliases for both a text and geo_point field.
// By default, the geo_point field should be ignored when building the query.
/*
//www.apache.org/licenses/LICENSE-2.0
// test that script_score works as expected:
// 1) only matched docs retrieved
// 2) score is calculated based on a script with params
// 3) min score applied
// applying min score
// test that when the internal query is rewritten script_score works well
// the query should be rewritten to from:null
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// see #3952
// return no docs
// see https://github.com/elastic/elasticsearch/issues/3177
// see #3521
// see #3521
// Issue #3540
//the mapping needs to be provided upfront otherwise we are not sure how many failures we get back
//as with dynamic mappings some shards might be lacking behind and parse a different query
// Issue #7880
//the mapping needs to be provided upfront otherwise we are not sure how many failures we get back
//as with dynamic mappings some shards might be lacking behind and parse a different query
// Issue #10477
//the mapping needs to be provided upfront otherwise we are not sure how many failures we get back
//as with dynamic mappings some shards might be lacking behind and parse a different query
// Timezone set with dates
// Same timezone set with time_zone
// We set a timezone which will give no result
// Same timezone set with time_zone but another timezone is set directly within dates which has the precedence
// repeat..., with terms
// wildcard check
// object check
// this uses dismax so scores are equal and the order can be arbitrary
// Operator only applies on terms inside a field! Fields are always OR-ed together.
// Operator only applies on terms inside a field! Fields are always OR-ed together.
// Operator only applies on terms inside a field! Fields are always OR-ed together.
// Test lenient
//when the number for shards is randomized and we expect failures
//we can either run into partial or total failures depending on the current number of shards
// Fields are ORed together
// Min should match > # optional clauses returns no docs.
// Only one should clause is defined, returns no docs.
//github.com/elastic/elasticsearch/issues/43144")
// test partial matching
// test valid type, but no matching terms
// same as above, just on the _id...
// another search with same parameters...
// index "lookup" id "missing" document does not exist: ignore the lookup terms
// index "lookup3" has the source disabled: ignore the lookup terms
// This made 2826 fail! (only with bit based filters)
// This made #2979 fail!
// see #2926
//issue manifested only with shards>=2
// see #2994
// see #3797
// Now in UTC+1
// We explicitly define a time zone in the from/to dates so whatever the time zone is, it won't be used
// We define a time zone to be applied to the filter and from/to have no time zone
/**
// unset cluster setting
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Tests boost value setting. In this case doc 1 should always be ranked above the other
// two matches.
// test case from #13884
// sub
// fields
// body
// properties
// type1
// Issue #7967
// the bug is that this would be parsed into basically a match_all
// query and this would match both documents
// https://github.com/elastic/elasticsearch/issues/18202
// nested doesn't match because it's hidden
// bool doesn't match
// binary doesn't match
// suggest doesn't match
// geo_point doesn't match
// geo_shape doesn't match
// properties
// type1
// The wildcard field matches aliases for both a text and boolean field.
// By default, the boolean field should be ignored when building the query.
/** Sole constructor. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// shard context will only need indicesQueriesRegistry for building Query objects nested in query rescorer
// shard context will only need indicesQueriesRegistry for building Query objects nested in query rescorer
/**
/**
// move to first token, this is where the internal fromXContent
// only increase the boost to make it a slightly different query
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// aggressive filter caching so that we can assert on the number of iterations of the script filters
/*
//www.apache.org/licenses/LICENSE-2.0
// no replicas, as they might be ordered differently
// we need to control refreshes as they might take different merges into account
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// and now, the last one is one
// a the last is zero
// Whether we actually clear a scroll, we can't know, since that information isn't serialized in the
// free search context response, which is returned from each node we want to clear a particular scroll.
// Fails during base64 decoding (Base64-encoded string must have at least four characters)
// Other invalid base64
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// Enforces that only one shard can only be allocated to a single node
/*
//www.apache.org/licenses/LICENSE-2.0
// We build a json version of the search_after first in order to
// ensure that every number type remain the same before/after xcontent (de)serialization.
// This is not a problem because the final type of each field value is extracted from associated sort field.
// This little trick ensure that equals and hashcode are the same when using the xcontent serialization.
// TODO add equals tests with mutating the original object
// BIG_INTEGER
// BIG_DECIMAL
// ignore json and yaml, they parse floating point numbers as floats/doubles
/**
// Get a valid one
// Now replace its values with one containing the passed in object
/*
//www.apache.org/licenses/LICENSE-2.0
// Convert Integer, Short, Byte and Boolean to Long in order to match the conversion done
// by the internal hits when populating the sort values.
/*
//www.apache.org/licenses/LICENSE-2.0
// randomPreference should not start with '_' (reserved for known preference types (e.g. _shards)
// id is not indexed, but lets see that we automatically convert to
// id is not indexed, but lets see that we automatically convert to
// Note that this is the RESULT window.
// Note that this is the RESULT window
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// test _doc sort
// test numeric sort
/*
//www.apache.org/licenses/LICENSE-2.0
// numSlices > numShards
// numShards > numSlices
// numShards == numSlices
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Returns random sort that is put under test */
/** Returns mutated version of original so the returned sort is different in terms of equals/hashcode */
/** Parse the sort from xContent. Just delegate to the SortBuilder's static fromXContent method. */
/**
/*
// assert potential warnings based on the test sort configuration. Do nothing by default, subtests can overwrite
/**
/**
/**
/**
/* The cast below is required to make Java 9 happy. Java 8 infers the T in copyWriterable to be the same as AbstractSortTestCase's
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// sort mode should also be set by build() implicitly to MIN or MAX if not set explicitly on builder
/**
// need to skip until parser is located on second START_OBJECT
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// sort DESC
// sort ASC
// at most 25 days in the month
// hour of the day
// now check with score tracking
// reindex and refresh
// reindex - no refresh
// force merge
// ignore
// STRING
// BYTE
// SHORT
// INTEGER
// LONG
// FLOAT
// DOUBLE
// TODO: WTF?
//we check that it's a parse failure rather than a different shard failure
// nested field
// nestedQuery
// assertWarnings(ID_FIELD_DATA_DEPRECATION_MESSAGE);
// unset cluster setting
/**
// We sort on nested field
// We sort on nested fields with max_children limit
// We sort on nested sub field
// missing nested path
// Use an ip field, which uses different internal/external
// representations of values, to make sure values are both correctly
// rendered and parsed (search_after)
// Create two indices and add the field 'route_length_miles' as an alias in
// one, and a concrete field in the other.
// Create two indices and add the field 'route_length_miles' as an alias in
// one, and a concrete field in the other.
//*** 1. sort DESC on long_field
/*** 1. sort DESC on long_field
// check the correct sort order
//*** 2. sort ASC on long_field
/*** 2. sort ASC on long_field
// check the correct sort order
/*
//www.apache.org/licenses/LICENSE-2.0
// to NY: 5.286 km
// to NY: 0.4621 km
// to NY: 1.055 km
// to NY: 1.258 km
// to NY: 2.029 km
// to NY: 8.572 km
// Order: Asc
// Order: Asc, Mode: max
// Order: Desc
// Order: Desc, Mode: min
// expected
// Regression bug:
// https://github.com/elastic/elasticsearch/issues/2851
// to NY: 5.286 km
// to NY: 0.4621 km
// Order: Asc
// Order: Desc
// Doc with missing geo point is first, is consistent with 0.20.x
// to NY: 5.286 km
// to NY:
// 0.4621
// km
// to NY: 1.055 km
// to NY: 1.258
// km
// to NY:
// 2.029
// km
// to NY:
// 8.572 km
// Order: Asc
// Order: Asc, Mode: max
// Order: Desc
// Order: Desc, Mode: min
//expected
/**
// to NY: 5.286 km
// to NY: 0.4621 km
// Order: Asc
// Order: Desc
// Doc with missing geo point is first, is consistent with 0.20.x
// Make sure that by default the unmapped fields continue to fail
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**   q     d1       d2
/*
//www.apache.org/licenses/LICENSE-2.0
// don't fully randomize here, GeoDistanceSort is picky about the filters that are allowed
// all good
// make sure the below tests test something...
// The common case should use LatLonDocValuesField.newDistanceSort
// however this might be disabled by fancy options
// 2 points -> plain SortField with a custom comparator
// km rather than m -> plain SortField with a custom comparator
// descending means the max value should be considered rather than min
// can't use LatLon optimized sorting with nested fields
// can't use LatLon optimized sorting with DESC sorting
/**
/**
// also use MultiValueMode.Max if no Mode set but order is DESC
// use MultiValueMode.Min if no Mode and order is ASC
// need to use distance unit other than Meters to not get back a LatLonPointSortField
// need to use distance unit other than Meters to not get back a LatLonPointSortField
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// test that filter gets rewritten
// test that inner nested sort gets rewritten
// test that both filter and inner nested sort get rewritten
// test that original stays unchanged if no element rewrites
// test that rewrite works recursively
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// need to skip until parser is located on second START_OBJECT
// need to skip until parser is located on second START_OBJECT
// all good
/*
//www.apache.org/licenses/LICENSE-2.0
// change one of the constructor args, copy the rest over
// script sort type String only allows MIN and MAX, so we only switch
// we rely on these ordinals in serialization, so changing them breaks bwc.
/**
/**
// check that without mode set, order ASC sets mode to MIN, DESC to MAX
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// STRING script
// add some docs that don't have values in those fields
// test the long values
// test the double values
// test the string values
// test the geopoint values
// TODO: sort shouldn't fail when sort field is mapped dynamically
// We have to specify mapping explicitly because by the time search is performed dynamic mapping might not
// be propagated to all nodes yet and sort operation fail when the sort field is not defined
// a query with docs just with null values
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// one element array, see https://github.com/elastic/elasticsearch/issues/17257
// test two spellings for _geo_disctance
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// we rely on these ordinals in serialization, so changing them breaks bwc.
/*
//www.apache.org/licenses/LICENSE-2.0
/** Check that ordinals remain stable as we rely on them for serialisation. */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/elastic/elasticsearch/issues/5132
/*
//www.apache.org/licenses/LICENSE-2.0
// clear all stats first
// we make sure each node gets at least a single shard...
// THERE WILL BE AT LEAST 2 NODES HERE SO WE CAN WAIT FOR GREEN
// create shards * docs number of docs and attempt to distribute them equally
// this distribution will not be perfect; each shard will have an integer multiple of docs (possibly zero)
// we do this so we have a lot of pages to scroll through
// refresh the stats now that scroll contexts are opened
// the number of queries executed is equal to at least the sum of number of pages in shard over all shards
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// have to set the text because we don't know if the global text was set
/**
/**
/**
// we need to skip the start object and the name, those will be parsed by outer SuggestBuilder
/**
/**
// change ither one of the shared SuggestionBuilder parameters, or delegate to the specific tests mutate method
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// "contexts" is an object consisting of key/array pairs, we shouldn't add anything random there
// also there can be inner search hits fields inside this option, we need to exclude another couple of paths
// where we cannot add random stuff
/*
//www.apache.org/licenses/LICENSE-2.0
// requires custom completion format
/**
// test that suggestion text takes precedence over global text
/**
// the weight is 1000 divided by string length, so the results are easy to to check
// analyzer which removes stopwords... so may not be the simple one
// edit distance 1
// edit distance 2
// suggestion with a character, which needs unicode awareness
// removing unicode awareness leads to no result
// increasing edit distance instead of unicode awareness works again, as this is only a single character
// Index two entities
// load the fst index into ram
// Get all stats
// regexes
// Higher weight so it's ranked first:
// stop word complete, gets ignored on query time, makes it "feed" only
// stopword gets removed, but position increment kicks in, which doesnt work for the prefix suggester
// see #3555
// we have 2 docs in a segment...
// update the first one and then merge.. the target segment will have no value in FIELD
// see #3596
// can cause stack overflow without the default max_input_length
// see #3648
// can cause stack overflow without the default max_input_length
// see #5930
/*
//www.apache.org/licenses/LICENSE-2.0
// requires custom completion format
// filter only on context cat
// filter only on context type
// boost only on context cat
// boost only on context type
// boost on both contexts
// query context order should never matter
// the score of each suggestion is the maximum score among the matching contexts
// Enable store and disable indexing sometimes
// pin
// location
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// explicit about type parameters, see: https://bugs.eclipse.org/bugs/show_bug.cgi?id=481649
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// "contexts" is an object consisting of key/array pairs, we shouldn't add anything random there
// also there can be inner search hits fields inside this option, we need to exclude another couple of paths
// where we cannot add random stuff
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// note: size will not be rendered via "toXContent", only passed on internally on transport layer
// CompletionSuggestion can have max. one entry
// also occasionally test zero entries
// - "contexts" is an object consisting of key/array pairs, we shouldn't add anything random there
// - there can be inner search hits fields inside this option where we cannot add random stuff
// - the root object should be excluded since it contains the named suggestion arrays
// We don't parse size via xContent, instead we set it to -1 on the client side
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// see #3196
// Always, otherwise the results can vary between requests.
// Always, otherwise the results can vary between requests.
// Always, otherwise the results can vary between requests.
// Always, otherwise the results can vary between requests.
// see #3037
/**
// see #2729
// Always, otherwise the results can vary between requests.
// Always, otherwise the results can vary between requests.
// Always, otherwise the results can vary between requests.
// use SuggestMode.ALWAYS, otherwise the results can vary between requests.
// The commented out assertions fail sometimes because suggestions are based off of shard frequencies instead of index frequencies.
/*, "prefix_aaad" */);
// assertThat(suggest.get(3).getSuggestedWords().get("prefix_abcd").get(4).getTerm(), equalTo("prefix_abcc"));
// assertThat(suggest.get(3).getSuggestedWords().get("prefix_abcd").get(4).getTerm(), equalTo("prefix_accd"));
// see #2817
// Check the "text" field this one time.
// Ask for highlighting
// pass in a correct phrase
// pass in a correct phrase - set confidence to 2
// pass in a correct phrase - set confidence to 0.99
// set all mass to trigrams (not indexed)
// set all mass to bigrams
// distribute mass
// try all smoothing methods
// check tokenLimit
// Check the name this time because we're repeating it which is funky
// we allow a size of 2 now on the shard generator level so "god" will be found since it's LD2
// see #3469
// Note that the last document has to have about the same length as the other or cutoff rechecking will remove the useful suggestion
// When searching on a shard with a non existing mapping, we should fail
// When searching on a shard which does not hold yet any document of an existing type, we should not fail
// see #3469
// prevents occasional scoring glitches due to multi segments
// test phrase suggestion on completely empty index
// test phrase suggestion but nothing matches
// finally indexing a document that will produce some meaningful suggestion
/**
// If there isn't enough chaf per shard then shards can become unbalanced, making the cutoff recheck this is testing do more harm
// then good.
// A single shard will help to keep the tests repeatable.
// We're going to be searching for:
//   united states house of representatives elections in washington 2006
// But we need to make sure we generate a ton of suggestions so we add a bunch of candidates.
// Many of these candidates are drawn from page names on English Wikipedia.
// Tons of different options very near the exact query term
// Six of these are near enough to be viable suggestions, just not the top one
// But we can't stop there!  Titles that are just a year are pretty common so lets just add one per year
// since 0.  Why not?
// That ought to provide more less good candidates for the last term
// Now remove or add plural copies of every term we can
// Now some possessive
// And some conjugation
// And other stuff
// Highway in Malaysia
// The last name or the German word, whichever.
// A film?
// A town in England
// Lots of places have this name
// Ditto
// Yup, also a town
// Book
// Can't forget lists, glorious lists!
// Lots of people are named Washington <Middle Initial>. LastName
// Lets just add some more to be evil
// Setting a silly high size helps of generate a larger list of candidates for testing.
// This too
// Just to prove that we've run through a ton of options
// The collate query setter is hard coded to use mustache, so lets lie in this test about the script plugin,
// which makes the collate code thinks mustache is evaluating the query.
// A single shard will help to keep the tests repeatable.
// suggest without collate
// suggest with collate
// collate suggest with no result (boundary case)
// collate suggest with bad query
// expected
// suggest with collation
// collate suggest with bad query
//expected
// collate script failure due to no additional params
// expected
// collate script with additional params
// collate query request with prune set to true
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Ignore doc values
// Ignore stored field
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// lazy initialization of context names and mappings, cannot be done in some init method because other test
// also create random CompletionSuggestionBuilder instances
// lazy initialization of context names and mappings, cannot be done in some init method because other test
// also create random CompletionSuggestionBuilder instances
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// some instances of StringDistance don't support equals, just checking the class here
/**
// test missing fieldname
// test unknown field
// test bad value for field (e.g. size expects an int)
// test unexpected token
// test that it doesn't overflow
// test that it doesn't overflow
// test that it doesn't overflow
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// normalize so parameters sum to 1
/**
// swap two values permute original lambda values
// swap first two
// swap last two
// swap first and last
/*
//www.apache.org/licenses/LICENSE-2.0
// Test some of the highlighting corner cases
// test synonyms
// Make sure that user supplied text is not marked as highlighted in the presence of a synonym filter
// only use forward with constant prefix
// Test a special case where one of the suggest term is unchanged by the postFilter, 'II' here is unchanged by the reverse analyzer.
//        assertThat(corrections[0].join(new BytesRef(" ")).utf8ToString(), equalTo("american ape"));
// test synonyms
/*
//www.apache.org/licenses/LICENSE-2.0
// collate query prune and parameters will only be used when query is set
// preTag, postTag
// preTag, postTag
// simply double both values
// test missing field name
// test empty field name
// currently, "direct_generator" is the only one available. Only compare size of the lists
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// go to start token, real parsing would do that in the outer element parser
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// do nothing
// test missing field name
// test empty field name
// test invalid accuracy values
// test invalid max edit distance values
// test invalid max inspections values
// test invalid max term freq values
// test invalid min doc freq values
// test invalid min word length values
// test invalid prefix length values
// test invalid size values
// null values not allowed for enums
// distance implementations don't implement equals() and have little to compare, so we only check class
/*
//www.apache.org/licenses/LICENSE-2.0
// ignore
/*
//www.apache.org/licenses/LICENSE-2.0
// Rebalancing is causing some checks after restore to randomly fail
// due to https://github.com/elastic/elasticsearch/issues/9421
// Make sure that snapshot clean up operations are finished
/*
//www.apache.org/licenses/LICENSE-2.0
// Write blobs in different formats
// Assert that all checksum blobs can be read by all formats
// This can happen if corrupt the byte length
// Block before finishing writing
// signalling
/*
//www.apache.org/licenses/LICENSE-2.0
// Workaround to simulate BwC situation: taking a snapshot without indices here so that we don't create any new version shard
// generations (the existence of which would short-circuit checks for the repo containing old version snapshots)
/*
//www.apache.org/licenses/LICENSE-2.0
//github.com/elastic/elasticsearch/issues/37485")
// Shouldn't be returned as part of API response
// But should still be in state
// Pick one node and block it
// Pick one node and block it
// Remove it from the list of available nodes
// Make sure that abort makes some progress
// When master node is closed during this test, it sometime manages to delete the snapshot files before
// completely stopping. In this case the retried delete snapshot operation on the new master can fail
// with SnapshotMissingException
// Expect two files to remain in the repository:
//   (1) index-(N+1)
//   (2) index-latest
// don't refresh test-idx-some it will take 30 sec until it times out...
// There is slight delay between snapshot being marked as completed in the cluster state and on the file system
// After it was marked as completed in the cluster state - we need to check if it's completed on the file system as well
// Make sure the first node is elected as master
// Register mock repositories
// Register mock repositories
/**
// to get a new node id
// snapshot could be taken before or after a day rollover
// add few docs - less than initially
// create another snapshot
// total size has to grow and has to be equal to files on fs
//  drop 1st one to avoid miscalculation as snapshot reuses some files of prev snapshot
// check that snapshot completes
// check that snapshot completes with both failed shards being accounted for in the snapshot result
// Wait for green so the close does not fail in the edge case of coinciding with a shard recovery that hasn't fully synced yet
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/// This test uses a snapshot/restore plugin implementation that
// counts the number of times metadata are loaded
// Creating a snapshot does not load any metadata
// Getting a snapshot does not load any metadata
// Getting the status of a snapshot loads indices metadata but not global metadata
// Restoring a snapshot loads indices metadata but not the global state
// Restoring a snapshot with selective indices loads only required index metadata
// Restoring a snapshot including the global state loads it with the index metadata
// Deleting a snapshot does not load the global metadata state but loads each index metadata
/** Compute a map key for the given snapshot and index names **/
/** A mocked repository that counts the number of times global/index metadata are accessed **/
/** A plugin that uses CountingMockRepository as implementation of the Repository **/
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// divide by 2 to not overflow when adding to this number for the pending generation below
// remove some elements
// add some elements
/*
//www.apache.org/licenses/LICENSE-2.0
// We have tests that check by-timestamp order
// Test restore after index deletion
// Finish flush
// Write a document
// We don't restore any indices here
// We don't restore any indices here
// We don't restore any indices here
// At least something should be stored
// If we are here, that means we didn't have any failures, let's check it
// If we are here, that means we didn't have any failures, let's check it
// Verify that snapshot status also contains the same failures
// Test restore after index deletion
// All shards were restored, we must find the exact number of hits
// One or more shards failed to be restored. This can happen when there is
// only 1 data node: a shard failed because of the random IO exceptions
// during restore and then we don't allow the shard to be assigned on the
// same node again during the same reroute operation. Then another reroute
// operation is scheduled, but the RestoreInProgressAllocationDecider will
// block the shard to be assigned again because it failed during restore.
// Test restore after index deletion
// we have to delete the index here manually, otherwise the cluster will keep
// trying to allocate the shards for the index, even though the restore operation
// is completed and marked as failed, which can lead to nodes having pending
// cluster states to process in their queue when the test is finished
/**
// No lucene corruptions, we want to test retries
// Restoring a file will never complete
/**
// remove the shard allocation filtering settings and use the Reroute API to retry the failed shards
/** Execute the unrestorable test use case **/
// create a test repository
// create a test index
// index some documents
// create a snapshot
// delete the test index
// update the test repository
// attempt to restore the snapshot with the given settings
// check that all shards failed during restore
// check that there is no restore in progress
// check that the shards have been created but are not assigned
// check that every primary shard is unassigned
// update the test repository in order to make it work
// execute action to eventually fix the situation
// delete the index and restore again
// Wait for the shards to be assigned
// Fail completely
// Test restore after index deletion
//  that would mean that recovery process started and failing
// Now read restore results and make sure it failed
// Store number of files after each snapshot
// Wait for at least 1ms to ensure that snapshots can be ordered by timestamp deterministically
/** Tests that a snapshot with a corrupted global state file can still be deleted */
// Delete the global state metadata file
// Truncate the global state metadata file
// Expected
// Expected
// Expected
// Expected
// Expected
// Expected
// Create index on 2 nodes and make sure each node has a primary by setting no replicas
// Pick one node and block it
// Create index on 2 nodes and make sure each node has a primary by setting no replicas
// Pick one node and block it
// do not do verification itself as snapshot threads could be fully blocked
// Create index on 2 nodes and make sure each node has a primary by setting no replicas
// Pick one node and block it
// We blocked the node during data write operation, so at least one shard snapshot should be in STARTED stage
// We blocked the node during data write operation, so at least one shard snapshot should be in STARTED stage
// test that getting an unavailable snapshot status throws an exception if ignoreUnavailable is false on the request
// test that getting an unavailable snapshot status does not throw an exception if ignoreUnavailable is true on the request
// test getting snapshot status for available and unavailable snapshots where ignoreUnavailable is true
// (available one should be returned)
// Create index on two nodes and make sure each node has a primary by setting no replicas
// only one shard
// we flush before the snapshot such that we have to process the segments_N files plus the .del file
// soft-delete generates DV files.
// Make sure that number of shards didn't change
// delete everything we can delete
// Make sure that number of shards didn't change
// compute current index settings (as we cannot query them if they contain SETTING_BLOCKS_METADATA)
// non-partial snapshots do not allow close / delete operations on indices where snapshot has not been completed
// unblock even if the try block fails otherwise we will get bogus failures when we delete all indices in test teardown.
// unblock even if the try block fails otherwise we will get bogus failures when we delete all indices in test teardown.
/** Tests that a snapshot with a corrupted global state file can still be restored */
/**
// Choose a random index from the snapshot
// Truncate the index metadata file
/**
// with ignore unavailable set to true, should not throw an exception
// Create index on 2 nodes and make sure each node has a primary by setting no replicas
// make sure we return only the in-progress snapshot when taking the first snapshot on a clean repository
// take initial snapshot with a block, making sure we only get 1 in-progress snapshot returned
// block a node so the create snapshot operation can remain in progress
// wait for block to kick in
// unblock node
// timeout after 10 seconds
// add documents so there are data files to block on
// block a node so the create snapshot operation can remain in progress
// wait for block to kick in
// with ignore unavailable set to true, should not throw an exception
// use _current plus the individual names of the finished snapshots
// unblock node
/**
//github.com/elastic/elasticsearch/issues/20876
// TODO: Fix repo cleanup logic to handle these leaked snap-file and only exclude test-repo (the mock repo) here.
// test that we can take a snapshot after a failed one, even if a partial index-N was written
// the less the number of shards, the less control files we have, so we are giving a higher probability of
// triggering an IOException toward the end when writing the pending-index-* files, which are the files
// that caused problems with writing subsequent snapshots if they happened to be lingering in the repository
// sometimes, the snapshot will fail with a top level I/O exception
// set shard allocation to none so the primary cannot be
// allocated - simulates a "bad" index that fails to snapshot
// verify a FAILED status is returned instead of a 500 status code
// see https://github.com/elastic/elasticsearch/issues/23716
// index some additional docs (maybe) for each index
// create a gap in the sequence numbers
// 10 indexed docs and one "missing" op.
// 15 indexed docs and one "missing" op.
// Write a document
// Write a document
// open and close the index to increase the primary terms
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Only disconnect master if we have more than a single master and can simulate a failover
// The delete worked out, creating a third snapshot
// We end up with two snapshots no matter if the delete worked out or not
// create a few more indices to make it more likely that the subsequent index delete operation happens before snapshot
// finalization
// Recreate index by the same name to test that we don't snapshot conflicting metadata in this scenario
// Single shard for each index so we either get all indices or all except for the deleted index
// Make sure we snapshotted the metadata of this index and not the recreated version
// Index delete must be blocked for non-partial snapshots and we get a snapshot for every index
/**
// Index a few documents with different field names so we trigger a dynamic mapping update for each of them
// Connect all nodes to each other
// TODO: randomize replica count settings once recovery operations aren't blocking anymore
// Select from sorted list of nodes
/**
// o.w. some tests might block
// Remove and add back local node to update ephemeral id on restarts
// LinkedHashMap so we have deterministic ordering when iterating over the map in tests
/**
// Select from sorted list of data-nodes here to not have deterministic behaviour
// Select from sorted list of data-nodes here to not have deterministic behaviour
/**
/**
// don't do anything, and don't block
// Check if both nodes are still part of the cluster
// TODO: Remove this hack once recoveries are async and can be used in these tests
// Run half the tests with the eventually consistent repository
// eliminate thread name check as we create repo in the test thread
/*
//www.apache.org/licenses/LICENSE-2.0
// Retrieve snapshot status from the data node.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// remove some elements
// add some elements
// modify some elements
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel">AWS S3 docs</a>.
// Filters out all actions that are super-seeded by subsequent actions
// TODO: Remove all usages of this method, snapshots should not depend on consistent list operations
// eliminate thread name check as we create repo in the test thread
/**
// Eventual consistency is only simulated as long as this flag is false
/**
// Consistent read after write
// Randomly filter out the index-N blobs from a listing to test that tracking of it in latestKnownRepoGen and the cluster state
// ensures consistent repository operations
// Randomly filter out index-N blobs at the repo root to proof that we don't need them to be consistently listed
// We do some checks in case there is a consistent state for a blob to prevent turning it inconsistent.
// TODO: Ensure that it is impossible to ever decrement the generation id stored in index.latest then assert that
//       it never decrements here. Same goes for the metadata, ensure that we never overwrite newer with older
//       metadata.
// TODO: dry up the logic for reading SnapshotInfo here against the code in ChecksumBlobStoreFormat
// If the existing snapshotInfo differs only in the timestamps it stores, then the overwrite is not
// a problem and could be the result of a correctly handled master failover.
// No need to add a write for this since we didn't change content
// Rethrow as AssertionError here since kind exception might otherwise be swallowed and logged by
// the blob store repository.
// Since we are not doing any actual IO we don't expect this to throw ever and an exception would
// signal broken SnapshotInfo bytes or unexpected behavior of SnapshotInfo otherwise.
// Primaries never retry so any shard level snap- blob retry/overwrite even with the same content is
// not expected.
// No need to add a write for this since we didn't change content
/*
//www.apache.org/licenses/LICENSE-2.0
// Apply state once to initialize repo properly like RepositoriesService would
// We create a snap- blob for snapshot "foo" in the first generation
// We try to write another snap- blob for "foo" in the next generation. It fails because the content differs.
// We try to write another snap- blob for "foo" in the next generation. It fails because the content differs.
// We try to write yet another snap- blob for "foo" in the next generation.
// It passes cleanly because the content of the blob except for the timestamps.
/*
//www.apache.org/licenses/LICENSE-2.0
//status and headers hold arbitrary content, we can't inject random fields in them
/**
//with random fields insertion in the inner exceptions, some random stuff may be parsed back as metadata,
//but that does not bother our assertions, as we only want to test that we don't break.
//exceptions are not of the same type whenever parsed back
/*
//www.apache.org/licenses/LICENSE-2.0
//failures are tested separately, so we can test xcontent equivalence at least when we have no failures
//status and headers hold arbitrary content, we can't inject random fields in them
/**
//with random fields insertion in the inner exceptions, some random stuff may be parsed back as metadata,
//but that does not bother our assertions, as we only want to test that we don't break.
//exceptions are not of the same type whenever parsed back
/*
//www.apache.org/licenses/LICENSE-2.0
/*
// 8 for the task number, 1 for the string length of the uuid, 22 for the actual uuid
//The size of the serialized representation of the EMPTY_TASK_ID matters a lot because many requests contain it.
/*
//www.apache.org/licenses/LICENSE-2.0
//status and headers hold arbitrary content, we can't inject random fields in them
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
// Responses in Elasticsearch never output a leading startObject. There isn't really a good reason, they just don't.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// normalize min and max
/** Puts latitude in range of -90 to 90. */
//common case, and avoids slight double precision shifting
/** Puts longitude in range of -180 to +180. */
//common case, and avoids slight double precision shifting
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// cap geometry collection at 4 shapes (to save test time)
// due to world wrapping, and the possibility for ambiguous polygons, the random shape generation could bail with
// a null shape. We catch that situation here, and only increment the counter when a valid shape is returned.
// Not the most efficient but its the lesser of the evil alternatives
/**
// NOTE: multipolygon not yet supported. Overlapping polygons are invalid so randomization
// requires an approach to avoid overlaps. This could be approached by creating polygons
// inside non overlapping bounding rectangles
// for random testing having a maximum number of 10 points for a line string is more than sufficient
// if this number gets out of hand, the number of self intersections for a linestring can become
// (n^2-n)/2 and computing the relation intersection matrix will become NP-Hard
// random point order or random linestrings can lead to invalid self-crossing polygons,
// compute the convex hull for a set of points to ensure polygon does not self cross
// if points are in a line the convex hull will be 2 points which will also lead to an invalid polygon
// when all else fails, use the bounding box as the polygon
// This test framework builds semi-random geometry (in the sense that points are not truly random due to spatial
// auto-correlation) As a result of the semi-random nature of the geometry, one can not predict the orientation
// intent for ambiguous polygons. Therefore, an invalid oriented dateline crossing polygon could be built.
// The validate flag will check for these possibilities and bail if an incorrect geometry is created
// jts bug may occasionally misinterpret coordinate order causing an unhelpful ('geom' assertion)
// or InvalidShapeException
// between 3 and 6 degrees
/** creates a small random rectangle by default to keep shape test performance at bay */
//1/3rd
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO: This test do not check all permutations of linestrings. So the test
// fails if the holes of the polygons are not ordered the same way
// TODO: This test do not check all permutations. So the Test fails
// if the inner polygons are not ordered the same way in both Multipolygons
//We want to know the type of the shape because we test shape equality in a special way...
//... in particular we test that one ring is equivalent to another ring even if the points are rotated or reversed.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// some of the fixed thread pool are bound by the number of
// cores so we can not exceed that
// these tasks will consume the thread pool causing further
// submissions to queue
// wait for the submitted tasks to be consumed by the thread
// pool
// these tasks will fill the thread pool queue
// these tasks will be rejected
/*
//www.apache.org/licenses/LICENSE-2.0
// the defaults
// the defaults
// this while loop is the core of this test; if threads
// are correctly idled down by the pool, the number of
// threads in the pool will drop to the min for the pool
// but if threads are not correctly idled down by the pool,
// this test will just timeout waiting for them to idle
// down
/*
//www.apache.org/licenses/LICENSE-2.0
// simple test for successful scheduling, exceptions tested more thoroughly in EvilThreadPoolTests
// simple test for successful scheduling, exceptions tested more thoroughly in EvilThreadPoolTests
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// notify that the runnable is started
// wait for other thread to un-pause
// this call was made during construction of the runnable
// create a thread and start the runnable
// wait for the runnable to be started and ensure the runnable hasn't used the threadpool again
// un-pause the runnable and allow it to complete execution
// validate schedule was called again
// wait for the number of successful count down operations
// cancel
// wait for the cancellable to be present before we really start so we can accurately know we cancelled
// rarely throw an exception before counting down
// see if we have counted down to zero or below yet. the exception throwing could make us count below zero
// rarely throw an exception after execution
// wait for the runnable to finish
// the runnable should have cancelled itself
// rarely wait and make sure the runnable didn't run at the next interval
/*
//www.apache.org/licenses/LICENSE-2.0
// ignore some shared threads we know that are created within the same VM, like the shared discovery one
// or the ones that are occasionally come up from ESSingleNodeTestCase
// TODO: this can't possibly be right! single node and integ test are unrelated!
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// the delta can be large, we just care it is the same order of magnitude
// is not called recursively
/*
//www.apache.org/licenses/LICENSE-2.0
// we don't report on the "same" thread pool
// otherwise check we have the expected type
// try to create a too big thread pool
// keep alive does not apply to fixed thread pools
// if this throws then ThreadPool#shutdownNow did not interrupt
// This is ok it is a default thread pool
/*
//www.apache.org/licenses/LICENSE-2.0
// Closing compression stream does not close underlying stream
// The bytes should be zeroed out on close
// The bytes should be zeroed out on close
/*
//www.apache.org/licenses/LICENSE-2.0
// Only a single connection attempt should be open.
// No connections succeeded
// The connection manager will close all open connections
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Check that the thread context is not deleted.
// Check that deserialize does not overwrite current thread context.
// Check that the thread context is not deleted.
// Check that deserialize does not overwrite current thread context.
// Check that the thread context is not deleted.
// Check that deserialize does not overwrite current thread context.
// For handshake we are compatible with N-2
// force status byte to signal compressed on the otherwise uncompressed message
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Connections now pointing to transport2
// Connections not pointing to transport2 because the cluster name is different
// Should validate successfully
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// following two log lines added to investigate #41745, can be removed once issue is closed
// also test a failure, there is no handler for scroll registered
// this test is not perfect since we might reconnect concurrently but it will fail most of the time if we don't have
// the right calls in place in the RemoteAwareClient
/*
//www.apache.org/licenses/LICENSE-2.0
// that's fine we might close
// now close it, this should trigger an interrupt on the socket and we can move on
// we do cancel the operation and that means that if timing allows it, the caller
// of a blocking call as well as the handler will get the exception from the
// ExecutionCancelledException concurrently. unless that is the case we fail
// if we get called more than once!
// that's fine
// it's ok if we're shutting down
// test no nodes connected
// ignore, this is an expected exception
// Ignore
//always a direct connection as the remote node is already connected
// we don't use the transport service connection manager so we will get a proxy connection for the local node
// always a proxy connection as the target node is not connected
/*
//www.apache.org/licenses/LICENSE-2.0
// simple validation
// Add the cluster on a different thread to test that we wait for a new cluster to
// connect before returning.
// randomBoolean() ? TimeValue.MINUS_ONE :
//randomBoolean() ? TimeValue.MINUS_ONE :
// close all targets and check for the transport level failure path
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Add duplicate connect attempt to ensure that we do not get duplicate connections in the round robin
// Test round robin
// Test that the connection is cleared from the round robin list when it is closed
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Closing connections leads to RemoteClusterConnection.ConnectHandler.collectRemoteNodes
// Assert that one of the discovered nodes is connected. After that, disconnect the connected
// discovered node and ensure the other discovered node is eventually connected
// exec again we are already connected
// We start in order to get a valid address + port, but do not start accepting connections as we
// will not actually connect to these transports
// Use the address for the node that will not respond
// Do not actually try to connect because the node will not respond. Just capture the
// address for later assertion
// Connection to discoverable will fail due to the stubbable transport
// Should validate successfully
/*
//www.apache.org/licenses/LICENSE-2.0
/** Unit tests for {@link TcpTransport} */
/** Test ipv4 host with a default port works */
/** Test ipv4 host with port works */
/** Test unbracketed ipv6 hosts in configuration fail. Leave no ambiguity */
/** Test ipv6 host with a default port works */
/** Test ipv6 host with port works */
/*
//www.apache.org/licenses/LICENSE-2.0
// we use always a non-alpha or beta version here otherwise minimumCompatibilityVersion will be different for the two used versions
// this one supports dynamic tracer updates
// this one doesn't support dynamic tracer updates
// this one doesn't support dynamic tracer updates
/*
//www.apache.org/licenses/LICENSE-2.0
// When writing or reading a 6.6 stream, these are not serialized
// We check that the handshake we serialize for this test equals the actual request.
// Otherwise, we need to update the test.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// won't happen
// Test that the task has rescheduled itself
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// since static must set to null to be eligible for collection
// the name and version should be updated
/*
//www.apache.org/licenses/LICENSE-2.0
// Script logic is
// 1) New accounts take balance from "balance" in upsert doc and first payment is charged at 50%
// 2) Existing accounts subtract full payment from balance stored in elasticsearch
// Pay money from what will be a new account and opening balance comes from upsert doc
// provided by client
// Now pay money for an existing account where balance is stored in es
// Issue #3265
// check noop
// check delete
// check _source parameter
// check updates without script
// add new field
// change existing field
// recursive map
// Index some documents
// Update the first object and note context variables values
// Idem with the second object
// test infrastructure kills long-running tests by interrupting them, thus we handle this case separately
//We create an index with merging disabled so that deletes don't get merged away
//Wait for no-node to clear
//Just keep swimming
//Threads should have finished because of the latch.await
//If are no errors every request received a response otherwise the test would have timedout
//aquiring the request outstanding semaphores.
//Upsert all the ids one last time to make sure they are available at get time
//This means that we add 1 to the expected versions and attempts
//All the previous operations should be complete or failed at this point
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// detect_noop defaults to true
// Use random keys so we get random iteration order.
// Use random keys so we get variable iteration order.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Issue #3629
// prefix queries
// fuzzy queries
// more like this queries
// We are relying on specific routing behaviors for the result to be right, so
// we cannot randomize the number of shards or change ids here.
// prefix queries
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Test info: disrupt network for up to 8s in a number of rounds and check that we only get true positive CAS results when running
// multiple threads doing CAS updates.
// Wait up to 1 minute (+10s in thread to ensure it does not time out) for threads to complete previous round before initiating next
// round.
// +1 for main thread.
// heal cluster faster to reduce test time.
// we use either the latest observed or the latest successful version, to increase chance of getting successful
// CAS'es and races. If we were to use only the latest successful version, any CAS fail on own write would mean that
// all future CAS'es would fail unless we guess the seqno/term below. On the other hand, using latest observed
// version exclusively we risk a single CAS fail on a dirty read to cause the same. Doing both randomly and adding
// variance to seqno/term should ensure we progress fine in most runs.
// we should be able to remove timeout or fail hard on timeouts
// validate version and seqNo strictly increasing for successful CAS to avoid that overhead during
// linearizability checking.
// if we supplied an input version <= latest successful version, we can safely assume that any failed
// operation will no longer be able to complete after the next successful write and we can therefore terminate
// the operation wrt. linearizability.
// todo: collect the failed responses and terminate when CAS with higher output version is successful, since
// this is the guarantee we offer.
// if we supplied an input version <= to latest successful version, we can safely assume that any failed
// operation will no longer be able to complete after the next successful write and we can therefore terminate
// the operation wrt. linearizability.
// todo: collect the failed responses and terminate when CAS with higher output version is successful, since
// this is the guarantee we offer.
// interrupt often comes as a RuntimeException so check to stop here too.
// a thread can go here either because it completed before disruption ended, timeout on main thread causes broken
// barrier
// this is timeout on the barrier, unexpected.
/**
// latest version that was observed, possibly dirty read of a write that does not survive
// latest version for which we got a successful response on a write.
// Large histories can be problematic and have the linearizability checker run OOM
// Bound the time how long the checker can run on such histories (Values empirically determined)
// let the test pass
// implicitly test that we can serialize all histories.
// we dump base64 encoded data, since the nature of this test is that it does not reproduce even with same seed.
/**
/**
// parsing out the version increases chance of hitting races against CAS successes, since if we did not parse this out, no
// writes would succeed after a fail on own write failure (unless we were lucky enough to guess the seqNo/primaryTerm using the
// random futureTerm/futureSeqNo handling in CASUpdateThread).
/**
// nothing to write.
// Generate 3 increasing versions
// for version conflicts, we keep state version with lastFailed set, regardless of input/output version.
// for non version conflict failures, we keep state version with lastFailed set, regardless of input version.
/*
//www.apache.org/licenses/LICENSE-2.0
// Note - external version doesn't throw version conflicts on deletes of non existent records.
// This is different from internal versioning
// this should conflict with the delete command transaction which told us that the object was deleted at version 17.
// deleting with a lower version fails.
// Delete with a higher or equal version deletes all versions up to the given one.
// Deleting with a lower version keeps on failing after a delete.
// But delete with a higher version is OK.
// deleting with a lower version fails.
// Delete with a higher version deletes all versions up to the given one.
// Deleting with a lower version keeps on failing after a delete.
// But delete with a higher version is OK.
// TODO: This behavior breaks rest api returning http status 201
// good news is that it this is only the case until deletes GC kicks in.
// Make sure that the next delete will be GC. Note we do it on the index settings so it will be cleaned up
// gc works based on estimated sampled time. Give it a chance...
// And now we have previous version return -1
// expected
// search with versioning
// TODO: ADD SEQ NO!
// search without versioning
// the doc is deleted. Even when we hit the deleted seqNo, a conditional delete should fail.
// Poached from Lucene's TestIDVersionPostingsFormat:
// random simple
// random realistic unicode
// sequential
// zero-pad sequential
// random long
// zero-pad random long
// TODO: sometimes use _bulk API
// TODO: test non-aborting exceptions (Rob suggested field where positions overflow)
// TODO: not great we don't test deletes GC here:
// We test deletes, but can't rely on wall-clock delete GC:
// Generate random IDs:
// Attach random versions to them:
// 20% of the time we delete:
// Save highest version per id:
// Shuffle
//final Random threadRandom = RandomizedContext.current().getRandom();
// TODO: sometimes use bulk:
// OK: our version is too old
// OK: our version is too old
// Verify against truth:
// We require only one shard for this test, so that the 2nd delete provokes pruning the deletes map:
// Index a doc:
// Force refresh so the add is sometimes visible in the searcher:
// Delete it
// Real-time get should reflect delete:
// ThreadPool.relativeTimeInMillis has default granularity of 200 msec, so we must sleep at least that long; sleep much longer in
// case system is busy:
// Delete an unrelated doc (provokes pruning deletes from versionMap)
// Real-time get should still reflect delete:
// We test deletes, but can't rely on wall-clock delete GC:
// Index a doc:
// Force refresh so the add is sometimes visible in the searcher:
// Delete it
// Real-time get should reflect delete even though index.gc_deletes is 0:
//v2
// Make sure that these versions are replicated correctly
/*
//www.apache.org/licenses/LICENSE-2.0
// Make sure that first file is modified
// Create new file in subdirectory
// Create new subdirectory in subdirectory
// Delete a directory, check notifications for
/*
//www.apache.org/licenses/LICENSE-2.0
// checking the defaults
// checking bwc
// only applies to medium
// checking custom
// checking default freq
// checking custom freq
/*
//www.apache.org/licenses/LICENSE-2.0
// wait to be killed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*blockid=*", request)) {
/*comp=blocklist*", request)) {
/*", request)) {
/*", request)) {
/*", request)) {
// see Constants.HeaderConstants.STORAGE_RANGE_HEADER
/*", request)) {
// List Blobs (https://docs.microsoft.com/en-us/rest/api/storageservices/list-blobs)
// see Constants.HeaderConstants.CLIENT_REQUEST_ID_HEADER
// see Constants.HeaderConstants.STORAGE_RANGE_HEADER
// see Constants.HeaderConstants.ERROR_CODE
// See https://docs.microsoft.com/en-us/rest/api/storageservices/common-rest-api-error-codes
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// wait to be killed
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Request body is closed in the finally block
// List Objects https://cloud.google.com/storage/docs/json_api/v1/objects/list
// GET Bucket https://cloud.google.com/storage/docs/json_api/v1/buckets/get
/*", request)) {
// Batch https://cloud.google.com/storage/docs/json_api/v1/how-tos/batch
/*uploadType=multipart*", request)) {
/*uploadType=resumable*", request)) {
// not a Google Storage parameter, but it allows to pass the blob name
// Resumable upload https://cloud.google.com/storage/docs/json_api/v1/how-tos/resumable-upload
/* Resume Incomplete */, -1);
//" + InetAddresses.toUriString(address.getAddress()) + ":" + address.getPort();
// removes the trailing end "\r\n--__END_OF_PART__--\r\n" which is 23 bytes long
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// configure Paths
// hadoop-home/, so logs will not complain
// hdfs-data/, where any data is going
// configure cluster
// lower default permission: TODO: needed?
// optionally configure security
// Configure HA mode
// Configure contents of the filesystem
// Set the elasticsearch user directory up
// Install a pre-existing repository into HDFS
// write our PID file
// write our port file
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// 0.90 must be explicitly foregrounded
/*
//www.apache.org/licenses/LICENSE-2.0
// wait to be killed
/*
//www.apache.org/licenses/LICENSE-2.0
// http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html
/*
//www.apache.org/licenses/LICENSE-2.0
// https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*?uploads", request)) {
/*?uploadId=*&partNumber=*", request)) {
/*?uploadId=*", request)) {
/*", request)) {
/*", request)) {
/*", request)) {
// search next consecutive {carriage return, new line} chars and stop
/*
//www.apache.org/licenses/LICENSE-2.0
/* no construction */ }
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// we need a non-local master to test shard failures
// we need a non-local master to test shard failures
/**
/**
/**
// we need a non-local master to test shard failures
// we need a non-local master to test shard failures
/**
/**
/**
// no point in randomizing - node assignment later on does it too.
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: can we share more code with the non-test side here
// without making things complex???
// make sure java.io.tmpdir exists always (in case code uses it in a static initializer)
// just like bootstrap, initialize natives, then SM
// use the default bootstrap.memory_lock setting
// initialize probes
// initialize sysprops
// check for jar hell
// Log ifconfig output before SecurityManager is installed
// install security manager if requested
// initialize paths the same exact way as bootstrap
// java.io.tmpdir
// custom test config file
// jacoco coverage output file
// in case we get fancy and use the -integration goals later:
// intellij hack: intellij test runner wants setIO and will
// screw up all test logging without it!
// add bind permissions for testing
// ephemeral ports (note, on java 7 before update 51, this is a different permission)
// this should really be the only one allowed for tests, otherwise they have race conditions
// ... but tests are messy. like file permissions, just let them live in a fantasy for now.
// TODO: cut over all tests to bind to ephemeral ports
// read test-framework permissions
// intellij and eclipse don't package our internal libs, so we need to set the codebases for them manually
// implements union
// guarantee plugin classes are initialized first, in case they have one-time hacks.
// this just makes unit testing more realistic
/** Add the codebase url of the given classname to the codebases map, if the class exists. */
// no class, fall through to not add. this can happen for any tests that do not include
// the given class. eg only core tests include plugin-classloader
/**
// compute classpath minus obvious places, all other jars will get the permission.
// es core
// es test framework
// lucene test framework
// randomized runner
// junit library
// parse each policy file, with codebase substitution from the classpath
// consult each policy file for those codebases
// implements union
/**
// does nothing, just easy way to make sure the class is loaded.
/*
//www.apache.org/licenses/LICENSE-2.0
// if an unexpected exception is thrown, we log
// terminal output to aid debugging
// rethrow so the test fails
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** The terminal that execute uses. */
/** The last command that was executed. */
/** Creates a Command to test execution. */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// A deque would be a perfect data structure for the FIFO queue of input values needed here. However,
// to support the valid return value of readText being null (defined by Console), we need to be able
// to store nulls. However, java the java Deque api does not allow nulls because it uses null as
// a special return value from certain methods like peek(). So instead of deque, we use an array list here,
// and keep track of the last position which was read. It means that we will hold onto all input
// setup for the mock terminal during its lifetime, but this is normally a very small amount of data
// so in reality it will not matter.
// always *nix newlines for tests
/** Adds an an input that will be return from {@link #readText(String)}. Values are read in FIFO order. */
/** Adds an an input that will be return from {@link #readText(String)}. Values are read in FIFO order. */
/** Returns all output written to this terminal. */
/** Returns all output written to this terminal. */
/** Wipes the input and output. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Checks the given rest client has the provided default headers. */
// copy so we can remove as we check
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/** A lock {@link AllocationService} allowing tests to override time */
/**
// no-op
// no-op
/*
//www.apache.org/licenses/LICENSE-2.0
/** This is a marker plugin used to trigger MockNode to use this mock info service. */
// if no fakery should take place
// if no fakery should take place
/*
//www.apache.org/licenses/LICENSE-2.0
// check that runRandomly leads to reproducible results
// Updating the cluster state involves up to 7 delays:
// 1. submit the task to the master service
// 2. send PublishRequest
// 3. receive PublishResponse
// 4. send ApplyCommitRequest
// 5. apply committed cluster state
// 6. receive ApplyCommitResponse
// 7. apply committed state on master (last one to apply cluster state)
// The time it takes to complete an election
// Pinging all peers twice should be enough to discover all nodes
// Then wait for an election to be scheduled; we allow enough time for retries to allow for collisions
// Allow two round-trip for pre-voting and voting
// Then a commit of the new leader's first cluster state
// If leader just blackholed, need to wait for this to be detected
// then wait for a follower to be promoted to leader
// perhaps there is an election collision requiring another publication (which times out) and a term bump
// then wait for the new leader to notice that the old leader is unresponsive
// then wait for the new leader to commit a state without the old leader
// TODO does ThreadPool need a node name any more?
// null means construct a list from all the current nodes
// The first pinging discovers the master
// One message delay to send a join
// Commit a new cluster state with the new node(s). Might be split into multiple commits, and each might need a
// followup reconfiguration
// TODO Investigate whether 4 publications is sufficient due to batching? A bound linear in the number of nodes isn't great.
// TODO supporting (preserving?) existing disruptions needs implementing if needed, for now we just forbid it
// for randomized writes and reads
// for lambdas
// reboot random node
// TODO other random steps:
// - reboot a node
// - abdicate leadership
// This is ok: it just means a message couldn't currently be handled.
// Large histories can be problematic and have the linearizability checker run OOM
// Bound the time how long the checker can run on such histories (Values empirically determined)
// may be in a prior connection attempt which has been blackholed
// A 1-node cluster has no need for fault detection etc so will eventually run out of things to do.
// node is master-ineligible either before or after the restart ...
// ... and it's accepted some non-initial state so we can roll back ...
// ... and we're feeling lucky ...
// ... then we might not have reliably persisted the cluster state, so emulate a rollback
// adapt cluster state to new localNode instance and add blocks
// In the real-life IOError might be thrown, for example if state fsync fails.
// This will require node restart and we're not emulating it here.
// removes it from openPersistedStates
// suppress auto-bootstrap
//transportService.stop(); // does blocking stuff :/
//transportService.close(); // does blocking stuff :/
// generated deterministically for repeatable tests
// in this case, we know for sure that event was not processed by the system and will not change history
// remove event to help avoid bloated history and state space explosion in linearizability checker
// do not remove event from history, the write might still take place
// instead, complete history when checking for linearizability
// reads do not change state
// remove event to help avoid bloated history and state space explosion in linearizability checker
// initial configuration should not have a place holder for local node
// TODO we only currently care about per-node acks
// apply cluster state, but don't notify listener
// ignore result
// don't do anything, and don't block
// generated deterministically for repeatable tests
/**
/**
/**
/**
/**
// null input is read, non-null is write
// history is completed with null, simulating timeout, which assumes that read went through
// history is completed with null, simulating timeout, which assumes that write went through
// successful write 42 returns previous value 7
// write 42 times out
// successful read
// read times out
/*
//www.apache.org/licenses/LICENSE-2.0
// generate cluster UUID deterministically for repeatable tests
// generate cluster state UUID deterministically for repeatable tests
// master-ineligible nodes can't be trusted to persist the cluster state properly
// check node invariants after each iteration
// ignore
// check system invariants. It's sufficient to do this at the end as these invariants are monotonic.
// one master per term
// unique cluster state per (term, version) pair
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//dx.doi.org/10.1002/cpe.3928
//dx.doi.org/10.1007/978-3-319-19195-9_4
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// clone history before completing it
// complete history
// the current state of the datatype
// the linearized prefix of the history
// cache of explored <state, linearized prefix> pairs
// path we're currently exploring
// current entry
// check if we have already explored this linearization
/**
/**
/**
// first, create entries and link response events to invocation events
// map from event id to matching response entry
// sanity check
// now link entries together in history order, and add a sentinel node at the beginning
// null if current entry is a response, non-null if it's an invocation
// internal id, distinct from Event.id
// removes this entry from the surrounding structures
// reinserts this entry into the surrounding structures
/**
/**
// Get a unique set object per state permutation. We assume that the number of permutations of states are small.
// We thus avoid the overhead of the set data structure.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// hacks everywhere
// kill worker so that next one will be scheduled
// ensures we don't block
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// generate cluster UUID deterministically for repeatable tests
/*
//www.apache.org/licenses/LICENSE-2.0
// we can't assert the offset since if the length is smaller than the refercence
// the offset can be anywhere
// read single bytes one by one
// reset the stream for bulk reading
// buffer for bulk reads
// bulk-read 0 bytes: must not modify buffer
// read a few few bytes as ints
// bulk-read all
// continuing to read should now fail with EOFException
// yay
// try to read more than the stream contains
// read a bunch of single bytes one by one
// now do NOT reset the stream - keep the stream's offset!
// buffer to compare remaining bytes against bulk read
// randomized target buffer to ensure no stale slots
// bulk-read all
// test stream input over slice (upper half of original)
// single reads
// reset the slice stream for bulk reading
// bulk read
// compare slice content with upper half of original
// compare slice bytes with bytes read from slice via streamInput :D
// no more bytes to retrieve!
// +1 for the one byte we read above
// single-page optimization
// we need a length != (n * pagesize) to avoid page sharing at boundaries
// small PBR which would normally share the first page
// an offset to the end would be len 0
// original reference has pages
// orig ref has no pages ie. BytesArray
// test empty
// TODO: good way to test?
// get a BytesRef from a slice
// some impls optimize this to an empty instance then the offset will be 0
// note that these are only true if we have <= than a page, otherwise offset/length are shifted
// empty content must have hash 1 (JDK compat)
// test with content
// test hashes of slices
// get refs & compare
// test equality of slices
// test a slice with same offset but different length,
// unless randomized testing gave us a 0-length slice.
// test same instance
// test different length
// test changed bytes
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Sets a new default filesystem for testing */
/** Installs a mock filesystem for testing */
/** Resets filesystem back to the real system default */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// all lines should have the same nodeName and clusterName
//once the nodeId and clusterId are received, they should be the same on remaining lines
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Thread looks like a node thread so use the node name
/*
/*
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Merge the given secure settings into this one. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// not empty, we might be executing on a shared cluster that keeps on obtaining
// and releasing arrays, lets make sure that after a reasonable timeout, all master
// copy (snapshot) have been released
// remove all existing master copy we will report on
// rest tests don't run randomized and have no context
// will be cleared anyway
/*
//www.apache.org/licenses/LICENSE-2.0
// not empty, we might be executing on a shared cluster that keeps on obtaining
// and releasing pages, lets make sure that after a reasonable timeout, all master
// copy (snapshot) have been released
// remove all existing master copy we will report on
// we always initialize with 0 here since we really only wanna have some random bytes / ints / longs
// and given the fact that it's called concurrently it won't reproduces anyway the same order other than in a unittest
// for the latter 0 is just fine
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Match either any backlash-escaped characters, or a "%(param)" pattern.
// COMMENTS is specified to allow whitespace in this pattern, for clarity
/**
// Escaped characters are unchanged
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// MetaData upgrade is tested in GatewayMetaStateTests, we override this method to NOP to make mocking easier
// Just set localNode here, not to mess with ClusterService and IndicesService mocking
/*
//www.apache.org/licenses/LICENSE-2.0
// we use nextPolygon because it guarantees no duplicate points
// don't build too deep
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** A tuple of document id, sequence number, primary term, source and version of a document */
/*
//www.apache.org/licenses/LICENSE-2.0
// A default primary term is used by engine instances created in this test.
// TODO randomize more settings
// make sure this doesn't kick in on us
// some codecs are read only so we only take the ones that we have in the service and randomly
// selected by lucene test case.
/**
/**
// we don't need to notify anybody in this test
// delete
// shuffle ops but make sure legacy ops are first
// replicas don't really care to about creation status of documents
// this allows to ignore the case where a document was found in the live version maps in
// a delete state and return false for the created flag in favor of code simplicity
// as deleted or not. This check is just signal regression so a decision can be made if it's
// intentional
// Replicas don't really care to about found status of documents
// this allows to ignore the case where a document was found in the live version maps in
// a delete state and return true for the found flag in favor of code simplicity
// his check is just signal regression so a decision can be made if it's
// intentional
/**
// We have to skip non-root docs because its _id field is not stored (indexed only).
/**
/**
/**
// engine was closed
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// If a translog op is replayed on the primary (eg. ccr), we need to use external instead of null for its version type.
/*
//www.apache.org/licenses/LICENSE-2.0
// after version 5
/*
//www.apache.org/licenses/LICENSE-2.0
/** Base test case for subclasses of MappedFieldType */
/** Abstraction for mutating a property of a MappedFieldType */
/** The name of the property that is being modified. Used in test failure messages. */
/** True if this property is updateable, false otherwise. */
/** Modifies the property */
/**
// check that we can update if the analyzer is unchanged
// check that we can update if the similarity is unchanged
/**
/** Sets the null value used by the modifier for null value testing. This should be set in an @Before method. */
/** Create a default constructed fieldtype */
// TODO: remove this once toString is no longer final on FieldType...
// transitivity
// reflexive
// symmetric
// modify the same property and they are equal again
// no exception
// always symmetric when not strict
// not compatible whether strict or not
/*
//www.apache.org/licenses/LICENSE-2.0
//this filter doesn't filter any field out, but it's used to exercise the code path executed when the filter is not no-op
/*
//www.apache.org/licenses/LICENSE-2.0
// this sucks how much must be overridden just do get a dummy field mapper...
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// to be overridden by subclasses
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// only static methods
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// use random but fixed term for creating shards
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// add node id as name to settings for proper logging
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/** recovers a replica from the given primary **/
/** recovers a replica from the given primary **/
/**
// update both primary and replica shard state
/**
// advance local checkpoint
// manually replicate max_seq_no_of_updates
// advance local checkpoint
// advance local checkpoint
// manually replicate max_seq_no_of_updates
// advance local checkpoint
/** Recover a shard from a snapshot using a given repository **/
/**
/**
// This isn't a warmer but sometimes verify the content in the reader
/*
//www.apache.org/licenses/LICENSE-2.0
/** A dummy repository for testing which just needs restore overridden */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// exposed in ES
// this one "seems to mess up offsets". probably shouldn't be a tokenizer...
// exposed in ES
// TODO: these tokenfilters are not yet exposed: useful?
// suggest stop
// capitalizes tokens
// like length filter (but codepoints)
// puts hyphenated words back together
// repeats anything marked as keyword
// like limittokencount, but by offset
// like limittokencount, but by position
// ???
// removes duplicates at the same position (this should be used by the existing factory)
// ???
// puts the type into the payload
// puts the type as a synonym
// fingerprint
// for tee-sinks
// for token filters that generate bad offsets, which are now rejected since Lucene 7
// should we expose it, or maybe think about higher level integration of the
// fake term frequency feature (LUCENE-7854)
// LUCENE-8273: ProtectedTermFilterFactory allows analysis chains to skip
// particular token filters based on the attributes of the current token.
// LUCENE-8332
// LUCENE-8936
// TODO: these charfilters are not yet exposed: useful?
// handling of zwnj for persian
/**
/**
// for old indices
/**
// TODO drop this temporary shim when all the old style tokenizers have been migrated to new style
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Don't compare the timestamp of ingest metadata since it will differ between executions
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Never generates a dot:
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// we use the MockTransportService.TestPlugin class as a marker to create a network
// module with this MockNetworkService. NetworkService is such an integral part of the systme
// we don't allow to plug it in from plugins or anything. this is a test-only override and
// can't be done in a production env.
//do not configure this in tests as this is causing SetOnce to throw exceptions when jvm is used for multiple tests
/*
//www.apache.org/licenses/LICENSE-2.0
// just a marker plugin for MockNode to mock out BigArrays
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Utility methods for testing plugins */
/** convenience method to write a plugin properties file */
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// Drain input stream fully to avoid warnings from SDKs like S3 that don't like closing streams mid-way
/**
// Skip Lucene MockFS extraN directory
// Assert that for each snapshot, the relevant metadata was written to index and shard folders
// Skip Lucene MockFS extraN directory
// TODO: we shouldn't be leaking empty shard directories when a shard (but not all of the index it belongs to)
//       becomes unreferenced. We should fix that and remove this conditional once its fixed.
/**
/**
// Setting local node as master so it may update the repository metadata in the cluster state
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// override file, to check if we get latest contents
// does not raise when blobs don't exist
// does not raise when blobs don't exist
// Wait for green so the close does not fail in the edge case of coinciding with a shard recovery that hasn't fully synced yet
// don't delete on the first iteration
// Check number of documents in this iteration
// Wait for green so the close does not fail in the edge case of coinciding with a shard recovery that hasn't fully synced yet
// deleted index
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
//" + InetAddresses.toUriString(address.getAddress()) + ":" + address.getPort();
/**
/**
// first key is a unique identifier for the incoming HTTP request,
// value is the number of times the request has been seen
// ignored, stream is assumed to have been closed by previous handler
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** A non-typed compiler for a single custom context */
// Scripts are always resolved using the script's source. For inline scripts, it's easy because they don't have names and the
// source is always provided. For stored and file scripts, the source of the script must match the key of a predefined script.
// TODO: remove this once scripts know to look for params under params key
// TODO: remove this once scripts know to look for params under params key
// TODO: remove this once scripts know to look for params under params key
// TODO: remove this once scripts know to look for params under params key
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Throw an {@link AssertionError} if there are still in-flight contexts. */
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// We build a json version of the search_from first in order to
// ensure that every number type remain the same before/after xcontent (de)serialization.
// This is not a problem because the final type of each field value is extracted from associated sort field.
// This little trick ensure that equals and hashcode are the same when using the xcontent serialization.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// never cache a query
// TODO: now just needed for top_hits, this will need to be revised for other agg unit tests:
/* Store the release-ables so we can release them at the end of the test case. This is important because aggregations don't
/**
/**
/**
/**
/**
// sometimes do an incremental reduce
// now do the final reduce
// materialize any parent pipelines
// materialize any sibling pipelines at top level
/**
// this executes basic query checks and asserts that weights are normalized only once etc.
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// TODO we only change name and boost, we should extend by any sub-test supplying a "mutate" method that randomly changes one
// aspect of the object under test
// we use the streaming infra to create a copy of the query provided as
// argument
// ensure that we do not create 2 aggregations with the same name
/*
//www.apache.org/licenses/LICENSE-2.0
// we shouldn't use the full long range here since we sum doc count on reduce, and don't want to overflow the long range there
// since we shuffle xContent, we cannot rely on the order of the original inner aggregations for comparison
/*
//www.apache.org/licenses/LICENSE-2.0
// size is 0
/*
//www.apache.org/licenses/LICENSE-2.0
// TODO randomize the size and the params in here?
// creating an index to test the empty buckets functionality. The way it works is by indexing
// two docs {value: 0} and {value : 2}, then building a histogram agg with interval 1 and with empty
// buckets computed.. the empty bucket is the one associated with key "1". then each test will have
// to check that this bucket exists with the appropriate sub aggregations.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Allows blocking on writing the index-N blob; this is a way to enforce blocking the
/** Allows blocking on writing the snapshot file at the end of snapshot creation to simulate a died master node */
// TODO: use another method of testing not being able to read the test file written by the master...
// this is super duper hacky
// Clean blocking flags, so we wouldn't try to block again
// Delay operation after unblocking
// So, we can start node shutdown while this operation is still running.
//
/**
// for network based repositories, the blob may have been written but we may still
// get an error with the client connection, so an IOException here simulates this
// Simulate a failure between the write and move operation in FsBlobContainer
// Atomic write since it is potentially supported
// by the delegating blob container
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we have to prefer CURRENT since with the range of versions we support it's rather unlikely to get the current actually.
// we initialize the serviceHolder and serviceHolderWithNoType just once, but need some
// calls to the randomness source during its setup. In order to not mix these calls with
// the randomness source that is later used in the test method, we use the master seed during
// this setup
/**
/**
/**
/**
/**
// also add mappings for two inner field in the object field
// Simplistic index name matcher used for testing
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
// to find root query to add additional bogus field there
// we'd like to see the offending field name here
/**
// Adds the valid query to the list of queries to modify and test
// Adds the alternates versions of the query too
// different kinds of exception wordings depending on location
// of mutation, so no simple asserts possible here
/**
// Indicate if a part of the query can hold any arbitrary content
// Track the number of query mutations
// Parse the valid query and inserts a new object level called "newField"
// We reached the place in the object tree where we want to insert a new object level
// The query has one or more fields that hold arbitrary content. If the current
// field is one (or a child) of those, no exception is expected when parsing the mutated query.
// Jump to next token
// We are walking through the object tree, so we can safely copy the current node
// We did not reach the insertion point, there's no more mutations to try
// We reached the expected insertion point, so next time we'll try one step further
/**
/**
/**
/**
/**
/**
/**
/* we use a private rewrite context here since we want the most realistic way of asserting that we are cacheable or not.
//remove after assertLuceneQuery since the assertLuceneQuery impl might access the context as well
// query _name never should affect the result of toQuery, we randomly set it to make sure
// extra safety to fail fast - serialize the rewritten version to ensure it's serializable.
/**
/**
/**
/**
/**
/**
// TODO we only change name and boost, we should extend by any sub-test supplying a "mutate" method that randomly changes one
// aspect of the object under test
/**
//we use the streaming infra to create a copy of the query provided as argument
/**
// unicode in 10% cases
/**
// if no type is set then return a random field name
/**
/**
// now assert that we actually generate the same JSON
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// TODO: Make this abstract when all sub-classes implement this (https://github.com/elastic/elasticsearch/issues/25929)
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
// add a few random fields to check that the parser is lenient on new fields
/*
//www.apache.org/licenses/LICENSE-2.0
// when set to true, writers will acquire writes from a semaphore
/**
/**
/**
// always try to get at least one
// time out -> check if we have to stop.
// time out -> check if we have to stop.
/**
/** Pausing indexing by setting current document limit to 0 */
/**
/** Stop all background threads * */
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// make sure we increment versions as listener may depend on it for change
// don't do anything
// don't do anything
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// corrupt files
// Skip files added by Lucene's ExtrasFS
/**
// TODO: it is known that Lucene does not check the checksum of CFS file (CompoundFileS, like an archive)
// see note at https://github.com/elastic/elasticsearch/pull/33911
// so far, don't corrupt crc32 part of checksum (last 4 bytes) of cfs file
// checksum is 8 bytes: first 4 bytes have to be zeros, while crc32 value is not verified
// we need to add assumptions here that the checksums actually really don't match there is a small chance to get collisions
// in the checksum which is ok though....
// collision
// checksum corrupted
// read
// corrupt
// rewrite
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
// TODO not sure how useful the following test is
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// doesn't work with potential multi data path from test cluster yet
/**
/**
/** node names of the corresponding clusters will start with these prefixes */
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// see @SuiteScope
//In an integ test it doesn't make sense to keep track of warnings: if the cluster is external the warnings are in another jvm,
//if the cluster is internal the deprecation logger is shared across all nodes
/**
// TODO move settings for random directory etc here into the index based randomized settings.
// this is only used by mock plugins and if the cluster is not internal we just can't set it
// if the test class is annotated with SuppressCodecs("*"), it means don't use lucene's codec randomization
// otherwise, use it, it has assertions and so on that can find bugs.
// always default delayed allocation to 0 to make sure we have tests are not delayed
// keep this low so we don't stall tests
// just don't flush
// remove this cluster first
// all leftovers are gone by now... this is really just a double safety if we miss something somewhere
// only build if it's not there yet
// close the previous one and create a new one
// wipe after to make sure we fail in the test that didn't ack the delete
// it is ok to leave persistent / transient cluster state behind if scope is TEST
// if we failed here that means that something broke horribly so we should clear all clusters
// TODO: just let the exception happen, WTF is all this horseshit
// afterTestRule.forceFailure();
/**
//use either 0 or 1 replica, yet a higher amount when possible, but only rarely
/**
// 30% of the time
// always default delayed allocation to 0 to make sure we have tests are not delayed
/**
/**
/**
/**
/**
/**
/**
/** Ensures the result counts are as expected, and logs the results if different */
/**
/**
/**
/**
/**
// We currently often use ensureGreen or ensureYellow to check whether the cluster is back in a good state after shutting down
// a node. If the node that is stopped is the master node, another node will become master and publish a cluster state where it
// is master but where the node that was stopped hasn't been removed yet from the cluster state. It will only subsequently
// publish a second state where the old master is removed. If the ensureGreen/ensureYellow is timed just right, it will get to
// execute before the second cluster state update removes the old master and the condition ensureGreen / ensureYellow will
// trivially hold if it held before the node was shut down. The following "waitForNodes" condition ensures that the node has
// been removed by the master so that the health check applies to the set of nodes we expect to be part of the cluster.
/**
/**
/**
// indexing threads can wait for up to ~1m before retrying when they first try to index into a shard which is not STARTED.
// no progress - try to refresh for the next time
// count now acts like search and barfs if all shards failed...
/**
// if static init fails the cluster can be null
/**
// remove local node reference
// remove local node reference
// Check that the non-master node has the same version of the cluster state as the master and
// that the master node matches the master (otherwise there is no requirement for the cluster state to match)
// We cannot compare serialization bytes since serialization order of maps is not guaranteed
// but we can compare serialization sizes - they should be the same
// Compare JSON serialization
/**
// this is just a temporary thing but it's easier to change if it is encapsulated.
/**
/**
/**
/**
/**
/**
/**
// TODO RANDOMIZE with flush?
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// (index, type, id)
// inject some bogus docs
// We configure a routing key in case the mapping requires it
// If you are indexing just a few documents then frequently do it one at a time.  If many then frequently in bulk.
// re-index if rejected
// delete the bogus types again - it might trigger merges or at least holes in the segments and enforces deleted docs!
/** Disables an index block for the specified index */
/** Enables an index block for the specified index */
/** Sets or unsets the cluster read_only mode **/
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// if we are not annotated assume suite!
/**
// Default the watermarks to absurdly low to prevent the tests
// from failing on nodes without enough disk space
// by default we never cache below 10k docs in a segment,
// bypass this limit so that caching gets some testing in
// integration tests that usually create few documents
// wait short time for other active shards before actually deleting, default 30s not needed in tests
// randomly enable low-level search cancellation to make sure it does not alter results
// empty list disables a port scan for other nodes
// Sometimes adjust the minimum search thread pool size, causing
// QueueResizingEsThreadPoolExecutor to be used instead of a regular
// fixed thread pool
/**
//" + stringAddress);
// add both mock plugins - local and tcp if they are not there
// we do this in case somebody overrides getMockPlugins and misses to call super
/**
/** Returns {@code true} iff this test cluster should use a dummy http transport */
/**
/**
/** Return the mock plugins the cluster should use */
// sometimes run without those completely
/**
/**
/**
/**
// Deleting indices is going to clear search contexts implicitly so we
// need to check that there are no more in-flight search contexts before
// we remove indices
/**
// note we need to do this this way to make sure this is reproducible
/**
// If it's internal cluster - using existing registry in case plugin registered custom data
// If it's external cluster - fall back to the standard set
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// we must wait for the node to actually be up and running. otherwise the node might have started,
// elected itself master but might not yet have removed the
// SERVICE_UNAVAILABLE/1/state not recovered / initialized block
//the seed has to be created regardless of whether it will be used or not, for repeatability
// Create the node lazily, on the first test. This is ok because we do not randomize any settings,
// only the cluster name. This allows us to have overridden properties for plugins and the version to use.
//the seed can be created within this if as it will either be executed before every test method or will never be.
/**
/** The plugin classes that should be added to the node. */
/** Helper method to create list of plugins without specifying generic types. */
// due to type erasure, the varargs type is non-reifiable, which causes this warning
/** Additional settings to add when creating the node. Also allows overriding the default settings. */
/** True if a dummy http transport should be used, or false if the real http transport should be used. */
// TODO: use a consistent data path for custom paths
// This needs to tie into the ESIntegTestCase#indexSettings() method
// limit the number of threads created
// default the watermarks low values to prevent tests from failing on nodes without enough disk space
// turning on the real memory circuit breaker leads to spurious test failures. As have no full control over heap usage, we
// turn it off for these tests.
// empty list disables a port scan for other nodes
// allow test cases to provide their own settings or override these
/**
/**
/**
/**
/**
/**
/**
// Wait for the index to be allocated so that cluster state updates don't override
// changes that would have been done locally
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// 5 sec lingering
// we suppress pretty much all the lucene codecs for now, except asserting
// assertingcodec is the winner for a codec here: it finds bugs and gives clear exceptions.
// Allows distinguishing between parallel test processes
// shutdown hook so that when the test JVM exits, logging is shutdown too
// filter out joda timezones that are deprecated for the java time migration
// Enable Netty leak detection and monitor logger for logged leak errors
// -----------------------------------------------------------------
// Suite and test case setup/cleanup.
// -----------------------------------------------------------------
/**
/**
/** called after a test is finished, but only if successful */
// setup mock filesystems for this test run. we change PathUtils
// so that all accesses are plumbed thru any mock wrappers
// randomize content type for request builders
//github.com/bcgit/bc-java/issues/405");
/**
// We check threadContext != null rather than enableWarningsCheck()
// because after methods are still called in the event that before
// methods failed, in which case threadContext might not have been
// initialized
//Check that there are no unaccounted warning headers. These should be checked with {@link #assertWarnings(String...)} in the
//appropriate test
/**
/**
// "clear" context by stashing current values and dropping the returned StoredContext
// ensure that the status logger is set to the warn level so we do not miss any warnings with our Log4j usage
// Log4j will write out status messages indicating problems with the Log4j usage to the status logger; we hook into this logger and
// assert that no such messages were written out as these would indicate a problem with our logging configuration
// separate method so that this can be checked again after suite scoped cluster is shut down
// ensure no one changed the status logger level on us
// ensure that there are no status logger messages which would indicate a problem with our Log4j usage; we map the
// StatusData instances to Strings as otherwise their toString output is useless
// we clear the list so that status data from other tests do not interfere with tests within the same JVM
// this must be a separate method from other ensure checks above so suite scoped integ tests can call...TODO: fix that
// mockdirectorywrappers currently set this boolean if checkindex fails
// TODO: can we do this cleaner???
/** MockFSDirectoryService sets this: */
// -----------------------------------------------------------------
// Test facilities and facades for subclasses.
// -----------------------------------------------------------------
// TODO: decide on one set of naming for between/scaledBetween and remove others
// TODO: replace frequently() with usually()
/**
/**
/**
/**
/**
/**
/**
/**
/**
// formula below does not work with very large doubles
/** A random integer from 0..max (inclusive). */
/** Pick a random object from the given array. The array must not be empty. */
/** Pick a random object from the given array. The array must not be empty. */
/** Pick a random object from the given list. */
/** Pick a random object from the given collection. */
/** Pick a random object from the given collection. */
/**
/**
/**
// work around a JDK bug, where java 8 cannot parse the timezone GMT0 back into a temporal accessor
// see https://bugs.openjdk.java.net/browse/JDK-8138664
/**
/**
/**
/**
/**
// In case you've forgotten your high-school studies, log10(x) / log10(y) == log y(x)
/**
// After 1s, we stop growing the sleep interval exponentially and just sleep 1s until maxWaitTime
/**
/**
// we override LTC behavior here: wrap even resources with mockfilesystems,
// because some code is buggy when it comes to multiple nio.2 filesystems
// (e.g. FileSystemUtils, and likely some tests)
/** Returns a random number of temporary paths. */
/** Return consistent index settings for the provided index version. */
/**
/**
/**
/**
// Oh well, we didn't get enough unique things. It'll be ok.
/**
/**
/**
//we need a sorted map for reproducibility, as we are going to shuffle its keys and write XContent back
// shuffle fields of objects in the list, but not the list itself
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/** Returns the suite failure marker: internal use only! */
/** Compares two stack traces, ignoring module (which is not yet serialized) */
/** Compares two stack trace elements, ignoring module (which is not yet serialized) */
/*
// busy spin
/**
/**
/**
/**
/**
// upper bound is inclusive
// some tests use MockTransportService to do network based testing. Yet, we run tests in multiple JVMs that means
// concurrent tests could claim port that another JVM just released and if that test tries to simulate a disconnect it might
// be smart enough to re-connect depending on what is tested. To reduce the risk, since this is very hard to debug we use
// a different default port range per JVM unless the incoming settings override it
// use a non-default base port otherwise some cluster in this JVM might reuse a port
// We rely on Gradle implementation details here, the worker IDs are long values incremented by one  for the
// lifespan of the daemon this means that they can get larger than the allowed port range.
// Ephemeral ports on Linux start at 32768 so we modulo to make sure that we don't exceed that.
// This is safe as long as we have fewer than 224 Gradle workers running in parallel
// See also: https://github.com/elastic/elasticsearch/issues/44134
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// ExternalTestCluster does not check the request breaker,
// because checking it requires a network request, which in
// turn increments the breaker, making it non-0
/*
//www.apache.org/licenses/LICENSE-2.0
// FieldFilterLeafReader does not forward cache helpers
// since it considers it is illegal because of the fact
// that it changes the content of the index. However we
// want this behavior for tests, and security plugins
// are careful to only use the cache when it's valid
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/** Return an instance on an unmapped field. */
// For most impls, we use the same instance in the unmapped case and in the mapped case
// Sort aggs so that unmapped come last.  This mimicks the behavior of InternalAggregations.reduce()
// sometimes do an incremental reduce
//check that non final reduction never adds buckets
/**
// TODO populate pipelineAggregators
/** Return an instance on an unmapped field. */
// TODO populate pipelineAggregators
/*
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/* Sorted map to make traverse order reproducible.
/* Each shared node has a node seed that is used to start up the node and get default settings
// if set to 0, data nodes will also assume the master role
/**
// use a dedicated master, but only low number to reduce overhead to tests
// Default the watermarks to absurdly low to prevent the tests
// from failing on nodes without enough disk space
// Some tests make use of scripting quite a bit, so increase the limit for integration tests
// always reduce this - it can make tests really slow
/**
// 10% of the nodes have a very frequent check interval
// 90% of the time - 10% of the time we don't set anything
// sometimes set a
// randomize tcp settings
// turning on the real memory circuit breaker leads to spurious test failures. As have no full control over heap usage, we
// turn it off for these tests.
// do something crazy slow here
// if multiple maven task run on a single host we better have an identifier that doesn't rely on input params
// do not create unicast host file for this one node.
/**
/**
// prevent killing the master if possible and client nodes
// allow overriding the above
// force certain settings
/**
// reusing an existing node implies its transport service already started
// we clone this here since in the case of a node restart we might need it again
/**
/* Randomly return a client to one of the nodes in the cluster */
/**
/* Randomly return a client to one of the nodes in the cluster */
/**
// ensure node client master is requested
/**
// ensure node client non-master is requested
/**
// currently unused
/**
/**
// TODO: collapse these together?
/**
// delete data folders now, before we start other nodes that may claim it
// use a new seed to make sure we generate a fresh new node id if the data folder has been wiped
// clear all rules for mock transport services
// trash all nodes with id >= sharedNodesSeeds.length - they are non shared
// clean up what the nodes left that is unused
// start any missing node
// we want to start nodes in one go
// if we don't have dedicated master nodes, keep things default
/** ensure a cluster is formed with all published nodes. */
// all nodes have a master
// all nodes have the same master (in same term)
// all nodes know about all other nodes
/* reset all clients - each test gets its own client based on the Random instance created above. */
// Check that the operations counter on index shard has reached 0.
// The assumption here is that after a test there are no ongoing write operations.
// test that have ongoing write operations after the test (for example because ttl is used
// and not all docs have been purged after the test) and inherit from
// ElasticsearchIntegrationTest must override beforeIndexDeletion() to avoid failures.
//check that shards that have same sync id also contain same number of documents
// the engine is closed or if the shard is recovering
// all good
/**
// shard is closed
//just ignore - shard movement
// shard is closed - just ignore
//just ignore - shard movement
// shard is closed - just ignore
// the local knowledge on the primary of the global checkpoint equals the global checkpoint on the shard
/**
// only reset the clients on nightly tests, it causes heavy load...
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// filter out old masters
// ensure that new nodes can find the existing nodes when they start
// update once masters have joined
// cannot be a synchronized method since it's called on other threads from within synchronized startAndPublishNodesAndClients()
/**
/**
/**
/**
/**
// we have to validate cluster size to ensure that the restarted node has rejoined the cluster if it was master-eligible;
// If stopping few enough master-nodes that there's still a majority left, there is no need to withdraw their votes first.
// However, we do not yet have a way to be sure there's a majority left, because the voting configuration may not yet have
// been updated when the previous nodes shut down, so we must always explicitly withdraw votes.
// TODO add cluster health API to check that voting configuration is optimal so this isn't always needed
/**
// we want to start nodes in one go
// randomize start up order
/**
/**
/**
/**
// fast-path
/**
/**
/**
/**
/**
/**
// synchronized to prevent concurrently modifying the cluster.
/**
/**
/**
/**
/**
/** returns true if the restart should also validate the cluster has reformed */
// Checks that the breakers have been reset without incurring a
// network request, because a network request can increment one
// of the breakers
// Clean up the cache, ensuring that entries' listeners have been called
// Anything that uses transport or HTTP can increase the
// request breaker (because they use bigarrays), because of
// that the breaker can sometimes be incremented from ping
// requests from other clusters because Jenkins is running
// multiple ES testing jobs in parallel on the same machine.
// To combat this we check whether the breaker has reached 0
// in an assertBusy loop, so it will try for 10 seconds and
// fail if it never reached 0
// see #ensureEstimatedStats()
// ensure that our size accounting on transport level is reset properly
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// dummy address/info that can be read by code expecting objects from the relevant methods,
// but not actually used for a real connection
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// do some stuff
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Returns plugins that should be loaded on the node */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** Stores the posix attributes for a path and resets them on close. */
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
//JSON and YAML write the base64 format
//with CBOR we get back a float
//with SMILE we get back a double (this will change in Jackson 2.9 where it will return a Float)
//with JSON AND YAML we get back a double, but with float precision.
/**
//the source can be stored in any format and eventually converted when retrieved depending on the format of the response
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// ignore
// Happens if `action.destructive_requires_name` is set to true
// which is the case in the CloseIndexDisableCloseAllTests
/**
// ignore
/**
// if nothing is provided, delete all
// ignore
/**
// if nothing is provided, delete all
// ignore
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// no profiling
/*
//www.apache.org/licenses/LICENSE-2.0
/** Utilities for selecting versions in tests */
/**
// group versions into major version
// this breaks b/c 5.x is still in version list but master doesn't care about it!
//assert majorVersions.size() == 2;
// TODO: remove oldVersions, we should only ever have 2 majors in Version
// on master branch
// remove current
// on a stable or release branch, ie N.x
// remove the next maintenance bugfix
// remove next minor
// a minor is being staged, which is also unreleased
// remove the next bugfix
// If none of the previous major was released, then the last minor and bugfix of the old version was not released either.
// minor of the old version is being staged
// bugix of the old version is also being staged
// we add unreleased out of order, so need to sort here
// split the given versions into sub lists grouped by minor version
// move the last version of the last minor in versions to the unreleased versions
/**
/**
/**
/**
/**
/**
/** Returns the oldest released {@link Version} */
/** Returns a random {@link Version} from all available versions. */
/** Returns a random {@link Version} from all available versions, that is compatible with the given version. */
/** Returns a random {@link Version} between <code>minVersion</code> and <code>maxVersion</code> (inclusive). */
// minVersionIndex is inclusive so need to add 1 to this index
/** returns the first future compatible version */
/** Returns the maximum {@link Version} that is compatible with the given version. */
/**
/**
// TODO: change this to minimumCompatibilityVersion(), but first need to remove released/unreleased
// versions so getPreviousVerison returns the *actual* previous version. Otherwise eg 8.0.0 returns say 7.0.2 for previous,
// but 7.2.0 for minimum compat
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*", firstObj, secondObj);
/**
// insert here
// we can use NamedXContentRegistry.EMPTY here because we only traverse the xContent once and don't use it
/**
// parser.currentName() can be null for root object and unnamed objects in arrays
// dots in randomized field names need to be escaped, we use that character as the path separator
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/** A {@link Client} that randomizes request parameters. */
// we don't use the QUERY_AND_FETCH types that break quite a lot of tests
// given that they return `size*num_shards` hits instead of `size`
// '_' is a reserved character
// randomly use the default
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// logging has shared JVM locks; we may suspend a thread and block other nodes from doing their thing
// security manager is shared across all nodes and it uses synchronized maps internally
// SecureRandom instance from SecureRandomHolder class is shared by all nodes
/**
//bugs.openjdk.java.net/browse/JDK-8218446>JDK-8218446</a>
// we spawn a background thread to protect against deadlock which can happen
// if there are shared resources between caller thread and suspended threads
// see unsafeClasses to how to avoid that
// keep trying to suspend threads, until no new threads are discovered.
// best effort to signal suspending
// best effort;
/*
// block detection checks if other threads are blocked waiting on an object that is held by one
// of the threads that was suspended
// find ThreadInfo object of the blocking thread (if available)
// resume threads if failed
// best effort
/**
// suspends/resumes threads intentionally
// we didn't make enough space, retry
// we assume it is not safe to suspend the thread
/*
// double check the thread is not in a shared resource like logging; if so, let it go and come back
// it is definitely not safe to suspend the thread
/*
// for testing
// for testing
// for testing
// for testing
// suspends/resumes threads intentionally
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// wait while checking for a stopped
// try to wait again, we really want the cluster state thread to be freed up when stopping disruption
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
// only one suite closeable per Engine
/**
/*
/*
/*
/*
//www.apache.org/licenses/LICENSE-2.0
// lazy initialized since we need it already on super() ctor execution :(
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/** Increments for the requests ids **/
/** Current working directory of the fixture **/
/**
/// Writes the PID of the current Java process in a `pid` file located in the working directory
// Writes the address and port of the http server in a `ports` file located in the working directory
// Check if this is a request made by the AntFixture
// Wait to be killed
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/* node id */, Map<ShardId, ShardRouting>> knownAllocations = new HashMap<>();
// for now always return immediately what we know
// for now, just pretend no node has data
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
//when the number for shards is randomized and we expect failures
//we can either run into partial or total failures depending on the current number of shards
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
// has to be outside catch clause to get a proper message
// has to be outside catch clause to get a proper message
/**
/**
/**
/**
/**
//we tried comparing byte per byte, but that didn't fly for a couple of reasons:
//1) whenever anything goes through a map while parsing, ordering is not preserved, which is perfectly ok
//2) Jackson SMILE parser parses floats as double, which then get printed out as double (with double precision)
//Note that byte[] holding binary values need special treatment as they need to be properly compared item per item.
/**
/**
/**
/**
//byte[] is really a special case for binary values when comparing SMILE and CBOR, arrays of other types
//don't need to be handled. Ordinary arrays get parsed as lists.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// noinspection OptionalAssignedToNull
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
/*
/**
/*
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// Ignore assumptions.
// append Gradle test runner test filter string
// fallback to system property filter when tests contain "."
// Client yaml suite tests are a special case as they allow for additional parameters
// we handle our own environment settings
/**
// we don't want the iters to be in there!
//don't print out the test class, we print it ourselves in appendAllOpts
//without filtering out the parameters (needed for REST tests)
//don't print out the test method, we print it ourselves in appendAllOpts
//without filtering out the parameters (needed for REST tests)
// we always use the default prefix
// these properties only make sense for integration tests
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
// EMPTY and THROW are fine here because `.map` doesn't use named x content or deprecation
/**
/**
/**
/**
/**
/**
// absolute equality required in expected and actual.
// Some known warnings can safely be ignored
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Cleanup rollup before deleting indices.  A rollup job might have bulks in-flight,
// so we need to fully shut them down first otherwise a job might stall waiting
// for a bulk to finish against a non-existing index (and then fail tests)
// Clean up SLM policies before trying to wipe snapshots so that no new ones get started by SLM after wiping
// Repeatedly delete the snapshots until there aren't any
// At this point there should be no snaphots
// This will cause an error at the end of this method, but do the rest of the cleanup first
// wipe indices
// wipe index templates
/*
/*"));
// wipe cluster settings
// 404 here just means we had no indexes
/**
// All other repo types we really don't have a chance of being able to iterate properly, sadly.
/**
// Ignore 404s because they imply someone was racing us to delete this
// If bad request returned, ILM is not enabled.
// If bad request returned, SLM is not enabled.
// If bad request returned, CCR is not enabled.
/**
// Ignore the task list API - it doesn't count against us
/*
/**
/**
/**
// default to the same client settings
/**
/**
/**
/**
/**
/**
// once detailed recoveries works, remove this if.
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*allow_no_indices*</code>. This will
/**
// guard against accidentally matching everything as an empty string lead to the pattern ".*" which matches everything
// very simple transformation from wildcard to a proper regex
// support wildcard matches (within a single path segment)
// restore previously escaped ',' in paths.
// suffix match
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Raw requests don't use the rest spec at all and are configured entirely by their parameters
// All other parameters are url parameters
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
//the rest path to use is randomized out of the matching ones (if more than one)
//divide params between ones that go within query string and ones that go within path
//Encode rules for path and query string parameters are different. We use URI to encode the path. We need to encode each
// path part separately, as each one might contain slashes that need to be escaped, which needs to be done manually.
// We prepend "/" to the path part to handle parts that start with - or other invalid characters.
//manually escape any slash that each part may contain
//randomly test the GET with source param instead of GET/POST with body
//lazily build a new client in case we need to point to some specific node
// We check the warnings ourselves so we don't need the client to do it for us
// The API doesn't claim to support GET anyway
// Negative length means "unknown" or "huge" in this case. Either way we can't send it as a parameter
// Long bodies won't fit in the parameter and will cause a too_long_frame_exception
// We can only encode JSON or YAML this way.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
//makes a copy of the parameters before modifying them for this specific request
// By default ask for error traces, this my be overridden by params
//make a copy of the headers before modifying them for this specific request
// if we hit a bad exception the response is null
//we always stash the last response body
// pkg-private for testing
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//skip parsing if we got text back (e.g. if we called _cat apis)
/**
/**
//we only get here if there is no response body or the body is text
/**
//content-type null means that text was returned
//if the body is in a binary format and gets requested as a string (e.g. to log a test failure), we convert it to json
/**
/**
//special case: api that don't support body (e.g. exists) return true if 200, false if 404, even if no body
//is_true: '' means the response had no body but the client returned true (caused by 200)
//is_false: '' means the response had no body but the client returned false (caused by 404)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*"
/**
/**
/**
// admin context must be available for @After always, regardless of whether the test was blacklisted
/**
/**
// default to all tests under the test root
// yaml suites are grouped by directory (effectively by api)
//sort the candidates so they will always be in the same order before being shuffled, for repeatability
/** Find all yaml suites that match the given list of paths from the root test path. */
// pkg private for tests
/** Add a single suite file to the set of suites. */
// we simply go to the _cat/nodes API and parse all versions in the cluster
//skip test if it matches one of the blacklist globs
//skip test if the whole suite (yaml file) is disabled
//skip test if the whole suite (yaml file) is disabled
//skip test if test section is disabled
//let's check that there is something to run, otherwise there might be a problem with the test section
/**
// Dump the stash on failure. Instead of dumping it in true json we escape `\n`s so stack traces are easier to read
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
/**
// Safe because we check that all the map keys are string in unstashObject
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//move to first field name
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//make sure we never modify the parameters once returned
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//remove the file extension
//our yaml parser seems to be too tolerant. Each yaml suite must end with \n, otherwise clients tests might break.
//the "---" section separator is not understood by the yaml parser. null is returned, same as when the parser is closed
//we need to somehow distinguish between a null in the middle of a test ("---")
// and a null at the end of the file (at least two consecutive null tokens)
/*
//www.apache.org/licenses/LICENSE-2.0
// add support for matching objects ({a:b}) against list of objects ([ {a:b, c:d} ])
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// must be part of API call then
//multiple bodies are supported e.g. in case of bulk provided as a whole string
/**
/**
//client should throw validation error before sending request
//lets just return without doing anything as we don't have any client to test here
//the text of the error message matches regular expression
//remove delimiters from regex
/**
// LinkedHashSet so that missing expected warnings come back in a predictable order which is nice for testing
// We skip warnings related to the transform rename so that we can continue to run the many mixed-version tests.
/*
/**
// . as in haskell's "compose" operator
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//if the value is wrapped into / it is a regexp (e.g. /s+d+/)
//Double 1.0 is equal to Integer 1
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// Do not build.
//we are in the beginning, haven't called nextToken yet
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// some randomness per shard
// we do this on the index level
// TODO: make this test robust to virus scanner
// don't use the settings from indexSettings#getSettings() they are merged with node settings and might contain
// secure settings that should not be copied in here since the new IndexSettings ctor below will barf if we do
// We only attempt to check open/closed state if there were no other test
// failures.
// TODO: perform real close of the delegate: LUCENE-4058
// dir.close();
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/**
/**
/**
/**
/**
/**
/**
// send the request, which will blow up
/**
/**
/**
/**
/**
// don't send anything, the receiving node is unresponsive
// close to simulate that tcp-ip eventually times out and closes connection (necessary to ensure transport eventually
// responds).
/**
/**
// TODO: Replace with proper setting
// delayed sending - even if larger then the request timeout to simulated a potential late response from target node
// poor mans request cloning...
// store the request to send it once the rule is cleared.
/**
/**
/**
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// we use always a non-alpha or beta version here otherwise minimumCompatibilityVersion will be different for the two used versions
// This is a customized profile for this test case.
// this one supports dynamic tracer updates
// this one doesn't support dynamic tracer updates
// wait till all nodes are properly connected and the event has been sent, so tests in this class
// will not get this callback called on the connections done in this setup
// this should be a noop
// use assert busy as callbacks are called on a different thread
// use assert busy as callbacks are called on a different thread
// use assert busy as callbacks are called on a different thread
// use assert busy as callbacks are called on a different thread
// we don't really care what's going on B, we're testing through A
// B senders just generated activity so serciveA can respond, we don't test what's going on there
// capture now
// ok!
// ok
// simulate restart of nodeB
// make sure we are fully closed here otherwise we might run into assertions down the road
// don't send back a response
// now, try and send another request, this times, with a short timeout
// sometimes leave include empty (default)
// not set, coming from service A
// initial values, cause its serialized from version 0
// wait for the transport to process the sending failure and disconnect from node
// now try to connect again and see that it fails
// all is well
// all is well
// when we close C here we have to disconnect the service otherwise assertions mit trip with pending connections in tearDown
// since the disconnect will then happen concurrently and that might confuse the assertions since we disconnect due to a
// connection reset by peer or other exceptions depending on the implementation
// note - this test uses backlog=1 which is implementation specific ie. it might not work on some TCP/IP stacks
// on linux (at least newer ones) the listen(addr, backlog=1) should just ignore new connections if the queue is full which
// means that once we received an ACK from the client we just drop the packet on the floor (which is what we want) and we run
// into a connection timeout quickly. Yet other implementations can for instance can terminate the connection within the 3 way
// handshake which I haven't tested yet.
// connection with one connection and a large timeout -- should consume the one spot in the backlog queue
// now with the 1ms timeout we got and test that is it's applied
// this acts like a node that doesn't have support for handshakes
// sometimes wait until the other side sends the message
// makes sure it's reproducible
// makes sure it's reproducible
// do nothing
// here the concurrent disconnect was faster and invoked the listener first
// don't block on a network thread here
// don't block on a network thread here
// nothing transmitted / read yet
// netty for instance invokes this concurrently so we better use assert busy here
// we did a single round-trip to do the initial handshake
// netty for instance invokes this concurrently so we better use assert busy here
// request has ben send
// response has been received
// don't block on a network thread here
// nothing transmitted / read yet
// netty for instance invokes this concurrently so we better use assert busy here
// request has been sent
// netty for instance invokes this concurrently so we better use assert busy here
// request has been sent
// exception response has been received
// 49 bytes are the non-exception message bytes that have been received. It should include the initial
// handshake message and the header, version, etc bytes in the exception message.
// ensure that we have profile precedence
// ensure that we have profile precedence
// port is required
// publish host has no global fallback for the profile since we later resolve it based on
// the bound address
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// loop through all profiles and start them up, special handling for default one
// make sure we maintain at least the types that are supported by this profile even if we only use a single channel for them.
// Only check every 2s to not flood the logs on a blocked thread.
// We mostly care about long blocks and not random slowness anyway and in tests would randomly catch slow operations that block for
// less than 2s eventually.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// fails with negligible probability 2^{-100}
// fails if every task has zero variability -- vanishingly unlikely
/*
//www.apache.org/licenses/LICENSE-2.0
// check that this is only called once
// check that this is only called once
/*
//www.apache.org/licenses/LICENSE-2.0
// check that new task gets queued
// check that no more tasks are queued
/*
//www.apache.org/licenses/LICENSE-2.0
// Threads that are part of a node get the node name
// Test threads get the test name
// Suite initialization gets "suite"
// And stuff that doesn't match anything gets wrapped in [] so we can see it
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// All these are required or MockNode will fail to build.
/*
//www.apache.org/licenses/LICENSE-2.0
// The next line with throw an exception if the date looks wrong
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/elastic/elasticsearch/pull/33911
// corrupt only last 4 bytes!
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// TODO: rework this test to use a dummy Version class so these don't need to change with each release
// full range
// sub range
// unbounded lower
// unbounded upper
// range of one
// implicit range of one
// max or min can be an unreleased version
/**
// First check the index compatible versions
/* Java lists all versions from the 5.x series onwards, but we only want to consider
/* Java lists all versions from the 5.x series onwards, but we only want to consider
// Now the wire compatible versions
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// at least one locked and one none lock thread
// make sure some threads are under lock
/**
// give some chance to catch this stack trace
// make sure some threads are under lock
// no threads should own the lock
// at least one locked and one none lock thread
// at least one locked and one none lock thread
// make sure some threads of test_node are under lock
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// sends some requests to the majority side part
// check all connections are restored
// TODO assertBusy should not be here, see https://github.com/elastic/elasticsearch/issues/38348
// send a request that is guaranteed disrupted.
// give a bit of time to send something under disruption.
// since we have 3+ nodes, we are sure to find a disrupted pair, also for bridge disruptions.
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//Based on EsRestTestCase.expectVersionSpecificWarnings helper method but without ESRestTestCase dependency
/*
//www.apache.org/licenses/LICENSE-2.0
// suffix match
// exact match
// additional text at the end should not match
/*", "/suite/termvector/20_issue7121/test_first");
// do not cross segment boundaries
/*", "/suite/termvector/20_issue7121/test/first");
/*allow_no_indices*", "/suite/indices.get/10_basic/we_allow_no_indices");
/*allow_no_indices*", "/suite/indices.get/10_basic/we_allow_no_indices_at_all/here");
/*/*allow_no_indices*", "/suite/indices.get/20_basic/we_allow_no_indices_at_all");
/*/*allow_no_indices*", "/suite/path/to/test/indices.get/20_basic/we_allow_no_indices_at_all");
/*/10_basic\\,20_advanced/*foo*", "/suite/indices.get/all/10_basic,20_advanced/foo");
/*/10_basic\\,20_advanced/*foo*", "/suite/indices.get/all/10_basic,20_advanced/my_foo");
/*/10_basic\\,20_advanced/*foo*", "/suite/indices.get/all/10_basic,20_advanced/foo_bar");
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//extension .yaml is optional
//single directory
//multiple directories
//multiple paths, which can be both directories or yaml test suites (with optional file extension)
//files can be loaded from classpath and from file system too
/*
//www.apache.org/licenses/LICENSE-2.0
// Stashed value is whole property name
// Stash key has dots
// Stashed value is part of property name
// Stashed value is inside of property name
// Multiple stashed values in property name
// Stashed value is part of property name and has dots
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//www.elasticsearch.org/guide/\"," +
//www.elasticsearch.org/guide/\"," +
// see params section is broken, an inside param is missing
//www.elasticsearch.org/guide/\"," +
// see parts section is broken, an inside param is missing
//www.elasticsearch.org/guide/\"," +
/*
//www.apache.org/licenses/LICENSE-2.0
//www.elastic.co/guide/en/elasticsearch/reference/master/search-count.html\",\n" +
//www.elastic.co/guide/en/elasticsearch/reference/master/indices-templates.html\",\n" +
//www.elastic.co/guide/en/elasticsearch/reference/master/docs-index_.html\",\n" +
/*
//www.apache.org/licenses/LICENSE-2.0
//www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html\"\n"+
//www.elastic.co/guide/en/elasticsearch/reference/master/docs-index_.html\",\n" +
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// test may be skipped so we did not create a parser instance
//next token can be null even in the middle of the document (e.g. with "---"), but not too many consecutive times
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// No warning headers doesn't throw an exception
// Any warning headers fail
// But not when we expect them
// But if you don't get some that you did expect, that is an error
// It is also an error if you get some warning you want and some you don't want
//stringified body is taken as is
//stringified body is taken as is
//dummy]",
//dummy]",
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
//out of 10 shuffling runs we expect to have at least more than 1 distinct output.
//This is to make sure that we actually do the shuffling
//out of 10 shuffling runs we expect to have at least more than 1 distinct output for both top level keys and inner object2
// "normal" way of calling where the value is not null
/*
// Gradle worker IDs are 1 based
/*
//www.apache.org/licenses/LICENSE-2.0
// In a 5+ node cluster there must be at least one reconfiguration as the nodes are shut down one-by-one before we drop to 2 nodes.
// If the nodes shut down too quickly then this reconfiguration does not have time to occur and the quorum is lost in the 3->2
// transition, even though in a stable cluster the 3->2 transition requires no special treatment.
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// doesn't work with potential multi data path from test cluster yet
/**
// if we manage min master nodes, we need to lock down the number of nodes
// we need one stable node
// stopping a node half way shouldn't clean data
// starting a node should re-use data folders and not clean it
// the cluster should be reset for a new test, cleaning up the extra path we made
// a new unknown node used this path, it should be cleaned
// but leaving the structure of existing, reused nodes
// but leaving the structure of existing, reused nodes
// last node and still no master
/**
/*
//www.apache.org/licenses/LICENSE-2.0
// Will throw an exception without the check for testClassPackage != null in testRunStarted
/*
/**
/**
//example.com")
/**
//example.com")
/**
/**
//example.com")
//example.com")
/**
//example.com")
//example.com")
/**
/**
//example.com")
/**
/**
//example.com")
/**
//example.com")
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// produce some randomness
/*
//www.apache.org/licenses/LICENSE-2.0
/**
// produce some randomness
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//www.apache.org/licenses/LICENSE-2.0
// types which are subject to checking when used in logger. <code>TestMessage<code> is also declared here to
// make sure this functionality works
// used by tests
/**
// VARARGS METHOD: debug(Marker?, String, (Object...|Supplier...))
// MULTI-PARAM METHOD: debug(Marker?, String, Object p0, ...)
// all the rest: debug(Marker?, (Message|MessageSupplier|CharSequence|Object|String|Supplier), Throwable?)
// constructor invocation
//counts how many times argAndField  was called on the method chain
// don't check logger usage for logger.warn(someObject)
/*
//www.apache.org/licenses/LICENSE-2.0
/**
/*
//www.apache.org/licenses/LICENSE-2.0
# Smoke-tests a x-pack release candidate
#
# 1. Downloads the zip file from the staging URL
# 3. Installs x-pack plugin
# 4. Starts one node for zip package and checks:
#    -- if x-pack plugin is loaded
#    -- checks xpack info page, if response returns correct version and feature set info
#
# USAGE:
#
# python3 -B ./dev-tools/smoke_test_rc.py --version 5.0.0-beta1 --hash bfa3e47
#
# in case of debug, uncomment
# HTTPConnection.debuglevel = 4
#that is ok it might not be there yet
# we now get / and /_nodes to fetch basic infos like hashes etc and the installed plugins
# check if plugin is loaded
# check if license is the default one
# also sleep for few more seconds, as the initial license generation might take some time
# try reading the pid and kill the node
# console colors
/*
/**
/**
// all good here, we are done
/**
/*
/**
/**
/*
// TODO: in jopt-simple 5.0 we can use a PathConverter to take Path instead of File
/*
// TODO: with jopt-simple 5.0, we can make these requiredUnless each other
// which is effectively "one must be present"
// sign
// dump
/*
// TODO: with jopt-simple 5.0, we can make these requiredUnless each other
// which is effectively "one must be present"
// verify
/*
/*
/*
/*
/*
/*
/*
// TODO this should probably become more structured once Analytics plugin has more than just one agg
/*
/*
/*
/*
// Increment usage here since it is a good boundary between internal and external, and should correlate 1:1 with
// usage and not internal instantiations
/**
/**
/**
/*
/**
// We have to create a new HLL because otherwise it will alter the
// existing cardinality sketch and bucket value
/*
/**
/*
/**
// Unknown
// should be an object
// should be an field
// should be an array
// should be a number
// values must be in increasing order
// should be an array
// should be a number
// we do not add elements with count == 0
// close the subParser so we advance to the end of the object
/** re-usable {@link HistogramValue} implementation */
/** reset the value for the histogram */
/*
/** Read from a stream. */
// Compute the sum of double values with Kahan summation algorithm which is more
// accurate than naive summation.
/**
/** Calculate base 2 logarithm */
/*
/** Read from a stream. */
/**
/**
/*
/**
/** Option to show the probability distribution for each character appearing in all terms. */
/** Accummulates the total length of all fields. Used for calculate average length and char frequencies. */
/** Map that stores the number of occurrences for each character. */
// Update min/max length for string
// Parse string chars and count occurrences
// Convert Map entries: Character -> String and LongArray -> Long
// Include only characters that have at least one occurrence
/*
/*
/**
/*
/*
/*
//1
//1
//3
//1
//5
//1
//7
//1
//9
//10
// Histogram
// Date Histogram
// Auto Date Histogram
// Mocked "test" agg, should fail validation
/*
//default
/*
//default
//assertEquals(0L, hdr.state.getTotalCount());
//assertEquals(4L, hdr.state.getTotalCount());
//assertEquals(16L, hdr.state.getTotalCount());
/*
/*
/*
//github.com/elastic/elasticsearch/issues/50307")
/*
//default
// TODO: Fix T-Digest: this assertion should pass but we currently get ~15
// https://github.com/elastic/elasticsearch/issues/14851
// assertThat(rank.getPercent(), Matchers.equalTo(0d));
// TODO: Fix T-Digest: this assertion should pass but we currently get ~59
// https://github.com/elastic/elasticsearch/issues/14851
// assertThat(rank.getPercent(), Matchers.equalTo(100d));
/*
//default
//assertEquals(0L, hdr.state.getTotalCount());
//assertEquals(4L, hdr.state.getTotalCount());
//assertEquals(16L, hdr.state.getTotalCount());
/*
// Intentionally not writing any docs
//github.com/elastic/elasticsearch/issues/47469")
/**
/*
// Follow indices actively following leader indices before the downgrade to basic license remain to follow
// the leader index after the downgrade, so document with id 5 should be replicated to follower index:
// Index2 was created in leader cluster after the downgrade and therefor the auto follow coordinator in
// follow cluster should not pick that index up:
// parse the logs and ensure that the auto-coordinator skipped coordination on the leader cluster
// (does not work on windows...)
// Manually following index2 also does not work after the downgrade:
/*
/*
// randomly do source filtering on indexing
/*
// randomly do source filtering on indexing
// unfollow and then follow and then index a few docs in leader index:
/*
// This index should be auto followed:
// This index will be followed manually
// We need to wait until index following is active for auto followed indices:
// (otherwise pause follow may fail, if there are no shard follow tasks, in case this test gets executed too quickly)
/*
/*
/*
// create a single index "leader" on the leader
// follow "leader" with "follow-leader" on the follower
// now create an auto-follow pattern for "leader-*"
// create "leader-1" on the leader, which should be replicated to "follow-leader-1" on the follower
// the follower should catch up
// create "leader-2" on the leader, and index some additional documents into existing indices
// the followers should catch up
// one more index "leader-3" on the follower
// the follower should catch up
/*
// Make sure that there are no other ccr relates operations running:
// Make sure that there are no other ccr relates operations running:
// User does not have manage_follow_index index privilege for 'unallowedIndex':
// Verify that the follow index has not been created and no node tasks are running
// User does have manage_follow_index index privilege on 'allowed' index,
// but not read / monitor roles on 'disallowed' index:
// Verify that the follow index has not been created and no node tasks are running
// Cleanup by deleting auto follow pattern and pause following:
/*
/*
/**
/**
// constructed reflectively by the plugin infrastructure
/**
// internal actions
// stats action
// follow actions
// auto-follow actions
// forget follower action
// stats API
// follow APIs
// auto-follow APIs
// forget follower API
// Persistent action requests
// Task statuses
// auto-follow metadata, persisted into the cluster state as XContent
// persistent action requests
// task statuses
/**
/**
/*
/**
/**
/**
/**
/**
/**
// client.getRemoteClusterClient(...) can fail with a IllegalArgumentException if remote
// connection is unknown
/**
// we have to check the license on the remote cluster
// following an index in remote cluster, so use remote client to fetch leader index metadata
/**
// NOTE: Placed this method here; in order to avoid duplication of logic for fetching history UUIDs
// in case of following a local or a remote cluster.
// Ignore replica shards as they may not have yet started and
// we just end up overwriting slots in historyUUIDs
/**
/*
/*
// this setting is intentionally not registered, it is only used in tests
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
// The following fields are read and updated under a lock:
/*
// TODO: set non-compliant status on auto-follow coordination that can be viewed via a stats API
// TODO: set non-compliant status on auto-follow coordination that can be viewed via a stats API
/**
// This check exists to avoid two AutoFollower instances a single remote cluster.
// (If an auto follow pattern is deleted and then added back quickly enough then
// the old AutoFollower instance still sees that there is an auto follow pattern
// for the remote cluster it is tracking and will continue to operate, while in
// the meantime in updateAutoFollowers() method another AutoFollower instance has been
// started for the same remote cluster.)
// keep the list of the last known active patterns for this auto-follower
// if the list changed, we explicitly retrieve the last cluster state in
// order to avoid timeouts when waiting for the next remote cluster state
// version that might never arrive
// Also check removed flag here, as it may take a while for this remote cluster state api call to return:
// If an index with the same name exists, but it is not a follow index for this leader index then
// we should let the auto follower attempt to auto follow it, so it can fail later and
// it is then visible in the auto follow stats. For example a cluster can just happen to have
// an index with the same name as the new follower index.
// Execute if the create and follow api call succeeds:
// This function updates the auto follow metadata in the cluster to record that the leader index has been followed:
// (so that we do not try to follow it in subsequent auto follow runs)
// The coordinator always runs on the elected master node, so we can update cluster state here:
/*
// Leader indices can be in the cluster state, but not all primary shards may be ready yet.
// This checks ensures all primary shards have started, so that index following does not fail.
// If not all primary shards are ready, then the next time the auto follow coordinator runs
// this index will be auto followed.
// A delete auto follow pattern request can have removed the auto follow pattern while we want to update
// the auto follow metadata with the fact that an index was successfully auto followed. If this
// happens, we can just skip this step.
// A delete auto follow pattern request can have removed the auto follow pattern while we want to update
// the auto follow metadata with the fact that an index was successfully auto followed. If this
// happens, we can just skip this step.
// Remove leader indices that no longer exist in the remote cluster:
/**
/*
/**
// timeout on wait_for_metadata_version
// ask for the next version.
// a put-mapping-request on old versions does not have origin.
// an indices aliases request on old versions does not have origin
/*
/*
// Starting the clock in order to know how much time is spent on fetching operations:
// must capture after snapshotting operations to ensure this MUS is at least the highest MUS of any of these operations.
// must capture IndexMetaData after snapshotting operations to ensure the returned mapping version is at least as up-to-date
// as the mapping version that these operations used. Here we must not use IndexMetaData from ClusterService for we expose
// a new cluster state to ClusterApplier(s) before exposing it in the ClusterService.
/**
// - 1 is needed, because toSeqNo is inclusive
// Make it easy to detect this error in ShardFollowNodeTask:
// (adding a metadata header instead of introducing a new exception that extends ElasticsearchException)
/*
/**
/*
/*
// updates follower mapping, this gets us the leader mapping version and makes sure that leader and follower mapping are identical
// This is the only request, we can optimistically fetch more documents if possible but not enforce max_required_seqno.
// We sneak peek if there is any thing new in the leader.
// If there is we will happily accept
// Always clear fetch exceptions:
// do not count polls against fetch stats
// In order to process this read response (3), we need to check and potentially update the follow index's setting (1) and
// check and potentially update the follow index's mappings (2).
// 4) handle read response:
// 3) update follow index mapping:
// 2) update follow index settings:
// 1) update follow index aliases:
// Do restore from repository here and after that
// start() should be invoked and stats should be reset
// For now handle like any other failure:
// need a more robust approach to avoid the scenario where an outstanding request
// can trigger another restore while the shard was restored already.
// https://github.com/elastic/elasticsearch/pull/37562#discussion_r250009367
/** Called when some operations are fetched from the leading */
// update last requested seq no as we may have gotten more than we asked for and we don't want to ask it again.
// read is completed, decrement
// In case that buffer has more ops than is allowed then reads may all have been stopped,
// this invocation makes sure that we start a read when there is budget in case no reads are being performed.
// Only retry is the shard follow task is not stopped.
// Cap currentRetry to avoid overflow when computing n variable
// + 1 here, because nextInt(...) bound is exclusive and otherwise the first delay would always be zero.
// If user does not have sufficient privileges
// If leader index is closed or no elected master
// If follow index is closed
// These methods are protected for testing purposes:
// To avoid confusion when ccr didn't yet execute a fetch:
/*
// list of headers that will be stored when a job is created
/*
/**
// this task is not a shard follow task
// the index exists, do not clean this persistent task
/*
// If no settings have been changed then just propagate settings version to shard follow node task:
// Figure out which settings have been updated:
// Figure out whether the updated settings are all dynamic settings and
// if so just update the follower index's settings:
// If only dynamic settings have been updated then just update these settings in follower index:
// If one or more setting are not dynamic then close follow index, update leader settings and
// then open leader index:
/*
// partition the aliases into the three sets
// add the aliases the follower does not have
// we intentionally override that the alias is not a write alias as follower indices do not receive direct writes
// update the aliases that are different (ignoring write aliases)
// we intentionally override that the alias is not a write alias as follower indices do not receive direct writes
// skip this alias, the leader and follower have the same modulo the write index
// we intentionally override that the alias is not a write alias as follower indices do not receive direct writes
// remove aliases that the leader no longer has
/*
/*
// noinspection StatementWithEmptyBody
// note that we do not need to mark as system context here as that is restored from the original renew
/*
// if something else happened, we will attempt to renew again after another renew interval has passed
// we have to execute under the system context so that if security is enabled the management is authorized
// If commitStats is null then AlreadyClosedException has been thrown: TransportIndicesStatsAction#shardOperation(...)
// AlreadyClosedException will be retried byShardFollowNodeTask.shouldRetry(...)
// If seqNoStats is null then AlreadyClosedException has been thrown at TransportIndicesStatsAction#shardOperation(...)
// AlreadyClosedException will be retried byShardFollowNodeTask.shouldRetry(...)
/*
/*
/*
/*
/*
// hide tasks that are orphaned (see ShardFollowTaskCleaner)
/*
/*
/*
/*
// auto patterns are always overwritten
// only already followed index uuids are updated
// Mark existing leader indices as already auto followed:
/*
// Validates whether the leader cluster has been configured properly:
// If restoreInfo is null then it is possible there was a master failure during the
// restore.
/*
// Validates whether the leader cluster has been configured properly:
/**
// Make a copy, remove settings that are allowed to be different and then compare if the settings are equal.
// Validates if the current follower mapping is mergable with the leader mapping.
// This also validates for example whether specific mapper plugins have been installed
/**
// Remove settings that are always going to be different between leader and follow index:
// soft deletes setting is checked manually
// Follower index may be upgraded, while the leader index hasn't been upgraded, so it is expected
// that these settings are different:
/*
// we have to execute under the system context so that if security is enabled the removal is authorized
// treat as success
// Remove index.xpack.ccr.following_index setting
// Remove ccr custom metadata
/*
/*
/*
/*
// public for testing purposes only
// The existing operations below the global checkpoint won't be replicated as they were processed
// in every replicas already. However, the existing operations above the global checkpoint will be
// replicated to replicas but with the existing primary term (not the current primary term) in order
// to guarantee the consistency between the primary and replicas, and between translog and Lucene index.
// public for testing purposes only
// return a fresh global checkpoint after the operations have been replicated for the shard follow task
/*
/*
/*
/*
/*
// This is currently safe to do because calling `onResponse` will serialize the bytes to the network layer data
// structure on the same thread. So the bytes will be copied before the reference is released.
/*
/*
/*
/*
/*
/*
/**
/*
/**
/**
/*
// See the comment in #indexingStrategyForOperation for the explanation why we can safely skip this operation.
// See the comment in #indexingStrategyForOperation for the explanation why we can safely skip this operation.
// even though we're not generating a sequence number, we mark it as seen
// extra safe in production code
// a noop implementation, because follow shard does not own the history but the leader shard does.
// Don't need to look up term for operations before the global checkpoint for they were processed on every copies already.
// excludes the non-root nested documents which don't have primary_term.
// we have merged away the looking up operation.
// the value of the global checkpoint is not verified when the following engine is closed,
// allowing it to be closed even in the case where all operations have not been fetched and
// processed from the leader and the operations history has gaps. This way the following
// engine can be closed and reopened in order to bootstrap the follower index again.
/*
/**
/*
// sequence number should be set when operation origin is primary
/*
/**
/*
/**
// We set a single dummy index name to avoid fetching all the index data
// Validates whether the leader cluster has been configured properly:
// Adding the leader index uuid for each shard as custom metadata:
// Copy mappings from leader IMD to follow IMD
// We assert that insync allocation ids are not empty in `PrimaryShardAllocator`
// Both the Snapshot name and UUID are set to _latest_
// TODO: Add timeouts to network calls / the restore process.
// schedule renewals to run during the restore
// we have to execute under the system context so that if security is enabled the renewal is authorized
// TODO: There should be some local timeout. And if the remote cluster returns an unknown session
//  response, we should be able to retry by creating a new session.
// Some tests depend on closing session before cancelling retention lease renewal
/*
/*
/**
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
// Default the watermarks to absurdly low to prevent the tests
// from failing on nodes without enough disk space
// wait short time for other active shards before actually deleting, default 30s not needed in tests
// empty list disables a port scan for other nodes
// Let cluster state api return quickly in order to speed up auto follow tests:
/**
// normalize primary term as the follower use its own term
// Ignore this exception and try getting List<DocIdSeqNoAndSource> from other IndexShard instance.
// causes assertBusy to retry
/**
// indexing threads can wait for up to ~1m before retrying when they first try to index into a shard which is not STARTED.
// no progress - try to refresh for the next time
// count now acts like search and barfs if all shards failed...
/*
// restore not completed yet, wait for next cluster state update
/*
// Let cluster state api return quickly in order to speed up auto follow tests:
/*
// Enabling auto following:
// Delete auto follow pattern and make sure that in the background the auto follower has stopped
// then the leader index created after that should never be auto followed:
// expectedVal2 + 1, because logs-does-not-count is also marked as auto followed.
// (This is because indices created before a pattern exists are not auto followed and are just marked as such.)
// Ensure that there are no auto follow errors:
// (added specifically to see that there are no leader indices auto followed multiple times)
// Enabling auto following:
// Need to set this, because following an index in the same cluster
// Enabling auto following:
// Soft deletes are disabled:
// Soft deletes are enabled:
// index created in the remote cluster before the auto follow pattern exists won't be auto followed
// create the auto follow pattern
// index created in the remote cluster are auto followed
// pause the auto follow pattern
// indices created in the remote cluster are not auto followed because the pattern is paused
// sometimes create another index in the remote cluster and close (or delete) it right away
// it should not be auto followed when the pattern is resumed
// pattern is paused, none of the newly created indices has been followed yet
// resume the auto follow pattern, indices created while the pattern was paused are picked up for auto-following
// create an auto follow pattern for each prefix
// pick up some random pattern to pause
// all patterns should be active
// start creating new indices on the remote cluster
// wait for 3 leader indices to be created on the remote cluster
// now pause some random patterns
// wait for more leader indices to be created on the remote cluster
// resume auto follow patterns
// wait for more leader indices to be created on the remote cluster
// check that all leader indices have been correctly auto followed
// Need to set this, because following an index in the same cluster
/*
/*
// wait for the shard follow task to exist
/**
// we set a low poll timeout so that shard changes requests are responded to quickly even without indexing
// we set a low poll timeout so that shard changes requests are responded to quickly even without indexing
// we set a low poll timeout so that shard changes requests are responded to quickly even without indexing
/*
// we set a low poll timeout so that shard changes requests are responded to quickly even without indexing
/*
// ignore, it could have been deleted by another thread
// we must check serially because aliases exist will return true if any but not necessarily all of the requested aliases exist
// we must check serially because aliases exist will return true if any but not necessarily all of the requested aliases exist
/*
// TODO: Assert that x-pack ccr feature is not enabled once feature functionality has been added
/*
/*
// Add a regular index, to check that we do not take that one into account:
/*
/*
// Need to add mock log appender before submitting CS update, otherwise we miss the expected log:
// (Auto followers for new remote clusters are bootstrapped when a new cluster state is published)
// Update the cluster state so that we have auto follow patterns and verify that we log a warning
// in case of incompatible license:
/*
// UUID is changed so that we can follow indexes on same cluster
// Depending on when the timeout occurs this can fail in two ways. If it times-out when fetching
// metadata this will throw an exception. If it times-out when restoring a shard, the shard will
// be marked as failed. Either one is a success for the purpose of this test.
// This test sets individual action timeouts low to attempt to replicated timeouts. Although the
// clear session action is not blocked, it is possible that it will still occasionally timeout.
// By wiping the leader index here, we ensure we do not trigger the index commit hanging around
// assertion because the commit is released when the index shard is closed.
/*
// ensure that a retention lease has been put in place on each shard
// block the recovery from completing; this ensures the background sync is still running
/*
/*
// sample the leases after recovery
// sleep a small multiple of the renew interval
// now ensure that the retention leases are the same
// we assert that retention leases are being renewed by an increase in the timestamp
// we will sometimes fake that some of the retention leases are already removed on the leader shard
// we will disrupt requests to remove retention leases for these random shards
// wait until the follower global checkpoints have caught up to the leader
// now assert that the retention leases have advanced to the global checkpoints
// we assert that retention leases are being advanced
/*
/*
// sample the leases after pausing
// sleep a small multiple of the renew interval
// now ensure that the retention leases are the same
// we assert that retention leases are not being renewed by an unchanged timestamp
// this forces the background renewal from following to face a retention lease not found exception
/**
// ensure that a retention lease has been put in place on each shard, and grab a copy of them
// now ensure that the retention leases are being renewed
// we assert that retention leases are being renewed by an increase in the timestamp
/**
/*
/*
/*
// sometimes update
// Block the ClusterService from exposing the cluster state with the mapping change. This makes the ClusterService
// have an older mapping version than the actual mapping version that IndexService will use to index "doc1".
// Make sure the mapping is ready on the shard before we execute the index request; otherwise the index request
// will perform a dynamic mapping update which however will be blocked because the latch is remained closed.
// Make sure at least one read-request which requires mapping sync is completed.
// no effect if latch was counted down - this makes sure teardown can make progress.
/*
// Pause follower1 index and check the follower info api:
/*
/*
/**
/*
// Sometimes we want to index a lot of documents to ensure that the recovery works with larger files
// Concurrently index new docs with mapping changes
// Check that the index exists, would throw index not found exception if the index is missing
// Waiting for some document being index before following the index:
// Leader index does not exist.
// Follower index does not exist.
// Both indices do not exist.
// Indexing directly into index2 would fail now, because index2 is a follow index.
// We can't test this here because an assertion trips before an actual error is thrown and then index call hangs.
// Turn follow index into a regular index by: pausing shard follow, close index, unfollow index and then open index:
// Indexing succeeds now, because index2 is no longer a follow index:
// Always unset allocation enable setting to avoid other assertions from failing too when this test fails:
// Sanity check that the setting has not been set in follower index:
// Check that the setting has been set in follower index:
// Sets an index setting on leader index that is excluded from being replicated to the follower index and
// expects that this setting is not replicated to the follower index, but does expect that the settings version
// is incremented.
// Sanity check that the setting has not been set in follower index:
// we have to remove the retention leases on the leader shards to ensure the follower falls behind
/**
/*
// Both auto follow patterns and index following should be resilient to remote connection being missing:
// This triggers a cluster state update, which should let auto follow coordinator retry auto following:
// This new index should be picked up by auto follow coordinator
// This new document should be replicated to follower index:
/*
/*
/*
// Remote connection needs to be re-configured, because all the nodes in leader cluster have been restarted:
/*
/*
// Ignore, to avoid invoking updateAutoFollowMetadata(...) twice
// compute and return the local cluster state, updated with some auto-follow patterns
// cluster state #1 : one pattern is active
// cluster state #2 : still one pattern is active
// cluster state #3 : add a new pattern, two patterns are active
// cluster state #4 : still both patterns are active
// cluster state #5 : first pattern is paused, second pattern is still active
// cluster state #5 : second pattern is paused, both patterns are inactive
// to be aligned with local cluster state updates
// in this test, every time it fetches the remote cluster state new leader indices to follow appears
// Ignore, to avoid invoking updateAutoFollowMetadata(...) twice
// iteration #1 : only pattern "patternLogs" is active in local cluster state
// iteration #2 : only pattern "patternLogs" is active in local cluster state
// iteration #3 : both patterns "patternLogs" and "patternDocs" are active in local cluster state
//
// iteration #4 : both patterns "patternLogs" and "patternDocs" are active in local cluster state
//
// iteration #5 : only pattern "patternDocs" is active in local cluster state, "patternLogs" is paused
// patternLogs-5 exists in remote cluster state but patternLogs was paused
// patternDocs-0 does not exist in remote cluster state
// Ignore, to avoid invoking updateAutoFollowMetadata(...)
// 1 shard started and another not started:
// Start second shard:
// index is opened
// index is closed
// Return a cluster state with no patterns so that the auto followers never really execute:
// Add 3 patterns:
// Get a reference to auto follower that will get removed, so that we can assert that it has been marked as removed,
// when pattern 1 and 3 are moved. (To avoid a edge case where multiple auto follow coordinators for the same remote cluster)
// Remove patterns 1 and 3:
// Add pattern 4:
// Get references to auto followers that will get removed, so that we can assert that those have been marked as removed,
// when pattern 2 and 4 are moved. (To avoid a edge case where multiple auto follow coordinators for the same remote cluster)
// Remove patterns 2 and 4:
// Add 3 patterns:
// Make pattern 1 and pattern 3 inactive
// Add active pattern 4 and make pattern 2 inactive
// Ignore, to avoid invoking updateAutoFollowMetadata(...) twice
// Ignore, to avoid invoking updateAutoFollowMetadata(...) twice
/*
// sufficiently large to exercise that we do not stack overflow
/*
// randomly close the index
// Ignore, to avoid invoking updateAutoFollowMetadata(...) twice
/*
/*
// x-content loses the exception
/*
/*
/*
/*
/*
/*
/*
// follower index parameter is not part of the request body and is provided in the url path.
// So this field cannot be used for creating a test instance for xcontent testing.
/*
// follower index parameter and wait for active shards params are not part of the request body and
// are provided in the url path. So these fields cannot be used for creating a test instance for xcontent testing.
/*
/*
// follower index parameter is not part of the request body and is provided in the url path.
// So this field cannot be used for creating a test instance for xcontent testing.
/*
// A number of times, get operations within a range that exists:
// get operations for a range for which no operations exist
// get operations for a range some operations do not exist:
// Unexpected history UUID:
// invalid range
/*
/*
/*
// this emulates what the CCR persistent task will do for pulling
/*
// the failures were able to be retried so fetch failures should have cleared
// Emulate network thread and avoid SO:
// Emulate network thread and avoid SO:
// if too many invocations occur with the same from then AOBE occurs, this ok and then something is wrong.
// Sometimes add a random retryable error
// Simulates a leader shard copy not having all the operations the shard follow task thinks it has by
// splitting up a response into multiple responses AND simulates maxBatchSizeInBytes limit being reached:
// Sometimes add a random retryable error
// Sometimes add an empty shard changes response to also simulate a leader shard lagging behind
// Report toSeqNo to simulate maxBatchSizeInBytes limit being met or last op to simulate a shard lagging behind:
// Instead of rarely(), which returns true very rarely especially not running in nightly mode or a multiplier have not been set
/*
// if you change this constructor, reflect the changes in the hand-written assertions below
// x-content loses the exception
/*
// treat this a peak request
// need to set outstandingWrites to 0, other the write buffer gets flushed immediately
// Also invokes the coordinatesReads() method:
// no more reads, because write buffer count limit has been reached
// need to set outstandingWrites to 0, other the write buffer gets flushed immediately
// Also invokes the coordinatesReads() method:
// no more reads, because write buffer size limit has been reached
// The call the updateMapping is a noop, so noting happens.
// Also invokes the coordinatesReads() method:
// no more reads, because task has been cancelled
// no more writes, because task has been cancelled
// Also invokes the coordinatesReads() method:
// no more reads, because task has been cancelled
// no more writes, because task has been cancelled
// before each retry, we assert the fetch failures; after the last retry, the fetch failure should clear
// NUmber of requests is equal to initial request + retried attempts
// the fetch failure has cleared
// Cancel just before attempting to fetch operations:
// number of requests is equal to initial request + retried attempts
// the fetch failure should have been cleared:
// otherwise we will keep looping as if we were repeatedly polling and timing out
// one request for each request that we simulate timedout, plus our request that receives a reply, and then a follow-up request
// since there will be only one failure, this should only be invoked once and there should not be a fetch failure
// Also invokes coordinatesWrites()
// Also invokes coordinatesWrites()
// change to 4 outstanding writers
// Also invokes coordinatesWrites()
// Also invokes coordinatesWrites()
// Also invokes coordinatesWrites()
// Number of requests is equal to initial request + retried attempts:
// Also invokes coordinatesWrites()
// Also invokes coordinatesWrites()
// Also invokes coordinatesWrites()
// handleWrite() also delegates to coordinateReads
// The call the updateMapping is a noop, so noting happens.
/*
// Deletes should be replicated to the follower
// force the global checkpoint on the leader to advance
// force the global checkpoint on the leader to advance
// Simulates some bulk requests are completed on the primary and replicated to some (but all) replicas of the follower
// but the primary of the follower crashed before these requests completed.
// A follow-task retries these requests while the primary-replica resync is happening on the follower.
// We need to recover the replica async to release the main thread for the following task to fill missing
// operations between the local checkpoint and max_seq_no which the recovering replica is waiting for.
// noop, as mapping updates are not tested
// no-op as settings updates are not tested here
// no-op as alias updates are not tested here
// hard code mapping version; this is ok, as mapping updates are not tested here
/*
/*
/*
/*
/*
/*
/*
// only add index1 and index2
/*
/*
/*
// should fail because the recorded leader index uuid is not equal to the leader actual index
// should fail because the recorded leader index history uuid is not equal to the leader actual index history uuid:
// should fail because leader index does not have soft deletes enabled
// should fail because the follower index does not have soft deletes enabled
// should fail because the number of primary shards between leader and follow index are not equal
// should fail, because leader index is closed
// should fail, because index.xpack.ccr.following_index setting has not been enabled in leader index
// should fail, because leader has a field with the same name mapped as keyword and follower as text
// should fail because of non whitelisted settings not the same between leader and follow index
// should fail because the following index does not have the following_index settings
// should succeed
// should succeed, index settings are identical
// should succeed despite whitelisted settings being different
// We should be conscious which dynamic settings are replicated from leader to follower index.
// This is the list of settings that should be replicated:
// These fields need to be replicated otherwise documents that can be indexed in the leader index cannot
// be indexed in the follower index:
/*
/*
// test that we use the primary term on the follower when applying operations from the leader
// we use this primary on the operations yet we expect the applied operations to have the primary term of the follower
// The second bulk includes some operations from the first bulk which were processed already;
// only a subset of these operations will be included the result but with the old primary term.
/*
// promote the replica to primary:
// only flush source
/*
/*
// Do not apply optimization for deletes or updates
// Apply optimization for documents that do not exist
// extends the fetch range so we may deliver some overlapping operations more than once.
// Primary should reject duplicates
// Replica should accept duplicates
/**
// make adding operations to translog slower
/*
// simulate that the retention lease already exists on the leader, and verify that we attempt to renew it
// simulate that the retention lease already exists on the leader, expires before we renew, and verify that we attempt to add it
/*
// Would throw exception if missing
// Would throw exception if missing
// Would throw exception if missing
// The tasks will not be rescheduled as the sessions are closed.
// Would throw exception if missing
// Would throw exception if missing
// Request a second file to ensure that original file is not leaked
// Exception will be thrown if file is not closed.
// Session starts as not idle. First task will mark it as idle
// Task is still scheduled
// Accessing session marks it as not-idle
// Check session exists
// Task is still scheduled
// Task is cancelled when the session times out
/*
// Manual test specific object fields and if not just fail:
/*
// these random values do not need to be internally consistent, they are only for testing formatting
// Manual test specific object fields and if not just fail:
/*
// this controls the blockage
// regardless of CCR being enabled
// this controls the blockage
// this is controls the blockage
// this is controls the blockage
// since it's the default, we want to ensure we test both with/without it
/*
/**
/*
// SALT must be at least 128bits for FIPS 140-2 compliance
// This can be changed to 256 once Java 9 is the minimum version
// http://www.oracle.com/technetwork/java/javase/terms/readme/jdk9-readme-3852447.html#jce
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// fill the rest with random bytes
/*
// Try parsing using complete date/time format
// Try parsing using complete date/time format
// Fall back to the date only format
/*
/*
/**
/*
/**
/**
/**
/**
/**
// license not yet expired
// license duration is longer than maximum duration, delay it to the first match time
// no delay in valid time bracket
// passed last match time
// invalid after license expiry
// license not yet expired, delay it to the first match time
// license has expired
// license expiry duration is shorter than minimum duration, delay it to the first match time
// no delay in valid time bracket
// passed last match time
/**
// initial trigger and in time bracket, schedule immediately
// in time bracket, add frequency
// not in time bracket
/**
/*
/*
/*
/*
/*
/*
/**
/*
/*
/*
/*
/*
/*
/**
/**
// in 1.x: the acceptable values for 'subscription_type': none | dev | silver | gold | platinum
// in 2.x: the acceptable values for 'type': trial | basic | silver | dev | gold | platinum
// in 5.x: the acceptable values for 'type': trial | basic | standard | dev | gold | platinum
// in 6.x: the acceptable values for 'type': trial | basic | standard | dev | gold | platinum
// in 7.x: the acceptable values for 'type': trial | basic | standard | dev | gold | platinum | enterprise
/**
// bwc for 1.x subscription_type field
// bwc for 1.x subscription_type field
// bwc for 1.x subscription_type field
// bwc for 1.x subscription_type field
/**
/**
/**
/**
/**
/**
// TODO Add an explicit enterprise operating mode
/**
// We will validate that only a basic license can have the BASIC_SELF_GENERATED_LICENSE_EXPIRATION_MILLIS
// in the validate() method.
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Version for future extensibility
// Ignore unknown elements - might be new version of license
// It was probably created by newer version - ignoring
// It was probably created by newer version - ignoring
// not a license spec
// In case the signature is truncated/malformed we might end up with fewer than 4 bytes in the byteBuffer
// or with a string that cannot be base64 decoded. In either case return a more friendly error instead of
// just throwing the BufferUnderflowException or the IllegalArgumentException
// we take the absolute version, because negative versions
// mean that the license was generated by the cluster (see TrialLicense)
// and positive version means that the license was signed
// signature version is the source of truth
/**
// EMPTY is safe here because we don't call namedObject
// take the latest issued unexpired license
// Ignore all other fields - might be created with new version
/**
/*
/**
// pkg private for tests
/**
/**
/**
/**
/**
/**
/**
// logged when grace period begins
/**
// TODO: ack messages should be generated on the master, since another node's cluster state may be behind...
// needs acknowledgement
// This check would be incorrect if "basic" licenses were allowed here
// because the defaults there mean that security can be "off", even if the setting is "on"
// BUT basic licenses are explicitly excluded earlier in this method, so we don't need to worry
// TODO we should really validate that all nodes have xpack installed and are consistently configured but this
// should happen on a different level and not in this code
// security is on but TLS is not configured we gonna fail the entire request and throw an exception
// current license is not auto-generated
// and has a later issue date
/**
/**
// triggers a cluster changed event eventually notifying the current licensee
// clear current license
/**
// notify all interested plugins
// auto-generate license if no licenses ever existed or if the current license is basic and
// needs extended or if the license signature needs to be updated. this will trigger a subsequent cluster changed event
// implies license has been explicitly deleted
// We subtract the grace period from the current time to avoid overflowing on an expiration
// date that is near Long.MAX_VALUE
/**
// license can be null if the trial license is yet to be auto-generated
// in this case, it is a no-op
// remove operationModeFileWatcher to gc the old license object
// pkg private for tests
// when we encounter a license with a future issue date
// which can happen with autogenerated license,
// we want to schedule a notification on the license issue date
// so the license is notificed once it is valid
// see https://github.com/elastic/x-plugins/issues/983
// license is expired, no need to check again
/*
/**
/**
// This field describes the version of x-pack for which this cluster has exercised a trial. If the field
// is null, then no trial has been exercised. We keep the version to leave open the possibility that we
// may eventually allow a cluster to exercise a trial every time they upgrade to a new major version.
// no license
// has a license
/*
/**
/**
/*
/**
/**
/**
// only upgrade signature when all nodes are ready to deserialize the new signature
/*
/**
/**
/*
// Until this is moved out to its own plugin (its currently in XPackPlugin.java, we need to make sure that any edits to this file
// are also carried out in XPackClientPlugin.java
// Until this is moved out to its own plugin (its currently in XPackPlugin.java, we need to make sure that any edits to this file
// are also carried out in XPackClientPlugin.java
// Metadata
// Until this is moved out to its own plugin (its currently in XPackPlugin.java, we need to make sure that any edits to this file
// are also carried out in XPackClientPlugin.java
// TODO convert this wildcard to a real setting
/*
/*
/**
/**
// this UTF-8 conversion is much pickier than java String
// set this after the fact to prevent that we are jumping back and forth first setting to defautl and then reading the
// actual op mode resetting it.
/*
/**
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
/*
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// recurse to the next cluster
// check the license on the first cluster, and then we recursively check licenses on the remaining clusters
// we stash any context here since this is an internal execution and should not leak any existing context information
/**
/**
/**
/**
/**
/*
/*
/*
/**
// In 7.x, there was an opt-in flag to show "enterprise" licenses. In 8.0 the flag is deprecated and can only be true
// TODO Remove this from 9.0
// Default to pretty printing, but allow ?pretty=false to disable
/*
/*
/*
/*
// TODO: remove POST endpoint?
/*
// Set -version in signature
// Version in signature is -version, so check for -(-version) < 4
// EMPTY is safe here because we don't call namedObject
/*
/*
//www.elastic.co/legal/trial_license/. To begin your free trial, call /start_trial again and specify " +
/*
/**
// do not generate a license if any license is present
// extend the basic license expiration date if needed since extendBasic will not be called now
/*
/*
/*
/*
/*
/*
/*
/*
/**
/** Messages for each feature which are printed when the license expires. */
/**
// ^^ though technically it was already disabled, it's not bad to remind them
// ^^ though technically it doesn't change the feature set, it's not bad to remind them
/** A wrapper for the license mode and state, to allow atomically swapping. */
/** The current "mode" of the license (ie license type). */
/** True if the license is active, or false if it is expired. */
/**
/** Add a listener to be notified on license change */
/** Remove a listener */
/** Return the current license type. */
/** Return true if the license is currently within its time boundaries, false otherwise. */
/**
/**
/**
/**
/**
/** Classes of realms that may be available based on the license type. */
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// any license (basic and upwards)
/**
/**
/**
/**
/**
/**
// Should work on all active licenses
/**
// status is volatile
// Should work on all active licenses
/**
// status is volatile
// Should work on all active licenses
/**
/**
/**
/**
/**
/**
// status is volatile
// Should work on all active licenses
/**
/**
/**
/**
/**
/*
/**
/*
/**
// NOTE: this does *not* call super, THIS IS A BUG
/*
/**
// TODO move this constant to License.java once we move License.java to the protocol jar
/**
/**
/**
// if the user requested the license info, and there is no license, we should send
// back an explicit null value (indicating there is no license). This is different
// than not adding the license info at all
// backcompat reading native code info, but no longer used here
// this is separated out so that the removed description can be read from the stream on construction
// TODO: remove this for 8.0
/*
/*
/**
/** Return a map from feature name to usage information for that feature. */
/*
/**
/**
/**
/*
/**
/**
/**
/**
/*
/*
/**
/**
/**
//When deserializing from XContent we need to wait for all vertices to be loaded before
// Connection objects can be created that reference them. This class provides the interim
// state for connections.
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
// read vertices
/**
// reverse-engineer if detailed stats were requested -
// mainly here for testing framework's equality tests
/*
/**
/**
/**
// otherwise inherit settings from parent
/*
/**
/*
/**
/**
/**
/**
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-significantterms-aggregation.html">
/**
//www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-significantterms-aggregation.html">
/**
/**
/*
/**
/**
/**
/**
/**
/**
/*
/*
/*
/*
/*
/**
/*
/**
/*
/*
/*
/*
/*
/**
/*
/**
// Indicates that the check is not applicable to this index type, the next check will be performed
// Indicates that the check finds this index to be up to date - no additional checks are required
// The index should be reindex
// The index should go through the upgrade procedure
/*
/**
/*
/**
/**
/**
/*
/*
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
/**
/*
// fully deleted segments don't need to be processed
// we have this additional delete by query functionality to filter out documents before we snapshot them
// we can't filter after the fact since we don't have an index anymore.
// we drop the sort on purpose since the field we sorted on doesn't exist in the target index anymore.
/*
/**
// we process the index metadata at snapshot time. This means if somebody tries to restore
// a _source only snapshot with a plain repository it will be just fine since we already set the
// required engine, that the index is read-only and the mapping to a default mapping
// for a minimal restore we basically disable indexing on all fields and only create an index
// that is valid from an operational perspective. ie. it will have all metadata fields like version/
// seqID etc. and an indexed ID field such that we can potentially perform updates on them or delete documents.
// we don't need to obey any routing here stuff is read-only anyway and get is disabled
// read-only!
// if there is no mapping this is null
// TODO should we have a snapshot tmp directory per shard that is maintained by the system?
// do nothing;
// SourceOnlySnapshot will take care of soft- and hard-deletes no special casing needed here
// we will use the lucene doc ID as the seq ID so we set the local checkpoint to maxDoc with a new index UUID
/**
/**
/*
/*
// Otherwise we would return a value that makes no sense.
/*
/**
/**
/**
/**
/**
/**
/**
// no security headers, we will have to use the xpack internal user for
// our execution by specifying the origin
/**
// No headers (e.g. security not installed/in use) so execute as origin
// Otherwise stash the context and copy in the saved headers before executing
/*
/**
// not running from a jar (unit tests, IDE)
/**
/*
// TODO: merge this into XPackPlugin
// the only licensing one
//TODO split these settings up
// we add the `xpack.version` setting to all internal indices
// deprecation
// graph
// ML
// security
// upgrade
// watcher
// license
// x-pack
// rollup
// ILM
// Freeze
// Data Frame
// graph
// logstash
// beats
// ML - Custom metadata
// ML - Persistent action requests
// ML - Task states
// ML - Data frame analytics
// ML - Inference preprocessing
// ML - Inference models
// ML - Inference aggregators
// ML - Inference Results
// ML - Inference Configuration
// monitoring
// security
// security : conditional privileges
// security : role-mappings
// sql
// watcher
// licensing
// rollup
// ccr
// ILM
// SLM
// ILM - Custom Metadata
// ILM - LifecycleTypes
// ILM - Lifecycle Actions
// Data Frame
// Vectors
// Voting Only Node
// Frozen indices
// Spatial
// data science
// ML - Custom metadata
// ML - Persistent action requests
// ML - Task states
// watcher
// licensing
//rollup
// Data Frame
/*
/*
// These should be moved back to XPackPlugin once its moved to common
/** Name constant for the security feature. */
/** Name constant for the monitoring feature. */
/** Name constant for the watcher feature. */
/** Name constant for the graph feature. */
/** Name constant for the machine learning feature. */
/** Name constant for the Logstash feature. */
/** Name constant for the Beats feature. */
/** Name constant for the Deprecation API feature. */
/** Name constant for the upgrade feature. */
// inside of YAML settings we still use xpack do not having handle issues with dashes
/** Name constant for the sql feature. */
/** Name constant for the rollup feature. */
/** Name constant for the index lifecycle feature. */
/** Name constant for the snapshot lifecycle management feature. */
/** Name constant for the CCR feature. */
/** Name constant for the transform feature. */
/** Name constant for flattened fields. */
/** Name constant for the vectors feature. */
/** Name constant for the voting-only-node feature. */
/** Name constant for the frozen index feature. */
/** Name constant for spatial features. */
/** Name constant for the data science plugin. */
/** Name constant for the enrich plugin. */
/** Name constant for indices. */
/*
// TODO: clean up this library to not ask for write access to all system properties!
// invoke this clinit in unbound with permissions to access all system properties
// TODO: fix gradle to add all security resources (plugin metadata) to test classpath
// of watcher plugin, which depends on it directly. This prevents these plugins
// from being initialized correctly by the test framework, and means we have to
// have this leniency.
// some other bug
//private final Environment env;
// These should not be directly accessed as they cannot be overridden in tests. Please use the getters so they can be overridden.
// FIXME: The settings might be changed after this (e.g. from "additionalSettings" method in other plugins)
// We should only depend on the settings from the Environment object passed to createComponents
// overridable by tests
/**
/**
/**
// check that all nodes would be capable of deserializing newly added x-pack metadata
// just create the reloader as it will pull all of the loaded ssl configurations and start watching them
// It is useful to override these as they are what guice is injecting into actions
// overridable for tests
// overridable for tests
/*
/**
/**
/**
/** Setting for enabling or disabling data frame. Defaults to true. */
/** Setting for enabling or disabling security. Defaults to true. */
/** Setting for enabling or disabling monitoring. */
/** Setting for enabling or disabling watcher. Defaults to true. */
/** Setting for enabling or disabling graph. Defaults to true. */
/** Setting for enabling or disabling machine learning. Defaults to true. */
/** Setting for enabling or disabling rollup. Defaults to true. */
/** Setting for enabling or disabling auditing. Defaults to false. */
/** Setting for enabling or disabling document/field level security. Defaults to true. */
/** Setting for enabling or disabling Logstash extensions. Defaults to true. */
/** Setting for enabling or disabling Beats extensions. Defaults to true. */
/**
/**
/** Setting for enabling or disabling TLS. Defaults to false. */
/** Setting for enabling or disabling http ssl. Defaults to false. */
/** Setting for enabling or disabling the reserved realm. Defaults to true */
/** Setting for enabling or disabling the token service. Defaults to the value of https being enabled */
/** Setting for enabling or disabling the api key service. Defaults to the value of https being enabled */
/** Setting for enabling or disabling FIPS mode. Defaults to false */
/** Setting for enabling or disabling sql. Defaults to true. */
/** Setting for enabling or disabling flattened fields. Defaults to true. */
/** Setting for enabling or disabling vectors. Defaults to true. */
/*
// TLSv1.3 cipher has PFS, AEAD, hardware support
// PFS, AEAD, hardware support
// PFS, AEAD, hardware support
// PFS, hardware support
// PFS, hardware support
// PFS, hardware support
// PFS, hardware support
// AEAD, hardware support
// hardware support
// hardware support
// TLSv1.3 cipher has PFS, AEAD, hardware support
// TLSv1.3 cipher has PFS, AEAD
// PFS, AEAD, hardware support
// PFS, AEAD, hardware support
// PFS, AEAD
// PFS, hardware support
// PFS, hardware support
// PFS, hardware support
// PFS, hardware support
// AEAD, hardware support
// hardware support
// hardware support
/*
// BCJSSE in FIPS mode doesn't support TLSv1.3 yet.
// http specific settings
// transport specific settings
/** Returns all settings created in {@link XPackSettings}. */
/*
// Allow child classes to provide their own defaults if necessary
/*
/*
/**
// If there are no resources, there might be no mapping for the id field.
// This makes sure we don't get an error if that happens.
// Do not include a resource with the same ID twice
// If the resourceId is not _all or *, we should see if it is a comma delimited string with wild-cards
// e.g. id1,id2*,id3
/**
/**
/**
/**
/**
/**
/*
/*
/**
/**
/*
/**
/**
/*
/**
/**
/*
// overrideable for tests
// local actions are executed directly, not on a separate thread, so no thread safe collection is necessary
/*
// overrideable for tests
// the value sent back doesn't matter since our predicate keeps iterating
/*
/*
/**
/*
/*
/**
/*
/*
/*
/**
/*
/*
/*
/*
/*
/**
/**
/**
// if allowNoJobForWildcards == true then any number
// of jobs with any id is ok. Therefore no matches
// are required
// require something, anything to match
// matches are not required for wildcards but
// specific job Ids are
// Matches are required for wildcards
/**
/*
/**
/*
/**
/*
/*
// Nothing to hash atm, so just use the action name
/*
/*
/**
// No XContentContext.API, because the headers should not be serialized as part of clusters state api
/*
// note: the casts to the following Writeable.Reader<T> instances are needed by some IDEs (e.g. Eclipse 4.8) as a compiler help
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// sort by index name, then shard ID
/*
/*
/**
/**
/**
/**
/**
/**
/**
/*
/*
/*
/*
// Note that Request should be the Value class here for this parser with a 'parameters' field that maps to
// PutAutoFollowPatternParameters class. But since two minor version are already released with duplicate
// follow parameters in several APIs, PutAutoFollowPatternParameters is now the Value class here.
// This class only exists for reuse of the FollowParameters class, see comment above the parser field.
/*
// Note that Request should be the Value class here for this parser with a 'parameters' field that maps to
// PutFollowParameters class. But since two minor version are already released with duplicate follow parameters
// in several APIs, PutFollowParameters is now the Value class here.
/**
// This class only exists for reuse of the FollowParameters class, see comment above the parser field.
/*
// Note that Request should be the Value class here for this parser with a 'parameters' field that maps to FollowParameters class
// But since two minor version are already released with duplicate follow parameters in several APIs, FollowParameters
// is now the Value class here.
/*
/*
/**
/**
/**
/**
// we need to store the context here as there is a chance that this method is called from a thread outside of the ThreadPool
// like a LDAP connection reader thread and we can pollute the context in certain cases
// we need to store the context here as there is a chance that this method is called from a thread outside of the ThreadPool
// like a LDAP connection reader thread and we can pollute the context in certain cases
/*
/**
/**
/*
/**
/*
/*
/**
/*
/**
/*
// Do nothing
/**
// seconds
// not a number
// Could not do the conversion
/**
/**
/**
/**
/*
/**
// messages
/*
// stop here as we can not return a single dest index
// convenience method to make testing easier
// special case: if indexNameExpressionResolver gets an empty list it treats it as _all
// note: this is equivalent to the default for search requests
/**
/**
// We traverse the validations in reverse order as we chain the listeners from back to front
/**
// non-trivia: if source contains a wildcard index, which does not resolve to a concrete index
// the resolved indices might be empty, but we can check if source contained something, this works because
// of no wildcard index is involved the resolve would have thrown an exception
// we can only check this node at the moment, clusters with mixed CCS enabled/disabled nodes are not supported,
// see gh#50033
// this can throw
// do not return immediately but collect all errors and than return
/*
/**
/**
/*
/**
/**
/**
// pkg-private for tests
/*
/**
/*
/*
/*
/**
/*
/*
/*
/*
/*
// use a treemap to guarantee ordering in the set, then transform it to the list of named policies
/*
/*
/*
/*
/*
/*
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
/*
/**
// Index must have been since deleted, ignore it
// All the allocation attributes are already set so just need to check
// if the allocation has happened
/*
/**
/*
/**
// Wrap the original listener to handle exceptions caused by ongoing snapshots
/**
/**
// The index has since been deleted, mission accomplished!
// Re-invoke the performAction method with the new state
// TODO: what is a good timeout value for no new state received during this time?
// There was a second error trying to set up an observer,
// fail the original listener
/**
// No snapshots are running, new state is acceptable to proceed
// There is a snapshot running with this index name
// There are snapshots, but none for this index, so it's okay to proceed with this state
// This means the cluster is being shut down, so nothing to do here
/*
/**
/*
/**
/**
// super.nextStepKey is set to null since it is not used by this step
// Index must have been since deleted, ignore it
/**
/**
/**
/*
/**
// Index must have been since deleted, ignore it
// How many shards the node should have
// The id of the node the shards should be on
/*
/*
/**
/*
/**
/*
/**
// Index must have been since deleted, ignore it
// get source index
// get target shrink index
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
// a[13] == "age"
// check to make sure that step details are either all null or all set.
/*
/*
/*
/**
/**
/*
/**
// Index must have been since deleted, ignore it
/*
/**
/**
/**
/*
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
// final step so that policy can properly update cluster-state with last action completed
// add steps for each phase, in reverse
// add `after` step for phase before next
// after step should have the name of the previous phase since the index is still in the
// previous phase until the after condition is reached
// add steps for each action, in reverse
// The very first after step is in a phase before the hot phase so call this "new"
// init step so that policy is guaranteed to have
/**
/*
/*
/**
/**
/**
/**
/*
/**
// Default to 1:30am every day
// Test that the setting is a valid cron syntax
/*
/**
/**
/**
/**
/**
/*
/*
/**
/**
/**
/**
/*
/*
/**
/**
/**
/**
/**
/**
/*
/**
/*
/**
/**
/*
/**
/*
/**
/*
/**
// Calling rollover with no conditions will always roll over the index
/*
/**
// Force merging is best effort, so always return true that the condition has been met.
/*
/**
//package private for testing
/*
/**
// These allocation deciders were chosen because these are the conditions that can prevent
// allocation long-term, and that we can inspect in advance. Most other allocation deciders
// will either only delay relocation (e.g. ThrottlingAllocationDecider), or don't work very
// well when reallocating potentially many shards at once (e.g. DiskThresholdDecider)
// For each shard
// Can we allocate at least one shard copy to this node?
// Shuffle the list of nodes so the one we pick is random
// No nodes currently match the allocation rules so just wait until there is one that does
// There are no shards for the index, the index might be gone
/*
/**
/*
/**
// get source index
// get target shrink index
// copy over other aliases from original index
// inherit all alias properties except `is_write_index`
/*
/**
// need to remove the single shard
// allocation so replicas can be allocated
/*
/**
// Index must have been since deleted, ignore it
/*
/**
// We only want to make progress if all shards of the shrunk index are
// active
/*
/*
/**
/**
/*
/*
/**
/*
/**
// Find the next phase after `index` that exists in `phases` and return it
// if we have exhausted VALID_PHASES and haven't found a matching
// phase in `phases` return null indicating there is no next phase
// available
// Find the previous phase before `index` that exists in `phases` and return it
// if we have exhausted VALID_PHASES and haven't found a matching
// phase in `phases` return null indicating there is no previous phase
// available
// Find the next action after `index` that exists in the phase and return it
// if we have exhausted `validActions` and haven't found a matching
// action in the Phase return null indicating there is no next
// action available
/*
/**
// There are no settings to change, so therefor this action should be safe:
/*
/*
/**
// The index won't have RolloverInfo if this is a Following index and indexing_complete was set by CCR,
// so just use the current time.
// find the newly created index from the rollover and fetch its index.creation_date
/*
/**
/*
// Follow stats api needs to return stats for follower index and all shard follow tasks should be synced:
/*
// Index must have been since deleted, ignore it
/*
/**
// Index was probably deleted
/*
/**
// The order of the following checks is important in ways which may not be obvious.
// First, figure out if 1) The configured alias points to this index, and if so,
// whether this index is the write alias for this index
// The writeIndex() call returns a tri-state boolean:
// true  -> this index is the write index for this alias
// false -> this index is not the write index for this alias
// null  -> this alias is a "classic-style" alias and does not have a write index configured, but only points to one index
//          and is thus the write index by default
// If this index is still the write index for this alias, skipping rollover and continuing with the policy almost certainly
// isn't what we want, as something likely still expects to be writing to this index.
// If the alias doesn't point to this index, that's okay as that will be the result if this index is using a
// "classic-style" alias and has already rolled over, and we want to continue with the policy.
// If indexing_complete is *not* set, and the alias does not point to this index, we can't roll over this index, so error out.
// Similarly, if isWriteIndex is false (see note above on false vs. null), we can't roll over this index, so error out.
// We currently have no information to provide for this AsyncWaitStep, so this is an empty object
/*
/**
/*
/***
/*
/*
/*
/*
/*
/*
/*
/*
// Needs to be declared but not used in constructing the response object
/*
/*
/*
/*
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
// fire off the search. Note this is async, the method will return from here
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// ready for another job
// must be started again
// abort and exit
// This shouldn't matter, since onAbort() will kill the task first
// No-op. Shouldn't really be possible to get here (should have to go through
// STOPPING
// first which will be handled) but is harmless to no-op and we don't want to
// throw exception here
// any other state is unanticipated at this point
// allowPartialSearchResults is set to false, so we should never see shard failures here
// execute finishing tasks
// an iteration result might return an empty set of documents to be indexed
// TODO we should check items in the response and move after accordingly to
// resume the failing buckets ?
// check if indexer has been asked to stop, state {@link IndexerState#STOPPING}
// no documents need to be indexed, continue with search
// TODO probably something more intelligent than every-50 is needed
// ensure that partial results are not accepted and cause a search failure
/**
// normal state;
// Anything other than indexing, aborting or stopping is unanticipated
/*
/**
/*
/**
// Indexer is running, but not actively indexing data (e.g. it's idle)
// Indexer is actively indexing data
// Transition state to where an indexer has acknowledged the stop
// but is still in process of halting
// Indexer is "paused" and ignoring scheduled triggers
// Something (internal or external) has requested the indexer abort and shutdown
/*
/**
/**
/**
/**
/**
/*
/*
/*
/*
// This parser follows the pattern that metadata is parsed leniently (to allow for enhancements)
/**
// Adjust the request, adding security headers from the current thread context
/*
/**
/*
/**
/**
/**
/**
// If we haven't opened a job than there will be no persistent task, which is the same as if the job was closed
// A closed job has no persistent task
// the job is re-locating
// previous executor node failed while the job was closing - it won't
// be reopened on another node, so consider it CLOSED for most purposes
// previous executor node failed and current executor node didn't
// have the chance to set job status to OPENING
// TODO: report (task != null && task.getState() == null) as STARTING in version 8, and fix side effects
// If we haven't started a datafeed then there will be no persistent task,
// which is the same as if the datafeed was't started
// previous executor node failed while the job was stopping - it won't
// be restarted on another node, so consider it STOPPED for reassignment purposes
// we are relocating at the moment
/**
/**
/**
/**
/**
/**
/*
// A big state can take a while to persist.  For symmetry with the _open endpoint any
// changes here should be reflected there too.
// openJobIds are excluded
// openJobIds are excluded
// openJobIds are excluded
/*
/*
/*
/*
/*
/*
/*
/*
/**
/**
/*
/*
/*
/*
/*
/*
// This one is plural in FileStructure, but singular in FileStructureOverrides
/*
/*
// Max allowed duration: 10 years
/*
/*
/*
/*
/*
/*
/*
/*
// Used internally to store the expanded IDs
/**
// TODO: Have callers wrap the content with an object as they choose rather than forcing it upon them
/*
// Put our own defaults for backwards compatibility
/*
/*
/*
// used internally to expand _all jobid to encapsulate all jobs in cluster:
// TODO: Have callers wrap the content with an object as they choose rather than forcing it upon them
/*
/*
/**
/*
/*
/*
// Ingest stats is a fragment
/*
/*
/**
/*
/*
/*
/*
// A big state can take a while to restore.  For symmetry with the _close endpoint any
// changes here should be reflected there too.
// The job field is streamed but not persisted
/*
// isBackground for fwc
// isBackground for fwc
/*
// Set the calendar Id in case it is null
/*
// content stream not included
// content stream not included
/*
/*
// If we have both URI and body filter ID, they must be identical
/*
/*
/**
// If we have both URI and body ID, they must be identical
/**
/*
// If we have both URI and body filter ID, they must be identical
/*
// If we have both URI and body jobBuilder ID, they must be identical
// Validate the jobBuilder immediately so that errors can be detected prior to transportation.
// Validate that detector configs are unique.
// This validation logically belongs to validateInputFields call but we perform it only for PUT action to avoid BWC issues which
// would occur when parsing an old job config that already had duplicate detectors.
// Some fields cannot be set at create time
/*
// If we have model_id in both URI and body, they must be identical
// Validations are done against the builder so we can build the full config object.
// This allows us to not worry about serializing a builder class between nodes.
/*
/*
/*
/*
/*
/*
/*
/**
/*
/*
// If we have both URI and body filter ID, they must be identical
/*
/** Indicates an update that was not triggered by a user */
// only serialize the update, as the job id is specified as part of the url
/*
/*
/*
/*
// When jobs are PUT their ID must be supplied in the URL - assume this will
// be valid unless an invalid job ID is specified in the JSON to be validated
// Validate that detector configs are unique.
// This validation logically belongs to validateInputFields call but we perform it only for PUT action to avoid BWC issues which
// would occur when parsing an old job config that already had duplicate detectors.
// Some fields cannot be set at create time
/*
/**
/*
// Exposed for testing, but always use the aliases in non-test code
/**
// Only create the index or aliases if some other ML index exists - saves clutter if ML is never used.
// Create the annotations index if it doesn't exist already.
// Whether we are using native process is a good way to detect whether we are in dev / test mode:
// Possible that the index was created while the request was executing,
// so we need to handle that possibility
// Create the alias
// Recreate the aliases if they've gone even though the index still exists.
// Nothing to do, but respond to the listener
/*
/**
// For QueryPage
/**
/*
/**
/*
/*
/**
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
/*
/**
// Used for QueryPage
/**
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
// Headers are not parsed by the strict (config) parser, so headers supplied in the _body_ of a REST request will be rejected.
// (For config, headers are explicitly transferred from the auth headers by code in the put/update datafeed actions.)
/**
/**
// each of these writables are version aware
// This reads a boolean from the stream, if true, it sends the stream to the `fromStream` method
/**
/**
// TODO Remove in v8.0.0
// We only need this NamedXContentRegistry object if getParsedQuery() == null and getParsingException() == null
// This situation only occurs in past versions that contained the lazy parsing support but not the providers (6.6.x)
// We will still need `NamedXContentRegistry` for getting deprecations, but that is a special situation
// Certain thrown exceptions wrap up the real Illegal argument making it hard to determine cause for the user
/**
/**
// TODO refactor in v8.0.0
// We only need this NamedXContentRegistry object if getParsedQuery() == null and getParsingException() == null
// This situation only occurs in past versions that contained the lazy parsing support but not the providers (6.6.x)
// We will still need `NamedXContentRegistry` for getting deprecations, but that is a special situation
// Certain thrown exceptions wrap up the real Illegal argument making it hard to determine cause for the user
/**
/**
/**
// Each of these writables are version aware
// never null
// This writes a boolean to the stream, if true, it sends the stream to the `writeTo` method
/**
/**
// For testing only
// eat exception as it should never happen
// For testing only
// eat exception as it should never happen
/*
/**
// NULL implies we calculate on use and thus is always valid
/*
/*
// Visible for testing
/*
/**
// This consumes the list of types if there was one.
/**
// Write the now removed types to prior versions.
// An empty list is expected
/**
/**
// Adjust the request, adding security headers from the current thread context
/**
/*
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
/*
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
// TODO retains `dateHistogramInterval()`/`interval()` access for bwc logic, needs updating
/*
/**
// Headers are not parsed by the strict (config) parser, so headers supplied in the _body_ of a REST request will be rejected.
// (For config, headers are explicitly transferred from the auth headers by code in the put data frame actions.)
// Creation time is set automatically during PUT, so create_time supplied in the _body_ of a REST request will be rejected.
// Version is set automatically during PUT, so version supplied in the _body_ of a REST request will be rejected.
/**
/**
/**
/**
// Explicit setting lower than minimum is an error
// Explicit setting higher than limit is an error
// Default is silently capped if higher than limit
/*
/*
/**
/**
// Certain thrown exceptions wrap up the real Illegal argument making it hard to determine cause for the user
// Visible for testing
// First we check in the excludes as they are applied last
// Now we can check the includes
// Empty includes means no further exclusions
// At this stage sourcePattern is a concrete field name and path is not equal to it.
// We should check if path is a nested field of pattern.
// Let us take "foo" as an example.
// Fields that are "foo.*" should also be matched.
/*
// Before 7.5.0 there was no STARTING state and jobs for which
// tasks existed but were unassigned were considered STOPPED
/**
/*
/*
/**
/*
/**
/**
// C++ process uses int64_t type, so it is safe for the dependent variable to use long numbers.
// This restriction is due to the fact that currently the C++ backend only supports binomial classification.
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
/**
/**
/**
/**
/**
/**
/*
/*
/**
/*
/**
/*
/**
/**
/**
/**
/**
/**
// Verify existence of required fields
// Apply user-provided query
// Fetch aggregations requested by individual metrics
/**
/**
/**
/*
/**
/**
/**
/**
/**
/*
/**
/**
/*
/**
/**
// Evaluations
// Soft classification metrics
// Classification metrics
// Regression metrics
// Evaluations
// Evaluation metrics
// Evaluation metrics results
/*
/**
// Store given {@code actualField} for the purpose of generating error message in {@code process}.
// This means there were more than {@code maxClassesCardinality} buckets.
// We cannot calculate per-class accuracy accurately, so we fail.
/**
// Number of actual classes taken into account
// Total number of documents taken into account
// Start with the assumption that all the docs were predicted correctly.
// Subtract errors (false negatives)
// Subtract errors (false positives)
// Subtract errors (false negatives) for classes other than explicitly listed in confusion matrix
/** List of per-class results. */
/** Fraction of documents for which predicted class equals the actual class. */
/** Name of the class. */
/** Fraction of documents that are either true positives or true negatives wrt {@code className}. */
/*
/**
/**
/**
/**
/*
/**
// This is step 1
// This is step 2
/** List of actual classes. */
/** Number of actual classes that were not included in the confusion matrix because there were too many of them. */
/** Name of the actual class. */
/** Number of documents belonging to the {code actualClass} class. */
/** List of predicted classes. */
/** Number of documents that were not predicted as any of the {@code predictedClasses}. */
/*
/**
// Store given {@code actualField} for the purpose of generating error message in {@code process}.
// This is step 1
// This is step 2
// This means there were more than {@code MAX_CLASSES_CARDINALITY} buckets.
// We cannot calculate average precision accurately, so we fail.
/** List of per-class results. */
/** Average of per-class precisions. */
/** Name of the class. */
/** Fraction of documents predicted as belonging to the {@code predictedClass} class predicted correctly. */
/*
/**
// Store given {@code actualField} for the purpose of generating error message in {@code process}.
// This means there were more than {@code MAX_CLASSES_CARDINALITY} buckets.
// We cannot calculate average recall accurately, so we fail.
/** List of per-class results. */
/** Average of per-class recalls. */
/** Name of the class. */
/** Fraction of documents actually belonging to the {@code actualClass} class predicted correctly. */
/*
/**
// create static hash code from name as there are currently no unique fields per class instance
/*
/**
/**
/**
/**
/*
/**
// extendedStats.getVariance() is the statistical sumOfSquares divided by count
// create static hash code from name as there are currently no unique fields per class instance
/*
/*
/**
/**
/**
// Calculates AUC based on the trapezoid rule
/*
/**
/**
/**
/**
/*
/*
/*
/*
/*
/*
/*
/*
/**
// $FALL-THROUGH$
/*
/**
// 1 gb maximum
// Don't need the xcontent registry as we are not deflating named objects.
//Public for testing (for now)
/*
// PreProcessing Lenient
// PreProcessing Strict
// Model Lenient
// Output Aggregator Lenient
// Model Strict
// Output Aggregator Strict
// PreProcessing
// Model
// Output Aggregator
// Inference Results
// Inference Configs
/*
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
// Basic is always true
// The model license does not matter, this is the highest licensed level
// catch the rest, if the license is active and is at least the required model license
// We don't store the definition in the same document as the configuration
/**
// We require a definition to be available here even though it will be stored in a different doc
/*
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
/* Noop does not matter as we will throw if more than one is defined */ },
/*
/*
/**
/*
//github.com/google/cld3
/**
//github.com/google/cld3/blob/06f695f1c8ee530104416aab5dcf2d6a1414a56a/src/embedding_network.cc
// Order matters
/**
// "esIndex" stands for "embedding space index".
// Multiplier for each embedding weight.
// Iterate across columns for this row
/**
/*
/**
/**
/**
/**
// defSize:0 indicates that there is not a defined size. Finding the shallowSize of Double gives the best estimate
/*
/**
/*
/**
/**
/**
// defSize:0 does not do much in this case as sizeOf(String) is a known quantity
/*
/**
/**
/*
/**
/*
/**
/**
/**
/**
/**
// defSize:0 indicates that there is not a defined size. Finding the shallowSize of Double gives the best estimate
/*
/**
/*
/**
/*
/*
/**
/**
//stackoverflow.com/a/35148974/1818849
// Ensure truncation by having byte buffer = maxBytes
// Ignore an incomplete character
/**
// 1. Start with ' ', only if the string already does not start with a space
// 2. Replace punctuation and whitespace with ' '
// NOTE: we capture unicode letters AND marks as Nepalese and other languages
// 2.1. Replace spacing modifier characters
// 3. Add space at end
// 4. Remove multiple spaces (2 or more) with a single space
// 5. Replace Turkish İ with I (TODO - check this out better...)
/*
/**
/*
//github.com/google/cld3
/**
/**
// 'm' and 'r' are mixing constants generated offline.
// They're not really 'magic', they just happen to work well.
// Initialize the hash to a 'random' value
// Mix 4 bytes at a time into the hash
// use unsigned shift
// Handle the last few bytes of the input array
// Do a few final mixes of the hash to ensure the last few
// bytes are well-incorporated.
// use unsigned shift
// use unsigned shift
/*
//github.com/google/cld3
/**
// First add terminators:
// Split the text based on spaces to get tokens, adds "^"
// to the beginning of each token, and adds "$" to the end of each token.
// e.g.
// " this text is written in english" goes to
// "^$ ^this$ ^text$ ^is$ ^written$ ^in$ ^english$ ^$"
// Find the char ngrams
// ^$ ^this$ ^text$ ^is$ ^written$ ^in$ ^english$ ^$"
// nGramSize = 2
// [{h$},{sh},{li},{gl},{in},{en},{^$},...]
// We need to use the special hashing so that we choose the appropriate weight+ quantile
// when building the feature vector.
/*
//github.com/google/cld3
/**
// counts[s] is the number of characters with script s.
// Use treemap so results are sorted in scriptid order
// Get anything that is a letter, or anything complex enough warranting a check (more than one UTF-8 byte).
// cp > Byte.MAX_VALUE works as the first 127 codepoints are the same as the ASCII encoding,
// which is the same as one UTF-8 byte.
/*
//github.com/google/cld3
/**
//github.com/google/cld3/blob/master/src/script_span/generated_ulscript.h
//Zyyy
//Latn
//Grek
//Cyrl
//Armn
//Hebr
//Arab
//Syrc
//Thaa
//Deva
//Beng
//Guru
//Gujr
//Orya
//Taml
//Telu
//Knda
//Mlym
//Sinh
//Thai
//Laoo
//Tibt
//Mymr
//Geor
//Hani
//Ethi
//Cher
//Cans
//Ogam
//Runr
//Khmr
//Mong
//
//
//Bopo
//
//Yiii
//Ital
//Goth
//Dsrt
//Zinh
//Tglg
//Hano
//Buhd
//Tagb
//Limb
//Tale
//Linb
//Ugar
//Shaw
//Osma
//Cprt
//Brai
//Bugi
//Copt
//Talu
//Glag
//Tfng
//Sylo
//Xpeo
//Khar
//Bali
//Xsux
//Phnx
//Phag
//Nkoo
//Sund
//Lepc
//Olck
//Vaii
//Saur
//Kali
//Rjng
//Lyci
//Cari
//Lydi
//Cham
//Lana
//Tavt
//Avst
//Egyp
//Samr
//Lisu
//Bamu
//Java
//Mtei
//Armi
//Sarb
//Prti
//Phli
//Orkh
//Kthi
//Batk
//Brah
//Mand
//Cakm
//Merc
//Mero
//Plrd
//Shrd
//Sora
//Takr
// (based on testing cld3 vs java codepoints)
// (based on testing cld3 va java codepoints)
// (based on testing cld3 va java codepoints)
// Fall-through for unknown(s)
/*
//github.com/google/cld3
/**
// Unicode scripts we care about.  To get compact and fast code, we detect only
// a few Unicode scripts that offer a strong indication about the language of
// the text (e.g., Hiragana -> Japanese).
// Special value to indicate internal errors in the script detection code.
// Special values for all Unicode scripts that we do not detect.  One special
// value for Unicode characters of 1, 2, 3, respectively 4 bytes (as we
// already have that information, we use it).  kScriptOtherUtf8OneByte means
// ~Latin and kScriptOtherUtf8FourBytes means ~Han.
// Used primarily for Korean.
// Used primarily for Japanese.
// Used primarily for Japanese.
// Using blocks for the HANGUL vs HANGUL_JANO distinctions
// If one exists. Needs investigated
// Not one of our special cases, need to determine the utf8 byte size
// Fits in a single UTF-8 byte
/*
//github.com/google/cld3
/**
// Get script feature value for the string
// Out of the codepoints captured by ULScript_Hani, separately count those
// in Hangul (Korean script) and those in a script other than Hangul.
/*
/*
/*
/*
/*
/*
/*
/*
/**
/*
/**
// If we don't have the labels we should return the top classification values anyways, they will just be numeric
/*
/*
/**
/*
/*
/*
/*
/**
/**
/**
/**
/*
// TODO should we have regression/classification sub-classes that accept the builder?
/*Noop as it could be an array or object, it just has to be a one*/},
// Indicates that the config is useless and the caller just wants the raw value
// Adjust the probabilities according to the thresholds
// Average operations for each model and the operations required for processing and aggregating with the outputAggregator
// This is essentially a serialization error but the underlying xcontent parsing does not allow us to inject this requirement
// So, we verify the models were parsed in an ordered fashion here instead.
/*
/*
/*
/**
/**
/**
/**
/*
/*
/*
/*
//github.com/google/cld3
// adding bias
// multiplying weights
// adding bias
// multiplying softmax weights
/*
//github.com/google/cld3
/**
/*
// TODO should we have regression/classification sub-classes that accept the builder?
// Indicates that the config is useless and the caller just wants the raw value
/**
// If we are classification, we should assume that the inference return value is whole.
// If we are classification, we should assume that the largest leaf value is whole.
// TODO, eventually have TreeNodes contain confidence levels
// Grabbing the features from the doc + the depth of the tree
// allocate space in the root node and set to a leaf
/**
// allocate space for the child nodes
/**
/*
// leaf validations
/*
/**
/**
// We have reached the maximum, signal stream completion.
/**
/*
/**
/*
/**
/**
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
// This one is nasty - the syntax for analyzers takes either names or objects at many levels, hence it's not
// possible to simply declare whether the field is a string or object and a completely custom parser is required
/**
/**
/**
/**
/**
/**
/**
// remove empty strings
/**
// remove empty strings
// remove empty strings
// This cannot be builder.field(CATEGORIZATION_ANALYZER.getPreferredName(), categorizationAnalyzerConfig, params);
// because that always writes categorizationAnalyzerConfig as an object, and in the case of a global analyzer it
// gets written as a single string.
// We always assign sequential IDs to the detectors that are correct for this analysis config
/**
// We want to outlaw nested fields where a less nested field clashes with one of the nested levels.
// For example, this is not allowed:
// - a
// - a.b
// Nor is this:
// - a.b
// - a.b.c
// But this is OK:
// - a.b
// - a.c
// The sorted set makes it relatively easy to detect the situations we want to avoid.
/*
/**
/**
/**
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
// Be lenient when parsing cluster state - assume unknown fields are from future versions
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
// do nothing
/**
/**
/*
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
/*
/**
/**
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
/**
/**
/**
/**
/**
/**
/**
// negative means "unknown", which should only happen for a 5.4 job
// no point writing this to cluster state, as the indexes will get reassigned on reload anyway
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// negative means unknown, and is expected for v5.4 jobs
// check functions have required fields
// field names cannot contain certain characters
// partition, by and over field names cannot be duplicates
// by/over field names cannot be "count", "over', "by" - this requirement dates back to the early
// days of the ML code and could be removed now BUT ONLY IF THE C++ CODE IS CHANGED
// FIRST - see https://github.com/elastic/x-pack-elasticsearch/issues/858
/**
/*
/*
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
/*
/**
/*
// Used for QueryPage
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
/**
/**
// TODO: Use java.time for the Dates here: x-pack-elasticsearch#829
// for removed last_data_time field
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// analysis fields
// time field
// remove empty strings
// the categorisation field isn't an input field
/**
// for removed last_data_time field
// Class already extends from AbstractDiffable, so copied from ToXContentToBytes#toString()
/**
// for removed last_data_time field
/**
// for removed last_data_time field
/**
// Results index name not specified in user input means use the default, so is acceptable in this validation
// Creation time is NOT required in user input, hence validated only on build
/**
// cannot have a group name the same as the job id
/**
// While testing for equality, ignore detectorIndex field as this field is auto-generated.
/**
/**
// If at the build stage there are missing values from analysis limits,
// it means we are reading a pre 6.3 job. Since 6.1, the model_memory_limit
// is always populated. So, if the value is missing, we fill with the pre 6.1
// default. We do not need to check against the max limit here so we pass null.
// Creation time is NOT required in user input, hence validated only on build
// User-defined names are prepended with "custom"
// Conditional guards against multiple prepending due to updates instead of first creation
/*
/**
/**
/*
/**
/*
// For internal updates
// For parsing REST requests
// These fields should not be set by a REST request
/**
/*
/**
// For QueryPage
/*
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
/*
/**
// EQ was considered but given the oddity of such a
// condition and the fact that it would be a numerically
// unstable condition, it was rejected.
/*
/**
/*
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
/*
/*
/**
/*
/**
/*
/**
/**
/**
/*
/**
/**
/**
// ".write" rather than simply "write" to avoid the danger of clashing
// with the read alias of a job whose name begins with "write-"
/**
/**
/**
/**
// If it was created between our last check, and this request being handled, we should add the alias
// Adding an alias that already exists is idempotent. So, no need to double check if the alias exists
// as well.
/*
/*
/**
//www.elastic.co/guide/en/elasticsearch/guide/current/nested-objects.html
/**
/**
/**
/**
/**
// RuleScope is a map
// TODO Should be a ByteSizeValue
// Custom settings are an untyped map
// TODO should be TimeValue
// TODO should be TimeValue
// TODO should be TimeValue
/**
// re-used: CREATE_TIME
/**
/**
// Add result all field for easy searches in kibana
// end properties
// end type
// end mapping
/**
// Model Plot Output
/**
// Forecast Output
// Forecast Stats Output
// re-used: TIMESTAMP, PROCESSING_TIME_MS, PROCESSED_RECORD_COUNT, LATEST_RECORD_TIME
/**
/* Array of influences */
/**
/**
// re-used: BUCKET_COUNT
/**
// re-used: BUCKET_COUNT
// re-used: EXPONENTIAL_AVG_CALCULATION_CONTEXT
/**
/**
/**
// end model size stats properties
// end model size stats mapping
/**
// The index has never been created yet
/*
/*
/**
/**
/*
/**
/**
/**
/**
/*
/**
/* intentionally empty */}, INPUT_RECORD_COUNT);
// TODO: Use java.time for the Dates here: x-pack-elasticsearch#829
// throw away inputRecordCount
/**
/**
// processedFieldCount could be a -ve value if no
// records have been written in which case it should be 0
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
// this.id excluded here as it is generated by the datastore
/**
/*
/**
/**
// Used for QueryPage
/**
/**
/**
// The state documents count suffices are 1-based
/**
// The state documents count suffices are 1-based
/**
/**
// Stored snapshot documents created prior to 6.3.0 will have no
// value for min_version.
/*
/*
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/*
/**
/** Calculates total bucket processing time as a product of the all-time average bucket processing time and the number of buckets. */
// Visible for testing
/**
// Calculate the cumulative moving average (see https://en.wikipedia.org/wiki/Moving_average#Cumulative_moving_average) of
// bucket processing times.
// Calculate the exponential moving average (see https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average) of
// bucket processing times.
/**
/*
/**
/**
/**
/*
/**
/**
/**
// Used for QueryPage
/**
/**
/**
// As a record contains fields named after the data fields, the parser for the record should always ignore unknown fields.
// However, it makes sense to offer strict/lenient parsing for other members, e.g. influences, anomaly causes, etc.
// LinkedHashSet preserves insertion order when iterating entries
/**
/**
/*
/**
/*
// Used for QueryPage
/**
/* *
// Can't use emptyList as might be appended to
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/*
/**
// Used for QueryPage
/*
/**
/**
/*
/**
/**
/**
/**
/*
/*
/**
/**
/*
/**
/*
// Used for QueryPage
// Influencers contain data fields, thus we always parse them leniently
/*
/**
/**
/**
/*
/**
// Used for QueryPage
/**
/**
/**
/*
/**
/**
//re-use: TIMESTAMP
/**
/**
/**
/**
/*
/**
/**
/*
/*
/*
/*
//TODO this should be MODEL_ID...
/*
/**
/**
/**
/**
/**
/**
/*
/**
/*
/**
/*
/*
/**
// note: not using Math.min/max as some internal prefetch optimization causes an NPE
/*
/**
/**
/*
/**
// Visible for testing
// Visible for testing
// Visible for testing
/**
/**
// When we cross the boundary between windows, we update the exponential average with metric values accumulated so far in
// incrementalMetricValueMs variable.
// This is the first time {@link #setLatestRecordTimestamp} is called on this object.
/*
/**
/**
/**
/*
/**
/**
/*
/**
// we choose the first one found, we go down longer keys first
// short cut before search
// exit early, we reached the full path and found something
// we found another map, continue exploring down this path
//exit early
// early exit, no need to create sb
// Pointer to where to start exploring
// Where in the requested path are we
/*
/**
/**
/**
/**
/**
/*
/**
/**
/*
/*
/**
/**
// allowNoMatch only applies to wildcard expressions,
// this isn't so don't check the allowNoMatch here
/**
/**
/**
/**
/*
/**
/*
/*
/**
/**
/**
/*
/**
// Serializing a map creates an object, need to skip the start object for the aggregation parser
/**
/**
/*
/**
/**
//github.com/elastic/x-pack-elasticsearch/issues/3810")
/*
/**
/**
/**
/*
// Return an "unknown" monitored system
// that can easily be filtered out if
// a node receives documents for a new
// system it does not know yet
/**
/*
/*
/**
/**
// default value (7 days)
// minimum value
/*
/*
// We allow strings to be "" because Logstash 5.2 - 5.3 would submit empty _id values for time-based documents
/*
/**
/**
/**
/**
// MonitoringBulkRequest accepts a body request that has the same format as the BulkRequest
// we no longer accept non-timestamped indexes from Kibana, LS, or Beats because we do not use the data
// and it was duplicated anyway; by simply dropping it, we allow BWC for older clients that still send it
// builds a new monitoring document based on the index request
/*
/*
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
// we explicitly ignore this where it's used to maintain binary BWC
/*
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
//excluding alerts since 6.x watches use it
/**
/**
/**
/**
// e.g., { "index_patterns": [ ".monitoring-data-6*" ], "version": 6000002 }
/**
/**
/**
// remove the type
// ensure the data lands in the correct index
/**
// For now: We prepend the API version to the string so that it's easy to parse in the future; if we ever add metadata
//  to pipelines, then it would better serve this use case
/**
/*
/*
// we piggyback verbosity on "human" output
/*
/*
/*
// Fields that are used both in core Rollup actions and Rollup plugin
// a set of ALL our supported metrics, to be a union of all other supported metric types (numeric, date, etc.)
// these mapper types are used by the configs (metric, histo, etc) to validate field mappings
// have to add manually since scaled_float is in a module
/**
/**
/**
/**
/**
/**
/**
/*
/*
/*
/*
// If we are retrieving all the jobs, the task description just needs to start
// with `rollup_`
// Otherwise find the task by ID
// XContentBuilder does not support passing the params object for Iterables
/*
/*
/**
/*
/*
/**
// TODO now that these rollup caps are being used more widely (e.g. search), perhaps we should
// store the RollupJob and translate into FieldCaps on demand for json output.  Would make working with
// it internally a lot easier
// Create RollupFieldCaps for the date histogram
// Create RollupFieldCaps for the histogram
// Create RollupFieldCaps for the term
// Create RollupFieldCaps for the metrics
/*
/*
/*
/*
/**
/**
// validate fixed time
/**
/**
/**
/**
// validate interval
// and delay
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/*
/**
/*
/**
// TODO: replace these with an enum
/**
/**
// nothing to do as all metrics are supported by SUPPORTED_NUMERIC_METRICS currently
/*
/**
/*
/**
/*
/**
/**
/*
/**
// This can be removed in 9.0
// Optional to accommodate old versions of state, not used in ctor
// 7.x nodes serialize `upgradedDocumentID` flag.  We don't need it anymore, but
// we need to pull it off the stream
// This can go away completely in 9.0
// 7.x nodes expect a boolean `upgradedDocumentID` flag. We don't have it anymore,
// but we need to tell them we are upgraded in case there is a mixed cluster
// This can go away completely in 9.0
/*
/**
/**
/*
/**
//fisheye.terracotta.org/browse/Quartz/trunk/quartz-core/src/main/java/org/quartz/CronExpression.java?r=2426">
//quartz-scheduler.org/">QUARTZ</a> PROJECT
// '*'
// '?'
/**
/**
/**
// Computation is based on Gregorian year only.
// move ahead one second, since we're computing the time *after* the
// given time
// CronTrigger does not deal with milliseconds
// loop until we've computed the next time, or we've past the endTime
// prevent endless loop...
// get second.................................................
// get minute.................................................
// get hour...................................................
// '+ 1' because calendar is 0-based for this field, and we are
// 1-based
// get day...................................................
// get day by day of month rule
// ensure test of mon != tmon further below fails
// make sure we don't over-run a short month, such as february
// '- 1' because calendar is 0-based for this field, and we
// are 1-based
// get day by day of week rule
// are we looking for the last XXX day of
// the month?
// desired
// d-o-w
// current d-o-w
// did we already miss the
// last one?
// no '- 1' here because we are promoting the month
// find date of last occurrence of this day in this month...
// '- 1' here because we are not promoting the month
// are we looking for the Nth XXX day in the month?
// desired
// d-o-w
// current d-o-w
// no '- 1' here because we are promoting the month
// '- 1' here because we are NOT promoting the month
// current d-o-w
// desired
// d-o-w
// will we pass the end of
// the month?
// no '- 1' here because we are promoting the month
// are we swithing days?
// '- 1' because calendar is 0-based for this field,
// and we are 1-based
// dayOfWSpec && !dayOfMSpec
//                throw new UnsupportedOperationException(
//                        "Support for specifying both a day-of-week AND a day-of-month parameter is not implemented.");
// '+ 1' because calendar is 0-based for this field, and we are
// 1-based
// test for expressions that never generate a valid fire date,
// but keep looping...
//                throw new ElasticsearchIllegalArgumentException("given time is not supported by cron [" + formatter.print(time) + "]");
// get month...................................................
// '- 1' because calendar is 0-based for this field, and we are
// 1-based
// '- 1' because calendar is 0-based for this field, and we are
// 1-based
// get year...................................................
//                throw new ElasticsearchIllegalArgumentException("given time is not supported by cron [" + formatter.print(time) + "]");
// '- 1' because calendar is 0-based for this field, and we are
// 1-based
// while( !done )
/**
/**
////////////////////////////////////////////////////////////////////////////
//
// Expression Parsing Functions
//
////////////////////////////////////////////////////////////////////////////
// throw an exception if L is used with other days of the month
// throw an exception if L is used with other days of the week
// Copying the logic from the UnsupportedOperationException below
// is an increment specified?
// intentionally empty
// intentionally empty
// put in a marker, but also fill values
// if the end of the range is before the start, then we need to overflow into
// the next day, month etc. This is done by adding the maximum amount for that
// type, and using modulus max to determine the value being added.
// ie: there's no max to overflow over
// take the modulus to get the real value
// 1-indexed ranges should not include 0, and should include their max
/**
/*
/*
/**
/**
// do not allow exceptions to escape this method; we should continue to notify listeners and schedule the next run
/*
// ignoring rejections if the scheduler has been shut down already
/*
/**
// we do scroll by default lets see if we can get rid of this at some point.
// This function is MADNESS! But it works, don't think about it too hard...
// simon edit: just watch this if you got this far https://www.youtube.com/watch?v=W-lF106Dgk8
// Finally, return the list of the entity
// Finally, return the list of the entity
// lets clean up things
// attempt to clear the scroll request
// since this is expected to happen at times, we just call the listener with an empty list
/*
/**
/**
/** Returns the current user information, or null if the current request has no authentication info. */
/** Returns the authentication information, or null if the current request has no authentication info. */
/**
/** Writes the authentication to the thread context */
/**
/**
/*
/**
/**
/** Global settings for the current node */
/** Provides access to key filesystem paths */
/** An internal client for retrieving information/data from this cluster */
/** The Elasticsearch thread pools */
/** Provides the ability to monitor files for changes */
/** Access to listen to changes in cluster state and settings  */
/** Provides support for mapping users' roles from groups and metadata */
/**
/**
/**
/**
/**
/*
// A trial (or basic) license can have SSL without security.
// This is because security defaults to disabled on that license, but that dynamic-default does not disable SSL.
/*
/*
// default to security4
// TODO migrate to securesetting!
/*
/**
/**
// TODO: this seems bogus, the only way to get an ioexception here is from a corrupt or tampered
// auth header, which should be be audited?
/*
/**
// As we do not yet support the nanosecond precision when we serialize to JSON,
// here creating the 'Instant' of milliseconds precision.
// This Instant can then be used for date comparison.
/*
/**
/*
/**
/**
/*
/**
/*
/**
// As we do not yet support the nanosecond precision when we serialize to JSON,
// here creating the 'Instant' of milliseconds precision.
// This Instant can then be used for date comparison.
/*
/**
/*
/**
/*
/**
// always store expiration in seconds because this is how we "serialize" to JSON and we need to parse back
/*
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
/*
/**
/*
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
// we parse error_count but ignore it while constructing response
/*
/**
/*
/**
/**
/**
/**
/**
/*
/**
/*
/*
/*
/*
/*
/*
/**
/**
/**
//openid.net/specs/openid-connect-core-1_0.html#ThirdPartyInitiatedLogin">3rd party initiated authentication</a>, the
/*
/**
/*
/**
/*
/*
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/**
/*
/**
/**
// EMPTY is ok here because we never call namedObject
/*
/**
/**
/*
/*
/**
/**
/**
/**
/**
/**
/*
/**
/**
/*
/*
/**
/*
/**
/**
/**
/*
/**
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/**
/*
/**
/**
// we pass false as last parameter because we want to reject the request if field permissions
// are given in 2.x syntax
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/**
/*
/**
/*
/**
/**
/**
/*
/**
/*
/**
/*
/**
/*
/**
/**
/*
/**
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/**
/**
/**
/**
/*
/**
// only show the scope if it is not null
/*
/**
/*
/**
/**
/**
/*
/**
/**
/**
/**
/**
/*
/**
/*
/*
/*
// we cannot apply our validation rules here as an authenticate request could be for an LDAP user that doesn't fit our restrictions
/*
/*
/*
/*
/**
/**
/*
/**
/**
/**
// EMPTY is ok here because we never call namedObject
/*
/*
/**
/*
/**
/*
/*
/**
/*
/**
/*
/**
/**
/**
/**
/*
/**
/**
/*
/**
/**
// The use of TreeSet is to provide a consistent order that can be relied upon in tests
// The use of TreeSet is to provide a consistent order that can be relied upon in tests
/*
/**
/*
/**
/*
/*
/**
/*
/**
/*
/**
/**
/**
/*
/**
/**
/**
/*
/**
/**
/*
/**
/*
/**
// we do not check for a password hash here since it is possible that the user exists and we don't want to update the password
/**
/*
/**
/**
// EMPTY is ok here because we never call namedObject
/*
/**
/*
/**
/*
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/*
/**
/*
/**
/**
/*
// TODO(hub-cap) Clean this up after moving User over - This class can re-inherit its field AUTHENTICATION_KEY in AuthenticationField.
// That interface can be removed
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
/**
/**
/**
/**
/**
/**
/*
/*
/**
/*
/**
/**
/**
/**
/**
// If it is already present then it will replace the existing header.
/*
/**
/*
/*
/**
/**
/**
/**
/**
// If same order, compare based on the realm name
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/*
/**
// never render this to the user
// never sent this to a client
/*
/*
/**
/*
/**
/*
/**
/**
// Cache
// 100k users
/**
/*
/*
/**
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
/*
//100k users
/**
/*
// these settings will be used under the prefix xpack.security.authc.realms.REALM_NAME.
/**
/**
// Copyright (c) 2006 Damien Miller <djm@mindrot.org>
//
// Permission to use, copy, modify, and distribute this software for any
// purpose with or without fee is hereby granted, provided that the above
// copyright notice and this permission notice appear in all copies.
//
// THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
// WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
// MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
// ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
// WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
// ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
// OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
/**
// BCrypt parameters
// Blowfish parameters
// Initial contents of key schedule
// bcrypt IV: "OrpheanBeholderScryDoubt". The C implementation calls
// this "ciphertext", but it is really plaintext or an IV. We keep
// the name to make code comparison easier.
// Table for Base64 encoding
// Table for Base64 decoding
// Expanded Blowfish key
/**
/**
/**
/**
// Feistel substitution on left word
// Feistel substitution on right word
/**
/**
/**
/**
//www.openbsd.org/papers/bcrypt-paper.ps
/**
/**
// Extract number of rounds
/*************************** ES CHANGE START *************************/
/* original code before introducing SecureString
// the next lines are the SecureString replacement for the above commented-out section
/*************************** ES CHANGE END *************************/
/**
/**
/**
/**
/*************************** ES CHANGE START *************************/
// this method previously took a string and did its own constant time comparison
/*************************** ES CHANGE END *************************/
/*
/**
/**
/**
/**
/*
//100k users
/**
/*
/**
/*
/*
// Base64 string length : (4*(n/3)) rounded up to the next multiple of 4 because of padding, 12 for 8 bytes
/*
/**
/**
// This is either a non hashed password from cache or a corrupted hash string.
/**
// Base64 string length : (4*(n/3)) rounded up to the next multiple of 4 because of padding.
// n is 32 (PBKDF2_KEY_LENGTH in bytes) and 2 is because of the dollar sign delimiters.
// Base64 string length : (4*(n/3)) rounded up to the next multiple of 4 because of padding.
// n is 32 (PBKDF2_KEY_LENGTH in bytes), so tokenLength is 44
/**
/**
/**
/*
/**
/*
// authorization scheme check is case-insensitive
//TODO we still have passwords in Strings in headers. Maybe we can look into using a CharSequence?
// the header does not start with 'Basic ' so we cannot use it, but it may be valid for another realm
// if there is nothing after the prefix, the header is bad
/**
/*
/**
/**
/**
/**
/**
// null dn fields get the default NULL_PREDICATE
/**
/**
/**
/**
/**
/**
// If the pattern is "*,dc=example,dc=com" then the rule is actually trying to express a DN sub-tree match.
// We can use dn.isDescendantOf for that
// if the suffix has a wildcard, then it's not a pure sub-tree match
/*
/*
/**
/**
// skip the doc_type and type fields in case we're parsing directly from the index
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
// find the start of the DSL object
// parseArray requires that the parser is positioned at the START_ARRAY token
/*
/**
/*
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
//placeholder used in the security plugin to indicate that the request is authorized knowing that it will yield an empty response
/*
/**
/**
/**
/**
/**
/**
/** add a local index name */
/** adds the array of local index names */
/** adds the list of local index names */
/** adds the list of remote index names */
/** @return <code>true</code> if both the local and remote index lists are empty. */
/** @return a immutable ResolvedIndices instance with the local and remote index lists */
/*
/**
/**
/**
/**
// EMPTY is safe here because we never use namedObject
// validate name
// advance to the START_OBJECT token if needed
// consume object but just drop
// don't need it
// re-wrap in order to add the role name
// advance to the START_OBJECT token
// it is transient metadata, skip it
/**
// by default certain restricted indices are exempted when granting privileges, as they should generally be hidden for ordinary
// users. Setting this flag eliminates this special status, and any index name pattern in the permission will cover restricted
// indices as well.
// we treat just '*' as no FLS since that's what the UI defaults to
/*
/**
/**
/**
/**
/**
// Because this Set has been removed from the map, and the only update to the set is performed in a
// Map#compute call, it should not be possible to get a concurrent modification here.
/**
// If the bitsetKey isn't in the lookup map, then there's nothing to synchronize
// We push this to a background thread, so that it reduces the risk of blocking searches, but also so that the lock management is
// simpler - this callback is likely to take place on a thread that is actively adding something to the cache, and is therefore
// holding the read ("update") side of the lock. It is not possible to upgrade a read lock to a write ("eviction") lock, but we
// need to acquire that lock here.
// it's possible for the key to be back in the cache if it was immediately repopulated after it was evicted, so check
// key is no longer in the cache, make sure it is no longer in the lookup map either.
// Due to the order here, it is possible than a new entry could be added _after_ the keysByIndex map is cleared
// but _before_ the cache is cleared. This should get fixed up in the "onCacheEviction" callback, but if anything slips through
// and sits orphaned in keysByIndex, it will not be a significant issue.
// When the index is closed, the key will be removed from the map, and there will not be a corresponding item
// in the cache, which will make the cache-invalidate a no-op.
// Since the entry is not in the cache, if #getBitSet is called, it will be loaded, and the new key will be added
// to the index without issue.
/**
// This ensures all insertions into the set are guarded by ConcurrentHashMap's atomicity guarantees.
// A cache loader is not allowed to return null, return a marker object instead.
/**
/*
/**
/**
/**
// slow
// very slow, but necessary in order to be correct
// we don't use the cost
/**
// this one takes deletes into account
// Not configurable, this limit only exists so that if a role query is updated
// then we won't risk OOME because of old role queries that are not used anymore
// just count
// we don't use a volatile here because the bitset is resolved before numDocs in the synchronized block
// so any thread that see numDocs != -1 should also see the true value of the roleQueryBits (happens-before).
/**
// If we would return a <code>null</code> liveDocs then that would mean that no docs are marked as deleted,
// but that isn't the case. No docs match with the role query and therefore all docs are marked as deleted
// apply deletes when needed:
// we always return liveDocs and hide docs:
// Not delegated since we change the live docs
/*
/**
// based on lucene/test-framework's FieldFilterLeafReader.
/**
// wraps subreaders with fieldsubsetreaders.
/** Return the automaton that is used to filter fields. */
/** List of filtered fields */
/** An automaton that only accepts authorized fields. */
/** {@link Terms} cache with filtered stats for the {@link FieldNamesFieldMapper} field. */
/**
/** returns true if this field is allowed. */
// we need to check for emptyness, so we can return null:
/** Filter a map by a {@link CharacterRunAutomaton} that defines the fields to retain. */
/** Filter a list by a {@link CharacterRunAutomaton} that defines the fields to retain. */
/** Step through all characters of the provided string, and return the
// for _source, parse, filter out the fields we care about, and serialize back downstream
// we share core cache keys (for e.g. fielddata)
/**
// this information is not cheap, return -1 like MultiFields does:
// for the _field_names field, fields for the document
// are encoded as postings, where term is the field.
// so we hide terms for fields we filter out.
/**
// re-compute the stats for the field to take
// into account the filtered terms.
// it is costly to recompute this value so we assume that docCount == maxDoc.
/**
/** Return true if term is accepted (matches a field name in this reader). */
// we don't support ordinals, but _field_names is not used in this way
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
// No permissions have been defined for an index, so don't intercept the index reader for access control
/*
/**
/**
/**
/**
/**
/*" and "object/1") and may
/*" ]</li>
/*", "user/kimchy", "config/*"</code>.
/*
/**
/**
/**
/**
/**
/**
/**
// Automaton based permission check
// action, request based permission check
/*
/**
/**
/**
// If access is allowed on root doc then also access is allowed on all nested docs of that root document:
// at least one of the queries should match
/**
//github.com/elastic/x-plugins/issues/3145
/**
/**
/**
/*
/**
// assume a load factor of 50%
// for each entry, we need two object refs, one for the entry itself
// and one for the free space that is due to the fact hash tables can
// not be fully loaded
// an automaton that represents a union of one more sets of permitted and denied fields
/** Constructor that does not enable field-level security: all fields are accepted. */
/** Constructor that enables field-level security based on include/exclude rules. Exclude rules
/** Constructor that enables field-level security based on include/exclude rules. Exclude rules
// we only accept deterministic automata so that the CharacterRunAutomaton constructor
// directly wraps the provided automaton
// we cache the result of isTotal since this might be a costly operation
/**
// wild guess, better than 0
/**
// an automaton that includes metadata fields, including join fields created by the _parent field such
// as _parent#type
/**
/**
/** Return whether field-level security is enabled, ie. whether any field might be filtered out. */
/** Return a wrapped reader that only exposes allowed fields. */
/*
/**
/**
/**
/**
/*
/**
/*
/**
/**
/**
/**
// the index pattern produced the empty automaton, presumably because the requested pattern expands exclusively inside the
// restricted indices namespace - a namespace of indices that are normally hidden when granting/checking privileges - and
// the pattern was not marked as `allowRestrictedIndices`. We try to anticipate this by considering _explicit_ restricted
// indices even if `allowRestrictedIndices` is false.
// TODO The `false` result is a _safe_ default but this is actually an error. Make it an error.
/**
// now... every index that is associated with the request, must be granted
// by at least one indices permission group
// if more than one permission matches for a concrete index here and if
// a single permission doesn't have a role query then DLS will not be
// applied even when other permissions do have a role query
// by default certain restricted indices are exempted when granting privileges, as they should generally be hidden for ordinary
// users. Setting this flag true eliminates the special status for the purpose of this permission - restricted indices still have
// to be covered by the "indices"
/*
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
/*
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
// At least one role / indices permission set need to match with all the requested indices/aliases:
/*
/**
/**
/*
/**
/**
/**
/*
/**
/*", "admin/team/*"}).
/**
// Package level for testing
/**
/**
/**
// this is allowed and short-circuiting here makes the later validation simpler
/**
/**
/**
/*
/**
/**
/*
/**
/**
/*
/**
// shared automatons
/*");
/*");
/*");
/*");
/*");
/*");
/*", "cluster:monitor/data_frame/*",
/*", "cluster:monitor/xpack/watcher/*");
/*");
/*", "cluster:monitor/xpack/rollup/*");
/*", ClusterStateAction.NAME, HasPrivilegesAction.NAME);
/*");
/*", StartILMAction.NAME, StopILMAction.NAME, GetStatusAction.NAME);
/*");
/*"));
/**
/*
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/*"), requestPredicate);
/*
/*
/*");
/*", PutMappingAction.NAME);
/*")));
/*");
/*
/**
/*"));
/*
// API key cannot own any other API key so deny access
// API key id from authentication must match the id from request
/*
/**
/*
/*
// added for monitoring
/*", // added for monitoring
// for the bootstrap service
/*", // for the bootstrap service
// needed for recovery and shrink api
// needed for the TemplateUpgradeService
// needed for the TemplateUpgradeService
// needed for global checkpoint syncs
// needed for retention lease syncs
// needed for background retention lease syncs
// needed for CCR to add retention leases
// needed for CCR to remove retention leases
// needed for CCR to renew retention leases
// needed for DiskThresholdMonitor.markIndicesReadOnly
// Only allow a proxy action if the underlying action is allowed
/*
// reporting_user doesn't have any privileges in Elasticsearch, and Kibana authorizes privileges based on this role
// .apm-* is for APM's agent configuration index creation
// DEPRECATED: to be removed in 9.0.0
// DEPRECATED: to be removed in 9.0.0
/*
/**
/**
/**
/**
/**
/**
/*
/**
/**
// skip template query, this requires runtime information like 'User' information.
/**
/**
// pkg protected for testing
// actually only if percolate query is referring to an existing document then this is problematic,
// a normal percolate query does work. However we can't check that here as this query builder is inside
// another module. So we don't allow the entire percolate query. I don't think users would ever use
// a percolate query as role query, so this restriction shouldn't prohibit anyone from using dls.
/*
/**
/**
// EMPTY is safe here because we never use namedObject
/*
/*
// public for tests
// public for tests
/*
/**
/**
/**
/*
// these values are not final since we allow them to be set at runtime
// String equality with support for wildcards
// Char equality with support for wildcards
// Escape character
/**
/**
/**
/**
// it's a lucene regexp
/**
// explicit fallthrough at end of switch
// add the next codepoint instead, if it exists
// else fallthru, lenient parsing with a trailing \
// accessor for testing
/*
/**
// the lock is used in an odd manner; when iterating over the cache we cannot have modifiers other than deletes using the
// iterator but when not iterating we can modify the cache without external locking. When making normal modifications to the cache
// the read lock is obtained so that we can allow concurrent modifications; however when we need to iterate over the keys or values
// of the cache the write lock must obtained to prevent any modifications.
// the cache cannot be modified while doing this operation per the terms of the cache iterator
/*
/*
/*
/**
// Add the user details to the params
// Always enforce mustache script lang:
/*
/**
/*
// We only check against the space character here (U+0020) since it's the only whitespace character in the
// set that we allow.
//
// Note for the future if we allow the full unicode range: the String and Character methods that deal with
// whitespace don't work for the whole range. They match characters that are considered whitespace to the Java
// language, which doesn't include characters like IDEOGRAPHIC SPACE (U+3000). The best approach here may be
// to match against java.util.regex.Pattern's \p{Space} class (which is by default broader than \s) or make a
// list from the codepoints listed in this page https://en.wikipedia.org/wiki/Whitespace_character
/**
/*
// don't attempt to parse ssl settings from the profile;
// profiles need to be killed with fire
/*
// just close and ignore - we are already stopped and just need to make sure we release all resources
/*
/*
/**
// we create the socket based on the name given. don't reverse DNS
/*
/**
// this is possibly the same check but we should not let anything use the default name either
/*
/**
/*
/**
/*
/**
// used for testing in a different package
/*
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/** Return true if this user was not the originally authenticated user, false otherwise. */
// Probably incorrect - comparing Object[] arrays with Arrays.equals
// no backcompat necessary, since there is no inner user
// last user written, regardless of bwc, does not have an inner user
/** Write just the given {@link User}, but not the inner {@link #authenticatedUser}. */
// not a system user
/*
/*
/**
/*
/**
/*
/**
/*
/**
/*
/*
/**
/*
/**
/*
/**
// ID validation
// Snapshot name validation
// We generate a snapshot name here to make sure it validates after applying date math
// Schedule validation
// Repository validation, validation of whether the repository actually exists happens
// elsewhere as it requires cluster state
/**
// TODO: we are breaking the rules of UUIDs by lowercasing this here, find an alternative (snapshot names must be lowercase)
/**
/**
/*
/**
// For testing
/*
/**
// Note: this is on purpose. While usually we would use Strings.toString(this) to render
// this using toXContent, it may contain sensitive information in the headers and thus
// should not emit them in case it accidentally gets logged.
/*
/**
// First, if there's no expire_after and a more recent successful snapshot, we can delete all the failed ones
// There's no expire_after and there's a more recent successful snapshot, delete this failed one
// Next, enforce the maximum count, if the size is over the maximum number of
// snapshots, then allow the oldest N (where N is the number over the maximum snapshot
// count) snapshots to be eligible for deletion
// Next check the minimum count, since that is a blanket requirement regardless of time,
// if we haven't hit the minimum then we need to keep the snapshot regardless of
// expiration time
// Finally, check the expiration time of the snapshot, if it is past, then it is
// eligible for deletion
// Only the oldest N snapshots are actually eligible, since if we went below this we
// would fall below the configured minimum number of snapshots to keep
// This snapshot is *not* one of the N oldest snapshots, so even if it were
// old enough, the other snapshots would be deleted before it
// If nothing matched, the snapshot is not eligible for deletion
/*
/*
/**
/*
/*
/*
/*
/**
/*
/**
/*
/*
/*
/*
/**
/*
/**
/*
/**
/**
/**
// No alias or index exists with the expected names, so create the index with appropriate alias
// The index didn't exist before we made the call, there was probably a race - just ignore this
// alias does not exist but initial index does, something is broken
// The alias exists and has a write index, so we're good
// The alias does not have a write index, so we can't index into it
// This is not an alias, error out
// (slmHistory.isAlias() == true) but (slmHistory instanceof Alias == false)?
/*
/**
// history (please add a comment why you increased the version here)
// version 1: initial
// TODO use separate SLM origin?
/*
/*
/*
/**
/**
/*
/**
/**
/**
/**
// password must be non-null for keystore...
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/**
//github.com/groovenauts/jmeter_oauth_plugin/blob/master/jmeter/src/
// Constructed Flag
// Tag and data types
// getLength() can return any 32 bit integer, so ensure that a corrupted encoding won't
// force us into allocating a very large array
/**
// A single byte short length
// We can't handle length longer than 4 bytes
//$NON-NLS-1$
/**
/**
/**
//$NON-NLS-1$
/**
//$NON-NLS-1$
// octet string is basically a byte array
//$NON-NLS-1$
//$NON-NLS-1$
//$NON-NLS-1$
//$NON-NLS-1$
//$NON-NLS-1$
/*
/**
/**
/**
/*
/**
/**
/*
/**
/**
/*
/**
/**
// Verify that the key starts with the correct header before passing it to parseOpenSslEC
/**
// Verify that the key starts with the correct header before passing it to parseOpenSslDsa
/**
/**
// Parse PEM headers according to https://www.ietf.org/rfc/rfc1421.txt
/**
// Unencrypted
// Parse PEM headers according to https://www.ietf.org/rfc/rfc1421.txt
/**
// Unencrypted
// Parse PEM headers according to https://www.ietf.org/rfc/rfc1421.txt
/**
/**
//We only handle PEM encryption
//malformed pem
/**
/**
//www.openssl.org/docs/man1.1.0/crypto/PEM_write_bio_PrivateKey_traditional.html
// AES IV (salt) is longer but we only need 8 bytes
// MD5 digests are 16 bytes
// use previous round digest as IV
/**
/**
// version
/**
// (version) We don't need it but must read to get to modulus
/**
// (version) We don't need it but must read to get to p
// we don't need x
/**
// version
/*
/**
/*
/**
/**
// Get the DER object with explicit 0 tag
// The JRE's handling of OtherNames is buggy.
// The internal sun classes go to a lot of trouble to parse the GeneralNames into real object
// And then java.security.cert.X509Certificate just turns them back into bytes
// But in doing so, it ends up wrapping the "other name" bytes with a second tag
// Specifically: sun.security.x509.OtherName(DerValue) never decodes the tagged "nameValue"
// But: sun.security.x509.OtherName.encode() wraps the nameValue in a DER Tag.
// So, there's a good chance that our tagged nameValue contains... a tagged name value.
/*
/**
// nothing to do here
/**
/**
/*
/**
// These settings are never registered, but they exist so that we can parse the values defined under grouped settings. Also, some are
// implemented as optional settings, which provides a declarative manner for fallback as we typically fallback to values from a
// different configuration
/**
/**
/**
/**
/**
/**
/**
/**
// We only handle the default store password if it's a PKCS#11 token
/**
/*
/**
/**
/**
/**
/**
/*
/**
// public for PKI realm
/**
/**
/**
/**
/*
/**
/**
//docs.oracle.com/en/java/javase/11/docs/specs/security/standard-names.html#sslcontext-algorithms">
/**
/**
/**
/**
// we don't need to load anything...
/**
// normally we'd throw here but let's create a new one that is not cached and will not be monitored for changes!
/**
/**
/**
/**
/**
// By default, an SSLEngine will not perform hostname verification. In order to perform hostname verification
// we need to specify a EndpointIdentificationAlgorithm. We use the HTTPS algorithm to prevent against
// man in the middle attacks for all of our connections.
// we use the cipher suite order so that we can prefer the ciphers we set first in the list
// many SSLEngine options can be configured using either SSLParameters or direct methods on the engine itself, but there is one
// tricky aspect; if you set a value directly on the engine and then later set the SSLParameters the value set directly on the
// engine will be overwritten by the value in the SSLParameters
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Initialize sslContext
// check the supported ciphers and log them here to prevent spamming logs on every call
// A single configuration might be used in many place, if there are multiple, we just list "shared" because
// that is better than the alternatives. Just listing would be misleading (it might not be the right one)
// but listing all of them would be confusing (e.g. some might be the default realms)
// This needs to be a supplier (deferred evaluation) because we might load more configurations after this context is built.
// we only need to distinguishing between 0/1/many
/**
// Drop trailing '.' so that any exception messages are consistent
// Client Authentication _should_ be required, but if someone turns it off, then this check is no longer relevant
/**
/**
// we use the cipher suite order so that we can prefer the ciphers we set first in the list
/**
// an SSLSession could be null as there is no lock while iterating, the session cache
// could have evicted a value, the session could be timed out, or the session could
// have already been invalidated, which removes the value from the session cache in the
// sun implementation
/**
// Put this even if empty, so that the name will be mapped to the global SSL configuration
// Put this even if empty, so that the name will be mapped to the global SSL configuration
/**
//docs.oracle.com/en/java/javase/11/docs/specs/security/standard-names.html#sslcontext-algorithms">Java Security
/*
/**
/**
// since we support reloading the keystore, we must store the passphrase in memory for the life of the node, so we
// clone the password and never close it during our uses below
/*
/**
/**
// since we support reloading the truststore, we must store the passphrase in memory for the life of the node, so we
// clone the password and never close it during our uses below
/*
/**
/*
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/*
/**
// Specify private cert/key pair via keystore
// Specify private cert/key pair via key and certificate files
// Optional support for legacy (non secure) passwords
// pkg private for tests
/*
/**
/*
/*
/**
/*
/**
/*
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
// wait until the gateway has recovered from disk, otherwise we think may not have the index templates,
// while they actually do exist
// no master node, exit immediately
// if this node is newer than the master node, we probably need to add the template, which might be newer than the
// template the master node has, so we need potentially add new templates despite being not the master node
/*
/**
/**
/*
/**
/**
// TODO: should we handle this with a thrown exception?
/**
/**
/**
/**
/**
/**
/**
// check all mappings contain correct version in _meta
// we have to parse the source here which is annoying
// should always contain one entry with key = typename
// get the actual mapping entries
// pre 5.0, cannot be up to date
/*
// if there are no transforms, do not show any stats
/*
/*
// common parse fields
/**
// the timestamp of the checkpoint, mandatory
// checkpoint for for time based sync
// TODO: consider a lower bound for usecases where you want to transform on a window of a stream
// common strings
// deprecated REST API, to be removed for 8.0.0
// note: this is used to match tasks
// strings for meta information
/**
// internal document id
/*
/**
/**
/*
/*
/*
// XContentBuilder does not support passing the params object for Iterables
/*
// used internally to expand the queried id expression
// Only get tasks that we have expanded to
/*
// dest.index and ID are not required for Preview, so we just supply our own
// Users can still provide just dest.pipeline to preview what their data would look like given the pipeline ID
/**
/*
/**
/*
/*
// use the timeout value already present in BaseTasksRequest
// the base class does not implement hashCode, therefore we need to hash timeout ourselves
// the base class does not implement equals, therefore we need to compare timeout ourselves
/*
/**
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
// TODO add data_frame attributes when/if they are added
/**
/**
/**
/**
/*
// we store the query in 2 formats: the raw format and the parsed format, because:
// - the parsed format adds defaults, which were not part of the original and looks odd on XContent retrieval
// - if parsing fails (e.g. query uses removed functionality), the source can be retrieved
// we need 2 passes, but the parser can not be cloned, so we parse 1st into a map and then re-parse that for syntax checking
// remember the registry, needed for the 2nd pass
/*
// default handling: if the user does not specify a query, we default to match_all
/**
/**
// Using Arrays.hashCode as Objects.hash does not deeply hash nested arrays. Since we are doing Array.equals, this is necessary
/*
/**
/*
/*
/**
// the own checkpoint
// checkpoint of the indexes (sequence id's)
// ignored, only for internal storage: String docType = (String) args[5];
// note: this is never parsed from the outside where timestamp can be formatted as date time
/**
/**
// compare the timestamp, id, checkpoint and than call matches for the rest
/**
// quick check
// do the expensive deep equal operation last
/**
// else: both are transient
// get the sum of of shard operations (that are fully replicated), which is 1 higher than the global checkpoint for each shard
// note: we require shard checkpoints to strictly increase and never decrease
// ignore entries that aren't part of newCheckpoint, e.g. deleted indices
// Add 1 per shard as sequence numbers start at 0, i.e. sequence number 0 means there has been 1 operation
// Add 1 per shard as sequence numbers start at 0, i.e. sequence number 0 means there has been 1 operation
// this should not be possible
/*
/**
/**
// checkpointstats requires a non-negative checkpoint number
/**
/*
/**
/*
/**
// types of transforms
// headers store the user context from the creating user, which allows us to run the transform as this user
// the header only contains name, groups and other context but no authorization keys
// if the id has been specified in the body and the path, they must match
// ignored, only for internal storage: String docType = (String) args[5];
// on strict parsing do not allow injection of headers, transform version, or create time
// at least one function must be defined
/*
/**
/*
/*
// TODO remove when no longer needed for wire BWC
// This changes how much "weight" past calculations have.
// The shorter the window, the less "smoothing" will occur.
/**
// was transformId
// If all our exp averages are 0.0, just assign the new values.
/*
// If we are reading from an old document we need to convert docsRemaining to docsProcessed
// was not previously tracked
// What if our total docs number is `null` because we are in a continuous checkpoint, but are serializing to an old version?
// totalDocs was always incorrect in past versions when in a continuous checkpoint. So, just write 0
// which will imply documentsRemaining == 0.
/*
// 7.3 BWC: current_position only exists in 7.2.  In 7.3+ it is replaced by position.
// BWC handling, translate current_position to position iff position isn't set
/*
/**
// Prior to version 7.4 DataFrameTransformStats didn't exist, and we have
// to do the best we can of reading from a DataFrameTransformStoredDoc object
// (which is called DataFrameTransformStateAndStats in 7.2/7.3)
// Prior to version 7.4 DataFrameTransformStats didn't exist, and we have
// to do the best we can of writing to a DataFrameTransformStoredDoc object
// (which is called DataFrameTransformStateAndStats in 7.2/7.3)
// If we get here then the task state must be started, and that means we should have an indexer state
// This one is not deterministic, because an overall state of STOPPED could arise
// from either (STOPPED, null) or (STARTED, STOPPED).  However, (STARTED, STOPPED)
// is a very short-lived state so it's reasonable to assume the other, especially
// as this method is only for mixed version cluster compatibility.
/*
/**
/**
/*
/*
/*
/* Constants for internal indexes of the transform plugin
// internal index
// version is not a rollover pattern, however padded because sort is string based
// audit index
// gh #49730: upped version of audit index to 000002
/*
/*
// we store the query in 2 formats: the raw format and the parsed format
/*
/**
// Format was optional in 7.2.x, removed in 7.3+
// Format was optional in 7.2.x, removed in 7.3+
// no need for an extra range filter as this is already done by checkpoints
/*
/*
// be parsing friendly, whether the token needs to be advanced or not (similar to what ObjectParser does)
/*
// histograms are simple and cheap, so we skip this optimization
/*
// allow "aggs" and "aggregations" but require one to be specified
// if somebody specifies both: throw
// some group source do not implement change detection or not makes no sense, skip those
// sources
// TODO this will need to change once we allow multi-bucket aggs + field merging
/**
/*
/*
// TODO: add script
// either script or field
/*
/*
/*
/**
/**
/*
/**
/**
/**
/*
/*
// this is the required index.format setting for 6.0 services (watcher and security) to start up
// this index setting is set by the upgrade API or automatically when a 6.0 index template is created
/**
/*
/**
// for serialization
/**
/**
/*
/*
// Older versions recorded the number of sparse vector fields.
// Older versions recorded the number of sparse vector fields.
/*
/*
/*
/*
/*
/**
/**
/**
/**
/**
//3
/*
/**
/**
/**
/**
/**
/*
/**
/**
/*
/*
/*
/**
// check if we have mixed results, then set to partial failure
// it's the type of the action
/*
/*
/*
/**
/**
/*
/*
/*
/**
// falling back on the throttle period of the watch
// falling back on the default throttle period of watcher
/*
// throttling happened because a user actively acknowledged the action, which means it is muted until the condition becomes false
// the current implementation uses an implementation of a throttler to decide that an action should not be executed because
// it has been acked/muted before
// throttling happened because of license reasons
// time based throttling for a certain period of time
// no throttling, used to indicate a not throttledresult
/*
/*
/**
/*
/*
/**
/**
/**
/**
/**
/*
// All instances has to produce the same hashCode because they are all equal
/*
/**
// don't make this final - we can't mock final classes :(
// TODO: FAILURE status is never used, but a some code assumes that it is used
/*
/**
/**
/*
/**
//condition body
/*
/**
/*
/**
// the encryption used in this class was picked when Java 7 was still the min. supported
// version. The use of counter mode was chosen to simplify the need to deal with padding
// and for its speed. 128 bit key length is chosen due to the JCE policy that ships by
// default with the Oracle JDK.
// TODO: with better support in Java 8, we should consider moving to use AES GCM as it
// also provides authentication of the encrypted data, which is something that we are
// missing here.
/*
/**
/**
// Not encrypted
/**
/*
/**
/**
/**
/**
/**
/*
// awaiting execution of the watch
// initial phase, watch execution has started, but the input is not yet processed
// input is being executed
// condition phase is being executed
// transform phase (optional, depends if a global transform was configured in the watch)
// actions phase, all actions, including specific action transforms
// missing watch, failed execution of input/condition/transform,
// successful run
/*
// the condition of the watch was not met
// Execution has been throttled due to time-based throttling - this might only affect a single action though
// Execution has been throttled due to ack-based throttling/muting of an action - this might only affect a single action though
// regular execution
// an error in the condition or the execution of the input
// a rejection due to a filled up threadpool
// the execution was scheduled, but in between the watch was deleted
// even though the execution was scheduled, it was not executed, because the watch was already queued in the thread pool
// this can happen when a watch was executed, but not completely finished (the triggered watch entry was not deleted), and then
// watcher is restarted (manually or due to host switch) - the triggered watch will be executed but the history entry already
// exists
/*
/*
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
/*
/**
/*
/**
/*
// only emitted to xcontent in "debug" mode
// acknowledged as state wins because the user had explicitly set this, where as throttled may happen due to execution
/**
/**
/**
/*
/**
/**
/*
/*
/*
/*
/**
// TODO we can potentially remove this in 6.x
// This code is lifted almost straight from 2.x's TimeValue.java
// Allow this special value to be unit-less:
// Allow this special value to be unit-less:
/*
// history (please add a comment why you increased the version here)
// version 1: initial
// version 2: added mappings for jira action
// version 3: include watch status in history
// version 6: upgrade to ES 6, removal of _status field
// version 7: add full exception stack traces for better debugging
// version 8: fix slack attachment property not to be dynamic, causing field type issues
// version 9: add a user field defining which user executed the watch
// version 10: add support for foreach path in actions
// Note: if you change this, also inform the kibana team around the watcher-ui
/*
/*
/*
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// EMPTY is safe here because we never use namedObject
// EMPTY is safe here because we never use namedObject
/*
/*
/*
/**
/**
/**
/*
/*
/*
/*
/*
/**
/*
/**
/**
/**
/**
/*
/**
/*
/**
/**
/*
/**
/*
/**
/**
/**
/*
/**
/*
/**
/**
/*
/**
/*
/**
/**
/*
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/*
/**
/*
/**
/**
/**
/*
/**
/*
/**
/**
/*
/**
/*
/**
/**
/**
/**
/**
/*
/*
/**
/**
/*
/**
/**
/*
/**
/*
/**
/*
/**
/*
/**
/**
/**
/**
/**
/*
/*
/*
/*
/**
/**
/**
/**
/*
/*
/**
/**
// if the watch status doesn't have a state, we assume active
// this is to support old watches that weren't upgraded yet to
// contain the state
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
// Multi fields
/*
/**
/*
/**
// Package visible for testing
// Merges the per-run stats (the stats in "other") with the stats already present
/**
/**
/**
/**
/**
/**
/**
/**
/**
// maintain a consistent order when serializing
/*
/*
// it is really ridiculous we have nodes() and getNodes()...
/*
/*
// before license expiry
// on license expiry
// on license expiry
// bounds
// in match
// out of bounds
// after expiry and after max
// after expiry and after max
/*
/*
// If the future is done, it means request/license validation failed.
// In which case, this `actionGet` should throw a more useful exception than the verify below.
// If the future is done, it means request/license validation failed.
// In which case, this `actionGet` should throw a more useful exception than the verify below.
/*
/**
// assert 1.x BWC
// assert expected (2.x+) variant
// assert expected (2.x+) variant (note: no 1.x variant of BASIC)
// assert expected (2.x+) variant (note: no 1.x variant of STANDARD)
// assert expected (2.x+) variant (note: no different 1.x variant of GOLD)
// assert 1.x BWC
// assert expected (2.x+) variant
/*
/*
/*
// try installing a signed license
// ensure acknowledgement message was part of the response
// try installing a signed license with acknowledgement
// ensure license was installed and no acknowledgment message was returned
// try installing a signed license
// ensure acknowledgement message was part of the response
// try installing a signed license
// try installing a signed license with acknowledgement
// ensure license was installed and no acknowledgment message was returned
// try installing a signed license
// try installing a signed license with acknowledgement
// ensure license was installed and no acknowledgment message was returned
/*
/*
// should have an extra status field, human readable issue_data and expiry_date
// should have an extra status field, human readable issue_data and expiry_date
// should have an extra status field, human readable issue_data and expiry_date
// should have an extra status field, human readable issue_data and no expiry_date
/*
// enable http
// for license mode file watcher
// restart so that license is updated
//license updated
// restart once more and verify updated license is active
/*
/**
//openjdk.java.net/jeps/252
/**
/**
/**
// So we skip TLS checks
// If validation failed, the future might be done without calling the updater task.
/*
// TODO: Add test/feature blocking the registration of basic license
// put gold license
// put platinum license
// modify content of signed license
// ensure that the invalid license never made it to cluster state
// generate signed licenses
// remove signed licenses
/*
// makes sure LicensePlugin is registered in Custom MetaData
// random order of insertion
// serialize metadata
// deserialize metadata again
// check that custom metadata still present
// consume null
// consume "licenses"
// consume endObject
/*
//        newSettings.put(XPackSettings.MONITORING_ENABLED.getKey(), false);
//        newSettings.put(XPackSettings.WATCHER_ENABLED.getKey(), false);
// basic license is added async, we should wait for it
// put license
// get and check license
// put license source
// get and check license
// modify content of signed license
// try to put license (should be invalid)
// try to get invalid license
// try to put license (should be invalid)
// try to get invalid license
// get license should not return the expired license
// delete all licenses
// get licenses (expected no licenses)
/*
// We need a signature that parses correctly, but it doesn't need to verify
// When parsing a license, we read the signature bytes to verify the _version_.
// Random alphabetic sig bytes will generate a bad version
/*
/*
/*
/*
/*
/*
// enable http
// Testing that you can start a basic license when you have no license
/*
// enable http
// Test that starting will fail without acknowledgement
/*
//randomFrom("subscription", "internal", "development");
/**
/**
/*
/**
/** Creates a license state with the given license type and active state, and checks the given method returns expected. */
/**
/*
/*
/*
/**
/*
// we use indexRandom which might create holes ie. deleted docs
// make sure deletes do not work
// make sure deletes do not work
// now do a scroll with a slice
/*
// we processed the segments_N file plus _1.si, _1.fdx, _1.fnm, _1.fdt
// in total we have 4 more files than the previous snap since we don't count the segments_N twice
// we processed the segments_N file plus _1_1.liv
// in total we have 5 more files than the previous snap since we don't count the segments_N twice
/** Create a {@link Environment} with random path.home and path.repo **/
/** Create a {@link Repository} with a random name **/
// Apply state once to initialize repo properly like RepositoriesService would
/*
// we either use the soft deletes directly or manually delete them to test the additional delete functionality
// add a field only this segment has
// now add another doc
/*
/*
/*
/**
/*
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
// Uses #InetSocketAddress.getHostString() to prevent reverse dns lookups, eager binding, so we can find out host/port regardless
// if the webserver was already shut down
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
// Check that headers are not set
/**
/*
//Get around all the setOnce nonsense in the plugin
// End of the XPackPlugin overrides
// There can be only one.
// return the same default from MapperPlugin
// There can be only one.
/*
// disable
// disable
/*
/*
/*
/*
/*
/*
// now update synonyms file and trigger reloading
// now update synonyms file and trigger reloading
// same for synonym filters in multiplexer chain
/*
/*
/*
/*
/*
// we never really went async, its all chained together so verify this for sanity
// we never really went async, its all chained together so verify this for sanity
// we never really went async, its all chained together so verify this for sanity
// we never really went async, its all chained together so verify this for sanity
/*
/*
/*
/*
/*
// CCS tests: at time of writing it wasn't possible to mock RemoteClusterService, therefore it's not possible
// to test the whole validation but test RemoteSourceEnabledAndRemoteLicenseValidation
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
/*
/*
/**
/*
// Mock pause follow api call:
/*
/*
/*
/*
// Need to override this test because this is the one special step that
// is allowed to have ERROR as the step name
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
/*
/*
/*
/*
/*
/*
// Mock pause follow api call:
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// randomise whether the node had valid attributes or not but make sure at least one node is valid
// Need at least 2 nodes to have some nodes on a new version
// Since one shard is already on only new nodes, we should always pick a new node
// Need at least 2 nodes to have some nodes on a new version
// Need at least 2 nodes to have some nodes on a new version
// This shard should only be allocated to new nodes
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// always valid
/*
// Hot Phase
// Warm Phase
// Cold Phase
// Delete Phase
/**
/*
/*
// Mock unfollow api call:
// Mock unfollow api call:
/*
/*
/*
/*
/*
/*
// Trying to create a real IndicesStatsResponse requires setting up a ShardRouting, so just mock it
// Mock paths in a way that pass ShardPath constructor assertions
// Mock paths for ShardPath#getRootDataPath()
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// test the execution order
// block till latch has been counted down, simulating network latency
// counters
// test the execution order
/*
/*
// A missing task is a closed job
// A task with no status is opening
// A missing task is a stopped datafeed
/*
/*
/*
/*
/*
/*
/*
/*
/*
// Should never happen
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// Don't insert random fields into the job object as the
// custom_fields member accepts arbitrary fields and new
// fields inserted there will result in object inequality
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// no need to randomize JobUpdate this is already tested in: JobUpdateTests
// this call sets isInternal = true
/*
/*
/*
/*
/*
/*
/*
/*
// Check times are aligned with the bucket
/*
/*
// time span is required to be at least 1 millis, so we use a custom method to generate a time value here
// only manual mode allows a timespan
/*
/*
// can only test with a single agg as the xcontent order gets randomized by test base class and then
// the actual xcontent isn't the same and test fail.
// Testing with a single agg is ok as we don't have special list writeable / xcontent logic
//query:match:type stopped being supported in 6.x
//size: 0 stopped being supported in 6.x
// Unlike the config version of this test, the metadata parser should tolerate the unknown future field
// headers are only persisted to cluster state
// headers are not written without the FOR_INTERNAL_STORAGE param
// Two datafeeds with the same job id should have the same random query delay
// But the query delay of a datafeed with a different job id should differ too
// Assert that the parsed versions of our aggs and queries work as well
// Assert that the parsed versions of our aggs and queries work as well
// So equality check between the streamed and current passes
// Streamed DatafeedConfigs when they are before 6.6.0 require a parsed object for aggs and queries, consequently all the default
// values are added between them
// Assert that the parsed versions of our aggs and queries work as well
/*
/*
/*
// can only test with a single agg as the xcontent order gets randomized by test base class and then
// the actual xcontent isn't the same and test fail.
// Testing with a single agg is ok as we don't have special list writeable / xcontent logic
// So equality check between the streamed and current passes
// Streamed DatafeedConfigs when they are before 6.6.0 require a parsed object for aggs and queries, consequently all the default
// values are added between them
// Assert that the parsed versions of our aggs and queries work as well
/*
// time span is required to be at least 1 millis, so we use a custom method to generate a time value here
/*
/*
//query:match:type stopped being supported in 6.x
// match_all if parsed, adds default values in the options
// headers are only persisted to cluster state
// headers are not written without the FOR_INTERNAL_STORAGE param
// All these are different ways of specifying a limit that is lower than the minimum
/*
/*
// Should never happen
// Excludes take precedence
/*
/*
/*
/*
// Boundary condition: num_top_classes == 0
// Boundary condition: num_top_classes == 1000
// num_top_classes == null, default applied
// Boundary condition: training_percent == 1.0
// Boundary condition: training_percent == 100.0
// training_percent == null, default applied
/*
/*
// Boundary condition: training_percent == 1.0
// Boundary condition: training_percent == 100.0
// training_percent == null, default applied
/*
/*
/*
/*
// 13 false positives, 13 false negatives
//  8 false positives, 17 false negatives
// 13 false positives, 10 false negatives
/*
/**
// This is the last step, time to write evaluation result
/*
// allow unknown fields in the root of the object only
/*
/*
/*
/*
/*
/*
/**
/*
/*
/*
/*
/*
// not a good day to play in the lottery; let's add them all
/*
/*
/*
/*
/*
/*
/*
/*
// Did we inflate to the same object?
/*
//TODO these tests are temporary until the named objects are actually used by an encompassing class (i.e. ModelInferer)
// We only want to add random fields to the root, or the root of the named objects
/*
/*
/*
/*
/*
// Test where the value is some unknown Value
/*
// Test where the value is some unknown Value
/*
/*
// Test where the value is some unknown Value
/*
// Truncate to UTF8 boundary (no cut)
// Truncate to UTF8 boundary (cuts)
// Don't truncate
// Truncate to UTF8 boundary (cuts)
///1/2@@3winter"),
///1/2@@3winter "),
///1/2@@3winter "),
///1/2@@3winter "),
///1/2@@3winter "),
/*
/*
//github.com/google/cld3
/**
/**
/*
// One UTF8 character by itself.
// Empty string.
// Only whitespaces.
// Only numbers and punctuation.
// Only numbers, punctuation, and spaces.
// One UTF8 character with some numbers / punctuation / spaces: character at
// one extremity or in the middle.
/*
// Unrecognized 2-byte scripts.  For info on the scripts mentioned below, see
// http://www.unicode.org/charts/#scripts Note: the scripts below are uniquely
// associated with a language.  Still, the number of queries in those
// languages is small and we didn't want to increase the code size and
// latency, so (at least for now) we do not treat them specially.
// The following three tests are, respectively, for Armenian, Syriac and
// Thaana.
// Unrecognized 3-byte script: CJK Unified Ideographs. Not uniquely associated with a language.
// Unrecognized 4-byte script: CJK Unified Ideographs Extension C. Not uniquely associated with a language.
// Unrecognized 4-byte script: CJK Unified Ideographs Extension E. Not uniquely associated with a language.
/*
//github.com/google/cld3
// compare against cld3 expected text type
/*
/*
/*
/*
/*
/*
// if ae are turned off, then we should get a null value
// otherwise, we should expect an assertion failure telling us that the string is improperly formatted
/*
/*
// Tree with loop
// This should handle missing values and take the default_left path
// Test with NO aggregator supplied, verifies default behavior of non-weighted sum
/*
/*
/*
/*
/*
/*
/*
//github.com/google/cld3
/**
// The true language
// The language predicted by CLD3
// The probability of the prediction
// The raw text on which the prediction is based
/*
/*
// does not really matter as this is a stump
// Build a tree with 2 nodes and 3 leaves using 2 features
// The leaves have unique values 0.1, 0.2, 0.3
// This feature vector should hit the right child of the root node
// This should hit the left child of the left child of the root node
// i.e. it takes the path left, left
// This should hit the right child of the left child of the root node
// i.e. it takes the path left, right
// This should still work if the internal values are strings
// This should handle missing values and take the default_left path
// Build a tree with 2 nodes and 3 leaves using 2 features
// The leaves have unique values 0.1, 0.2, 0.3
// This feature vector should hit the right child of the root node
// This should hit the left child of the left child of the root node
// i.e. it takes the path left, left
// This should hit the right child of the left child of the root node
// i.e. it takes the path left, right
// Build a tree with 2 nodes and 3 leaves using 2 features
// The leaves have unique values 0.1, 0.2, 0.3
// This feature vector should hit the right child of the root node
// This should hit the left child of the left child of the root node
// i.e. it takes the path left, left
// This should handle missing values and take the default_left path
/*
/*
// indices will be deleted by the ESRestTestCase class
/*
// Single detector, not pre-summarised
// Single detector, pre-summarised
// Multiple detectors, not pre-summarised
/*
// the root cause is wrapped in an intermediate ElasticsearchParseException
/*
/*
// at least one of the two should be present
// no need for random condition (it is already tested)
/*
// no need for random DetectionRule (it is already tested)
// char, isValid?
// if nothing else is set the count functions (excluding distinct count)
// are the only allowable functions
// Nor rare
// Nor freq_rare
// some functions require a fieldname
// some functions require a fieldname
// some functions require a fieldname
// some functions require a fieldname
/*
/*
// Unlike the config version of this test, the metadata parser should tolerate the unknown future field
// JobConfigurationTests:
/**
// JobConfigurationVerifierTests:
// Assert parsing a job without version works as expected
// field name used here matches what's in createAnalysisConfig()
/*
/**
/*
/*
/*
// no validation error:
// no validation error:
// no validation error:
/*
/*
/*
/*
// These are not reserved because they're Elasticsearch keywords, not
// field names
// These are not reserved because they're data types, not field names
// These are not reserved because they're data types, not field names
// ModelPlotConfig has an 'enabled' the same as one of the keywords
// By comparing like this the failure messages say which string is missing
// check no mapping for the reserved field
// Only the mappings for the results index should be added below.  Do NOT add mappings for other indexes here.
// Only the mappings for the config index should be added below.  Do NOT add mappings for other indexes here.
/*
/*
/*
/*
/*
/*
/*
/**
/*
/*
// influencer fields with the same name as a by/over/partitiion field
// come second in the list
// 512 comes from IndexRequest.validate()
/*
/*
/*
/*
/*
/*
/*
/*
/*
// merge 4 into 3
// merge 3 into 2
// merger 2 into 1
/*
// same as accumulator
// merging the other way should yield the same results
/*
/*
// Not part of path
/*
/*
/*
/*
// Add all the default fields so they are not added dynamically when the object is parsed
//phrase stopped being supported for match in 6.x
//size being 0 in terms agg stopped being supported in 6.x
/*
/**
/*
/**
// now update synonyms file several times and trigger reloading
/*
//second
//minute
//hour
//day of month
//month
//day of week
//year
// large names so we don't accidentally collide
/*
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
/**
// previous way to deserialize a DateHistogramGroupConfig
// previous way to serialize a DateHistogramGroupConfig
/**
// Serialize the old format
// Deserialize the new format
// Serialize the new format
// Deserialize the old format
/*
/*
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
/*
/*
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
/*
/*
/*
/*
/*
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
/*
// we count down the latch after this exception is caught and mock logged in SchedulerEngine#notifyListeners
// this happens after the listener has been notified, threw an exception, and then mock logged the exception
// randomize the order and register the listeners
// only allow one triggering of the listeners
// now check that every listener was invoked
// latch for each invocation of nextScheduledTimeAfter, once for each scheduled run, and then a final time when we disable
/*
/*
/*
/*
// old version so the default for `ownedByAuthenticatedUser` is false
/*
/*
// old version so the default for `ownedByAuthenticatedUser` is false
/*
/*
/*
/*
/*
/*
/*")),
/*
/*")),
// wildcard app name
// invalid priv names
/*");
// no actions
// reserved metadata
// mixed
// Empty request
/*
/*
// Fail
/*");
// Fail
// Fail
/*
/*
/*
/*
/*
/*
/*
/*
/**
/**
// careful, don't "bury" this on the call stack, unless you know what you're doing
/**
/**
// we can not keep a reference to the event here because Log4j is using a thread
// local instance under the hood
/*
//secret.es.shield.gov/");
//secret.es.shield.gov/]"));
/*
/*
/*
/*
/*
// This value is based on the internal implementation details of lucene's FixedBitSet
// If the implementation changes, this can be safely updated to match the new ram usage for a single bitset
// Enough to hold exactly 2 bit-sets in the cache
// The first time through we have 1 entry, after that we have 2
// Older queries should get evicted, but the query from last iteration should still be cached
// This value is based on the internal implementation details of lucene's FixedBitSet
// If the implementation changes, this can be safely updated to match the new ram usage for a single bitset
// Enough to hold less than 1 bit-sets in the cache
// This value is based on the internal implementation details of lucene's FixedBitSet
// If the implementation changes, this can be safely updated to match the new ram usage for a single bitset
// Enough to hold slightly more than 1 bit-sets in the cache
// Force the cache to perform eviction
// Loop until the cache has less than 2 items, which mean that something we evicted
// Check that the original bitset is no longer in the cache (a new instance is returned)
// This value is based on the internal implementation details of lucene's FixedBitSet
// If the implementation changes, this can be safely updated to match the new ram usage for a single bitset
// Enough to hold slightly more than 1 bit-set in the cache
// BitSet1 has been evicted now, run the cleanup...
// Check that the original bitset is no longer in the cache (a new instance is returned)
// BitSet2 has been evicted now, run the cleanup...
// This value is based on the internal implementation details of lucene's FixedBitSet
// If the implementation changes, this can be safely updated to match the new ram usage for a single bitset
// Force cache evictions by setting the size to be less than the number of distinct queries we search on.
// Sleep for a small (random) length of time.
// This increases the likelihood that cache could have been modified between the eviction & the cleanup
// Due to cache evictions, we must get more bitsets than fields
// Due to cache evictions, we must have seen more bitsets than the cache currently holds
// Even under concurrent pressure, the cache should hit the expected size
// Need to do this nested, or else the cache will be cleared when the index reader is closed
/*
// We check it is empty at the end of the test, so make sure it is empty in the
// beginning as well so that we can easily distinguish from garbage added by
// this test and garbage not cleaned up by other tests.
// this doc has been marked as deleted:
/** Same test as in FieldSubsetReaderTests, test that core cache key (needed for NRT) is working */
// add two docs, id:0 and id:1
// open reader
// delete id:0 and reopen
// we should have the same cache key as before
// However we don't support caching on the reader cache key since we override deletes
/*
/** Simple tests for this filterreader */
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 points
// open reader
// see only one field
// size statistic
// doccount statistic
// min statistic
// max statistic
// bytes per dimension
// number of dimensions
// walk the trees: we should see stuff in fieldA
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 fields
// open reader
// see only one field
/**
// add document with 2 fields
// open reader
// see only one field
// include on top-level value
// include on inner wildcard
// include on leading wildcard
// include on inner value
// exclude on exact value
// exclude on wildcard
// include on inner array
// include on inner array 2
// exclude on inner array
// exclude on inner array 2
// json array objects that have no matching fields should be left empty instead of being removed:
// (otherwise nested inner hit source filtering fails with AOOB)
/**
// add document with 2 fields
// open reader
// see only one field
// seekExact
// seekExact with TermState
// first, collect TermState from underlying reader
// now try and seekExact with it
// seekCeil
/**
// add document with 2 fields
// open reader
// see only two fields
// seekExact
// seekCeil
/**
// add document with 2 fields
// open reader
// see only one field
// seekExact
// seekCeil
/**
// add document with 2 fields
// open reader
// see only one field
/** test that core cache key (needed for NRT) is working */
// add two docs, id:0 and id:1
// open reader
// delete id:0 and reopen
// we should have the same cache key as before
/**
// add document with 2 fields
// no vectors
// open reader
// sees no fields
/**
// open reader
// see no fields
// see no vectors
// see no stored fields
//TODO FLS filters out empty objects from source, although they are granted access.
//When filtering mappings though we keep them.
/*assertEquals(1, filtered.size());
/*
/*
// unfortunately DirectoryReader isn't mock friendly
// Presence of fields in a role with an empty array implies access to no fields except the meta fields
// make sure meta fields cannot be denied access to
// check we can add all fields with *
// same with null
// check we remove only excluded fields
// same with null
// some other checks
// empty array for allowed fields always means no field is allowed
// make sure all field can be explicitly allowed
/*
/*");
/*");
/*", "audit/*", "user/12345");
/*");
/*").grants(app2Read, "foo/bar"), equalTo(true));
/*").grants(app2Read, "bar/baz"), equalTo(false));
/*");
/*
/*"), Set.of("cluster:admin/ilm/*"));
/*"), Set.of());
/*"), requestPredicate);
/*
/*
/*
/*
// Map<String, ResourcePrivileges> expectedResourceToResourcePrivs = new HashMap<>();
/*");
/*");
/*"))
/*"))
/*"))
/*")).build();
/*
/*
/*
/*
// must start with lowercase
// must start with letter
// cannot contain special characters unless preceded by a "-" or "_"
// no wildcards
// no special characters with wildcards
// these should all be OK
// wildcards in the suffix
// must start with lowercase
// must start with letter
// cannot contain special characters
// these should all be OK
/*", "read/a_b.c-d+e%f#(g)")) {
/*", "action:login");
/*", "action:*");
/*", "data:write/*", "action:login"));
// pass
/*
/*
// ManageApplicationPrivileges.parse requires that the parser be positioned on the "manage" field.
/*
/*
/*");
/*");
/*");
// all implies monitor
// ClusterPrivilegeResolver.resolve() for a cluster action converts action name into a pattern by adding "*"
// check indices actions
// check non-ilm action
// check indices actions
// check non-ilm action
/*
/**
// This test might cease to be true if we ever have non-security restricted names
// but that depends on how users are supposed to perform snapshots of those new indices.
// ILM
// SAML and token
// Application Privileges
// Everything else
// inherits from 'all'
// read-only index access
// Beats management index
// we get this from the cluster:monitor privilege
// These tests might need to change if we add new non-security restricted indices that the monitoring user isn't supposed to see
// (but ideally, the monitoring user should see all indices).
// internal use only
// internal use only
// internal use only
// internal use only
// internal use only
// internal use only
/*
/*
/*
// expected
// set to the min value
// either pattern 1 or 2 should be evicted (in theory it should be 1, but we don't care about that level of precision)
/*
/*
/*
/*
/*
/*
/*
/*
/*
// New timestamp
// new policyId
// new repo name
/*
// The content of this IndexResponse doesn't matter, so just make it 100% random
// The content of this IndexResponse doesn't matter, so just make it 100% random
/*
//github.com/elastic/elasticsearch/issues/43950")
// Ignore this, it's verified in another test
// now delete one template from the cluster state and lets retry
// Ignore this, it's verified in another test
// Ignore this, it's verified in another test
// Ignore this, it's verified in another test
// -------------
/**
/*
/*
// read in keystore version
// read in keystore version
/*
/*
/*
// pass
/*
/**
/**
//Load HTTPClient only once. Client uses the same store as a truststore
//localhost:" + server.getPort())).close());
// The new server certificate is not in the client's truststore so SSLHandshake should fail
//localhost:" + server.getPort())).close()));
/**
//github.com/elastic/elasticsearch/issues/49094", inFipsJvm());
// Load HTTPClient once. Client uses a keystore containing testnode key/cert as a truststore
//localhost:" + server.getPort())).close());
// The new server certificate is not in the client's truststore so SSLHandshake should fail
//localhost:" + server.getPort())).close()));
/**
// Create the MockWebServer once for both pre and post checks
//localhost:" + server.getPort())).close());
// Client's truststore doesn't contain the server's certificate anymore so SSLHandshake should fail
//localhost:" + server.getPort())).close()));
/**
//github.com/elastic/elasticsearch/issues/49094", inFipsJvm());
// Create the MockWebServer once for both pre and post checks
//localhost:" + server.getPort())));//.close());
// Client doesn't trust the Server certificate anymore so SSLHandshake should fail
//localhost:" + server.getPort())).close()));
/**
// truncate the keystore
// do nothing
/**
// truncate the file
/**
// truncate the truststore
/**
// write bad file
// testclient.crt filename already used in #testPEMTrustReloadException
// Baseline checks
// modify
// checks after reload
/**
/**
/*
/*
// Pass settings in as component settings
/*
// Randomise the keystore type (jks/PKCS#12)
// The default is to use JKS. Randomly test with explicit and with the default value.
// Technically, we don't care whether xpack.http.ssl is valid for server - it's a client context, but we validate both of the
// server contexts (http & transport) during construction, so this is the only way to make a non-server-valid context.
// the order we set the protocols in is not going to be what is returned as internally the JDK may sort the versions
// the order we set the protocols in is not going to be what is returned as internally the JDK may sort the versions
// this just exhaustively verifies that the right things are called and that it uses the right parameters
// ensure it actually goes through and calls the real method
// Here we use a different ciphers for each context, so we can check that the returned SSLConfiguration matches the
// provided settings
// Execute a GET on a site known to have a valid certificate signed by a trusted public CA
// This will result in an SSLHandshakeException if the SSLContext does not trust the CA, but the default
// truststore trusts all common public CAs so the handshake will succeed
//www.elastic.co/")).close());
// Execute a GET on a site known to have a valid certificate signed by a trusted public CA which will succeed because the JDK
// certs are trusted by default
//www.elastic.co/")).close());
// Execute a GET on a site known to have a valid certificate signed by a trusted public CA
// This will result in an SSLHandshakeException if the SSLContext does not trust the CA, but the default
// truststore trusts all common public CAs so the handshake will succeed
// Execute a GET on a site known to have a valid certificate signed by a trusted public CA which will succeed because the JDK
// certs are trusted by default
// randomly select between default, and explicit enabled
/*
/*
/**
/**
/*
// randomise between default-false & explicit-false
// randomise between default-true & explicit-true
// randomise between default-false & explicit-false
// randomise between default-false & explicit-false
// it does not matter whether or not this is set, as security is not enabled.
// it does not matter whether or not this is set, as TLS is enabled.
// it does not matter whether this is set, or to which value.
// it does not matter whether this is set, or to which value.
/*
/*
// translate Windows line endings (\r\n) to standard ones (\n)
/*
/*
// for testing the test method, we can not assert directly, but wrap it with an exception, which also
// nicely encapsulate parsing errors thrown by MessageFormat itself
/*
/*
/**
/*
/*
/*
/*
/*
/*
// id & dest fields will be set by the parser
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
// register aggregations as NamedWriteable
/*
/*
/**
/*
/*
/*
/*
// should not happen
// create something broken but with a source
// lenient, passes but reports invalid
// strict throws
// lenient, passes but reports invalid
// strict throws
// lenient, passes but reports invalid
// strict throws
/*
// create something broken but with a source
// allow unknown fields in the root of the object only as QueryConfig stores a Map<String, Object>
/*
/*
// changesLastDetectedAt is not serialized to past values, so when it is pulled back in, it will be null
/*
/*
// same
// with copy
// other id
// other timestamp
// other checkpoint
// other index checkpoints
// other time upper bound
// no difference for same checkpoints, transient or not
// new vs transient new: ok
// transient new vs new: illegal
// new vs old: illegal
// corner case: the checkpoint appears older but the inner shard checkpoints are newer
// test cases where indices sets do not match
// remove something from old, so newer has 1 index more than old: should be equivalent to old index existing but empty
// remove same key: old and new should have equal indices again
// remove 1st index from new, now old has 1 index more, which should be ignored
/*
// else
/*
/*
/*
/*
// documentsIndexed are not in past versions, so it would be zero coming in
/*
// Will be false after BWC deserialization
/*
// changesLastDetectedAt aren't serialized back
/*
// Setting params for internal storage so that we can check XContent equivalence as
// DataFrameIndexerTransformStats does not write the ID to the XContentObject unless it is for internal storage
/*
/*
// Since the old version does not have the version serialized, the version NOW is 7.2.0
/*
// ensure that the unlikely does not happen: 2 aggs share the same name
// create something broken but with a source
// lenient, passes but reports invalid
// strict throws
// lenient, passes but reports invalid
// strict throws
/*
/*
// array of illegal characters, see {@link AggregatorFactories#VALID_AGG_NAME}
// ensure that the unlikely does not happen: 2 group_by's share the same name
// lenient, passes but reports invalid
// strict throws
// lenient, passes but reports invalid
// strict throws
// should not happen
/*
/*
// lenient passes but reports invalid
// lenient passes but reports invalid
/*
/*
/*
/*
// we always check the period first... so the result will come for the period throttler
/*
/*
/*
/**
/**
/** freeze the time for this clock, preventing it from advancing */
/** the clock will be reset to current time and will advance from now */
/** freeze the clock if not frozen and advance it by the given time */
/** freeze the clock if not frozen and advance it by the given amount of seconds */
/** freeze the clock if not frozen and rewind it by the given time */
/** freeze the clock if not frozen and rewind it by the given number of seconds */
/*
/*
/*
/*
// Also tests that failures are not counted towards the maximum
/*
/*
/**
/*
/**
/**
/*
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/" +
//www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-translog.html",
/*
/**
//www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-7.0.html#breaking_70_search_changes",
//www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-7.0.html" +
/*
/*
// Cluster is not affected but we look up repositories in metadata
/*
/*
/*
//www.elastic.co/guide/en/elasticsearch/reference/master/" +
//www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-translog.html",
/*
// TODO: once some query syntax has been removed from 8.0 and deprecated in 7.x reinstate this test
// to check that particular query syntax causes a deprecation warning
/*
//www.elastic.co/guide/en/elasticsearch/reference/master/breaking-changes-8.0.html" +
/*
// and that is ok
// Create source index:
// Create the policy:
// Add entry to source index and then refresh:
// Execute the policy:
// Create pipeline
// Index document using pipeline with enrich processor:
// Check if document has been enriched
// delete the pipeline so the policies can be deleted
// lets not delete the pipeline at first, to test the failure
// delete the pipelines so the policies can be deleted
// verify the delete did not happen
/*
/*
/*
/*
// This test is here because it requires a valid user that has permission to execute policy PUTs but should fail if the user
// does not have access to read the backing indices used to enrich the data.
/*
// If a document does not have the enrich key, return the unchanged document
// If the index is empty, return the unchanged document
// If the enrich key does not exist in the index, throw an error
// If no documents match the key, return the unchanged document
// used for testing only:
// used for testing only:
/*
/**
/*
/*
// Release policy lock, and throw a different exception
// Set task status to failed to avoid having to catch and rethrow exceptions everywhere
// Look up policy in policy store and execute it
// Be sure to unlock if submission failed.
// Unregister task in case of exception
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
// Check that no enrich policies are being executed
// Ensure that no enrich policy executions started while we were retrieving the snapshot of index data
// If executions were kicked off, we can't be sure that the indices we are about to process are a
// stable state of the system (they could be new indices created by a policy that hasn't been published yet).
// Find the policy on the index
// Check if index has a corresponding policy
// No corresponding policy. Index should be marked for removal.
// Check if index is currently linked to an alias
// Index is not currently published to the enrich alias. Should be marked for removal.
/*
/**
/**
/**
/**
// we ensure that we both have the pipeline and its version represents the current (or later) version
/**
// remove the id from the document so that documents from multiple indices will always be unique.
/*
// Collect the source index information
// First ensure mapping is set
// Validate the key and values
// Ensure that the current field is of object type only (not a nested type or a non compound field)
// Currently the only supported policy type is EnrichPolicy.MATCH_TYPE, which is a keyword type
// No need to also configure index_options, because keyword type defaults to 'docs'.
// Enable _source on enrich index. Explicitly mark key mapping type.
// No changes will be made to an enrich index after policy execution, so need to enable automatic refresh interval:
// This disables eager global ordinals loading for all fields:
// Check to make sure that the enrich pipeline exists, and create it if it is missing.
// Filter down the source fields to just the ones required by the policy
// Do we want to fail the request if there were failures during the reindex process?
// Force merge down to one segment successful
/*
/*
/**
/**
// The policy name is used to create the enrich index name and
// therefor a policy name has the same restrictions as an index name
// indices field in policy can contain wildcards, aliases etc.
/**
/**
/**
// Make a copy, because policies map inside custom metadata is read only:
/*
/*
/** used in tests **/
/*
/** used in tests **/
/*
/**
// Write tp is expected when executing enrich processor from index / bulk api
// Management tp is expected when executing enrich processor from ingest simulate api
// Use put(...), because if queue is full then this method will wait until a free slot becomes available
// The calling thread here is a write thread (write tp is used by ingest) and
// this will create natural back pressure from the enrich processor.
// If there are no write threads available then write requests with ingestion will fail with 429 error code.
// There may be room to for a new request now that numberOfOutstandingRequests has been decreased:
/*
/**
// This always executes on all ingest nodes, hence no node ids need to be provided.
/*
/*
/**
// validate that only a from, size, query and source filtering has been provided (other features are not supported):
// (first unset, what is supported and then see if there is anything left)
// can't set -1 to indicate not specified
/*
// the most lenient we can get in order to not bomb out if no indices are found, which is a valid case
// where a user creates and deletes a policy before running execute
// ensure the policy exists first
// delete all enrich indices for this policy
/*
// Report failures even if some node level requests succeed:
/*
// if we don't fail here then reindex will fail with a more complicated error.
// (EnrichPolicyRunner uses a pipeline with reindex)
/*
/*
/*
/*
/*
/*
/*
/*
// and that is okay
/*
// create enrich index
// point within match boundary
/*
/*
/*
/*
/**
/**
// Launch a fake policy run that will block until firstTaskBlock is counted down.
// Launch a second fake run that should fail immediately because the lock is obtained.
// Should throw exception on the previous statement, but if it doesn't, be a
// good citizen and conclude the fake runs to keep the logs clean from interrupted exceptions
// Conclude the first mock run
// Validate exception from second run
// Ensure that the lock from the previous run has been cleared
// Launch a two fake policy runs that will block until counted down to use up the maximum concurrent
// Launch a third fake run that should fail immediately because the lock is obtained.
// Should throw exception on the previous statement, but if it doesn't, be a
// good citizen and conclude the fake runs to keep the logs clean from interrupted exceptions
// Conclude the first mock run
// Validate exception from second run
// Ensure that the lock from the previous run has been cleared
/*
// Lock
// Ensure that locked policies are rejected
// Get exec state - should note as safe and revision 1 since nothing has happened yet
// Get another exec state - should still note as safe and revision 1 since nothing has happened yet
// Lock a policy and leave it open (a
// Get a third exec state - should have a new revision and report unsafe since execution is in progress
// Unlock the policy
// Get a fourth exec state - should have the same revision as third, and report no policies in flight since the previous execution
// is complete
// Create a fifth exec state, lock and release a policy, and check if the captured exec state is the same as the current state in
// the lock object
// Should report as not the same as there was a transient "policy execution" between getting the exec state and checking it.
// Lock
// Unlock
// Ensure locking again after release works
/*
// Create a test enabled maintenance service
// Add some random policies for the maintenance thread to reference
// Create some indices for the policies
// Ensure that the expected indices exist
// Do cleanup - shouldn't find anything to clean up
// Ensure that the expected indices still exist
// Replace a policy index with a new one
// Ensure all three indices exist
// Should clean up the first index for the first policy
// Ensure only the two most recent indices exist
// Remove a policy to simulate an abandoned index with a valid alias, but no policy
// Should cleanup the first index for the second policy
// Ensure only the first policy's index is left
// Clean up the remaining policy indices
// Extend the maintenance service to make the cleanUpEnrichIndices method a blocking method that waits for clean up to complete
/*
// Validate Index definition
// Validate Mapping
// Validate document structure
// Validate segments
// Validate Index is read only
// Validate Index definition
// Validate Mapping
// Validate document structure
// Validate segments
// Validate Index is read only
// Validate Index definition
// Validate Mapping
// Validate document structure
// Validate segments
// Validate Index is read only
// Validate Index definition
// Validate Mapping
// Validate document structure
// Validate removal of routing values
// Validate segments
// Validate Index is read only
// Validate Index definition
// Validate Mapping
// Validate document structure
// Validate segments
// Validate Index is read only
// Validate Index definition
// Validate Mapping
// Validate segments
// Validate Index is read only
// Validate Index definition
// Validate Mapping
// Validate segments
// Validate Index is read only
// Validate Index definition
// Validate Mapping
// Validate segments
// Validate Index is read only
// Validate Index definition
// Validate Mapping
// Validate segments
// Validate Index is read only
// The executor would wrap the listener in order to clean up the task in the
// task manager, but we're just testing the runner, so we make sure to clean
// up after ourselves.
// Put and flush a document to increase the number of segments, simulating not
// all segments were merged on the first try.
// Validate number of force merges
// Validate Index definition
// Validate Mapping
// Validate document structure
// Validate segments
// Validate Index is read only
// The executor would wrap the listener in order to clean up the task in the
// task manager, but we're just testing the runner, so we make sure to clean
// up after ourselves.
/*
// testFromXContent, always shuffles the xcontent and then byte wise the query is different, so we check the parsed version:
/*
/*
/*
// After full restart the policies should still exist:
/*
/*
// Run
// Check request
// Check result
/*
/*
// Run
// Check request
// Check result
// Run
// Check request
// Check result
// Run
// Check request
// Check result
// Execute
// Check request
// Check result
// Execute
// Check request
// Check result
/*
// First batch of search requests have been sent off:
// (However still 5 should remain in the queue)
// Nothing should happen now, because there is an outstanding request and max number of requests has been set to 1:
// Replying a response and that should trigger another coordination round
// Replying last response, resulting in an empty queue and no outstanding requests.
// All individual action listeners for the search requests should have been invoked:
// First batch of search requests have been sent off:
// (However still 5 should remain in the queue)
// All individual action listeners for the search requests should have been invoked:
// First batch of search requests have been sent off:
// (However still 5 should remain in the queue)
// Replying a response and that should trigger another coordination round
// All individual action listeners for the search requests should have been invoked:
/*
/*
/*
/*
/*
/*
// the tests shuffle around the policy query source xcontent type, so this is needed here
// since the backing store is a treemap the list will be sorted so we can just check each
// instance is the same
/*
/*
// if the enrich policy does not exist, then just keep going
// fail if the state of this is left locked
// fail if the state of this is left locked
/*
// if the enrich policy does not exist, then just keep going
// fail if the state of this is left locked
// empty or null should return the same
// save a second one to verify the count below on GET
// save a second one to verify the count below on GET
/*
// Manual test specific object fields and if not just fail:
/*
// this controls the blockage
// regardless of enrich being enabled
// this controls the blockage
// this is controls the blockage
// this is controls the blockage
// since it's the default, we want to ensure we test both with/without it
/*
// Manual test specific object fields and if not just fail:
/*
/**
// we record the segment stats here - that's what the reader needs when it's open and it give the user
// an idea of what it can save when it's closed
// we fake an empty DirectoryReader for the ReadOnlyEngine. this reader is only used
// to initialize the reference manager and to make the refresh call happy which is essentially
// a no-op now
// always current
// TODO maybe we can return an empty commit?
// it might look awkward that we have to check here if the keys match but if we concurrently
// access the lastOpenedReader there might be 2 threads competing for the cached reference in
// a way that thread 1 counts down the lastOpenedReader reference and before thread 1 can execute
// the close listener we already open and assign a new reader to lastOpenedReader. In this case
// the cache key doesn't match and we just ignore it since we use this method only to null out the
// lastOpenedReader member to ensure resources can be GCed
// special case for can_match phase - we use the cached point values reader
// special case we only want to report segment stats if we have a reader open. in that case we only get a reader if we still
// have one open at the time and can inc it's reference.
// we just hand out a searcher on top of an empty reader that we opened for the ReadOnlyEngine in the #open(IndexCommit)
// method. this is the case when we don't have a reader open right now and we get a stats call any other that falls in
// the category that doesn't trigger a reopen
// don't call close here we manage reference ourselves
/*
// also register a release resource in this case if we have multiple roundtrips like in DFS
/**
// volatile since it might be closed concurrently
// we are lenient here it's ok to double close
// only do this if we are not closed already
// we end up in this case when we are not closed but in an intermediate
// state were we want to release all or the real leaf readers ie. in between search phases
// but still want to keep this Lazy reference open. In oder to let the heavy real leaf
// readers to be GCed we need to null our the references.
// ensure we fail early and with good exceptions
/**
// empty reader here to make FilterLeafReader happy
// don't register in reader as a subreader here.
// this is mainly for tests
/*
/**
// except of a couple of selected methods everything else will
// throw a UOE which causes a can_match phase to just move to the actual phase
// later such that we never false exclude a shard if something else is used to rewrite.
// might not be in this reader
/*
/*
/*
/*
// only unfreeze if we are frozen and only freeze if we are not frozen already.
// this prevents all indices that are already frozen that match a pattern to
// go through the cycles again.
// TODO improve FreezeResponse so that it also reports failures from the close index API
/*
/*
// we don't merge we want no background merges to happen to ensure we have consistent breaker stats
// first flush to make sure we have a commit that we open in the frozen engine blow.
// pull the reader to account for RAM in the breaker.
// even though we don't want this to be searched concurrently we better make sure we release all resources etc.
// The point of these checks is to ensure that methods from the super class
// are overwritten to make sure we never miss a method from FilterLeafReader / FilterDirectoryReader
// here we make sure we catch any change to their super classes FilterLeafReader / FilterDirectoryReader
/*
// index more documents while one shard copy is offline
/*
/**
/*
// now scroll
// in total 4 refreshes 1x query & 1x fetch per shard (we have 2)
// sometimes close it
// never opened a reader
// we don't resolve to closed indices
/*
/*
/*
/*
/*
/*
/*
/**
// Each "hop" is recorded here using hopNumber->fieldName->vertices
/**
// Either we gathered no leads from the last hop or we have
// reached the final hop
// A single sample pool of docs is built at the root of the aggs tree.
// For quality's sake it might have made more sense to sample top docs
// for each of the terms from the previous hop (e.g. an initial query for "beatles"
// may have separate doc-sample pools for significant root terms "john", "paul", "yoko" etc)
// but I found this dramatically slowed down execution - each pool typically had different docs which
// each had non-overlapping sets of terms that needed frequencies looking up for significant terms.
// A common sample pool reduces the specialization that can be given to each root term but
// ultimately is much faster to run because of the shared vocabulary in a single sample set.
// Add any user-supplied criteria to the root query as a must clause
// Build a MUST clause that matches one of either
// a:) include clauses supplied by the client or
// b:) vertex terms from the previous hop.
//Now build the agg tree that will channel the content ->
//   base agg is terms agg for terms from last wave (one per field),
//      under each is a sig_terms agg to find next candidates (again, one per field)...
// Map execution mode used because Sampler agg keeps us
// focused on smaller sets of high quality docs and therefore
// examine smaller volumes of terms
//We have the potential for self-loops as we are looking at the same field so add 1 to the requested size
// because we need to eliminate fieldA:termA -> fieldA:termA links that are likely to be in the results.
//                        nextWaveSigTerms.significanceHeuristic(new PercentageScore.PercentageScoreBuilder());
//Had some issues with no significant terms being returned when asking for small
// number of final results (eg 1) and only one shard. Setting shard_size higher helped.
// Alternative choices of significance algo didn't seem to be improvements....
//                        nextWaveSigTerms.significanceHeuristic(new GND.GNDBuilder(true));
//                        nextWaveSigTerms.significanceHeuristic(new ChiSquare.ChiSquareBuilder(false, true));
// Originally I thought users would always want the
// same number of results as listed in the include
// clause but it may be the only want the most
// significant e.g. in the lastfm example of
// plotting a single user's tastes and how that maps
// into a network showing only the most interesting
// band connections. So line below commmented out
// nextWaveSigTerms.size(includes.length);
// Map execution mode used because Sampler agg keeps us
// focused on smaller sets of high quality docs and therefore
// examine smaller volumes of terms
// nextWavePopularTerms.size(includes.length);
// Execute the search
// System.out.println(source);
// System.out.println(searchResponse);
// We think of the total scores as the energy-level pouring
// out of all the last hop's connections.
// Each new node encountered is given a score which is
// normalized between zero and one based on
// what percentage of the total scores its own score
// provides
// Signal output can be zero if we did not encounter any new
// terms as part of this stage
// Potentially run another round of queries to perform next"hop" - will terminate if no new additions
// Add new vertices and apportion share of total signal along
// connections
// Gather all matching terms into the graph and propagate
// signals
// There were no terms from the previous phase that needed pursuing
// As we travel further out into the graph we apply a
// decay to the signals being propagated down the various channels.
// Avoid self-joins
// Decay the signal by the weight attached to the source vertex
// We cannot (without further querying) determine an accurate number
// for the foreground count of the toVertex term - if we sum the values
// from each fromVertex term we may actually double-count occurrences so
// the best we can do is take the maximum foreground value we have observed
// Decay the signal by the weight attached to the source vertex
// Having let the signals from the last results rattle around the graph
// we have adjusted weights for the various vertices we encountered.
// Now we review these new additions and remove those with the
// weakest weights.
// A priority queue is used to trim vertices according to the size settings
// requested for each field.
// For each of the fields
// Nothing to trim
// Get the top vertices for this field
// Remove weak new nodes and their dangling connections from the main graph
//TODO right now we only trim down to the best N vertices. We might also want to offer
// clients the option to limit to the best M connections. One scenario where this is required
// is if the "from" and "to" nodes are a client-supplied set of includes e.g. a list of
// music artists then the client may be wanting to draw only the most-interesting connections
// between them. See https://github.com/elastic/x-plugins/issues/518#issuecomment-160186424
// I guess clients could trim the returned connections (which all have weights) but I wonder if
// we can do something server-side here
// Helper method - compute the total signal of all scores in the search results
// Signal is based on significance score
// don't count self joins (term A obviously co-occurs with term A)
// Signal is based on popularity (number of
// documents)
// don't count self joins (term A obviously co-occurs with term A)
// We can afford to build a Boolean OR query with individual
// boosts for interesting terms
// Too many terms - we need a cheaper form of query to execute this
/**
// Add any user-supplied criteria to the root query as a should clause
// If any of the root terms have an "include" restriction then
// we add a root-level MUST clause that
// mandates that at least one of the potentially many terms of
// interest must be matched (using a should array)
// Map execution mode used because Sampler agg
// keeps us focused on smaller sets of high quality
// docs and therefore examine smaller volumes of terms
// It is feasible that clients could provide a choice of
// significance heuristic at some point e.g:
// sigBuilder.significanceHeuristic(new
// PercentageScore.PercentageScoreBuilder());
// Min doc count etc really only applies when we are
// thinking about certainty of significance scores -
// perhaps less necessary when considering popularity
// termsBuilder.field(vr.fieldName()).shardMinDocCount(shardMinDocCount)
//       .minDocCount(minDocCount).executionHint("map").size(vr.size());
// Run the search
// System.out.println(source);
// Determine the total scores for all interesting terms
// Now gather the best matching terms and compute signal weight according to their
// share of the total signal strength
// Expand out from these root vertices looking for connections with other terms
// Helper method - Provides a total signal strength for all terms connected to the initial query
// Signal is based on significance score
// Signal is based on popularity (number of documents)
// Too many terms - we need a cheaper form of query to execute this
// We have a sufficiently low number of terms to use the per-term boosts.
// Lucene boosts are >=1 so we baseline the provided boosts to start
// from 1
// Actual resolution of timer is granularity of the interval
// configured globally for updating estimated time.
/*
/**
/*
/*
// Supply a doc ID for deterministic routing of docs to shards
// Ensure single segment with no deletes. Hopefully solves test instability in
// issue https://github.com/elastic/x-pack-elasticsearch/issues/918
// members of beatles
// friends of members of beatles
// Disable security otherwise authentication failures happen creating indices.
//        newSettings.put(XPackSettings.SECURITY_ENABLED.getKey(), false);
//        newSettings.put(XPackSettings.MONITORING_ENABLED.getKey(), false);
//        newSettings.put(XPackSettings.WATCHER_ENABLED.getKey(), false);
// Tests use of a client-provided query to steer exploration
// members of beatles
//70s friends of beatles
// friends of members of beatles
// members of beatles
//00s friends of beatles
//90s friends of friends of beatles
// Turning off the significance feature means we reward popularity
// members of beatles
// friends of members of beatles
// members of beatles
//00s friends of beatles
// A query that should cause a timeout
// Most of the test runs we reach dave in the allotted time before we hit our
// intended delay but sometimes this doesn't happen so I commented this line out.
// checkVertexDepth(response, 1, "dave");
// members of beatles
// friends of members of beatles
// *Very* rarely I think the doc delete randomization and background merges conspire to
// make this test fail. Scores vary slightly due to deletes I suspect.
/*
// Policy with the same name must exist in follower cluster too:
// Sanity check that following_index setting has been set, so that we can verify later that this setting has been unset:
// Ensure that 'index.lifecycle.indexing_complete' is replicated:
// ILM should have placed both indices in the warm phase and there these indices are read-only:
// ILM should have unfollowed the follower index, so the following_index setting should have been removed:
// (this controls whether the follow engine is used)
// Create the repository before taking the snapshot.
// start snapshot
// add policy and expect it to trigger unfollow immediately (while snapshot in progress)
// Ensure that 'index.lifecycle.indexing_complete' is replicated:
// ILM should have unfollowed the follower index, so the following_index setting should have been removed:
// (this controls whether the follow engine is used)
// Following index should have the document
// ILM should have completed the unfollow
// assert that snapshot succeeded
// Create a policy on the leader
// Policy with the same name must exist in follower cluster too:
// Set up an auto-follow pattern
// Create an index on the leader using the template set up above
// Check that the new index is created
// Check that it got replicated to the follower
// check that the alias was replicated
// Sanity check that following_index setting has been set, so that we can verify later that this setting has been unset:
// Wait for the index to roll over on the leader
// Wait for the next index should have been created on the leader
// And the old index should have a write block and indexing complete set
// Wait for the setting to get replicated to the follower
// ILM should have unfollowed the follower index, so the following_index setting should have been removed:
// (this controls whether the follow engine is used)
// The next index should have been created on the follower as well
// and the alias should be on the next index
// And the previously-follower index should be in the warm phase
// Clean up
// this policy won't exist on the leader, that's fine
// Create a policy with just a Shrink action on the follower
// Follow the index
// Make sure it actually took
// This should now be in the "warm" phase waiting for the index to be ready to unfollow
// Set the indexing_complete flag on the leader so the index will actually unfollow
// Wait for the setting to get replicated
// Wait for the index to continue with its lifecycle and be shrunk
// assert the aliases were replicated
// Wait for the index to complete its policy
// this policy won't exist on the leader, that's fine
// Create a policy with just a Shrink action on the follower
// Follow the index
// Make sure it actually took
// This should now be in the "warm" phase waiting for the index to be ready to unfollow
// Set the indexing_complete flag on the leader so the index will actually unfollow
// Wait for the setting to get replicated
// We can't reliably check that the index is unfollowed, because ILM
// moves through the unfollow and shrink actions so fast that the
// index often disappears between assertBusy checks
// Wait for the index to continue with its lifecycle and be shrunk
// Wait for the index to complete its policy
// Set up the policy and index, but don't attach the policy yet,
// otherwise it'll proceed through shrink before we can set up the
// follower
// Policy with the same name must exist in follower cluster too:
// Now we can set up the leader to use the policy
// Sanity check that following_index setting has been set, so that we can verify later that this setting has been unset:
// We should get into a state with these policies where both leader and followers are waiting on each other
// Index a bunch of documents and wait for them to be replicated
// Then make sure both leader and follower are still both waiting
// Manually set this to kick the process
// The shrunken index should now be created on the leader...
// And both of these should now finish their policies
// this policy won't exist on the leader, that's fine
// Set up the follower
// Pause ILM so that this policy doesn't proceed until we want it to
// Set indexing complete and wait for it to be replicated
// Remove remote cluster alias:
// Then add it back with an incorrect seed node:
// (unfollow api needs a remote cluster alias)
// Start ILM back up and let it unfollow
// Wait for the policy to be complete
// Ensure the "follower" index has successfully unfollowed
// Sometimes throw in an extraneous unfollow just to check it doesn't break anything
// Sometimes throw in an extraneous unfollow just to check it doesn't break anything
/*
/**
// create policy_1 and policy_2
// PUT policy_1 and policy_2
// create the test-index index and set the policy to policy_1
// wait for the shards to initialize
// Check the index is on the attempt rollover step
// Change the policy to policy_2
// Check the index is still on the attempt rollover step
// Index a single document
// Check the index goes to the warm phase and completes
// Check index is allocated on integTest-1 and integTest-2 as per policy_2
/*
// create policy
// update policy on index
// index document {"foo": "bar"} to trigger rollover
/*
// asserts that rollover was called
// asserts that shrink deleted the original index
// asserts that the delete phase completed for the managed shrunken index
// create policy
// update policy on index
// move to a step
// update policy on index
// move to a step
// index document to trigger rollover
/*
// asserts that rollover was called
// asserts that shrink deleted the original index
// asserts that the delete phase completed for the managed shrunken index
// update policy to be correct
// retry step
// assert corrected policy is picked up and index is shrunken
// create policy
// update policy on index
// index document {"foo": "bar"} to trigger rollover
// create policy
// update policy on index
// index document {"foo": "bar"} to trigger rollover
// Create the repository before taking the snapshot.
// create delete policy
// create index without policy
// index document so snapshot actually does something
// start snapshot
// add policy and expect it to trigger delete immediately (while snapshot in progress)
// assert that index was deleted
// assert that snapshot is still in progress and clean up
// Create the repository before taking the snapshot.
// create delete policy
// create index without policy
// required so the shrink doesn't wait on SetSingleNodeAllocateStep
// index document so snapshot actually does something
// start snapshot
// add policy and expect it to trigger shrink immediately (while snapshot in progress)
// assert that index was shrunk and original index was deleted
// assert that snapshot succeeded
// Create the repository before taking the snapshot.
// create delete policy
// create index without policy
// index document so snapshot actually does something
// start snapshot
// add policy and expect it to trigger delete immediately (while snapshot in progress)
// assert that the index froze
// assert that snapshot is still in progress and clean up
// Set up a policy with rollover
// Index a document
// Wait for rollover to happen
// Remove the policy from the original index
// Add the policy again
// Wait for everything to be copacetic
// Move to a step from the injected unfollow action
// If we get an OK on this request we have successfully moved to the injected step
// Make sure we actually move on to and execute the shrink action
// Move to the same step, which should re-read the policy
// Make sure we actually rolled over
// Re-start ILM so that subsequent tests don't fail
// Create a "shrink-only-policy"
// PUT policy
// create index without alias so the rollover action fails and is retried
// create the index as readonly and associate the ILM policy to it
// wait for ILM to start retrying the step
// remove the read only block
// index is not readonly so the ILM should complete successfully
// Set up a policy with rollover
// Index a document
// Manual rollover
// Index another document into the original index so the ILM rollover policy condition is met
// Wait for the rollover policy to execute
// ILM should manage the second index after attempting (and skipping) rolling the original index
// index some documents to trigger an ILM rollover
// ILM should rollover the second index even though it skipped the first one
// create the rolled index so the rollover of the first index fails
// Using {@link #waitUntil} here as ILM moves back and forth between the {@link WaitForRolloverReadyStep} step and
// {@link org.elasticsearch.xpack.core.ilm.ErrorStep} in order to retry the failing step. As {@link #assertBusy}
// increases the wait time between calls exponentially, we might miss the window where the policy is on
// {@link WaitForRolloverReadyStep} and the move to `attempt-rollover` request will not be successful.
// Similar to above, using {@link #waitUntil} as we want to make sure the `attempt-rollover` step started failing and is being
// retried (which means ILM moves back and forth between the `attempt-rollover` step and the `error` step)
// the rollover step should eventually succeed
// moving ILM to the "update-rollover-lifecycle-date" without having gone through the actual rollover step
// the "update-rollover-lifecycle-date" step will fail as the index has no rollover information
// manual rollover the index so the "update-rollover-lifecycle-date" ILM step can continue and finish successfully as the index
// will have rollover information now
//github.com/elastic/elasticsearch/issues/50353")
// Index a document
//github.com/elastic/elasticsearch/issues/50353")
// Index a document
//github.com/elastic/elasticsearch/issues/50353")
// Index should be created and then deleted by ILM
// Stop ILM so that the initialize step doesn't run
// Create the index with the origination parsing turn *off* so it doesn't prevent creation
// Wait until an error has occurred.
// Turn origination date parsing back off
// Index a document
// Wait for the index to enter the check-rollover-ready step
// Update the policy to allow rollover at 1 document instead of 100
// Index should now have been able to roll over, creating the new index and proceeding to the "complete" step
// This method should be called inside an assertBusy, it has no retry logic of its own
// This method should be called inside an assertBusy, it has no retry logic of its own
// For a failure, print out whatever history we *do* have for the index
// Throw AssertionError instead of an exception if the search fails so that assertBusy works as expected
// Finally, check that the history index is in a good state
// PUT policy
// wait for the shards to initialize
// wait for the shards to initialize
/*
// Create a snapshot repo
// Check that the snapshot was actually taken
// Check that the last success date was written to the cluster state
// Check that the stats are written
// Create a policy with ignore_unvailable: false and an index that doesn't exist
// Check that the failure is written to the cluster state
//github.com/elastic/elasticsearch/issues/48531")
// Create a snapshot repo
// Check that the executed snapshot is created
// Create a snapshot repo
// Stop SLM so nothing happens
// Check that the executed snapshot is created
// Sleep for up to a second, but at least 1 second since we scheduled the policy so we can
// ensure it *would* have run if SLM were running
// Retention and the manually executed policy should still have run,
// but only the one we manually ran.
//github.com/elastic/elasticsearch/issues/48017")
// Create a snapshot repo
// Create a policy with a retention period of 1 millisecond
// Manually create a snapshot
// Check that the executed snapshot is created
// Run retention every second
// Check that the snapshot created by the policy has been removed by retention
// We expect a failed response because the snapshot should not exist
// Unset retention
// Create a snapshot repo
// Create a policy with a retention period of 1 millisecond
// Check that the executed snapshot is created
// Wait for stats to be updated
/**
// This method should be called inside an assertBusy, it has no retry logic of its own
// Throw AssertionError instead of an exception if the search fails so that assertBusy works as expected
// Finally, check that the history index is in a good state
/*
// as default timeout seems not enough on the jenkins VMs
/*
/**
/**
// test_ilm user does not have permissions on this index
// Set up two roles and users, one for reading SLM, another for managing SLM
/*\", \"cluster:admin/snapshot/*\"]," +
// Build two high level clients, each using a different user
// great, we want it to not exist
// test_ilm user has permissions to view
/**
//github.com/elastic/elasticsearch/issues/41440")
//github.com/elastic/elasticsearch/issues/41440")
/*
// test_user: index docs using alias in the newly created index
// wait so the ILM policy triggers rollover action, verify that the new index exists
// test_user: index docs using alias, now should be able write to new index
// verify that the doc has been indexed into new write index
/*
/**
// This index doesn't exist any more, there's nothing to execute currently
// We can do cluster state steps all together until we
// either get to a step that isn't a cluster state step or a
// cluster state wait step returns not completed
// cluster state action step so do the action and
// move the cluster state to the next step
// set here to make sure that the clusterProcessed knows to execute the
// correct step if it an async action
// set here to make sure that the clusterProcessed knows to execute the
// correct step if it an async action
// cluster state wait step so evaluate the
// condition, if the condition is met move to the
// next step, if its not met return the current
// cluster state so it can be applied and we will
// wait for the next trigger to evaluate the
// condition again
// We may have executed a step and set "nextStepKey" to
// a value, but in this case, since the condition was
// not met, we can't advance any way, so don't attempt
// to run the current step
// There are actions we need to take in the event a phase
// transition happens, so even if we would continue in the while
// loop, if we are about to go into a new phase, return so that
// other processing can occur
// either we are no longer the master or the step is now
// not the same as when we submitted the update task. In
// either case we don't want to do anything now
// After the cluster state has been processed and we have moved
// to a new step, we need to conditionally execute the step iff
// it is an `AsyncAction` so that it is executed exactly once.
/*
// overridable by tests
// This registers a cluster state listener, so appears unused but is not.
// the template registry is a cluster state listener
// Custom Metadata
// Lifecycle Types
// Lifecycle Actions
/*
/*
/**
/**
/**
/**
// Only phase changing and async wait steps should be run through periodic polling
// Only proceed to the next step if enough time has elapsed to go into the next phase
/**
/**
// Delete needs special handling, because after this step we
// will no longer have access to any information about the
// index since it will be... deleted.
/**
// Only proceed to the next step if enough time has elapsed to go into the next phase
/**
/**
/**
/**
/**
/**
// This index may have been deleted and has no metadata, so ignore it
/**
// Register that the delete phase is now "complete"
/**
// This index may have been deleted and has no metadata, so ignore it
/*
/**
/**
// We manually validate here, because any API must correctly specify the current step key
// when moving to an arbitrary step key (to avoid race conditions between the
// check-and-set). moveClusterStateToStep also does its own validation, but doesn't take
// the user-input for the current step (which is why we validate here for a passed in step)
// true until proven false by a run policy
// If we just became master, we need to kick off any async actions that
// may have not been run due to master rollover
// ILM is trying to stop, but this index is in a Shrink step (or other dangerous step) so we can't stop
// Don't rethrow the exception, we don't want a failure for one index to be
// called to cause actions not to be triggered for further indices
// pkg-private for testing
// pkg-private for testing
// don't create scheduler if the node is shutting down
// scheduler could be null if the node might be shutting down
// only act if we are master, otherwise
// keep idle until elected
/**
// true until proven false by a run policy
// loop through all indices in cluster state and filter for ones that are
// managed by the Index Lifecycle Service they have a index.lifecycle.name setting
// associated to a policy
// ILM is trying to stop, but this index is in a Shrink step (or other dangerous step) so we can't stop
// Don't rethrow the exception, we don't want a failure for one index to be
// called to cause actions not to be triggered for further indices
// this assertion is here to ensure that the check we use in maybeScheduleJob is accurate for detecting a shutdown in
// progress, which is that the cluster service is stopped and closed at some point prior to closing plugins
/**
/*
/**
/**
// policy could be updated in-between execution
/**
/**
// as an initial step we'll mark the failed step as auto retryable without actually looking at the cause to determine
// if the error is transient/recoverable from
// maintain the retry count of the failed step as it will be cleared after a successful execution
/**
// manual retries don't update the retry count
/**
// clear any step info or error-related settings from the current step
// The "new" phase is the initialization phase, usually the phase
// time would be set on phase transition, but since there is no
// transition into the "new" phase, we set it any time in the "new"
// phase
/**
/**
// This index doesn't exist anymore, we can't do anything
/**
// Index doesn't exist so fail it
/**
/*
/*
/**
// Doesn't close the wrapped client since this client object is shared
// among multiple instances
/*
// Index must have been since deleted, ignore it
// either the policy has changed or the step is now
// not the same as when we submitted the update task. In
// either case we don't want to do anything now
/*
// Index must have been since deleted, ignore it
// either the policy has changed or the step is now
// not the same as when we submitted the update task. In
// either case we don't want to do anything now
/*
/*
/**
/*
// keeps track of existing policies in the cluster state
// keeps track of what the first step in a policy is, the key is policy name
// keeps track of a mapping from policy/step-name to respective Step, the key is policy name
// Use a non-diffable value serializer. Otherwise actions in the same
// action and phase that are changed show up as diffs instead of upserts.
// We want to treat any change in the policy as an upsert so the map is
// correctly rebuilt
// This is never called
// This is never called
// It is ok to re-use potentially modified policy here since we are in an initialization or completed phase
// if the current phase definition describes an internal step/phase, do not parse
// Build a list of steps that correspond with the phase the index is currently in
// parse phase steps from the phase definition in the index settings
// Return the step that matches the given stepKey or else null if we couldn't find it
/**
// These built in phases should never wait
// We don't have that phase registered, proceed right through it
/*
// Index must have been since deleted, ignore it
// either the policy has changed or the step is now
// not the same as when we submitted the update task. In
// either case we don't want to do anything now
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// very lightweight operation, no need to fork
// parse existing phase steps from the phase definition in the index settings
// If this is requesting only errors, only include indices in the error step or which are using a nonexistent policy
/*
// if no policies explicitly provided, behave as if `*` was specified
/*
// no need to actually install metadata just yet, but safe to say it is not stopped
/*
// The index has somehow been deleted - there shouldn't be any opportunity for this to happen, but just in case.
/*
/**
// headers from the thread context stored by the AuthenticationService to be shared between the
// REST layer and the Transport layer here must be accessed within this thread and not in the
// cluster state thread in the ClusterStateUpdateTask below since that thread does not share the
// same context, and therefore does not have access to the appropriate security headers.
// first time using index-lifecycle feature, bootstrap metadata
// Revert to the non-refreshed state
/**
/**
/**
// The index is on a step that doesn't exist in the new policy, we
// can't safely re-read the JSON
// The new and old phase have the same stepkeys for this current phase, so we can
// refresh the definition because we know it won't change the execution flow.
/**
/**
// No need to update anything if the policies are identical in contents
/*
/*
// The index has somehow been deleted - there shouldn't be any opportunity for this to happen, but just in case.
/*
/*
/*
/**
// In the unlikely case that we cannot generate an exception string,
// try the best way can to encapsulate the error(s) with at least
// the message
/*
/**
// Prior to actually performing the bulk, we should ensure the index exists, and
// if we were unable to create it or it was in a bad state, we should not
// attempt to index documents.
/**
// TODO: remove the threadpool wrapping when the .add call is non-blocking
//  (it can currently execute the bulk request occasionally)
//  see: https://github.com/elastic/elasticsearch/issues/50440
/**
// No alias or index exists with the expected names, so create the index with appropriate alias
// The index didn't exist before we made the call, there was probably a race - just ignore this
// alias does not exist but initial index does, something is broken
// The alias exists and has a write index, so we're good
// The alias does not have a write index, so we can't index into it
// This is not an alias, error out
/*
/**
// history (please add a comment why you increased the version here)
// version 1: initial
/*
/**
/*
/*
/*
/**
// TODO: change this not to use 'this'
// SLM is currently stopped, so don't schedule jobs
// Only used for testing
/**
/**
/**
// Retrieve all of the expected policy job ids from the policies in the metadata
// Cancel all jobs that are *NOT* in the scheduled tasks map
/**
// Find and cancel any existing jobs for this policy
// Find all jobs matching the `jobid-\d+` pattern
// Filter out a job that has not been changed (matches the id exactly meaning the version is the same)
// Cancel existing job so the new one can be scheduled
// Now atomically schedule the new job and add it to the scheduled tasks map. If the jobId
// is identical to an existing job (meaning the version has not changed) then this does
// not reschedule it.
/**
/**
/**
/**
/*
// Would be cleaner if we could use Optional#ifPresentOrElse
/**
// Check that there are no failed shards, since the request may not entirely
// fail, but may still have failures (such as in the case of an aborted snapshot)
// Add each failed shard's exception as suppressed, the exception contains
// information about which shard failed
// Call the failure handler to register this as a failure and persist it
// This shouldn't happen unless there's an issue with serializing the original exception, which shouldn't happen
/**
/**
/*
/**
// The schedule has changed, so reschedule the retention job
// Only used for testing
// The schedule has been unset, so cancel the scheduled retention job
/**
/*
/**
// Skip running retention if SLM is disabled, however, even if it's
// disabled we allow manual running.
// Defined here so it can be re-used without having to repeat it
// Find all SLM policies that have retention enabled
// For those policies (there may be more than one for the same repo),
// return the repos that we need to get the snapshots for
// Finally, asynchronously retrieve all the snapshots, deleting them serially,
// before updating the cluster state with the new metrics and setting 'running'
// back to false
// Find all the snapshots that are past their retention date
// Finally, delete the snapshots that need to be deleted
// This snapshot has no metadata, it is not eligible for deletion
// policyId was null in the metadata, so it's not eligible
// This snapshot was taking by a policy that doesn't exist, so it's not eligible
// Retention is not configured
// Retrieve the predicate based on the retention policy, passing in snapshots pertaining only to *this* policy and repository
// Skip retrieving anything if there are no repositories to fetch
// Only return snapshots in the SUCCESS state
/**
// This shouldn't happen unless there's an issue with serializing the original exception
// Check whether we have exceeded the maximum time allowed to spend deleting
// snapshots, if we have, short-circuit the rest of the deletions
/**
// Deletes cannot occur simultaneously, so wait for this
// deletion to complete before attempting the next one
// Cannot delete during a snapshot
// Cannot delete during an existing delete
// Cannot delete while a repository is being cleaned
// Cannot delete during a restore
// It's okay to delete snapshots
/**
// This means the cluster is being shut down, so nothing to do here
/*
/**
/*
/**
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// Check that the policy exists in the first place
/*
/*
/*
// no need to actually install metadata just yet, but safe to say it is not stopped
/*
/*
/*
// headers from the thread context stored by the AuthenticationService to be shared between the
// REST layer and the Transport layer here must be accessed within this thread and not in the
// cluster state thread in the ClusterStateUpdateTask below since that thread does not share the
// same context, and therefore does not have access to the appropriate security headers.
/*
/*
/*
/**
/*
// Reset the index to use the "allClusterPolicyName"
// Unset the index's phase/action/step to simulate starting from scratch
/*
/*
/*
// This is necessary to prevent ILM and SLM installing a lifecycle policy, these tests assume a blank slate
// start master node
// test get-lifecycle behavior when IndexLifecycleMetaData is null
// assert version and modified_date
// start node
// using AtomicLong only to extract a value from a lambda rather than the more traditional atomic update use-case
// set the origination date setting to an older value
// set the origination date setting to null
// complete the step
// start node
// disabling the lifecycle parsing would maintain the parsed value as that was set as the origination date
// setting the lifecycle origination date setting to null should make the lifecyle date fallback on the index creation date
// setting the lifecycle origination date to an explicit value overrides the date parsing
// start master node
// start data node
// check that the scheduler was started on the appropriate node
// start one server
// start another server
// first wait for 2 nodes in the cluster
// check step in progress in lifecycle
// this checks that the phase execution is picked up from the phase definition settings
// kill the first server
// check that index lifecycle picked back up where it
// complete the step
// start master node
// assert version and modified_date
// assert ILM is still stopped
// update the poll interval
/*
/*
// The cluster state can take a few extra milliseconds to update after the steps are executed
// The cluster state can take a few extra milliseconds to update after the steps are executed
// State changes should not run AsyncAction steps
// Wait for the cluster state action step
// Wait for the async action step
// verify that no exception is thrown
// First step is retrieved because there are no settings for the index
// The step that was written into the metadata is retrieved
// With no time, always transition
// Index is not old enough to transition
// Set to the fuuuuuttuuuuuuurre
// Come back to the "present"
/** A real policy steps registry where getStep can be overridden so that JSON doesn't have to be parsed */
/*
// test all the shrink action steps that ILM can be stopped during (basically all of them minus the actual shrink)
// Check that ILM can stop when in the shrink action on the provided step
// disabling the parsing origination date setting should prevent the validation from throwing exception
/*
// test going from null lifecycle settings to next step
// test going from set currentStep settings to nextStep
/*
/*
/**
/*
/*
/**
/*
/*
/*
/*
// start with empty registry
// add new policy
// remove policy
// add new policy
// swap out policy
// TODO(talevy): assert changes... right now we do not support updates to policies. will require internal cleanup
// Modify the policy
// start with empty registry
// add new policy
// Update the policy with the new policy, but keep the phase the same
// Update the policies
/*
/*
/*
// start with random epoch between 1/1/1970 and 31/12/2035 so that start is not
// so large such that (start + interval) > Long.MAX
/*
// Success case, it can be updated even though the configuration for the
// rollover and set_priority actions has changed
// Failure case, can't update because the step we're currently on has been removed in the new policy
// Failure case, can't update because the future step has been deleted
// Failure case, index doesn't have enough info to check
// Failure case, the phase JSON is unparseable
// Check that no other execution state changes have been made
// Check that the phase definition has been refreshed
// No change, because the policies were identical
// No change, because the index doesn't have a lifecycle.name setting for this policy
// Check that no other execution state changes have been made
// Check that the phase definition has been refreshed
/*
/*
// The content of this BulkResponse doesn't matter, so just make it have the same number of responses
// The content of this BulkResponse doesn't matter, so just make it have the same number of responses with failures
/**
/*
/**
// Create a snapshot repo
// Check that the executed snapshot shows up in the SLM output
// Cancel/delete the snapshot
// ignore
// Create a snapshot and wait for it to be complete (need something that can be deleted)
// Wait for all running snapshots to be cleared from cluster state
// Take another snapshot, but before doing that, block it from completing
// Check that the executed snapshot shows up in the SLM output as in_progress
// Run retention
// Check that the snapshot created by the policy has been removed by retention
// Trigger a cluster state update so that it re-checks for a snapshot in progress
// Great, we wanted it to be deleted!
// Cancel the ongoing snapshot (or just delete it if it finished)
// just wait and retry
// Assert that the history document has been written for taking the snapshot and deleting it
// Setup
// Create a snapshot repo
// Create a failed snapshot
// Run retention - we'll check the results later to make sure it's had time to run.
// Take a successful snapshot
// Check that the failed snapshot from before still exists, now that retention has run
// Run retention again and make sure the failure was deleted
// This is what we want to happen
// Concurrent status calls and write operations may lead to failures in determining the current repository generation
// TODO: Remove this hack once tracking the current repository generation has been made consistent
// Create a snapshot repo
// Check that the executed snapshot shows up in the SLM output
// This is what we want to happen
// Concurrent status calls and write operations may lead to failures in determining the current repository generation
// TODO: Remove this hack once tracking the current repository generation has been made consistent
// Cancel/delete the snapshot
// ignore
// Convert this to an AssertionError so that it can be retried in an assertBusy - this is often a transient error because
// concurrent status calls and write operations may lead to failures in determining the current repository generation
// TODO: Remove this hack once tracking the current repository generation has been made consistent
/**
/*
/*
// Fri Mar 15 2019 21:09:06 UTC
// 67 is the smallest field count with these sizes that causes an error
// chosen arbitrarily
// chosen arbitrarily
/* bytes of overhead per key/value pair */) + 1;
/*
// Since the service does not think it is master, it should not be triggered or scheduled
// Since the service is stopping, jobs should have been cancelled
// Since the service is stopped, jobs should have been cancelled
// No jobs should be scheduled when service is closed
/**
//github.com/elastic/elasticsearch/issues/44997")
// Since the service does not think it is master, it should not be triggered or scheduled
// Change the service to think it's on the master node, events should be scheduled now
// Make sure the job got updated
// Create a state simulating the policy being deleted
// The existing job should be cancelled and no longer trigger
// When the service is no longer master, all jobs should be automatically cancelled
// Make sure at least one triggers and the job is scheduled
// Signify becoming non-master, the jobs should all be cancelled
/**
/*
// Trigger the event, but since the job name does not match, it should
// not run the function to create a snapshot
// This verifying client will verify that we correctly invoked
// client.admin().createSnapshot(...) with the appropriate
// request. It also returns a mock real response
// Trigger the event with a matching job name for the policy
// Trigger the event with a matching job name for the policy
/**
/*
// Service should not scheduled any jobs once closed
/*
// Test with no SLM metadata
// Test with empty SLM metadata
// Test with metadata containing only a policy without retention
// Test with metadata containing a couple of policies
// Test when user metadata is null
// Test when no retention is configured
// Test when user metadata is a map that doesn't contain "policy"
// Test with an ancient snapshot that should be expunged
// Test with a snapshot that's start date is old enough to be expunged (but the finish date is not)
// Test with a fresh snapshot that should not be expunged
// We're expected two deletions before they hit the "taken too long" test, so have a latch of 2
// Don't pause until snapshot 2
/*
/**
//internal representation of typeless templates requires the default "_doc" type, which is also required for internal templates
/*
/*
/*
/*
/*
/*
/*
/**
/**
// We require range queries to specify both bounds because an unbounded query could incorrectly match
// values from other keys. For example, a query on the 'first' key with only a lower bound would become
// ("first\0value", null), which would also match the value "second\0value" belonging to the key 'second'.
/**
/**
/*
/**
// Note that we throw an exception here just to be safe. We don't actually expect to reach
// this case, since XContentParser verifies that the input is well-formed as it parses.
/*
/**
/**
/**
/**
/**
/**
/**
/**
/*
// Add a flattened object field.
// Add a short alias to that field.
// Add a longer alias to that field.
// Update the long alias to refer to a non-flattened object field.
/*
// Check the root fields.
// Check the keyed fields.
// Check that there is no 'field names' field.
// First verify the default behavior when depth_limit is not set.
// Set a lower value for depth_limit and check that the field is rejected.
// First verify the default behavior when ignore_above is not set.
// Set a lower value for ignore_above and check that the field is skipped.
/*
/*
// Set up the index service.
// Add some documents.
// Load global field data for subfield 'key'.
// Load global field data for the subfield 'other_key'.
/*
//www.elastic.co")
// Check that queries are split on whitespace.
//www.elastic.co")
// Add a random number of documents containing a flat object field, plus
// a small number of dummy documents.
// Test the root flat object field.
// Test two keyed flat object fields.
// linear counting should be picked, and should be accurate
// error is not bound, so let's just make sure it is > 0
// Aggregate on the root 'labels' field.
// Aggregate on the 'priority' subfield.
// Aggregate on the 'release' subfield.
// Aggregate on the 'priority' subfield with a min_doc_count of 0.
//www.elastic.co");
// Check 'include' filtering.
// Check 'exclude' filtering.
//www.elastic.co"));
/*
// Do not include the term 'avocado' in the mock document.
// Nothing to do.
/*
/*
/*
// The job name is invalid because it contains a space
// If validation of the invalid job is not done until after transportation to the master node then the
// root cause gets reported as a remote_transport_exception.  The code in PubJobAction is supposed to
// validate before transportation to avoid this.  This test must be done in a multi-node cluster to have
// a chance of catching a problem, hence it is here rather than in the single node integration tests.
// Ensure all data is searchable
// feed some more data points
// unintuitive: should return the earliest record timestamp of this feed???
// counts should be summed up
/*
/**
/*
// Cannot use expectThrows here because blacklisted tests will throw an
// InternalAssumptionViolatedException rather than an AssertionError
// Some tests assert on searches of wildcarded ML indices rather than on ML endpoints.  For these we expect no hits.
/*
/*
/**
// We should have got here if and only if the only ML endpoints in the test were in the allowed list
/*
/**
//github.com/elastic/elasticsearch/issues/32033\")", Constants.WINDOWS);
// Set the memory limit to 30MB
// It's important that the values used here are either always represented in less than 16 UTF-8 bytes or
// always represented in more than 22 UTF-8 bytes.  Otherwise platform differences in when the small string
// optimisation is used will make the results of this test very different for the different platforms.
// Assert we haven't violated the limit too much
// Set the memory limit to 30MB
// It's important that the values used here are either always represented in less than 16 UTF-8 bytes or
// always represented in more than 22 UTF-8 bytes.  Otherwise platform differences in when the small string
// optimisation is used will make the results of this test very different for the different platforms.
// Assert we haven't violated the limit too much
// Set the memory limit to 30MB
// It's important that the values used here are either always represented in less than 16 UTF-8 bytes or
// always represented in more than 22 UTF-8 bytes.  Otherwise platform differences in when the small string
// optimisation is used will make the results of this test very different for the different platforms.
// Assert we haven't violated the limit too much
// Set the memory limit to 110MB
// It's important that the values used here are either always represented in less than 16 UTF-8 bytes or
// always represented in more than 22 UTF-8 bytes.  Otherwise platform differences in when the small string
// optimisation is used will make the results of this test very different for the different platforms.
// Assert we haven't violated the limit too much
/*
/**
//github.com/elastic/elasticsearch/issues/44613", Constants.WINDOWS);
// Setting the record score to 10.0, to avoid the low score records due to multibucket trailing effect
// This is the key assertion: if renormalization never happened then the record_score would
// be the same as the initial_record_score on the anomaly record that happened earlier
// First anomaly is 10 events
// Second anomaly is 100, should get the highest score and should bring the first score down
/*
// Get the job stats
/*
/**
// To compare Java/C++ tokenization performance:
// 1. Change false to true in this assumption
// 2. Run the test several times
// 3. Change MachineLearning.CATEGORIZATION_TOKENIZATION_IN_JAVA to false
// 4. Run the test several more times
// 5. Check the timings that get logged
// 6. Revert the changes to this assumption and MachineLearning.CATEGORIZATION_TOKENIZATION_IN_JAVA
/*
/*
// Let's just assert there's both training and non-training results
//
// Wait until state is one of REINDEXING or ANALYZING, or until it is STOPPED.
// Now let's start it again
// That means the job had managed to complete
// Index one more document with a class different than the two already used.
// Index one more document with a class different than the two already used.
// Should not throw
// We use 100 rows as we can't set this too low. If too low it is possible
// we only train with rows of one of the two classes which leads to a failure.
// Let's run both jobs in parallel and wait until they are finished
// Now we compare they both used the same training rows
// Call _delete_expired_data API and check nothing was deleted
// Delete the config straight from the config index
// Now calling the _delete_expired_data API should remove unused state
/**
// Assert that all the predicted class names come from the set of dependent variable values.
// Assert that the first class listed in top classes is the same as the predicted class.
// Assert that all the class probabilities lie within [0, 1] interval.
// Assert that the top classes are listed in the order of decreasing scores.
// Accuracy
// MulticlassConfusionMatrix
// Precision
// Recall
/*
// Datafeed did not do anything yet, hence search_count is equal to 0.
// Datafeed processed numDocs documents so search_count must be greater than 0.
// Datafeed did not do anything yet, hence search_count is equal to 0.
// Datafeed processed numDocs documents so search_count must be greater than 0.
// Change something different than jobId, here: queryDelay.
// Search_count is still greater than 0 (i.e. has not been reset by datafeed update)
// Datafeed should auto-stop...
// ...and should have auto-closed the job too
// It's practically impossible to assert that a stop request has waited
// for a concurrently executing request to finish before returning.
// But we can assert the data feed has stopped after the request returns.
// The idea is to hit the situation where one request waits for
// the other to complete. This is difficult to schedule but
// hopefully it will happen in CI
// The UI now force deletes datafeeds, which means they can be deleted while running.
// The first step is to isolate the datafeed.  But if it was already being stopped then
// the datafeed may not be running by the time the isolate action is executed.  This
// test will sometimes (depending on thread scheduling) achieve this situation and ensure
// the code is robust to it.
// This is OK - it means the thread running the delete fully completed before the stop started to execute
/**
// Use lots of chunks so we have time to stop the lookback before it completes
// At this point, stopping the datafeed will have submitted a request for the job to close.
// Depending on thread scheduling, the following kill request might overtake it.  The Thread.sleep()
// call here makes it more likely; to make it inevitable for testing also add a Thread.sleep(10)
// immediately before the checkProcessIsAlive() call in AutodetectCommunicator.close().
// This should close very quickly, as we killed the process.  If the job goes into the "failed"
// state that's wrong and this test will fail.
/*
// This user has admin rights on machine learning, but (importantly for the tests) no rights
// on any of the data indexes
// This user has admin rights on machine learning, and read access to the network-data index
// space in 'time stamp' is intentional
// Create index with source = enabled, doc_values = enabled, stored = false + multi-field
// space in 'time stamp' is intentional
// Create index with source = enabled, doc_values = disabled (except time), stored = false
// Create index with source = disabled, doc_values = enabled (except time), stored = true
// Create index with nested documents
// Create index with multiple docs per time interval for aggregation testing
// space in 'time stamp' is intentional
// Create index with source = enabled, doc_values = enabled, stored = false + multi-field
// This should be disallowed, because even though the ml_admin user has permission to
// create a datafeed they DON'T have permission to search the index the datafeed is
// configured to read
//want to search, but no admin access
// This should be disallowed, because ml_admin is trying to preview a datafeed created by
// by another user (x_pack_rest_user in this case) that will reveal the content of an index they
// don't have permission to search directly
// The derivative agg won't have values for the first bucket of each host
// At the time we create the datafeed the user can access the network-data index that we have access to
// Change the role so that the user can no longer access network-data
// We expect that no data made it through to the job
// There should be a notification saying that there was a problem extracting data
// At the time we create the datafeed the user can access the network-data index that we have access to
// Change the role so that the user can no longer access network-data
// There should be a notification saying that there was a problem extracting data
// Model state should be persisted at the end of lookback
// test a model snapshot is present
// Don't check rollup jobs because we clear them in the superclass.
/*
// A job with a bucket_span of 2s
// Datafeed with aggs
// Create stuff and open job
// Now let's index the data
// Index a doc per second from a minute ago to a minute later
// And start datafeed in real-time mode
// Wait until we finalize a bucket after now
// Wrap up
// Assert we have not dropped any data - final buckets should contain 2 events each
/*
// Get the latest finalized bucket
// Simply adding data within the current delayed data detection, the choice of 43100000 is arbitrary and within the window
// for the DatafeedDelayedDataDetector
// Assert that the are returned in order
// Get the latest finalized bucket
// Write our missing data in the bucket right before the last finalized bucket
// Assert that the are returned in order
// Get the latest finalized bucket
// Simply adding data within the current delayed data detection, the choice of 43100000 is arbitrary and within the window
// for the DatafeedDelayedDataDetector
// Assert that the are returned in order
/*
// We are going to create data for last 2 days
// Tests that nothing goes wrong when there's nothing to delete
// Index some unused state documents (more than 10K to test scrolling works)
// Start all jobs
// Run up to a day ago
// Now let's wait for all jobs to be closed
// Update snapshot timestamp to force it out of snapshot retention window
// Now let's create some forecasts
// We must set a very small value for expires_in to keep this testable as the deletion cutoff point is the moment
// the DeleteExpiredDataAction is called.
// Refresh to ensure the snapshot timestamp updates are visible
// We need to wait a second to ensure the second time around model snapshots will have a different ID (it depends on epoch seconds)
// FIXME it would be better to wait for something concrete instead of wait for time to elapse
// Run up to now
// Verify forecasts were created
// Before we call the delete-expired-data action we need to make sure the unused state docs were indexed
// Now call the action under test
// no-retention job should have kept all data
// Verify short expiry forecasts were deleted only
// Verify .ml-state doesn't contain unused state documents
// Assert at least one state doc for each job
// We need to refresh to ensure the updates are visible
/*
/**
// each half of the buckets contains one anomaly for each by field value
// push the data for the first half buckets
// Update rules so that the anomalies suppression is inverted
// push second half
// Let's send a bunch of random IPs with counts of 1
// Now send anomalous counts for our filtered IPs plus 333.333.333.333
// Some more normal buckets
// Now let's update the filter
// Wait until the notification that the filter was updated is indexed
// Send another anomalous bucket
// Some more normal buckets
// We have 2 IPs and they're both safe-listed.
// Ignore if ip in safe list AND actual < 10.
// First, 20 buckets with a count of 1 for both IPs
// Now send anomalous count of 9 for 111.111.111.111
// and 10 for 222.222.222.222
// Some more normal buckets
/*
// To test the source query is applied when we extract data,
// we set up a job where we have a query which excludes all but one document.
// We then assert the memory estimation is low enough.
// We insert one odd value out of 5 for one feature
/*
// Now we can start doing forecast requests
// Now let's verify forecasts
// Set the memory limit to 30MB
//github.com/elastic/elasticsearch/issues/44609", Constants.WINDOWS);
// flushing the job forces an index refresh, see https://github.com/elastic/elasticsearch/issues/31173
// run forecast a 2nd time
/*
/*
/**
// We should have 2 interim records
// Second batch
// This should fix the mean for 'foo'
// Then advance time and send normal data to force creating final results for previous bucket
// No other interim results either
/*
// push some data, flush job, verify no interim results
// push some more data, flush job, verify no interim results
// push some data up to a 1/4 bucket boundary, flush (with interim), check interim results
// We might need to retry this while waiting for a refresh
// push 1 more record, flush (with interim), check same interim result
// push rest of data, close, verify no interim results
// Verify interim results have been replaced with finalized results
// push some data, flush job, verify no interim results
// advance time and request interim results
// We expect there are no records. The bucket count is low but at the same time
// it is too early into the bucket to consider it an anomaly. Let's verify that.
/*
// Explicit _all
// Implicit _all
// Explicit _all
// Implicit _all
// tests the _xpack/usage endpoint
// With security enabled GET _aliases throws an index_not_found_exception
// if no aliases have been created. In multi-node tests the alias may not
// appear immediately so wait here.
// Use _cat/indices/.ml-anomalies-* instead of _cat/indices/_all to workaround https://github.com/elastic/elasticsearch/issues/45652
//create jobId1 docs
//create jobId2 docs
// check that indices still exist, but no longer have job1 entries and aliases are gone
//job2 still exists
// Delete the second job and verify aliases are gone, and original concrete/custom index is gone
// Check the index mapping contains the first by_field_name
// Check the index mapping now contains both fields
// Check the index mapping contains the first by_field_name
// Check the index mapping now contains both fields
// we should get the friendly advice nomatter which way around the clashing fields are seen
// Use _cat/indices/.ml-anomalies-* instead of _cat/indices/_all to workaround https://github.com/elastic/elasticsearch/issues/45652
// check that the index still exists (it's shared by default)
// check that the job itself is gone
// documents related to the job do not exist yet
// TimingStats doc exists, 2 buckets have been processed
// when job is being deleted, it also deletes all related documents from the shared index
// check that the TimingStats documents got deleted
// check that the job itself is gone
// Use _cat/indices/.ml-anomalies-* instead of _cat/indices/_all to workaround https://github.com/elastic/elasticsearch/issues/45652
// Wait for task to complete
// check that the index still exists (it's shared by default)
// check that the job itself is gone
// Use _cat/indices/.ml-anomalies-* instead of _cat/indices/_all to workaround https://github.com/elastic/elasticsearch/issues/45652
// Manually delete the index so that we can test that deletion proceeds
// normally anyway
// check index was deleted
// With security enabled cat aliases throws an index_not_found_exception
// if no aliases have been created. In multi-node tests the alias may not
// appear immediately so wait here.
// Manually delete the aliases so that we can test that deletion proceeds
// normally anyway
// check aliases were deleted
// Make the job's results span an extra two indices, i.e. three in total.
// To do this the job's results alias needs to encompass all three indices.
// Use _cat/indices/.ml-anomalies-* instead of _cat/indices/_all to workaround https://github.com/elastic/elasticsearch/issues/45652
// Add some documents to each index to make sure the DBQ clears them out
// Also index a few through the alias for the first job
// check for the documents
// Delete
// check that the indices still exist but are empty
// Immediately after the first deletion finishes, recreate the job.  This should pick up
// race conditions where another delete request deletes part of the newly created job.
// The idea is to hit the situation where one request waits for
// the other to complete. This is difficult to schedule but
// hopefully it will happen in CI
// This looks redundant but the check is done so we can
// print the exception's error message
// 404s are ok as it means the job had already been deleted.
// The idea of the code above is that the deletion is sufficiently time-consuming that
// all threads enter the deletion call before the first one exits it.  Usually this happens,
// but in the case that it does not the job that is recreated may get deleted.
// It is not a error if the job does not exist but the following assertions
// will fail in that case.
// Check that the job aliases exist.  These are the last thing to be deleted when a job is deleted, so
// if there's been a race between deletion and recreation these are what will be missing.
// The job does not exist
// The job aliases should be deleted
/*
/**
// ignore
// ignore
// ignore
// ignore
/*
/**
// ignore
/**
// Make sure we wrote to the audit
// Since calls to write the AbstractAuditor are sent and forgot (async) we could have returned from the start,
// finished the job (as this is a very short analytics job), all without the audit being fully written.
// TODO: Consider restoring this assertion when we are sure all the audit messages are available at this point.
// assertThat("Messages: " + allAuditMessages, allAuditMessages, hasSize(expectedAuditMessagePrefixes.length));
/*
/**
// we need to wrap node clients because we do not specify a user for nodes and all requests will use the system
// user. This is ok for internal n2n stuff but the test framework does other things like wiping indices, repositories, etc
// that the system user cannot do. so we wrap the node client with a user that can do these things since the client() calls
// return a node client
// We need to refresh to ensure the deletion is visible
// remove local node reference
// remove local node reference
// Check that the non-master node has the same version of the cluster state as the master and
// that the master node matches the master (otherwise there is no requirement for the cluster state to match)
// We cannot compare serialization bytes since serialization order of maps is not guaranteed
// but we can compare serialization sizes - they should be the same
// Compare JSON serialization
/*
// We are going to create data for last day
// As the initial time is random, there's a chance the first record is
// aligned on a bucket start. Thus we check the buckets are in [23, 24]
// As the initial time is random, there's a chance the first record is
// aligned on a bucket start. Thus we check the buckets are in [23, 24]
// As the initial time is random, there's a chance the first record is
// aligned on a bucket start. Thus we check the buckets are in [23, 24]
/*
// 5 docs with valid numeric value and missing categorical field (which should be ignored as it's not analyzed)
// Add a doc with missing field
// Add a doc with numeric being array which is also treated as missing
/*
/**
// 2017-01-01T00:00:00Z
// Check we get equal number of overall buckets on a default request
// Check overall buckets are half when the bucket_span is set to double the job bucket span
// Check overall score filtering works when chunking takes place
/*
// Persisting the job will create a model snapshot
// check that state is persisted after time has been advanced even if no new data is seen in the interim
// open and run a job with a small data set
// Check that state has been persisted
// To generate unique snapshot IDs ensure that there is at least a 1s delay between the
// time each job was closed
// re-open the job
// advance time
// Check that a new state record exists.
// Check an edge case where time is manually advanced before any valid data is seen
// Manually advance time.
// Check that state has been persisted
// now check that the job can be happily restored - even though no data has been seen
// Check an edge case where a job is opened and then immediately closed
// Check that state has not been persisted
// Check that results have not been persisted
/*
// TODO reenable this assertion when the backend is stable
// it seems for this case values can be as far off as 2.0
// double featureValue = (double) destDoc.get(NUMERICAL_FEATURE_FIELD);
// double predictionValue = (double) resultsObject.get("variable_prediction");
// assertThat(predictionValue, closeTo(10 * featureValue, 2.0));
// Let's just assert there's both training and non-training results
// Wait until state is one of REINDEXING or ANALYZING, or until it is STOPPED.
// Now let's start it again
// That means the job had managed to complete
// Let's run both jobs in parallel and wait until they are finished
// Now we compare they both used the same training rows
// Call _delete_expired_data API and check nothing was deleted
// Delete the config straight from the config index
// Now calling the _delete_expired_data API should remove unused state
/*
/**
// 2017-01-01T00:00:00Z
/*
/**
// Create the job, post the data and close the job
// Forecast should fail when the model has seen no data, ie model state not initialized
// Reopen the job and check forecast works
// In a multi-node cluster the replica may not be up to date
// so wait for the change
/*
/**
// We need to wait a second to ensure the second time around model snapshot will have a different ID (it depends on epoch seconds)
// Check model has grown since a new series was introduced
// Check quantiles have changed
// Snapshots are sorted in descending timestamp order so we revert to the last of the list/earliest.
// Check model_size_stats has been reverted
// Check quantiles have been reverted
// Re-run 2nd half of data
/*
// We insert one odd value out of 5 for one feature
// Check we've got all docs
// Check they all have an outlier_score
// State here could be any of STARTED, REINDEXING or ANALYZING
// We stopped before we even created the destination index
// Check we've got all docs
// Check they all have an outlier_score
// Check we've got all docs
// Check they all have an outlier_score
// This number of rows should make memory usage estimate greater than 1MB
// Assuming a 1TB job will never fit on the test machine - increase this when machines get really big!
// Due to lazy start being allowed, this should succeed even though no node currently in the cluster is big enough
// Wait until state is STARTING, there is no node but there is an assignment explanation.
// Wait until state is one of REINDEXING or ANALYZING, or until it is STOPPED.
// Now let's start it again
// That means the job had managed to complete
// Check we've got all docs
// Check they all have an outlier_score
// We insert one odd value out of 5 for one feature
/*
// add 10 min event smaller than the bucket
// Run 6 days of data
// Check tags on the buckets during the first event
// Following buckets have 0 events
// The second event bucket
// Following buckets have 0 events
// The 3rd event buckets
// Following buckets have 0 events
// It is unlikely any anomaly records have been created but
// ensure there are non present anyway
// The event starts 10 buckets in and lasts for 2
// write data up to and including the event
// flush the job and get the interim result during the event
/**
// Open the job
// write some buckets of data
// Now create a calendar and events for the job while it is open
// Wait until the notification that the process was updated is indexed
// write some more buckets of data that cover the scheduled event period
// and close
// the first buckets have no events
// 7th and 8th buckets have the event
/**
// Open the job
// write some buckets of data
// Create a new calendar referencing groupName
// Put events in the calendar
// Update the job to be a member of the group
// Wait until the notification that the job was updated is indexed
// write some more buckets of data that cover the scheduled event period
// and close
// the first 6 buckets have no events
// 7th and 8th buckets have the event but the last one does not
// register for clean up
/*
// Assert appropriate task state and assignment numbers
// Set the upgrade mode setting
// Assert state for tasks still exists and that the upgrade setting is set
//Disable the setting
/*
/*
/**
// Have to use JNA for Windows named pipes
// https://msdn.microsoft.com/en-us/library/windows/desktop/aa365150(v=vs.85).aspx
// https://msdn.microsoft.com/en-us/library/windows/desktop/aa365146(v=vs.85).aspx
// https://msdn.microsoft.com/en-us/library/windows/desktop/ms724211(v=vs.85).aspx
// https://msdn.microsoft.com/en-us/library/windows/desktop/aa365467(v=vs.85).aspx
// https://msdn.microsoft.com/en-us/library/windows/desktop/aa365747(v=vs.85).aspx
// This won't be used in the *nix version
// ERROR_PIPE_CONNECTED means the pipe was already connected so
// there was no need to connect it again - not a problem
// ERROR_PIPE_CONNECTED means the pipe was already connected so
// there was no need to connect it again - not a problem
// Ignore it if the previous block caught an exception, as this probably means we failed to create the pipe
// Ignore it if the previous block caught an exception, as this probably means we failed to create the pipe
// Timeout is 10 seconds for the very rare case of Amazon EBS volumes created from snapshots
// being slow the first time a particular disk block is accessed.  The same problem as
// https://github.com/elastic/x-pack-elasticsearch/issues/922, which was fixed by
// https://github.com/elastic/x-pack-elasticsearch/pull/987, has been observed in CI tests.
// If this doesn't join quickly then the server thread is probably deadlocked so there's no
// point waiting a long time.
// Timeout is 10 seconds for the very rare case of Amazon EBS volumes created from snapshots
// being slow the first time a particular disk block is accessed.  The same problem as
// https://github.com/elastic/x-pack-elasticsearch/issues/922, which was fixed by
// https://github.com/elastic/x-pack-elasticsearch/pull/987, has been observed in CI tests.
// In some rare cases writer can close before the reader has had a chance
// to read what is written. On Windows this can cause ConnectNamedPipe to
// error with ERROR_NO_DATA
// If this doesn't join quickly then the server thread is probably deadlocked so there's no
// point waiting a long time.
/*
// Test cases from https://github.com/john-kurkowski/tldextract/tree/master/tldextract/tests
// .info is a valid TLD
// These are not a valid DNS names
//tests.add(new TestConfiguration("192.168", "62.9\143\127", "192.168.62.9\143\127"));
// no part of the DNS name can be longer than 63 octets
/*
// [Zach] This breaks the script's JSON encoding, skipping for now
//String bad = "0u1aof\209\1945\188hI4\236\197\205J\244\188\247\223\190F\2135\229gVE7\230i\215\231\205Qzay\225UJ\192
// pw\216\231\204\194\216\193QV4g\196\207Whpvx.fVxl\194BjA\245kbYk\211XG\235\198\218B\252\219\225S\197\217I\2538n\229
// \244\213\252\215Ly\226NW\242\248\244Q\220\245\221c\207\189\205Hxq5\224\240.\189Jt4\243\245t\244\198\199p\210\1987
// r\2050L\239sR0M\190w\238\223\234L\226\2242D\233\210\206\195h\199\206tA\214J\192C\224\191b\188\201\251\198M\244h
// \206.\198\242l\2114\191JBU\198h\207\215w\243\228R\1924\242\208\191CV\208p\197gDW\198P\217\195X\191Fp\196\197J\193
// \245\2070\196zH\197\243\253g\239.adz.beacon.base.net";
//hrd = "base.net";
//tests.add(new TestConfiguration(bad.substring(0, bad.length() - (hrd.length() + 1)), hrd, bad));
// checkHighestRegisteredDomain() tests
// TLD with only 1 rule.
// TLD with some 2-level rules.
// more complex TLD
//tests.add(new TestConfiguration(null, "b.c.kobe.jp", "b.c.kobe.jp"));
//tests.add(new TestConfiguration(null, "b.c.kobe.jp", "a.b.c.kobe.jp"));
//tests.add(new TestConfiguration(null, "食狮.com.cn", "食狮.com.cn"));
//tests.add(new TestConfiguration(null, "食狮.公司.cn", "食狮.公司.cn"));
//tests.add(new TestConfiguration(null, "食狮.公司.cn", "www.食狮.公司.cn"));
//tests.add(new TestConfiguration(null, "shishi.公司.cn", "shishi.公司.cn"));
//tests.add(new TestConfiguration(null, "食狮.中国", "食狮.中国"));
//tests.add(new TestConfiguration(null, "食狮.中国", "www.食狮.中国"));
//tests.add(new TestConfiguration(null, "shishi.中国", "shishi.中国"));
// domainSplit() tests had subdomain, testHighestRegisteredDomainCases() do not
//github.com/elastic/elasticsearch/issues/32966")
// Create job
// Create index to hold data
// Index some data
// domainSplit() tests had subdomain, testHighestRegisteredDomainCases() did not, so we need a special case for sub
// Anomaly has 100 docs, but we don't care about the value
// Non-anomalous values will be what's seen when the anomaly is reported
// Create and start datafeed
// domainSplit() tests had subdomain, testHighestRegisteredDomainCases() do not
/*
/*
// if the license has expired, close jobs and datafeeds
/*
// This is for performance testing.  It's not exposed to the end user.
// Recompile if you want to compare performance with C++ tokenization.
// This is not used in v7 and higher, but users are still prevented from setting it directly to avoid confusion
/**
// Values higher than 100% are allowed to accommodate use cases where swapping has been determined to be acceptable.
// Anomaly detector jobs only use their full model memory during background persistence, and this is deliberately
// staggered, so with large numbers of jobs few will generally be persisting state at the same time.
// Settings higher than available memory are only recommended for OEM type situations where a wrapper tightly
// controls the types of jobs that can be created, and each job alone is considerably smaller than what each node
// can handle.
// Before 8.0.0 this needs to match the max allowed value for xpack.ml.max_open_jobs,
// as the current node could be running in a cluster where some nodes are still using
// that setting.  From 8.0.0 onwards we have the flexibility to increase it...
// This setting is cluster-wide and can be set dynamically. However, prior to version 7.1 it was
// a non-dynamic per-node setting. n a mixed version cluster containing 6.7 or 7.0 nodes those
// older nodes will not react to the dynamic changes. Therefore, in such mixed version clusters
// allocation will be based on the value first read at node startup rather than the current value.
// Undocumented setting for integration test purposes
// TODO: stop setting this attribute in 8.0.0 but disallow it (like mlEnabledNodeAttrName below)
// The ML UI will need to be changed to check machineMemoryAttrName instead before this is done
// This is not used in v7 and higher, but users are still prevented from setting it directly to avoid confusion
// overridable by tests
// special holder for @link(MachineLearningFeatureSetUsage) which needs access to job manager, empty if ML is disabled
// special holder for @link(MachineLearningFeatureSetUsage) which needs access to job manager if ML is enabled
// The low level cause of failure from the named pipe helper's perspective is almost never the real root cause, so
// only log this at the lowest level of detail.  It's almost always "file not found" on a named pipe we expect to be
// able to connect to, but the thing we really need to know is what stopped the native process creating the named pipe.
// factor of 1.0 makes renormalization a no-op
// Inference components
// Data frame analytics components
// Components shared by anomaly detection and data frame analytics
// this object registers as a license state listener, and is never removed, so there's no need to retain another reference to it
// Perform node startup operations
// These thread pools scale such that they can accommodate the maximum number of jobs per node
// that is permitted to be configured.  It is up to other code to enforce the configured maximum
// number of jobs per node.
// 4 threads per job process: for input, c++ logger output, result processing and state processing.
// This pool is used by renormalization, plus some other parts of ML that
// need to kick off non-trivial activities that mustn't block other threads.
// Whether we are using native process is a good way to detect whether we are in dev / test mode:
// Our indexes are small and one shard puts the
// least possible burden on Elasticsearch
// Our indexes are small and one shard puts the
// least possible burden on Elasticsearch
// Our indexes are small and one shard puts the
// least possible burden on Elasticsearch
// TODO review these settings
// Sacrifice durability for performance: in the event of power
// failure we can lose the last 5 seconds of changes, but it's
// much faster
// set the default all search field
/**
// mem <= 0 means the value couldn't be obtained for some reason
/*
/*
/*
// Step 5. extract trained model config count and then return results
// Step 4. Extract usage from ingest statistics and gather trained model config count
// Step 3. Extract usage from data frame analytics stats and then request ingest node stats
// Step 2. Extract usage from datafeeds stats and then request stats for data frame analytics
// Step 1. Extract usage from jobs stats and then request stats for all datafeeds
// Step 0. Kick off the chain of callbacks by requesting jobs stats
// Replace non-alpha-numeric characters with underscores because
// the values from custom settings become keys in the usage data
//TODO separate out ours and users models possibly regression vs classification
//TODO separate out ours and users models possibly regression vs classification
/*
/**
/*
/**
/**
/**
/**
/*
/**
/**
// We have successfully snapshotted the ML configs so we don't need to try again
// Exposed for testing
// If there are no tasks in the cluster state metadata to begin with, this could be null.
/**
// copy and update the job parameters
// replace with the updated params
// replace with the updated params
/**
// public for testing
// the snapshot already exists
// Pre v5.5 (ml beta) jobs do not have a version.
// These jobs cannot be opened, we rely on the missing version
// to indicate this.
// See TransportOpenJobAction.validate()
/**
/**
/**
/**
// prioritise datafeed and job pairs
// are there jobs without datafeeds to migrate
/**
/*
/**
/**
/**
/**
/*
// Wait until the gateway has recovered from disk.
// The atomic flag prevents multiple simultaneous attempts to create the
// index if there is a flurry of cluster state updates in quick succession
/** For testing */
/** For testing */
/*
// This prevents datafeeds from sending data to autodetect processes WITHOUT stopping the
// datafeeds, so they get reassigned.  We have to do this first, otherwise the datafeeds
// could fail if they send data to a dead autodetect process.
// This kills autodetect processes WITHOUT closing the jobs, so they get reassigned.
// We're stopping anyway, so don't let this complicate the shutdown sequence
/*
// We fork in innerTaskOperation(...), so we can use ThreadPool.Names.SAME here:
// Delegates close job to elected master node, so it becomes the coordinating node.
// See comment in OpenJobAction.Transport class for more information.
/*
// This should not happen, because openJobIds was
// derived from the same tasks metadata as jobTask
// This is the easy case - the job is not currently assigned to a node, so can
// be gracefully stopped simply by removing its persistent task.  (Usually a
// graceful stop cannot be achieved by simply removing the persistent task, but
// if the job has no running code then graceful/forceful are basically the same.)
// The listener here can be a no-op, as waitForJobClosed() already waits for
// these persistent tasks to disappear.
/**
// If there are failed jobs force close is true
// we need to fork because we are now on a network threadpool and closeJob method may take a while to complete:
// number of resolved jobs should be equal to the number of tasks,
// otherwise something went wrong
// This can happen we the actual task in the node no longer exists,
// which means the job(s) have already been closed.
// If there are no open or closing jobs in the request return
// No jobs to close but we still want to wait on closing jobs in the request
// Wait for job to be marked as closed in cluster state, which means the job persistent task has been removed
// This api returns when job has been closed, but that doesn't mean the persistent task has been removed from cluster state,
// so wait for that to happen here.
/*
// Delete calendar and events
/*
// Get the calendar first so we check the calendar exists before checking the event exists
/*
// the task has been removed in between
// Check datafeed is stopped
/*
/**
// We clean up the memory tracker on delete because there is no stop; the task stops by itself
// Step 3. Delete the config
// Step 2. Delete state
// Step 1. Get the config to check if it exists
/*
// TODO: make configurable in the request
// Removing expired ML data and artifacts requires multiple operations.
// These are queued up and executed sequentially in the action listener,
// the chained calls must all run the ML utility thread pool NOT the thread
// the previous action returned in which in the case of a transport_client_boss
// thread is a disaster.
/*
/*
//Getting the max RestStatus is sort of arbitrary, would the user care about 5xx over 4xx?
//Unsure of a better way to return an appropriate and possibly actionable cause to the user.
//since these documents are not updated, a conflict just means it was deleted previously
/*
/**
// Check if there is a deletion task for this job already and if yes wait for it to complete
// The listener that will be executed at the end of the chain will notify all listeners
// First check that the job exists, because we don't want to audit
// the beginning of its deletion if it didn't exist in the first place
// We clean up the memory tracker on delete rather than close as close is not a master node action
// Step 4. When the job has been removed from the cluster state, return a response
// -------
// Step 3. When the physical storage has been deleted, delete the job config document
// -------
// Don't report an error if the document has already been deleted
// Step 2. Remove the job from any calendars
// Step 1. Delete the physical storage
// Step 8. If we did not drop the indices and after DBQ state done, we delete the aliases
// no action was taken by DBQ, assume indices were deleted
// Step 7. If we did not delete the indices, we run a delete by query
// We did not execute DBQ, no need to delete aliases or check the response
// Step 6. If we have any hits, that means we are NOT the only job on these indices, and should not delete the indices.
// If we do not have any hits, we can drop the indices and then skip the DBQ and alias deletion.
// We need to run DBQ and alias deletion
// If we have deleted the index, then we don't need to delete the aliases or run the DBQ
// skip DBQ && Alias
// assume the index is already deleted
// skip DBQ && Alias
// Step 5. Determine if we are on shared indices by looking at whether the initial index was ".ml-anomalies-shared"
// or whether the indices that the job's results alias points to contain any documents from other jobs.
// TODO: this check is currently assuming that a job's results indices are either ALL shared or ALL
// dedicated to the job.  We have considered functionality like rolling jobs that generate large
// volumes of results from shared to dedicated indices.  On deletion such a job would have a mix of
// shared indices requiring DBQ and dedicated indices that could be simply dropped.  The current
// functionality would apply DBQ to all these indices, which is safe but suboptimal.  So this functionality
// should be revisited when we add rolling results index functionality, especially if we add the ability
// to switch a job over to a dedicated index for future results.
// The job may no longer be using the initial shared index, but if it started off on a
// shared index then it will still be on a shared index even if it's been reindexed
// don't bother searching the index any further, we are on the default shared
// don't bother searching the index any further - it's already been closed or deleted
// Step 4. Get the job as the initial result index name is required
// Step 3. Delete quantiles done, delete the categorizer state
// Step 2. Delete state done, delete the quantiles
// Step 1. Delete the model state
// The quantiles type and doc ID changed in v5.5 so delete both the old and new format
// Just use ID here, not type, as trying to delete different types spams the logs with an exception stack trace
// It's not a problem for us if the index wasn't found - it's equivalent to document not found
// The categorizer state type and doc ID changed in v5.5 so delete both the old and new format
// Just use ID here, not type, as trying to delete different types spams the logs with an exception stack trace
// If we successfully deleted a document try the next one; if not we're done
// There's an assumption here that there won't be very many categorizer
// state documents, so the recursion won't go more than, say, 5 levels deep
// It's not a problem for us if the index wasn't found - it's equivalent to document not found
// first find the concrete indices associated with the aliases
// remove the aliases from the concrete indices found in the first step
// don't error if the job's aliases have already been deleted - carry on and delete the
// rest of the job's data
// The response includes _all_ indices, but only those associated with
// the aliases we asked about will have associated AliasMetaData
// 3. Delete the job
// 2. Cancel the persistent task. This closes the process gracefully so
// the process should be killed first.
// Killing the process marks the task as completed so it
// may have disappeared when we get here
// 1. Kill the job's process
/*
// Verify the snapshot exists
// Verify the snapshot is not being used
// Delete the snapshot and any associated state files
// We don't care about the bulk response, just that it succeeded
/*
/**
/*
/**
// Add one task only. Other tasks will be added as needed by the nextTask method itself.
/*
/**
/**
/**
/**
/*
/*
// As determining the file structure might take a while, we run
// in a different thread to avoid blocking the network thread.
/*
// ThreadPool.Names.SAME, because operations is executed by autodetect worker thread
/*
// ThreadPool.Names.SAME, because operations is executed by autodetect worker thread
// tmp storage might be null, we do not log here, because it might not be
// required
// paranoia case, it should not happen that we do not retrieve a result
// special case: if forecast failed due to insufficient disk space, log the setting
// paranoia case, it should not be possible to have an empty message list
/*
/*
// is the request Id a group?
/*
/*
/*
// Check for duplicate datafeeds
// Merge cluster state and index configs
// ignore
/*
/*
/*
// While finalResponse has all the stats objects we need, we should report the count
// from the get response
/*
/*
/*
/*
// Up until now we gathered the stats for jobs that were open,
// This method will fetch the stats for missing jobs, that was stored in the jobs index
/*
/*
// As computing and potentially aggregating overall buckets might take a while,
// we run in a different thread to avoid blocking the network thread.
// If top_n is 1, we can use the request bucket_span in order to optimize the aggregations
/*
/*
/*
/*
// run through all tasks
// Always fail immediately and return an error
/*
// No running datafeed task to isolate
/*
/**
// TODO: Hacking around here with TransportTasksAction. Ideally we should have another base class in core that
// redirects to a single node only
// We need to check whether there is at least an assigned task here, otherwise we cannot redirect to the
// node running the job task.
// no need to accumulate sub responses, since we only perform an operation on one task only
// not ideal, but throwing exceptions here works, because higher up the stack there is a try-catch block delegating to
// the actionlistener's onFailure
/*
/*
/*
/*
/**
// Indices are created on demand from templates.
// It is not an error if the index doesn't exist yet
// There is no snapshot to restore or the min model snapshot version is 5.5.0
// which is OK as we have already checked the node is >= 5.5.0.
// This api doesn't do heavy or blocking operations (just delegates PersistentTasksService),
// so we can do this on the network thread
// We only delegate here to PersistentTasksService, but if there is a metadata writeblock,
// then delegating to PersistentTasksService doesn't make a whole lot of sense,
// because PersistentTasksService will then fail.
// Clear job finished time once the job is started and respond
// Wait for job to be started
// Start job task
// Tell the job tracker to refresh the memory requirement for this job and all other jobs that have persistent tasks
// Get the job config
// We want to return to the caller without leaving an unassigned persistent task, to match
// what would have happened if the error had been detected in the "fast fail" validation
// Not a critical error so continue
// We succeeded in cancelling the persistent task, but the
// problem that caused us to cancel it is the overall result
// If the task parameters do not have a job field then the job
// was first opened on a pre v6.6 node and has not been migrated
// If we are waiting for an upgrade to complete, we should not assign to a node
// If we already know that we can't find an ml node because all ml nodes are running at capacity or
// simply because there are no ml nodes in the cluster then we fail quickly here:
// If the job is failed then the Persistent Task Service will
// try to restart it on a node restart. Exiting here leaves the
// job in the failed state and it must be force closed.
/**
// This means we are awaiting a new node to be spun up, ok to return back to the user to await node creation
// This logic is only appropriate when opening a job, not when reallocating following a failure,
// and this is why this class must only be used when opening a job
// Assignment has failed on the master node despite passing our "fast fail" validation
// The persistent task should be cancelled so that the observed outcome is the
// same as if the "fast fail" validation on the coordinating node had failed
// The OPENING case here is expected to be incredibly short-lived, just occurring during the
// time period when a job has successfully been assigned to a node but the request to update
// its task state is still in-flight.  (The long-lived OPENING case when a lazy node needs to
// be added to the cluster to accommodate the job was dealt with higher up this method when the
// magic AWAITING_LAZY_ASSIGNMENT assignment was checked for.)
/*
// ThreadPool.Names.SAME, because operations is executed by autodetect worker thread
/*
/*
// ThreadPool.Names.SAME, because operations is executed by autodetect worker thread
/*
// NB: this is using the client from the transport layer, NOT the internal client.
// This is important because it means the datafeed search will fail if the user
// requesting the preview doesn't have permission to search the relevant indices.
// Fake DatafeedTimingStatsReporter that does not have access to results index
/** Visible for testing */
// Since we only want a preview, it's worth limiting the cost
// of the search in the case of non-aggregated datafeeds.
// We do so by setting auto-chunking. This ensures to find
// a sensible time range with enough data to preview.
// When aggregations are present, it's best to comply with
// what the datafeed is set to do as it can reveal problems with
// the datafeed config (e.g. a chunking config that would hit circuit-breakers).
/** Visible for testing */
// DataExtractor returns single-line JSON but without newline characters between objects.
// Instead, it has a space between objects due to how JSON XContenetBuilder works.
// In order to return a proper JSON array from preview, we surround with square brackets and
// we stick in a comma between objects.
// Also, the stream is expected to be a single line but in case it is not, we join lines
// using space to ensure the comma insertion works correctly.
/*
// Make it an error to overwrite an existing calendar
/*
// If security is enabled only create the datafeed if the user requesting creation has
// permission to read the indices the datafeed is going to read from
// This means no rollup indexes are in the config
/**
/*
/*
/*
/*
/*
// 3. Revert the state
// 2. Verify the job exists
// 1. Verify/Create the state index and its alias exists
// If we need to delete buckets that occurred after the snapshot, we
// wrap the listener with one that invokes the OldDataRemover on
// acknowledged responses
/*
// Don't want folks spamming this endpoint while it is in progress, only allow one request to be handled at a time
// Noop, nothing for us to do, simply return fast to the caller
// <4> We have unassigned the tasks, respond to the listener.
// Wait for our tasks to all stop
// There is a chance that we failed un-allocating a task due to allocation_id being changed
// This call will timeout in that case and return an error
// Handle potential node timeouts,
// these should be considered failures as tasks as still potentially executing
// <3> After isolating the datafeeds, unassign the tasks
/*
// State change was not acknowledged, we either timed out or ran into some exception
// We should not continue and alert failure to the end user
// There are no tasks to worry about starting/stopping
// Did we change from disabled -> enabled?
// Wait for jobs to not be "Awaiting upgrade"
// Wait for datafeeds to not be "Awaiting upgrade"
//<1> Change MlMetadata to indicate that upgrade_mode is now enabled
/**
// We want to always have the same ordering of which tasks we un-allocate first.
// However, the order in which the distributed tasks handle the un-allocation event is not guaranteed.
// Another process could modify tasks and thus we cannot find them via the allocation_id and name
// If the task was removed from the node, all is well
// We handle the case of allocation_id changing later in this transport class by timing out waiting for task completion
// Consequently, if the exception is ResourceNotFoundException, continue execution; circuit break otherwise.
/*
/* This class extends from TransportMasterNodeAction for cluster state observing purposes.
//Get the deprecation warnings from the parsed query and aggs to audit
// This api doesn't do heavy or blocking operations (just delegates PersistentTasksService),
// so we can do this on the network thread
// Verify data extractor factory can be created, then start persistent task
/** Creates {@link DataExtractorFactory} solely for the purpose of validation i.e. verifying that it can be created. */
// Fake DatafeedTimingStatsReporter that does not have access to results index
// We only delegate here to PersistentTasksService, but if there is a metadata writeblock,
// then delagating to PersistentTasksService doesn't make a whole lot of sense,
// because PersistentTasksService will then fail.
// We want to return to the caller without leaving an unassigned persistent task, to match
// what would have happened if the error had been detected in the "fast fail" validation
// We succeeded in cancelling the persistent task, but the
// problem that caused us to cancel it is the overall result
/* only pck protected for testing */
// If the persistent task framework wants us to stop then we should do so immediately and
// we should wait for an existing datafeed import to realize we want it to stop.
// Note that this only applied when task cancel is invoked and stop datafeed api doesn't use this.
// Also stop datafeed api will obey the timeout.
/**
// Assignment has failed despite passing our "fast fail" validation
/*
/**
// This api doesn't do heavy or blocking operations (just delegates PersistentTasksService),
// so we can do this on the network thread
// We only delegate here to PersistentTasksService, but if there is a metadata writeblock,
// then delegating to PersistentTasksService doesn't make a whole lot of sense,
// because PersistentTasksService will then fail.
// Wait for analytics to be started
// Start persistent task
// Perform memory usage estimation for this config
// Get start context
// Tell the job tracker to refresh the memory requirement for this job and all other jobs that have persistent tasks
// Validate that model memory limit is sufficient to run the analysis
// Refresh memory requirement for jobs
// Step 7. Validate that there are analyzable data in the source index
// Step 6. Validate mappings can be merged
// Step 5. Validate dest index is empty if task is starting for first time
// Step 4. Check data extraction is possible
// Step 3. Validate source and dest
// Validate the query parses
// Validate source/dest are valid
// Step 2. Get stats to recover progress
// Step 1. Get the config
// The job has been deleted in between
// We want to return to the caller without leaving an unassigned persistent task, to match
// what would have happened if the error had been detected in the "fast fail" validation
/**
// This means we are awaiting a new node to be spun up, ok to return back to the user to await node creation
// Assignment has failed despite passing our "fast fail" validation
// The STARTING case here is expected to be incredibly short-lived, just occurring during the
// time period when a job has successfully been assigned to a node but the request to update
// its task state is still in-flight.  (The long-lived STARTING case when a lazy node needs to
// be added to the cluster to accommodate the job was dealt with higher up this method when the
// magic AWAITING_LAZY_ASSIGNMENT assignment was checked for.)
// We succeeded in cancelling the persistent task, but the
// problem that caused us to cancel it is the overall result
// If we are waiting for an upgrade to complete, we should not assign to a node
// Pass an effectively infinite value for max concurrent opening jobs, because data frame analytics jobs do
// not have an "opening" state so would never be rejected for causing too many jobs in the "opening" state
// If we are "stopping" there is nothing to do
// If we are "failed" then we should leave the task as is; for recovery it must be force stopped.
/*
/**
// The STARTING state is not used anywhere at present, so this should never happen.
// At present datafeeds that have a persistent task that hasn't yet been assigned
// a state are reported as STOPPED (which is not great).  It could be considered a
// breaking change to introduce the STARTING state though, so let's aim to do it in
// version 8.  Also consider treating STARTING like STARTED for stop API behaviour.
// Delegates stop datafeed to elected master node, so it becomes the coordinating node.
// See comment in TransportStartDatafeedAction for more information.
// This should not happen, because startedDatafeeds was derived from the same tasks that is passed to this method
// This is the easy case - the datafeed is not currently assigned to a valid node,
// so can be gracefully stopped simply by removing its persistent task.  (Usually
// a graceful stop cannot be achieved by simply removing the persistent task, but
// if the datafeed has no running code then graceful/forceful are the same.)
// The listener here can be a no-op, as waitForDatafeedStopped() already waits for
// these persistent tasks to disappear.
// wait for started and stopping datafeeds
// Map datafeedId -> datafeed task Id.
// A node has dropped out of the cluster since we started executing the requests.
// Since stopping an already stopped datafeed is not an error we can try again.
// The datafeeds that were running on the node that dropped out of the cluster
// will just have their persistent tasks cancelled.  Datafeeds that were stopped
// by the previous attempt will be noops in the subsequent attempt.
// We validated that the datafeed names supplied in the request existed when we started processing the action.
// If the related tasks don't exist at this point then they must have been stopped by a simultaneous stop request.
// This is not an error.
// This should not happen, because startedDatafeeds and stoppingDatafeeds
// were derived from the same tasks that were passed to this method
// we need to fork because we are now on a network threadpool
// We validated that the datafeed names supplied in the request existed when we started processing the action.
// If the related task for one of them doesn't exist at this point then it must have been removed by a
// simultaneous force stop request.  This is not an error.
// the task has disappeared so must have stopped
// Wait for datafeed to be marked as stopped in cluster state, which means the datafeed persistent task has been removed
// This api returns when task has been cancelled, but that doesn't mean the persistent task has been removed from cluster state,
// so wait for that to happen here.
// number of resolved data feeds should be equal to the number of
// tasks, otherwise something went wrong
// This can happen when the actual task in the node no longer exists,
// which means the datafeed(s) have already been stopped.
/*
/**
/** Visible for testing */
// This should not be possible; we filtered started analytics thus the task should exist
// This means the task has not been assigned to a node yet so
// we can stop it by removing its persistent task.
// The listener is a no-op as we're already going to wait for the task to be removed.
// This can happen when the actual task in the node no longer exists,
// which means the data frame analytic(s) have already been closed.
// the task has disappeared so must have stopped
/*
/*
// Check datafeed is stopped
/*
// Check if removed items are present to avoid typos
/*
/*
// The quantiles can be large, and totally dominate the output -
// it's clearer to remove them
/*
// ThreadPool.Names.SAME, because operations is executed by autodetect worker thread
/*
/*
/*
//15 minutes in ms
// start time is before last checkpoint, thus continue from checkpoint
// start time is after last checkpoint, thus we need to skip time
// Keep track of the last bucket time for which we did a missing data check
// Get the end of the last bucket and make it milliseconds
// Have we an annotation that covers the same area with the same message?
// Cannot use annotation.equals(other) as that checks createTime
// Creating a warning in addition to updating/creating our annotation. This allows the issue to be plainly visible
// in the job list page.
/**
/**
// A storage for errors that should only be thrown after advancing time
// When extraction problems are encountered, we do not want to advance time.
// Instead, it is preferable to retry the given interval next time an extraction
// is triggered.
// For aggregated datafeeds it is possible for our users to use fields without doc values.
// In that case, it is really useful to display an error message explaining exactly that.
// Unfortunately, there are no great ways to identify the issue but search for 'doc values'
// deep in the exception.
// a conflict exception means the job state is not open any more.
// we should therefore stop the datafeed.
// When an analysis problem occurs, it means something catastrophic has
// happened to the c++ process. We sent a batch of data to the c++ process
// yet we do not know how many of those were processed. It is better to
// advance time in order to avoid importing duplicate data.
// We can now throw any stored error as we have updated time.
// If the datafeed was stopped, then it is possible that by the time
// we call flush the job is closed. Thus, we don't flush unless the
// datafeed is still running.
// We find the timestamp of the start of the next frequency interval.
// The goal is to minimize any lag. To do so,
// we offset the time by the query delay modulo frequency.
// For example, if frequency is 60s and query delay 90s,
// we run 30s past the minute. If frequency is 1s and query delay 10s,
// we don't add anything and we'll run every second.
// a conflict exception means the job state is not open any more.
// we should therefore stop the datafeed.
// When an analysis problem occurs, it means something catastrophic has
// happened to the c++ process. We sent a batch of data to the c++ process
// yet we do not know how many of those were processed. It is better to
// advance time in order to avoid importing duplicate data.
/**
/*
// Step 5. Build datafeed job object
// Context building complete - invoke final listener
// Create data extractor factory
// Collect data counts
// Collect latest bucket
// Get the job config and re-validate
// Re-validation is required as the config has been re-read since
// the previous validation
// Get the datafeed config
/*
// Use allocationId as key instead of datafeed id
// The task was stopped in the meantime, no need to do anything
/**
/**
// TODO: it's not ideal that this "isolate" method does something a bit different to the one below
// This calls get() rather than remove() because we expect that the persistent task will
// be removed shortly afterwards and that operation needs to be able to find the holder
// Important: Holder must be created and assigned to DatafeedTask before setting state to started,
// otherwise if a stop datafeed call is made immediately after the start datafeed call we could cancel
// the DatafeedTask without stopping datafeed, which causes the datafeed to keep on running.
// Notify that a lookback-only run found no data
// In this case we auto-close the job, as though a lookback-only datafeed stopped
/**
// To ensure that we wait until lookback / realtime search has completed before we stop the datafeed
// It is crucial that none of the calls this "finally" block makes throws an exception for minor problems.
/**
/*
// Given that the UI force-deletes the datafeed and then force-deletes the job, it's
// quite likely that the auto-close here will get interrupted by a process kill request,
// and it's misleading/worrying to log an error in this case.
// This clearing of the thread context is not strictly necessary.  Every action performed by the
// datafeed _should_ be done using the MlClientHelper, which will set the appropriate thread
// context.  However, by clearing the thread context here if anyone forgets to use MlClientHelper
// somewhere else in the datafeed code then it should cause a failure in the same way in single
// and multi node clusters.  If we didn't clear the thread context here then there's a risk that
// a context with sufficient permissions would coincidentally be in force in some single node
// tests, leading to bugs not caught in CI due to many tests running in single node test clusters.
/*
// lets try again later when the job has been opened:
// We cannot verify remote indices
/**
/*
/**
/** Interface used for persisting current timing stats to the results index. */
/** Does nothing by default. This behavior is useful when creating fake {@link DatafeedTimingStatsReporter} objects. */
/** Persisted timing stats. May be stale. */
/** Current timing stats. */
/** Object used to persist current timing stats. */
/** Whether or not timing stats will be persisted by the persister object. */
/** Gets current timing stats. */
/**
/**
/** Finishes reporting of timing stats. Makes timing stats persisted immediately. */
// Don't flush if current timing stats are identical to the persisted ones
/** Disallows persisting timing stats. After this call finishes, no document will be persisted. */
// Since persisting datafeed timing stats is not critical, we just log a warning here.
/**
/**
/**
/**
/**
// 10s
/*
/**
/**
/**
/**
/**
/**
/*
/**
/**
// We only care about the situation when data is added to the indices
// Older data could have been removed from the indices, and should not be considered "missing data"
/*
/*
/**
// There are eight 15min buckets in a two hour span, so matching that number as the fallback for very long buckets
// 2 hours in Milliseconds
/**
// we should provide a good default as the user did not specify a window
/*
/**
/**
/**
/*
/**
// This means no rollup indexes are in the config
// Rollup indexes require aggregations
// If we have remote indices in the data feed, don't bother checking for rollup support
// Rollups + CCS is not supported
/*
/**
/**
// For derivative aggregations the first bucket will always be null
// so query one extra histogram bucket back and hope there is data
// in that bucket
/*
/**
/*
/*
/*
/**
/**
/**
// This means we reached a bucket aggregation without sub-aggs. Thus, we can flush the path written so far.
// Sort into leaf and bucket aggregations.
// The leaf aggregations will be processed first.
// Skip a level down for single bucket aggs, if they have a sub-agg that is not
// a bucketed agg we should treat it like a leaf in this bucket
// If on the current level (indicated via bucketAggregations) or one of the next levels (singleBucketAggregations)
// we have more than 1 `MultiBucketsAggregation`, we should error out.
// We need to make the check in this way as each of the items in `singleBucketAggregations` is treated as a separate branch
// in the recursive handling of this method.
// Ignore bucket aggregations that don't contain a field we
// are interested in. This avoids problems with pipeline bucket
// aggregations where we would create extra docs traversing a
// bucket agg that isn't used but is required for the pipeline agg.
// we support more than one `SingleBucketAggregation` at each level
// However, we only want to recurse with multi/single bucket aggs.
// Non-bucketed sub-aggregations were handle as leaf aggregations at this level
// If there are no more bucket aggregations to process we've reached the end
// and it's time to write the doc
// buckets are ordered by time, once we get to a bucket past the
// start time we no longer need to check the time.
// skip buckets outside the required time range
/*
/**
/**
/**
/*
/**
/*
/*
/**
/** Let us set a minimum chunk span of 1 minute */
// This is the first time next is called
// search is over
// First search or the current search finished; we can advance to the next search
// If it was a new search it means it returned 0 results. Thus,
// we reconfigure and jump to the next time interval where there are data.
/**
// TODO: once RollupSearchAction is changed from indices:admin* to indices:data/read/* this branch is not needed
/* this branch is not needed
/**
/**
/*
/*
// When the datafeed uses aggregations and in order to accommodate derivatives,
// an extra bucket is queried at the beginning of each search. In order to avoid visiting
// the same bucket twice, we need to search buckets aligned to the histogram interval.
// This allows us to steer away from partial buckets, and thus avoid the problem of
// dropping or duplicating data.
/*
/**
// In case of error make sure we clear the scroll context
// This could be a transient error with the scroll Id.
// Reinitialise the scroll and try again but only once.
/*
/*
// Step 2. Contruct the factory and notify listener
// Step 1. Get field capabilities necessary to build the information of how to extract fields
// We need capabilities for all fields matching the requested fields' parents so that we can work around
// multi-fields that are not in source.
// This response gets discarded - the listener handles the real response
/*
/*
/**
/*
/**
/**
// Filter any values in headers that aren't security fields
// the dafafeed already exists
/**
/**
// There cannot be more than one datafeed per job
/**
/**
/**
// some required datafeeds were not found
/**
// TODO A better way to handle this rather than just ignoring the error?
// some required datafeeds were not found
// match all
/*
/**
// Metadata fields
/**
/**
// If the source field is an alias, fetch the concrete field that the alias points to.
// We may have updated the value of {@code sourceFieldMapping} in the "if" block above.
// Hence, we need to check the "instanceof" condition again.
// We have validated the destination index should match a single index
// Fetch mappings from destination index
// Verify that the results field does not exist in the dest index
// Determine mappings to be added to the destination index
// Add the mappings to the destination index
/*
/**
// With config in hand, determine action to take
// If we are STARTED, it means the job was started because the start API was called.
// We should determine the job's starting state based on its previous progress.
// The task has fully reindexed the documents and we should continue on with our analyses
// If we are already at REINDEXING, we are not 100% sure if we reindexed ALL the docs.
// We will delete the destination index, recreate, reindex
// Retrieve configuration
// Make sure the state index and alias exist
// The task was requested to stop before we started reindexing
// Reindexing is complete; start analytics
// Reindex
// Create destination index if it does not exist
// Ensure we mark reindexing is finished for the case we are recovering a task that had finished reindexing
// Update state to ANALYZING and start process
// Task has stopped
// TODO This could fail with errors. In that case we get stuck with the copied index.
// We could delete the index in case of failure or we could try building the factory before reindexing
// to catch the error early on.
/*
// It is possible that the stop API has been called in the meantime and that
// may also cause this method to be called. We check whether we have already
// been marked completed to avoid doing it twice. We need to capture that
// locally instead of relying to isCompleted() because of the asynchronous
// persistence of progress.
// We should log the error but it shouldn't stop us from stopping the task
// We need to update reindexing progress before we cancel the task
// There is a chance that the task is finished by the time we cancel it in which case we'll get
// a ResourceNotFoundException which we can ignore.
// We set reindexing progress at least to 1 for a running process to be able to
// distinguish a job that is running for the first time against a job that is restarting.
// The task is not present which means either it has not started yet or it finished.
// We keep track of whether the task has finished so we can use that to tell whether the progress 100.
// The task is not present which means either it has not started yet or it finished.
// We keep track of whether the task has finished so we can use that to tell whether the progress 100.
// This may happen if there is no reindexing task id set which means we either never started the task yet or we're finished
/**
/*
/**
/*
/*
/*
/**
// We've set allow_partial_search_results to false which means if something
// goes wrong the request will throw.
// Request was successful so we can restore the flag to retry if a future failure occurs
// This ensures the search throws if there are failures and the scroll context gets cleared automatically
// if values is empty then it means it's a missing value
// we are here if we have a missing value but the analysis does not support those
// or the value type is not supported (e.g. arrays, etc.)
// This could be a transient error with the scroll Id.
// Reinitialise the scroll and try again but only once.
/*
/*
/**
/**
/*
/**
// If the user has not explicitly included fields we'll include all compatible fields
// If the inclusion set does not match anything, that means the user's desired fields cannot be found in
// the collection of supported field types. We should let the user know.
// If the exclusion set does not match anything, that means the fields are already not present
// no need to raise if nothing matched
// Re-wrap our exception so that we throw the same exception type when there are no fields.
// Check requirements first
// If both are multi-fields it means there are several. In this case parent is the previous multi-field
// we selected. We'll just keep that.
// If we prefer source only the parent may support it. If it does we pick it immediately.
// If any of the two is a doc_value field let's prefer it as it'd support aggregations.
// We check the parent first as it'd be a shorter field name.
// None is aggregatable. Let's pick the parent for its shorter name.
// We convert boolean fields to integers with values 0, 1 as this is the preferred
// way to consume such features in the analytics process regardless of:
//  - analysis type
//  - whether or not the field is categorical
//  - whether or not the field is a dependent variable
/*
/**
// Step 4. Create cardinality by field map and build detector
// Step 3. Get cardinalities for fields with limits
// Step 2. Get field capabilities necessary to build the information of how to extract fields
// Step 1. Get doc value fields limit
// This response gets discarded - the listener handles the real response
/*
// Filter any values in headers that aren't security fields
/**
/**
/*
// Nothing to persist
/*
/*
/**
/**
/*
/**
/**
/**
/**
/**
/*
/*
/**
/*
// Visible for testing
// The task was requested to stop before we created the process context
// Refresh the dest index to ensure data is searchable
// Fetch existing model state (if any)
// Errors during task stopping are expected but we still want to log them just in case.
// This results in marking the persistent task as complete
// Note: We are not marking the task as failed here as we want the user to be able to inspect the failure reason.
// The extra fields are for the doc hash and the control field (should be an empty string)
// The value of the control field should be an empty string for data frame rows
// We add 2 extra fields, both named dot:
//   - the document hash
//   - the control message
// Visible for testing
// Only set the new reason if there isn't one already as we want to keep the first reason (most likely the root cause).
/**
// The job was stopped before we started the process so no need to start it
// If we have no rows, that means there is no data so no point in starting the native process
// just finish the task
/*
/**
// TODO When java 9 features can be used, we will not need the local variable here
// No need to log error as it's due to stopping
/*
// If we are in failed state we drop the results but we let the processor
// parse the output
/*
// For memory estimation the model memory limit here should be set high enough not to trigger an error when C++ code
// compares the limit to the result of estimation.
// The handler passed here will never be called as AbstractNativeProcess.detectCrash method returns early when
// (processInStream == null) which is the case for MemoryUsageEstimationProcess.
/**
/*
// Nothing to persist
/*
// The extra 2 are for the checksum and the control field
/*
/*
// Memory estimation process does not use the input pipe, hence null.
/*
/**
/*
/*
/**
// Let's make sure we have at least one training row
/*
// TODO change back to STRICT_PARSER once native side is aligned
/*
/*
/*
/*
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
// We check if the parent is an object which is indicated by field caps containing an "object" entry.
// If an object, it's not a multi field
/**
/*
// Entry is of the form "38.897676, -77.03653"
/*
// Entry is of the form "POINT (-77.03653 38.897676)"
/*
/*
/*
/*
// doc_value field with the epoch_millis format
// pre-6.0 field
/*
// Even if the column names are overridden we need to know if there's a
// header in the file, as it affects which rows are considered records
// The column names are the header names but with dots replaced with underscores and blanks named column1, column2, etc.
// null to allow GC before timestamp search
// We make the assumption that the timestamp will be on the first line of each record.  Therefore, if the
// timestamp is the last column then either our assumption is wrong (and the approach will completely
// break down) or else every record is on a single line and there's no point creating a multiline config.
// This is why the loop excludes the last column.
// Tolerate extra columns if and only if they're empty
// Tolerate an incomplete last row
// This should have been enforced by canCreateFromSample()
// SuperCSV will put nulls in the header if any columns don't have names, but empty strings are better for us
// Check lengths
// Check edit distances between short fields
// The reason that only short fields are included is that sometimes
// there are "message" fields that are much longer than the other
// fields, vary enormously between rows, and skew the comparison.
/**
/**
/**
/**
// There are some examples with pretty pictures of the matrix on Wikipedia here:
// http://en.wikipedia.org/wiki/Levenshtein_distance
// Populate the left column
// Calculate the other entries in the matrix
// We could allocate a new array for currentCol here, but it's more efficient to reuse the one that's now redundant
// Do the strings differ at the point we've reached?
// No, they're the same => no extra cost
// Yes, they differ, so there are 3 options:
// 1) Deletion => cell to the left's value plus 1
// 2) Insertion => cell above's value plus 1
// 3) Substitution => cell above left's value plus 1
// Take the cheapest option of the 3
// Result is the value in the bottom right hand corner of the matrix
// Logstash's CSV parser won't tolerate fields where just part of the
// value is quoted, whereas SuperCSV will, hence this extra check
// Tolerate extra columns if and only if they're empty
// Tolerate an incomplete last row
/*
/**
/*
/**
/**
/**
// Dates are treated like strings for top hits
/**
// This should not happen in the usual context this class is used in within the file structure finder,
// as "double" should be big enough to hold any value that the file structure finder considers numeric
/**
// Updating a running mean like this is more numerically stable than using (sum / count)
// Simple case - median is middle value
// More complicated case - median is average of two middle values
// Both target values are the same
/**
/*
/**
/**
/*
/**
/**
/**
/*
/**
/**
// NDJSON will often also be valid (although utterly weird) CSV, so NDJSON must come before CSV
/**
/**
// Creating the reader will throw if the specified character set does not exist
// Add a dummy exception containing the explanation so far - this can be invaluable for troubleshooting as incorrect
// decisions made early on in the structure analysis can result in seemingly crazy decisions or timeouts later on
// We need an input stream that supports mark and reset, so wrap the argument
// in a BufferedInputStream if it doesn't already support this feature
// This is from ICU4J
// Determine some extra characteristics of the input to compensate for some deficiencies of ICU4J
// If the input is pure ASCII then many single byte character sets will match.  We want to favour
// UTF-8 in this case, as it avoids putting a bold declaration of a dubious character set choice
// in the config files.
// Input wasn't pure ASCII, so use the best matching character set that's supported by both Java and Go.
// Additionally, if the input contains zero bytes then avoid single byte character sets, as ICU4J will
// suggest these for binary files but then
// This extra test is to avoid trying to read binary files as text.  Running the structure
// finding algorithms on binary files is very slow as the binary files generally appear to
// have very long lines.
// Some character sets cannot be encoded.  These are extremely rare so it's likely that
// they've been chosen based on incorrectly provided binary data.  Therefore, err on
// the side of rejecting binary data.
// If a precise delimiter is specified, we only need one structure finder
// factory, and we'll tolerate as little as one column in the input
// The delimiter is not specified, but some other aspect of delimited files is,
// so clone our default delimited factories altering the overridden values
// We can use the default factories, but possibly filtered down to a specific format
// Don't include any byte-order-marker in the sample.  (The logic to skip it works for both
// UTF-8 and UTF-16 assuming the character set of the reader was correctly detected.)
/*
/**
/*
// NUMBER Grok pattern doesn't support scientific notation, so we extend it
/**
// Accept the first match from the first sample that is compatible with all the other samples
// Get candidate timestamps from the possible field(s) of the first sample record
// Construct the TimestampFormatFinder outside the no-op catch because an exception
// from the constructor indicates a problem with the overridden format
// No possible timestamp format found in this particular field - not a problem
/**
// We can get here if all the records that contained a given field had a null value for it.
// In this case it's best not to make any statement about what the mapping type should be.
// Elasticsearch fields can be either arrays or single values, but array values must all have the same type
/**
/**
// To be mapped as type "date" all the values must match the same timestamp format - if
// they don't we'll end up here, and move on to try other possible mappings
/**
/**
/**
// This removes the interim timestamp field used for semi-structured text formats
/*
/**
/**
/**
// Can't use \b as the breaks, because slashes are not "word" characters
// TODO: would be nice to have IPORHOST here, but HOSTNAME matches almost all words
// A time with no date cannot be stored in a field of type "date", hence "keyword"
// This already includes pre/post break conditions
// Disallow +, - and . before numbers, as well as "word" characters, otherwise we'll pick
// up numeric suffices too eagerly
// TODO: also unfortunately can't have USERNAME in the list as it matches too broadly
// Fixing these problems with overly broad matches would require some extra intelligence
// to be added to remove inappropriate matches.  One idea would be to use a dictionary,
// but that doesn't necessarily help as "jay" could be a username but is also a dictionary
// word (plus there's the international headache with relying on dictionaries).  Similarly,
// hostnames could also be dictionary words - I've worked on machines called "hippo" and
// "scarf" in the past.  Another idea would be to look at the adjacent characters and
// apply some heuristic based on those.
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// The (?m) here has the Ruby meaning, which is equivalent to (?s) in Java
/**
// If the pattern doesn't match then captures will be null
// If the mapping is type "date" with no format, try to adjust it to include the format
// This feels like it shouldn't happen, but there may be some obscure edge case
// where it does, and in production it will cause less frustration to just return
// a mapping type of "date" with no format than to fail the whole analysis
/**
// If the pattern doesn't match then captures will be null
/**
/**
/**
// If the pattern doesn't match then captures will be null
// Exclude the time field because that will be dropped and replaced with @timestamp
/*
/**
/*
/**
/*
// Is it appropriate to treat a file that is neither structured nor has
// a regular pattern of timestamps as a log file?  Probably not...
// Simple case
// If message is null here then the sample probably began with the incomplete ending of a previous message
// We count lines before the first message as consumed (just like we would
// for the CSV header or lines before the first XML document starts)
// This check avoids subsequent problems when a massive message is unwieldy and slow to process
// Don't add the last message, as it might be partial and mess up subsequent pattern finding
// null to allow GC before Grok pattern search
// We can't parse directly into @timestamp using Grok, so parse to some other time field, which the date filter will then remove
/*
// This works because, by default, dot doesn't match newlines
/**
/*
/**
// only accessed within synchronized methods
/**
/**
/**
/**
// If a timeout has occurred then this check will overwrite any timeout exception thrown by Grok.captures() and this
// is intentional - the exception from this class makes more sense in the context of the find file structure API
// Even though close() cancels the timer, it's possible that it can already be running when close()
// is called, so this check prevents the effects of this method occurring after close() returns
/**
/*
/**
// The ? characters in this must match INDETERMINATE_FIELD_PLACEHOLDER
// above, but they're literals in this regex to aid readability
/**
// The simple regex here is based on the fact that the %{MONTH} Grok pattern only matches English and German month names
// The simple regex here is based on the fact that the %{DAY} Grok pattern only matches English and German day names
/**
/**
// The TOMCAT_DATESTAMP format has to come before ISO8601 because it's basically ISO8601 but
// with a space before the timezone, and because the timezone is optional in ISO8601 it will
// be recognised as that with the timezone missed off if ISO8601 is checked first
// In DATESTAMP the month may be 1 or 2 digits, but the day must be 2
// In DATE the month may be 1 or 2 digits, but the day must be 2
// This one is an ISO8601 date with no time, but the TIMESTAMP_ISO8601 Grok pattern doesn't cover it
/**
// These two are not volatile because the class is explicitly not for use from multiple threads.
// But if it ever were to be made thread safe, making these volatile would be one required step.
/**
/**
/**
// Special case of fractional seconds
// No need to append to the Grok pattern as %{SECOND} already allows for an optional fraction,
// but we need to remove the separator that's included in %{SECOND} (and that might be escaped)
/**
// First check for a special format string
// Next check for a built-in candidate that incorporates the override, and prefer this
// If the override is not a valid format then one or other of these two calls will
// throw, and that is how we'll report the invalid format to the user
// This timestamp (2001-02-03T04:05:06,123456789+0545) is chosen such that the month, day and hour all have just 1 digit.
// This means that it will distinguish between formats that do/don't output leading zeroes for month, day and hour.
// Additionally it has the full 9 digits of fractional second precision, to avoid the possibility of truncating the fraction.
// Modify the built-in candidate so it prefers to return the user supplied format
// if at all possible, and only falls back to standard logic for other situations
// TODO consider support for overriding the locale too
// But since Grok only supports English and German date words ingest
// via Grok will fall down at an earlier stage for other languages...
// None of the out-of-the-box formats were close, so use the built Grok pattern and simple regex for the override
/**
// Since a search in a long string that has sections that nearly match will be very slow, it's
// worth doing an initial sanity check to see if the relative positions of digits necessary to
// get a match exist first
/**
// Sharing formats considerably reduces the memory usage during the analysis
// when there are many samples, so reconstruct the match with a shared format
/**
// Nothing to do
/**
// The highest possible weight is 1, so if the difference between the two highest weights
// is less than the number of lines remaining then the leader cannot possibly be overtaken
/**
/**
/**
/**
// If the selected format is already at the beginning of the list there's nothing to do
// Swap the selected format with the one that's currently at the beginning of the list
/**
/**
// If errorOnNoTimestamp is set and we get here it means no samples have been added, which is likely a programmer mistake
/**
// If errorOnNoTimestamp is set and we get here it means no samples have been added, which is likely a programmer mistake
/**
// If errorOnNoTimestamp is set and we get here it means no samples have been added, which is likely a programmer mistake
/**
// If errorOnNoTimestamp is set and we get here it means no samples have been added, which is likely a programmer mistake
/**
// If errorOnNoTimestamp is set and we get here it means no samples have been added, which is likely a programmer mistake
/**
// With multiple formats, only consider the matches that correspond to the first
// in the list (which is what we're returning information about via the getters).
// With just one format it's most efficient not to bother checking formats.
/**
// This method needs rework if the class is ever made thread safe
/**
/**
// Inconsistency
// Inconsistency
/**
// Valid indeterminate day/month numbers will be in the range 1 to 31.
// -1 is used to mean "not present", and we ignore that here.
// If there are many more values of one number than the other then assume that's the day
// This happens in the following cases:
// - No indeterminate numbers (in which case the answer is irrelevant)
// - Only one indeterminate number (in which case we favour month over day)
// firstCardinality can be 0, but then secondCardinality should have been 0 too
/**
// Fall back to whether the day comes before the month in the default short date format for the server locale.
// Can't use 1 as that occurs in 1970, so 3rd Feb is the earliest date that will reveal the server default.
/**
/**
/**
// If errorOnNoTimestamp is set and we get here it means no samples have been added, which is likely a programmer mistake
/**
// There's no format for TAI64N in the timestamp formats used in mappings
/**
/**
//akira.ruc.dk/~keld/teaching/algoritmedesign_f08/Artikler/09/Baeza92.pdf">A New Approach to Text Searching</a>
// Note that this only compares up to the highest bit that is set, so trailing non digit characters will not participate
// in the comparison.  This is not currently a problem for this class, but is something to consider if this functionality
// is ever reused elsewhere.  The solution would be to use a wrapper class containing a BitSet and a separate int to store
// the length to compare.
// 63 here is the largest bit position (starting from 0) in a long
// Since we control the input we should avoid the situation
// where the pattern to find has more bits than a single long
// ~1L means all bits set except the least significant
// This array has one entry per "character" in the "alphabet" (which for this method consists of just 0 and 1)
// ~0L means all bits set
/**
/**
/**
/**
/**
/**
/**
// Do the merge like this to preserve ordering
// The merged format is exactly the same as this format, so there's no need to create a new object
/**
// This picks out punctuation that is likely to represent a field separator.  It deliberately
// leaves out punctuation that's most likely to vary between field values, such as dots.
// Used for deciding whether an ISO8601 timestamp contains a timezone.
/**
/**
/**
/**
// Parse leniently under the assumption the first sequence of hashes is day and the
// second is month - this may not be true but all we do is extract the numbers
// TODO consider support for overriding the locale too
// But it's not clear-cut as Grok only knows English and German date
// words and for indeterminate formats we're expecting numbers anyway
// Now parse again leniently under the assumption the first sequence of hashes is month and the
// second is day - we have to do it twice and extract day as the lenient parser will wrap months > 12
// TODO consider support for overriding the locale too
// But it's not clear-cut as Grok only knows English and German date
// words and for indeterminate formats we're expecting numbers anyway
// Move on to the next format
/**
// This means that in the case of a literal Z, XXX is preferred
// The (?m) here has the Ruby meaning, which is equivalent to (?s) in Java
// The Elasticsearch ISO8601 parser requires a literal T between the date and time, so
// longhand formats are needed if there's a space instead
// Seconds are optional in ISO8601
// Add fractional seconds pattern if appropriate
// Add timezone if appropriate - in the case of a literal Z, XX is preferred
// This method should not have been called if the example didn't include the bare minimum of date and time
// INDETERMINATE_FIELD_PLACEHOLDER here could represent either a day number (d) or month number (M) - it
// will get changed later based on evidence from many examples
// The Grok pattern should ensure we got at least as far as the year
// If we haven't consumed the whole example then we should have got as far as
// the (whole) seconds, and the bit afterwards should be the fractional seconds
/*
// Tolerate an incomplete last record as long as we have one complete record
// null to allow GC before timestamp search
// If we get here the XML parser should have confirmed this
//apache.org/xml/features/disallow-doctype-decl", true);
// The next 5 should be irrelevant given the previous 1, but it doesn't hurt to set them just in case
//xml.org/sax/features/external-parameter-entities", false);
//xml.org/sax/features/external-general-entities", false);
//apache.org/xml/features/nonvalidating/load-external-dtd", false);
/*
/**
// This processing is extremely complicated because it's necessary
// to create a new XML stream reader per document, but each one
// will read ahead so will potentially consume characters from the
// following document.  We must therefore also recreate the string
// reader for each document.
// Find the position that's one character beyond end of the end element.
// The next document (if there is one) must start after this (possibly
// preceeded by whitespace).
// Line and column numbers start at 1, not 0
/*
// How many total inference processors are allowed to be used in the cluster.
// Used for testing
// If multiple inference processors are in the same pipeline, it is wise to tag them
// The tag will keep default value entries from stepping on each other
// Package private for testing
/*
/*
/*
/**
/**
/**
/**
// If we the model is not loaded and we did not kick off a new loading attempt, this means that we may be getting called
// by a simulated pipeline
/**
// It is referenced by a pipeline, but the cache does not contain it
// If the loaded model is referenced there but is not present,
// that means the previous load attempt failed or the model has been evicted
// Attempt to load and cache the model if necessary
// if the cachedModel entry is null, but there are listeners present, that means it is being loaded
// synchronized (loadingListeners)
// If there is no loadingListener that means the loading was canceled and the listener was already notified as such
// Consequently, we should not store the retrieved model
// synchronized (loadingListeners)
// synchronized (loadingListeners)
// If we failed to load and there were listeners present, that means that this model is referenced by a processor
// Alert the listeners to the failure
// If ingest data has not changed or if the current node is not an ingest node, don't bother caching models
// The listeners still waiting for a model and we are canceling the load?
// If we had models still loading here but are no longer referenced
// we should remove them from loadingListeners and alert the listeners
// Remove all cached models that are not referenced by any processors
// Remove the models that are no longer referenced
// Remove all that are still referenced, i.e. the intersection of allReferencedModelKeys and referencedModels
// Populate loadingListeners key so we know that we are currently loading the model
// synchronized (loadingListeners)
// Execute this on a utility thread as when the callbacks occur we don't want them tying up the cluster listener thread pool
/*
/**
// the configurations are expected to be small
// do not allow anything outside of the defined schema
// Add the doc_type field
/*
/**
// These parsers follow the pattern that metadata is parsed leniently (to allow for enhancements), whilst config is parsed strictly
// Opting for a `-` and not `#` for HTML encoding pains
/*
// TODO should we check length against allowed stream size???
// use sort to get the last
// use sort to get the last
/**
// We previously expanded the IDs.
// If the config has gone missing between then and now we should throw if allowNoResources is false
// Otherwise, treat it as if it was never expanded to begin with.
// Ensure sorted even with the injection of locally resourced models
// If there are no resources, there might be no mapping for the id field.
// This makes sure we don't get an error if that happens.
// we only care about the item id's
// If the resourceId is not _all or *, we should see if it is a comma delimited string with wild-cards
// e.g. id1,id2*,id3
// This should never happen. If we were able to deserialize the object (from Native or REST) and then fail to serialize it again
// that is not the users fault. We did something wrong and should throw.
/*
/**
/**
/**
// TODO JIndex we shouldn't be building the job here
// Try to get the job from the cluster state
/**
/**
// Check for duplicate jobs
// Merge cluster state and index jobs
// ignore
/**
/**
// Check for the job in the cluster state first
// the underlying error differs depending on which way around the clashing fields are seen
// A job has the same Id as one of the group names
// error with the first in the list
// check the new groups are not job Ids
// Autodetect must be updated if the fields that the C++ uses are changed
// No need to do anything
// calendarJobIds may be a group or job
// Merge the expended group members with the request Ids.
// Ids that aren't jobs will be filtered by isJobOpen()
// Step 3. After the model size stats is persisted, also persist the snapshot's quantiles and respond
// -------
// The quantiles can be large, and totally dominate the output -
// it's clearer to remove them as they are not necessary for the revert op
// Step 2. When the model_snapshot_id is updated on the job, persist the snapshot's model size stats with a touched log time
// so that a search for the latest model size stats returns the reverted one.
// -------
// Step 1. update the job
// -------
/*
/**
/**
/**
/*
/**
/**
// TODO: remove in 8.0.0
// Try to allocate jobs according to memory usage, but if that's not possible (maybe due to a mixed version cluster or maybe
// because of some weird OS problem) then fall back to the old mechanism of only considering numbers of assigned jobs
// First check conditions that would rule out the node regardless of what other tasks are assigned to it
// Assuming the node is eligible at all, check loading
// TODO: remove this in 8.0.0
// If this will be the first job assigned to the node then it will need to
// load the native code shared libraries, so add the overhead for this
// If we cannot get the job memory requirement,
// fall back to simply allocating by job count
// If we cannot get the available memory on any machine in
// the cluster, fall back to simply allocating by job count
// Means we have lazy nodes left to allocate
// find all the anomaly detector job tasks assigned to this node
// Don't count CLOSED or FAILED jobs, as they don't consume native memory
// find all the data frame analytics job tasks assigned to this node
// Don't count stopped and failed df-analytics tasks as they don't consume native memory
// The native process is only running in the ANALYZING and STOPPING states, but in the STARTED
// and REINDEXING states we're committed to using the memory soon, so account for it here
// if any jobs are running then the native code will be loaded, but shared between all jobs,
// so increase the total memory usage of the assigned jobs to account for this
/*
/**
/*
/**
/**
/**
// Ignore empty tokens for categorization
/**
/**
/*
/**
/**
// Can't use \b as the breaks, because slashes are not "word" characters
// TODO: would be nice to have IPORHOST here, but HOST matches almost all words
// This already includes pre/post break conditions
// Disallow +, - and . before numbers, as well as "word" characters, otherwise we'll pick
// up numeric suffices too eagerly
// TODO: also unfortunately can't have USERNAME in the list as it matches too broadly
// Fixing these problems with overly broad matches would require some extra intelligence
// to be added to remove inappropriate matches.  One idea would be to use a dictionary,
// but that doesn't necessarily help as "jay" could be a username but is also a dictionary
// word (plus there's the international headache with relying on dictionaries).  Similarly,
// hostnames could also be dictionary words - I've worked on machines called "hippo" and
// "scarf" in the past.  Another idea would be to look at the adjacent characters and
// apply some heuristic based on those.
/**
// The first string in this array will end up being the empty string, and it doesn't correspond
// to an "in between" bit.  Although it could be removed for "neatness", it actually makes the
// loops below slightly neater if it's left in.
//
// E.g., ".*?cat.+?sat.+?mat.*" -> [ "", "cat", "sat", "mat" ]
// Create a pattern that will capture the bits in between the fixed parts of the regex
//
// E.g., ".*?cat.+?sat.+?mat.*" -> Pattern (.*?)cat(.+?)sat(.+?)mat(.*)
// E.g., if the input regex was ".*?cat.+?sat.+?mat.*" then the example
// "the cat sat on the mat" will result in "the ", " ", " on the ", and ""
// being added to the 4 "in between" collections in that order
// We should never get here.  If we do it implies a bug in the original categorization,
// as it's produced a regex that doesn't match the examples.
// Finally, for each collection of "in between" bits we look for the best Grok pattern and incorporate
// it into the overall Grok pattern that will match the each example in its entirety
// Remember (from the first comment in this method) that the first element in this array is
// always the empty string
/**
/**
// If the pattern doesn't match then captures will be null.  But we expect this
// method to only be called after validating that the pattern does match.
/**
/**
/**
/*
/**
/**
// We're at the first character of a candidate token, so record the offset
// We don't return tokens that are hex numbers, and it's most efficient to keep a running note of this
// Count dots and dashes as numeric
// If we get here, we've found a separator character having built up a candidate token
// The candidate token is valid to return
// The candidate token is not valid to return, i.e. it's hex or begins with a digit, so wipe it and carry on searching
// We need to recheck whether we've got a valid token after the loop because
// the loop can also be exited on reaching the end of the stream
// Strip dots, dashes and underscores at the end of the token
// Characters that may exist in the term attribute beyond its defined length are ignored
// Set final offset
// Adjust any skipped tokens
/*
/**
/*
/*
/*
/*
/*
/**
/**
/*
/**
/*
/**
/**
/**
// If not using the default sort field (timestamp) add it as a secondary sort
/*
/**
/**
/**
/*
/**
/**
/**
/*
/**
/**
// the job already exists
/**
/**
/**
// Applying the update may result in a validation error
/**
/**
// Applying the update may result in a validation error
/**
/**
/**
/**
// some required jobs were not found
/**
// TODO A better way to handle this rather than just ignoring the error?
// some required jobs were not found
/**
/**
/**
// TODO A better way to handle this rather than just ignoring the error?
/**
// match all
// field exists only when the job is marked as deleting
/*
/**
/**
/**
/*
/**
// _doc is the most efficient sort order and will also disable scoring
/**
// _doc is the most efficient sort order and will also disable scoring
/**
// _doc is the most efficient sort order and will also disable scoring
/**
// _doc is the most efficient sort order and will also disable scoring
// Wrapper to ensure safety
/*
/**
/**
/**
/*
/**
/**
// If the supplied bucket has records then create a copy with records
// removed, because we never persist nested records in buckets
/**
/**
/**
/**
// for testing
/**
// Don't commit as we expect masses of these updates and they're not
// read again by this process
/**
/**
/**
/**
/**
/**
/**
// We refresh using the read alias in order to ensure all indices will
// be refreshed even if a rollover occurs in between.
// Refresh should wait for Lucene to make the data searchable
/**
// Refresh should wait for Lucene to make the data searchable
/**
/*
/**
// Consider the possibility that some of the responses are exceptions
// There's a further complication, which is that msearch doesn't translate a
// closed index cluster block exception into a friendlier index closed exception
// Don't wrap the original exception, because then it would be the root cause
// and Kibana would display it in preference to the friendlier exception
/**
// Our read/write aliases should point to the concrete index
// If the initial index is NOT an alias, either it is already a concrete index, or it does not exist yet
// SHOULD NOT be closed as in typical call flow checkForLeftOverDocuments already verified this
// if it is closed, we bailout and return an error
// Indices can be shared, so only create if it doesn't exist already. Saves us a roundtrip if
// already in the CS
// This assumes the requested mapping will be merged with mappings from the template,
// and may need to be revisited if template merging is ever refactored
// Possible that the index was created while the request was executing,
// so we need to handle that possibility
// Add the term field mappings and alias.  The complication is that the state at the
// beginning of the operation doesn't have any knowledge of the index, as it's only
// just been created.  So we need yet another operation to get the mappings for it.
// Expect one index.  If this is not the case then it means the
// index has been deleted almost immediately after being created, and this is
// so unlikely that it's reasonable to fail the whole operation.
// take into account object and nested fields:
// Put the whole mapping, not just the term fields, otherwise we'll wipe the _meta section of the mapping
/**
// look for both old and new formats
/**
// These next two document IDs never need to be the legacy ones due to the rule
// that you cannot open a 5.4 job in a subsequent version of the product
/**
/**
/**
/**
/**
// This now gets the first 10K records for a bucket. The rate of records per bucket
// is controlled by parameter in the c++ process and its default value is 500. Users may
// change that. Issue elastic/machine-learning-cpp#73 is open to prevent this.
// Find the records using the time stamp rather than a parent-child
// relationship.  The parent-child filter involves two queries behind
// the scenes, and Elasticsearch documentation claims it's significantly
// slower.  Here we rely on the record timestamps being identical to the
// bucket timestamp.
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Step 2. Find the count, mean and standard deviation of memory usage over the time span of the last N bucket results,
//         where N is the number of buckets required to consider memory usage "established"
// model size stats haven't changed in the last N buckets,
// so the latest (older) ones are established
// no need to do an extra search in the case of exactly one document being aggregated
// is there sufficient stability in the latest model size stats readings?
// yes, so return the latest model size as established
// no - we don't have an established model size
// Step 1. Find the time span of the most recent N bucket results, where N is the number of buckets
//         required to consider memory usage "established"
// Find all the calendars used by the job then the events for those calendars
/**
/*
/**
/*
/**
// Including interim results does not stop final results being
// shown, so including interim results means no filtering on the
// isInterim field
// Implemented as "NOT isInterim == true" so that not present and null
// are equivalent to false.  This improves backwards compatibility.
// Also, note how for a boolean field, unlike numeric term queries, the
// term value is supplied as a string.
/*
/**
/**
/**
/*
/**
/**
/**
// First try to restore model state.
// Secondly try to restore categorizer state. This must come after model state because that's
// the order the C++ process expects.  There are no snapshots for this, so the IDs simply
// count up until a document is not found.  It's NOT an error to have no categorizer state.
/*
/**
/** Persisted timing stats. May be stale. */
/** Current timing stats. */
/** Object used to persist current timing stats. */
// Don't flush if current timing stats are identical to the persisted ones
/**
/**
/**
/*
/*
/*
/*
/*
/**
/**
/**
/*
/**
/**
// report at various boundaries
/**
/**
/**
/**
/**
/**
/**
/**
// Start reducing the logging rate after a million records have been seen
/*
/**
/**
/*
/**
/**
/*
/**
/**
// Though this setting is dynamic, it is only set when a new job is opened. So, already running jobs will not get the updated value.
/**
/**
// 3 hours
/**
// 6 hours
/**
/**
/**
/**
/**
// Input is always length encoded
// Limit the number of output records
// always set the time field
// Supply a URL for persisting/restoring model state unless model
// persistence has been explicitly disabled.
// Persist model state every few hours even if the job isn't closed
/**
/**
/**
// createTempFile has a race condition where it may return the same
// temporary file name to different threads if called simultaneously
// from multiple threads, hence add the thread ID to avoid this
// write to a temporary field config file
/*
/**
/**
/**
// In this case the original exception is spurious and highly misleading
// Filters have to be written before detectors
// Add detector rules
// Add scheduled events; null means there's no update but an empty list means we should clear any events in the process
// We also have to wait for the normalizer to become idle so that we block
// clients from querying results in the middle of normalization.
/**
// Don't log here - it just causes double logging when the exception gets logged
// Don't log here - it just causes double logging when the exception gets logged
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
/*
// If the process is missing but the task exists this is most likely
// due to 2 reasons. The first is because the job went into the failed
// state then the node restarted causing the task to be recreated
// but the failed process wasn't. The second is that the job went into
// the failed state and the user tries to remove it force-deleting it.
// Force-delete issues a kill but the process will not be present
// as it is cleaned up already. In both cases, we still need to remove
// the task from the TaskManager (which is what the kill would do)
/**
/**
/**
/**
// Step 3. Set scheduled events on message and write update process message
// Step 2. Set the filter on the message and get scheduled events
// Step 1. Get the filter
// Start the process
// We need to fork, otherwise we restore model state from a network thread (several GET api calls):
// No need to log here as the persistent task framework will log it
// Don't leave a partially initialised process hanging around
// Make sure the state index and alias exist
// Try adding the results doc mapping - this updates to the latest version if an old mapping is present
// At this point we lock the process context until the process has been started.
// The reason behind this is to ensure closing the job does not happen before
// the process is started as that can result to the job getting seemingly closed
// but the actual process is hanging alive.
// Now that the process is running and we have updated its state we can unlock.
// It is important to unlock before we initialize the communicator (ie. load the model state)
// as that may be a long-running method.
// Copy for consistency within a single method call
// Closing jobs can still be using some or all threads in MachineLearning.JOB_COMMS_THREAD_POOL_NAME
// that an open job uses, so include them too when considering if enough threads are available.
// TODO: in future this will also need to consider jobs that are not anomaly detector jobs
// A TP with no queue, so that we fail immediately if there are no threads available
// If submitting the operation to read the results from the process fails we need to close
// the process too, so that other submitted operations to threadpool are stopped.
/**
// don't remove the process context immediately, because we need to ensure
// it is reachable to enable killing a job while it is closing
// The only way we can get here is if 2 close requests are made very close together.
// The other close has done the work so it's safe to return here without doing anything.
// If the close failed because the process has explicitly been killed by us then just pass on that exception
// to ensure the contract that multiple simultaneous close calls for the same job wait until
// the job is closed is honoured, hold the lock throughout the close procedure so that another
// thread that gets into this method blocks until the first thread has finished closing the job
// delete any tmp storage
/*
/*
// if shutdown with tasks pending notify the handlers
/*
/**
/**
// Create a custom iterator here, because LinkedBlockingDeque iterator and stream are not blocking when empty:
/*
/**
// TODO: should we fail to start?
/*
// The extra 1 is the control field
// if state is null or empty it will be ignored
// else it is used to restore the quantiles
/*
/**
/**
/**
/*
/**
/*
/*
/**
// only used from the process() thread, so doesn't need to be volatile
/**
// Visible for testing
// If a function call in this throws for some reason we don't want it
// to kill the results reader thread as autodetect will be blocked
// trying to write its output.
// Don't log the stack trace in this case.  Log just enough to hint
// that it would have been better to close jobs before shutting down,
// but we now fully expect jobs to move between nodes without doing
// all their graceful close activities.
// Don't log the stack trace to not shadow the root cause.
// We should only get here if the iterator throws in which
// case parsing the autodetect output has failed.
// Delete any existing interim results generated by a Flush command
// which have not been replaced or superseded by new results.
// persist after deleting interim results in case the new
// results are also interim
// execute the bulk request only in some cases or in doubt
// otherwise rely on the count-based trigger
// We need to refresh in order for the snapshot to be available when we try to update the job with it
// We need to make all results written up to these quantiles available for renormalization
// Commit previous writes here, effectively continuing
// the flush from the C++ autodetect process right
// through to the data store
// Interim results may have been produced by the flush,
// which need to be
// deleted when the next finalized results come through
// This blocks the main processing thread in the unlikely event
// there are 2 model snapshots queued up. But it also has the
// advantage of ensuring order
// Although the results won't take 30 minutes to finish, the pipe won't be closed
// until the state is persisted, and that can take a while
// Input stream has been completely processed at this point.
// Wait for any updateModelSnapshotOnJob calls to complete.
// These lines ensure that the "completion" we're awaiting includes making the results searchable
/**
/**
/*
// acknowledgeFlush(...) could be called before waitForFlush(...)
// a flush api call writes a flush command to the analytical process and then via a different thread the
// result reader then reads whether the flush has been acked.
/*
/*
/*
/**
/**
/**
/**
/*
/**
/**
/**
/**
// because 0 means never expire, the default is -1
/*
/**
/**
/*
// epoch in seconds
/**
// The time field doesn't count
/**
// header is all the analysis input fields + the time field + control field
// Write the header
/**
// -2 because last field is the control field, and last but one is the pre-tokenized tokens field
/**
// Using the CsvEncoder directly is faster than using a CsvLineWriter with end-of-line set to the empty string
/**
// Records have epoch seconds timestamp so compare for out of order in seconds
// out of order
// record this timestamp even if the record won't be processed
/**
/**
// TODO header could be empty
/**
// time field
// field for categorization tokens
// control field
/**
/**
/**
/**
/*
/*
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
/**
/**
/**
// null if EoF
// time field doesn't count
/*
/**
/**
/*
/**
/**
/**
/**
/*
/**
/**
/*
/**
/*
/**
/**
/*
/**
/*
// Note: for the Engine API summarycountfield is currently passed as a
// command line option to autodetect rather than in the field config file
/**
// Filters have to be written before the detectors
// As values are written as entire settings rather than part of a
// clause no quoting is needed
// CATEGORIZATION_TOKENIZATION_IN_JAVA is used for performance testing
/*
/**
/**
// The underlying stream, MarkSupportingStreamInputWrapper, doesn't care about
// readlimit, so just set to -1.  We could pick a value, but I worry that if the
// underlying implementation changes it may cause strange behavior, whereas -1 should
// blow up immediately
// marker & 0xFF to deal with Java's lack of unsigned bytes...
// We never expect to get the control field or categorization tokens field
// time field doesn't count
/**
/**
/*
/*
/*
/*
/**
/*
/**
/**
// null token means EOF
// Only do the donkey work of converting the field value to a
// string if we need it
// Convert any scalar values in the array to a comma delimited
// string.  (Arrays of more complex objects are ignored.)
// Scalar values don't need any extra skip code
// Consume the whole array but do nothing with it
/**
/**
/**
/*
/**
// Initialize earliest/latest times
/*
/**
// simplistic way to calculate data sparsity, just take the log and
// check the difference
/**
/**
// flush all we know
/**
/*
/*
// Do nothing as it is not holding the parent score.
/*
/*
/*
/**
/*
/**
// This isn't great as the order must match the order in Normalizer.normalize(),
// but it's only for developers who cannot run the native processes
// Write lineified JSON
// Nothing to do
// Nothing to do
// Nothing to do
// Sanity check: make sure the process hasn't terminated already
/*
/**
// nothing to persist
/*
/*
/**
/**
/**
/**
/*
/**
/**
// Wait for the results handler to finish
/**
/**
/**
/*
/**
/**
/*
/*
/**
/**
/*
/**
/**
/*
/**
/**
/*
// nothing to do
/*
/**
/**
/**
/**
/*
/**
/**
// 30 days
/**
/**
// The updates will have been persisted in batches throughout the renormalization
// process - this call just catches any leftovers
/*
/**
/**
// This will throw NPE if quantiles is null, so do it first
// Needed to ensure work is not added while the tryFinishWork() method is running
// Must add to latchDeque before quantilesDeque
// We cannot tolerate more than one thread running this loop at any time,
// but need a different lock to the other synchronized parts of the code
// We have to wait until idle to avoid a raft of exceptions as other parts of the
// system are stopped after this method returns.  However, shutting down the
// scoresUpdater first means it won't do all pending work; it will stop as soon
// as it can without causing further errors.
// We discard all but the latest quantiles
// Count down the latches associated with any discarded quantiles
// We cannot tolerate new work being added in between the isEmpty() check and releasing the semaphore
// We cannot allow new quantiles to be added while we are failing from a previous renormalization failure.
// We discard all but the earliest quantiles, if they exist
// Count down all the latches as they no longer matter since we failed
// Keep the earliest quantile so that the next call to doRenormalizations() will include as much as the failed normalization
// window as possible.
// Since this latch is already countedDown, there is no reason to put it in the `latchDeque` again
// Exit immediately if another normalization is in progress.  This means we don't hog threads.
// Note that if there is only one set of quantiles in the queue then both these references will point to the same quantiles.
// We could end up with latestQuantilesWithLatch being null if the thread running this method
// was preempted before the tryStartWork() call, another thread already running this method
// did the work and exited, and then this thread got true returned by tryStartWork().
// We could end up with earliestQuantiles being null if quantiles were
// added between getting the earliest and latest quantiles.
// If we're going to skip quantiles, renormalize using the latest quantiles
// over the time ranges implied by all quantiles that were provided.
// Loop if more work has become available while we were working, because the
// tasks originally submitted to do that work will have exited early.
/**
/*
/**
// No more markers in this block
// Ignore blank lines
/*
/*
/**
// maybe null if the batched iterator search return no results
/**
/**
/**
// currentBatch is either null or all its elements have been iterated.
// get the next currentBatch
// BatchedJobsIterator.hasNext maybe true if searching the first time
// but no results are returned.
/*
/**
// _doc is the most efficient sort order and will also disable scoring
// _doc is the most efficient sort order and will also disable scoring
/*
/**
/**
// No snapshot to remove
/*
/**
// Delete the documents gradually.
// With DEFAULT_SCROLL_SIZE = 1000 this implies we spread deletion of 1 million documents over 5000 seconds ~= 83 minutes.
// _doc is the most efficient sort order and will also disable scoring
/*
/*
/**
// not a managed state document id
// TODO Once at 8.0, we can stop searching for jobs in cluster state
// and remove cluster service as a member all together.
// _doc is the most efficient sort order and will also disable scoring
/*
/*
/*
/*
/**
/**
/**
// Do not detect crash when the process is being closed or killed.
// Do not detect crash when the process has been closed automatically.
// This is possible when the process does not have input pipe to hang on and closes right after writing its output.
// The log message doesn't say "crashed", as the process could have been killed
// by a user or other process (e.g. the Linux OOM killer)
/**
// closing its input causes the process to exit
// wait for the process to exit by waiting for end-of-file on the named pipe connected
// to the state processor - it may take a long time for all the model state to be
// indexed
// the log processor should have stopped by now too - assume processing the logs will
// take no more than 5 seconds longer than processing the state (usually it should
// finish first)
// The PID comes via the processes log stream. We do wait here to give the process the time to start up and report its PID.
// Without the PID we cannot kill the process.
// Wait for the process to die before closing processInStream as if the process
// is still alive when processInStream is closed it may start persisting state
// Ignore it - we're shutting down and the method itself has logged a warning
// Ignore it - we're shutting down and the method itself has logged a warning
// Sanity check: make sure the process hasn't terminated already
// Do nothing
// Given we are closing down the process there is no point propagating IO exceptions here
/*
/**
// no-op
/*
/**
// The original implementation of this loop created very deeply nested
// CompositeBytesReference objects, which caused problems for the bulk persister.
// This new implementation uses an intermediate List of blocks that don't contain
// end markers to avoid such deep nesting in the CompositeBytesReference that
// eventually gets created.
/**
// No more zero bytes in this block
// Ignore completely empty chunks
// No validation - assume the native process has formatted the state correctly
/*
/**
/**
/**
/*
/**
/*
/**
/**
// We never terminate the phaser
// If there are no registered parties or no unarrived parties then there is a flaw
// in the register/arrive/unregister logic in another method that uses the phaser
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// A refresh is already in progress, so don't do another
// It's critical that we empty out the current listener list on
// error otherwise subsequent retries to refresh will be ignored
// persistentTasks will be null if there's never been a persistent task created in this cluster
// Do the next iteration in a different thread, otherwise stack overflow
// can occur if the searches happen to be on the local node, as the huge
// chain of listeners are all called in the same thread if only one node
// is involved
/**
// The phaser prevents searches being started after the memory tracker's stop() method has returned
// Phases above 0 mean we've been stopped, so don't do any operations that involve external interaction
// Although recent versions of the code enforce a non-null model_memory_limit
// when parsing, the job could have been streamed from an older version node in
// a mixed version cluster
// TODO: does this also happen if the .ml-config index exists but is unavailable?
// However, note that we wait for the .ml-config index to be available earlier on in the
// job assignment process, so that scenario should be very rare, i.e. somebody has closed
// the .ml-config index (which would be unexpected and unsupported for an internal index)
// during the memory refresh.
/*
/**
/**
// The controller process should already be running by the time this class tries to connect to it, so the timeout
// can be short (although there's a gotcha with EBS volumes restored from snapshot, so not TOO short)
/*
// Sanity check to avoid hard-to-debug errors - tabs and newlines will confuse the controller process
// The C++ process will exit when it gets EOF on the command stream
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
// do not allow any usage below this threshold
// A map to keep track of allocated native storage by resource id
/**
/**
/**
// do not allow to breakout from the tmp storage provided
//bugs.openjdk.java.net/browse/JDK-8162520 */
/* See: https://bugs.openjdk.java.net/browse/JDK-8162520 */
/*
/**
/**
/**
// The way the pipe names are formed MUST match what is done in the controller main()
// function, as it does not get any command line arguments when started as a daemon.  If
// you change the code here then you MUST also change the C++ code in controller's
// main() function.
/**
// The following are specified using two arguments, as the C++ processes could already accept input from files on disk
/**
// The order here is important.  It must match the order that the C++ process tries to connect to the pipes, otherwise
// a timeout is guaranteed.  Also change api::CIoManager in the C++ code if changing the order here.
// Distinguish between pipe not wanted and pipe wanted but not successfully connected
// Distinguish between pipe not wanted and pipe wanted but not successfully connected
// Distinguish between pipe not wanted and pipe wanted but not successfully connected
// Distinguish between pipe not wanted and pipe wanted but not successfully connected
// Distinguish between pipe not wanted and pipe wanted but not successfully connected
// Distinguish between pipe not wanted and pipe wanted but not successfully connected
/*
/**
// if start of an array ignore it, we expect an array of results
/*
/*
/**
// The source bytes are already UTF-8.  The C++ process wants UTF-8, so we
// can avoid converting to a Java String only to convert back again.
// There's a complication that the source can already have trailing 0 bytes
// This is dictated by RapidJSON on the C++ side; it treats a '\0' as end-of-file
// even when it's not really end-of-file, and this is what we need because we're
// sending multiple JSON documents via the same named pipe.
/*
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
// check if there is some leftover from log summarization
// if the process crashed, a non-delimited JSON string might still be in the pipe
/**
// There's an assumption here that 0 is not a valid PID.  This is certainly true for
// userland processes.  On Windows the "System Idle Process" has PID 0 and on *nix
// PID 0 is for "sched", which is part of the kernel.
/**
/**
// If this happens it probably means someone has changed the format in lib/ver/CBuildInfo.cc
// in the ml-cpp repo without changing the pattern above to match
/**
// No more markers in this block
// Ignore blank lines
// This is to work around the problem of log4cxx on Windows
// outputting UTF-16 instead of UTF-8.  For full details see
// https://github.com/elastic/machine-learning-cpp/issues/385
// This isn't expected to ever happen
// Keep the last few error messages to report if the process dies
// get out of here quickly if level isn't of interest
// log message summarization is disabled for debug
// log summarization: log 1st message, count all consecutive messages arriving
// in a certain time window and summarize them as 1 message
// this is a repeated message, so do not log it, but count
// not similar, flush last summary if necessary
// log last message with summary
// TODO: Is there a way to preserve the original timestamp when re-logging?
// add version information, so it's conveniently next to the crash log
// edge case: for 1 repeat, only log the message as is
/*
/**
/**
/**
// todo(hendrikm): workaround, see
// https://github.com/elastic/machine-learning-cpp/issues/123
/**
// Write blank values for all fields other than the control field
// The control field comes last
/*
/**
/**
// This will be used to convert 32 bit integers to network byte order
// 4 == sizeof(int)
/**
/**
/**
// number fields
/**
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
// endpoints that support body parameters must also accept POST
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
// A calendar can be created with just a name or with an optional body
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
/*
/*
// We need to consume the body before returning
/*
/*
/*
/*
/*
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
/*
/*
/*
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
// Send task description id instead of waiting for the message
// We do not want to log anything due to a delete action
// The response or error will be returned to the client when called synchronously
// or it will be stored in the task result when called asynchronously
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// Even though these are null, setting up the defaults in case
// we want to change them later
// TODO: remove deprecated endpoint in 8.0.0
// endpoints that support body parameters must also accept POST
// endpoints that support body parameters must also accept POST
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
// A timestamp in the URL overrides any timestamp that may also have been set in the body
// Check if the REST param is set first so mutually exclusive
// options will cause an error if set
// multiple bucket options
// single and multiple bucket options
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
// endpoints that support body parameters must also accept POST
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
/* parts.subList(i, partsSize).each(joiner::add); */
/* Excluded domains (e.g. !nhs.uk) use the next highest
// NOTE: we don't check SpecialPermission because this will be called (indirectly) from scripts
/**
/* special-snowflake rules now... */
/* for the case where the host is internal like .local so is not a recognised public suffix */
/* HRD is the top private domain */
/*
/**
/*
/**
/**
/**
// Do nothing - the only reason there's a constructor is to allow mocking
/**
// The return type is String because we don't want any (too) clever path processing removing
// the seemingly pointless . in the path used on Windows.
// Use the Java temporary directory.  The Elasticsearch bootstrap sets up the security
// manager to allow this to be read from and written to.  Also, the code that spawns our
// daemon passes on this location to the C++ code using the $TMPDIR environment variable.
// All these factors need to align for everything to work in production.  If any changes
// are made here then CNamedPipeFactory::defaultPath() in the C++ code will probably
// also need to be changed.
/**
/**
// Can't use Files.isRegularFile() on on named pipes on Windows, as it renders them unusable,
// but luckily there's an even simpler check (that's not possible on *nix)
// Try to open the file periodically until the timeout expires, then, if
// it's still not available throw the exception from FileInputStream
// On Windows Files.isRegularFile() will render a genuine named pipe unusable
/**
/**
/**
// Can't use File.isFile() on Windows, but luckily there's an even simpler check (that's not possible on *nix)
// Try to open the file periodically until the timeout expires, then, if
// it's still not available throw the exception from FileOutputStream
/**
// Periodically check whether the file exists until the timeout expires, then, if
// it's still not available throw a FileNotFoundException
// There's a race condition here in that somebody could delete the named pipe at this point
// causing the line below to create a regular file.  Not sure what can be done about this
// without using low level OS calls...
/**
/**
/**
/*
/**
/**
// noinspection Java9CollectionFactory (because the list can contain null entries)
// noinspection Java9CollectionFactory (because the list can contain null entries)
/**
/*
/**
/*
/**
/*
/**
/**
/**
/**
/**
/**
/*
/**
/*
// Having an exponent higher than this causes integer overflow
// Since we exponentially increase, we don't want force randomness to have an excessively long sleep
// Exponential backoff calculation taken from: https://en.wikipedia.org/wiki/Exponential_backoff
// Its good to have a random window along the exponentially increasing curve
// so that not all bulk requests rest for the same amount of time
// We should only retry the docs that failed.
// If we failed, lets set the bulkRequest to be a collection of the failed requests
/*
// Pick a license that does not allow machine learning
// test that license restricted apis do not work
// Pick a license that does allow machine learning
// test that license restricted apis do now work
// test that license restricted apis do now work
// Pick a license that does not allow machine learning
// test that license restricted apis do not work
// Pick a license that does allow machine learning
// now that the license is invalid, the job should get closed:
// test that license restricted apis do now work
// test that license restricted apis do now work
// Pick a license that does not allow machine learning
// test that license restricted apis do not work
// Pick a license that does allow machine learning
// test that license restricted apis do now work
// put job
// put datafeed
// open job
// start datafeed
// now that the license is invalid, the job should be closed and datafeed stopped:
// open job
// start datafeed
// now that the license is invalid, the job should be closed and datafeed stopped:
// test that license restricted apis do now work
// Pick a license that does not allow machine learning
// now that the license is invalid, the job should get closed:
// test that license restricted apis do not work
// Pick a license that does allow machine learning
// test that license restricted apis do now work
// re-open job now that the license is valid again
// test that license restricted apis do now work
// the stop datafeed due to invalid license happens async, so check if the datafeed turns into stopped state:
// the close due to invalid license happens async, so check if the job turns into closed state:
// test that license restricted apis do now work
// the close due to invalid license happens async, so check if the job turns into closed state:
// test that license restricted apis do now work
// Pick a random license
// test that license restricted apis do now work
// Pick a random license
// Creating a pipeline should work
// Pick a license that does not allow machine learning
// Inference against the previous pipeline should still work
// Creating a new pipeline with an inference processor should work
// Inference against the new pipeline should fail since it has never previously succeeded
// Simulating the pipeline should fail
// Pick a license that does allow machine learning
// test that license restricted apis do now work
//both ingest pipelines should work
// Pick a license that does not allow machine learning
// inferring against a model should now fail
// Inferring with previously Licensed == true should pass, but indicate license issues
// Pick a license that does allow machine learning
/*
/**
/*
/*
/*
// set local node master
// no longer master
// set local node master
// no longer master
// set local node master
// no longer master
/*
// index is present but no routing
// create a datafeed without aggregations or anything
// else that may cause validation errors
/*
// v5.4 jobs did not have a version and should not have a new one set
// assert that for each datafeed its corresponding job is selected
// assert that for each datafeed its corresponding job is selected
// job tasks
// datafeed tasks
// The unallocated task should be modifed
// the allocated task should not be modified
// unallocated datafeed should be updated
// allocated datafeed will not be updated
// create a datafeed without aggregations or anything
// else that may cause validation errors
/*
/*
/*
// Because we check if the job for the datafeed exists and we don't
// allow two datafeeds to exist for a single job we have to add both
// a job and a datafeed here
/*
/**
// Disable native ML autodetect_process as the c++ controller won't be available
// Disable security otherwise delete-by-query action fails to get authorized
/**
//github.com/elastic/elasticsearch/issues/38952
// Delete the ML indices apart from the annotations index.  The annotations index will be deleted by the
// base class cleanup.  We want to delete all the others first so that the annotations index doesn't get
// automatically recreated.
// block until the templates are installed
/*
// force close so not an error for the failed job
// not a force close so is an error
// hack but it saves a lot of mocking
// This method should return immediately because the job is already closed.
// Check that the listener is called. If a different code path was taken the
// listener wouldn't be called without extensive mocking
/*
/**
/*
/*
/*
/*
/*
// An index being unavailable should take precedence over waiting for a lazy node
/*
/*
/*
// At present the only critical index is the config index
/*
/*
/*
/*
// We need to return empty counts so that the lookback doesn't update the last end time
// advance time
// Execute a second valid time, but do so in a smaller window than the interval
// Execute a third time, but this time make sure we exceed the data check interval, but keep the delayedDataDetector response
// the same
// Execute a fourth time, this time we return a new delayedDataDetector response to verify annotation gets updated
// What we expect the updated annotation to be indexed as
// Execute a fifth time, no changes should occur as annotation is the same
// We should not get 3 index requests for the annotations
/*
// Check with multiples
// Now non-multiples
/*
// Verify datafeed has not started running yet as job is still opening
// Still no run
// Now it should run as the job state changed to OPENED
// Verify datafeed has not started running yet as job doesn't have an open autodetect communicator
// Still no run
// Now it should run as the autodetect communicator is open
// Verify datafeed has not started running yet as job is stale (i.e. even though opened it is part way through relocating)
// Still no run
// Now it should run as the job state chanded to OPENED
// Verify datafeed has not started running yet as job is still opening
// Verify task never run and got stopped
// Verify datafeed has not started running yet as job is still opening
// Stop the datafeed
// Update job state to opened
// Verify no datafeed was run
/*
// Using wildcard index name to test for index resolving as well
// Using wildcard index name to test for index resolving as well
// Using wildcard index name to test for index resolving as well
// Set to lower allocationId, so job task is stale:
// Here we test that when there are 2 problems, the most critical gets reported first.
// In this case job is Opening (non-critical) and the index does not exist (critical)
/*
/*
// This call would normally trigger persisting but because of the "disallowPersisting" call above it will not.
/*
/*
// Should not throw
// Should not throw
// Should not throw
// Should not throw
/*
// Test with remote index, aggregation, and no chunking
// Test with remote index, aggregation, and chunking
// Test with remote index, no aggregation, and no chunking
// Test with remote index, no aggregation, and chunking
/*
/*
// Each bucket is 4 key-value pairs and there are 2 terms, thus 600 buckets will be 600 * 4 * 2 = 4800
// key-value pairs. They should be processed in 5 batches.
/*
/*
/*
/*
// 200 * 1_000 == 200_000
// 300K millis * 1000 * 10 / 15K docs = 200000
// 300K millis * 500 * 10 / 15K docs = 100000
// 30K millis * 1000 * 10 / 150K docs = 2000 < min of 60K
// 100 millis * 1000 * 10 / 10 docs = 100000
// 300K millis * 500 * 10 / 15K docs = 100000
// This one is empty
// Now we have: 200K millis * 500 * 10 / 5K docs = 200000
// This is the last one
// do nothing
/*
// We should clear the scroll context twice: once for the first search when we retry
// and once after the retry where we'll have an exception
// first response is good
// this should recover from the first shard failure and try again
// A second failure is not tolerated
// the new start time after error is the last record timestamp +1
// this will throw a SearchPhaseExecutionException
// this will throw a SearchPhaseExecutionException
// first response is good
// this should recover from the SearchPhaseExecutionException and try again
// A second failure is not tolerated
// We should clear the scroll context twice: once for the first search when we retry
// and once after the retry where we'll have an exception
// Check for the scripts
/*
/*
// Prior to 6.x, timestamps were simply `long` milliseconds-past-the-epoch values
/*
/*
/*
/*
/*
// First batch
// Second batch
// Third batch is empty
// First batch
// Second batch
// Third batch should return empty
// Now let's assert we're sending the expected search request
// Check continue scroll requests had correct ids
// Check we cleared the scroll with the latest scroll id
// First search will fail
// Next one will succeed
// Last one
// First batch expected as normally since we'll retry after the error
// Next batch should return empty
// Check we cleared the scroll with the latest scroll id
// First search will fail
// Next one fails again
// Search will succeed
// But the first continue scroll fails
// The next one succeeds and we shall recover
// Last one
// First batch expected as normally since we'll retry after the error
// We get second batch as we retried after the error
// Next batch should return empty
// Notice we've done two searches and two continues here
// Check we cleared the scroll with the latest scroll id
// Search will succeed
// But the first continue scroll fails
// As well as the second
// First batch expected as normally since we'll retry after the error
// We get second batch as we retried after the error
// Explicit cast of ExtractedField args necessary for Eclipse due to https://bugs.eclipse.org/bugs/show_bug.cgi?id=530915
// First and only batch
// Empty
// First batch
// Third batch should return empty
// First and only batch
// Empty
// First batch
// Third batch should return empty
// Explicit cast of ExtractedField args necessary for Eclipse due to https://bugs.eclipse.org/bugs/show_bug.cgi?id=530915
/*
// some_boolean is a non-required, numerical feature in outlier detection analysis
// some_boolean is a non-required, numerical feature in regression analysis
// some_boolean is a non-required, numerical feature in classification analysis
// some_boolean is a required, categorical dependent variable in classification analysis
/**
/*
/*
/*
/**
// Make sure the process context did not leak
// 'processData' and 'processResults' threads
// startProcess
// stop
/*
// This test verifies the processor knows how to handle a failure on storing the model and completes normally
/*
/*
/*
// As all these rows have no dependent variable value, they're not for training and should be unaffected
// We should pick them all as training percent is 100
// Now we need to calculate sensible bounds to assert against.
// We'll use 5 variances which should mean the test only fails once in 7M
// And, because we're doing multiple runs, we'll divide the variance with the number of runs to narrow the bounds
// We have some non-training rows and then a training row to check
// we maintain the first training row and not just the first row
/*
/*
/*
/*
/*
/*
// doc_value field
/*
/*
/*
/*
/*
/*
// CSV - no need to check NDJSON or XML because they come earlier in the order we check formats
// TSV - no need to check NDJSON, XML or CSV because they come earlier in the order we check formats
// Semi-colon delimited - no need to check NDJSON, XML, CSV or TSV because they come earlier in the order we check formats
// Pipe delimited - no need to check NDJSON, XML, CSV, TSV or semi-colon delimited
// values because they come earlier in the order we check formats
/*
// It's obvious the first row really should be a header row, so by overriding
// detection with the wrong choice the results will be completely changed
// note that this last record is truncated
// Default timestamp field is the first field from the start of each row that contains a
// consistent timestamp format, so if we want the second we need an override
// The exclude pattern needs to work on the raw text, so reflects the unmodified field names
/*
/*
// This input should never match a single byte character set.  ICU4J will sometimes decide
// that it matches a double byte character set, hence the two assertion branches.
// This input should never match a single byte character set.  ICU4J will probably decide
// that it matches both UTF-16BE and UTF-16LE, but we should reject these as there's no
// clear winner.
// Need to change the quote character from the default of double quotes
// otherwise the quotes in the NDJSON will stop it parsing as CSV
// Every line of the text sample has two colons, so colon delimited is possible, just very weird
// The number of lines might need increasing in the future if computers get really fast,
// but currently we're not even close to finding the structure of this much data in 10ms
// This is not just junk; this is comma separated junk
// Expected if timeout occurs and the input stream is closed before junk generation is complete
// This shouldn't take anything like 10 seconds, but VMs can stall so it's best to
// set the timeout fairly high to avoid the work that spurious failures cause
/*
// This doesn't need closing because it has an infinite timeout
/*
// 12345678901234567890 is too long for long
// After removing the two expected fields there should be nothing left in the pipeline
// After removing the two expected fields there should be nothing left in the pipeline
/*
// It seems sensible that we don't detect these suffices as either base 10 or base 16 numbers
// We don't want the .1. in the middle to get detected as a hex number
//www.elastic.co/ with trailing slash",
//www.elastic.co/guide/en/x-pack/current/ml-configuring-categories.html#ml-configuring-categories is a section",
//www.elastic.co/downloads");
// Two timestamps: one local, one UTC
// Two timestamps: one ISO8601, one indeterminate day/month
// Two timestamps: one custom, one built-in
// Two timestamps: one with date, one without
//semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) " +
//semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) " +
//semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) " +
//semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) " +
// Two timestamps: one local, one UTC
// Two timestamps: one local, one UTC
/*
/*
/*
// No need to check NDJSON, XML, CSV, TSV, semi-colon delimited values or pipe
// delimited values because they come earlier in the order we check formats
/*
// In this case the "message" field was output by the Grok pattern, so "message"
// at the end of the processing will _not_ contain a complete sample message
// This Grok pattern cannot be matched against the messages in the sample because the fields are in the wrong order
// This sample causes problems because the (very weird) primary timestamp format
// is not detected but a secondary format that only occurs in one line is detected
/*
//github.com/elastic/elasticsearch/issues/48861")
/*
// Override is a special format
// Override is covered by a built-in format
// Can't compare Grok objects as Grok doesn't implement equals()
// Exact format supplied is returned if it matches
// Other supported formats are returned if exact format doesn't match
// Override is supported but not covered by any built-in format
// Simplest case - nothing is indeterminate
// US with padding
// US without padding
// EU with padding
// EU without padding
// Indeterminate at the beginning of the pattern
// Indeterminate in the middle of the pattern
// No separators
// It's unreasonable to expect a variable length format like 'd' or 'M' to work without separators
// This is based on the fact that %{MONTHNUM} can match a single digit whereas %{MONTHDAY} cannot
// This is based on the fact that %{MONTHNUM} can match a single digit whereas %{MONTHDAY} cannot
// Inconsistent so no decision
// Second number has 3 values, first only 1, so guess second is day
// First number has 3 values, second only 1, so guess first is day
// Insufficient evidence to decide
// Similar to the test above, but with the possibility that the secondary
// ISO8601 formats cause confusion - this test proves that they don't
// Second number has 3 values, first only 1, so guess second is day
// First number has 3 values, second only 1, so guess first is day
// Insufficient evidence to decide
// Locale fallback is the only way to decide
// The US locale should work for both FIPS and non-FIPS
// Non-US locales may not work correctly in a FIPS JVM - see https://github.com/elastic/elasticsearch/issues/45140
// TAI64N doesn't necessarily contain digits, so this functionality cannot guarantee that it won't match somewhere in the text
//apiserv:8080/engine/v2/jobs HTTP/1.1\" 201 42 \"-\" \"curl/7.46.0\" 384");
// These two don't match because they're too far in the future
// when interpreted as seconds/milliseconds from the epoch
// (they need to be 10 or 13 digits beginning with 1 or 2)
// TIMESTAMP_ISO8601 doesn't match ISO8601 if it's only a date with no time
// Note: some of the time formats give millisecond accuracy, some second accuracy and some minute accuracy
//apiserv:8080/engine/v2/jobs HTTP/1.1\" 201 42 \"-\" \"curl/7.46.0\" 384", "192.168.62.101 - - [", "HTTPDATE",
// Differs from the above as the required format is specified
// Non-matching required format specified
// Even though many lines have a timestamp near the end (in the Lucene version information),
// these are so far along the lines that the weight of the timestamp near the beginning of the
// first line should take precedence
// The override should force the seemingly inferior choice of timestamp
// TODO - this won't work any more :-(
// All the test times are for Tue May 15 2018 16:14:56 UTC, which is 17:14:56 in London.
// This is the timezone that will be used for any text representations that don't include it.
// All formats tested have either both or neither of hour and minute
// Seconds automatically defaults to 0
// This next line parses the textual date without any default timezone, so if
// the text doesn't contain the timezone then the resulting temporal accessor
// will be incomplete (i.e. impossible to convert to an Instant).  You would
// hope that it would be possible to specify a timezone to be used only in this
// case, and in Java 9 and 10 it is, by adding withZone(zone) before the
// parse(text) call.  However, with Java 8 this overrides any timezone parsed
// from the text.  The solution is to parse twice, once without a default
// timezone and then again with a default timezone if the first parse didn't
// find one in the text.
// TODO: when Java 8 is no longer supported remove the two
// lines and comment above and the closing brace below
// If the last one isn't right then propagate
// If the last one throws then propagate
/**
/*
// No need to check NDJSON because it comes earlier in the order we check formats
/*
/*
/*
/*
// Test with labels
/*
// Test invalidate cache for model3
// It is not referenced, so called eagerly
// Should have been loaded from the cluster change event
// Verify that we have at least loaded all three so that evictions occur in the following loop
// Only reference models 1 and 2, so that cache is only invalidated once for model3 (after initial load)
// Only loaded requested once on the initial load from the change event
// Load model 3, should invalidate 1
// Load model 1, should invalidate 2
// Load model 2, should invalidate 3
// Test invalidate cache for model3
// Now both model 1 and 2 should fit in cache without issues
/*
// Should be OK as we don't make any client calls
/*
// Should be OK as we don't make any client calls
/*
// Ask a few times to increase the chance of failure if the .ml-annotations index is created when no other ML index exists
// Creating a document in the .ml-notifications-000001 index should cause .ml-annotations
// to be created, as it should get created as soon as any other ML index exists
/*
// Records are not persisted to Elasticsearch as an array within the bucket
// documents, so remove them from the expected bucket before comparing
// this will persist the interim results
// and this will delete the interim results
// Records are not persisted to Elasticsearch as an array within the bucket
// documents, so remove them from the expected bucket before comparing
// this will persist the interim results
// and this will delete the interim results and persist the new interim bucket & records
// this deletes the previous interim and persists final bucket & records
// Records are not persisted to Elasticsearch as an array within the bucket
// documents, so remove them from the expected bucket before comparing
// bucket triggers persistence
// between 1970 and 2065
/*
// at least the primary shards of the indices a job uses should be started
// replicas must be assigned, otherwise we could lose a whole index
// replicas must be assigned, otherwise we could lose a whole index
// at least the primary shards of the indices a job uses should be started
// replicas must be assigned, otherwise we could lose a whole index
// replicas must be assigned, otherwise we could lose a whole index
// at least the primary shards of the indices a job uses should be started
// start 2 non ml node that will never get a job allocated. (but ml apis are accessible from this node)
// start ml node
// the default is based on 'xpack.ml.enabled', which is enabled in base test class.
// at least the primary shards of the indices a job uses should be started
// job should get and remain in a failed state and
// the status remains to be opened as from ml we didn't had the chance to set the status to failed:
// job should be re-opened:
// start non ml node, but that will hold the indices
// Sample each cs update and keep track each time a node holds more than `maxConcurrentJobAllocations` opening jobs.
// at least the primary shards of the indices a job uses should be started
// fork so stopping all ml nodes proceeds quicker:
// This test is designed to check that a job will not open when the .ml-state
// or .ml-anomalies-shared indices are not available. To do this those indices
// must be allocated on a node which is later stopped while .ml-config is
// allocated on a second node which remains active.
// start non ml node that will hold the state and results indices
// start an ml node for the config index
// Create the indices (using installed templates) and set the routing to specific nodes
// State and results go on the state-and-results node, config goes on the config node
// at least the primary shards of the indices a job uses should be started
/*
// Create datafeed config
// Read datafeed config
// Headers are set by the putDatafeedConfig method so they
// must be added to the original config before equality testing
// Update
// Only security headers are updated, grab the first one
// Read the updated config
// Delete
// Create datafeed config
// cannot create another with the same id
// delete
// error deleting twice
// Test job IDs only
// Test full job config
// Only security headers are updated, grab the first one
/*
/*
// Create job
// Create job
// Create job
// Read Job
// Update Job
// Delete Job
// Read deleted job
// Delete deleted job
// and again with errorIfMissing set false
// Create job
// Create job
// update with the no-op validator
// Update with a validator that errors
// Job Ids
// Job builders
// Test job IDs only
// Test full job config
// This config is not valid because it uses aggs but the job's
// summary count field is not set
// repeat the update for good measure
/*
//github.com/elastic/elasticsearch/issues/40134")
// Each job should result in one extra field being added to the results index mappings: field1, field2, field3, etc.
// Due to all being created simultaneously this test may reveal race conditions in the code that updates the mappings.
// Start the requests as close together as possible, without waiting for each to complete before starting the next one.
// Only after all requests are in-flight, wait for all the requests to complete.
// Assert that the mappings contain all the additional fields: field1, field2, field3, etc.
// Test time filters
// Lands halfway through the second event which should be returned
// Lands halfway through the 3rd event which should be returned
// index the param docs
// events in the past should be filtered out
// events
// filters
// datacounts
// model size stats
// model snapshot
// quantiles
/*
/**
// If the deletion of aliases touches the unrelated index with the block
// then the line above will throw a ClusterBlockException
/*
// Add a job to the index
// Same as index job but has extra fields in its custom settings
// which will be used to check the config was overwritten
// put a job representing a previously migrated job
// try to write foo and 'job-already-migrated' which does not have the custom setting field
// Check job foo has been indexed and job-already-migrated has been overwritten
// this job won't have been marked as migrated as calling
// MlConfigMigrator.writeConfigToIndex directly does not do that
// do the migration
// the first time this is called mlmetadata will be snap-shotted
// Verify that we have custom values in the new cluster state and that none of them is null
// check the jobs have been migrated
// check datafeeds are migrated
// define the configs
// index a doc with the same Id as the config snapshot
// do the migration
// writing the snapshot should fail because the doc already exists
// in which case the migration should continue
// check the jobs have been migrated
// and jobs and datafeeds clusterstate
// do the migration
// check the jobs have been migrated
// check datafeeds are migrated
// Add empty ML metadata
// do the migration
// and jobs and datafeeds clusterstate
// do the migration
// check the jobs have not been migrated
// check datafeeds have not been migrated
// and jobs and datafeeds clusterstate
// if the cluster state has a job config and the index does not
// exist it should be created
/*
// index some datafeed data
// stop the only ML node
// replicas must be assigned, otherwise we could lose a whole index
// Job state is opened but the job is not assigned to a node (because we just killed the only ML node)
// An unassigned datafeed can be stopped either normally or by force
// Since 7.5 we can also stop an unassigned job either normally or by force
// index some datafeed data
// Job state should be opened here
// Post the job a record that will result in the job receiving a timestamp in epoch
// seconds equal to the maximum integer - this makes the blackhole autodetect fail.
// It's better to do this than the approach of directly updating the job state using
// the approach used below for datafeeds, because when the job fails at the "process"
// level it sets off a more realistic chain reaction in the layers that wrap the "process"
// (remember it's not a real native process in these internal cluster tests).
// Confirm the job state is now failed
// It's impossible to reliably get the datafeed into a stopping state at the point when the ML node is removed from the cluster
// using externally accessible actions.  The only way this situation could occur in reality is through extremely unfortunate
// timing.  Therefore, to simulate this unfortunate timing we cheat and access internal classes to set the datafeed state to
// stopping.
// Confirm the datafeed state is now stopping
// Stop the node running the failed job/stopping datafeed
// replicas must be assigned, otherwise we could lose a whole index
// We should be allowed to force stop the unassigned datafeed even though it is stopping and its job has failed
// Confirm the datafeed state is now stopped
// We should be allowed to force stop the unassigned failed job
// index some datafeed data
// Stop the datafeed normally
// Force stop the datafeed without waiting for the normal stop to return first
// Confirm that the normal stop also reports success - whichever way the datafeed
// ends up getting stopped it's not an error to stop a stopped datafeed
// Open 4 small jobs.  Since there is only 1 node in the cluster they'll have to go on that node.
// Expand the cluster to 3 nodes.  The 4 small jobs will stay on the
// same node because we don't rebalance jobs that are happily running.
// Wait for the cluster to be green - this means the indices have been replicated.
// Open a big job.  This should go on a different node to the 4 small ones.
// Stop the current master node - this should be the one with the 4 small jobs on.
// If memory requirements are used to reallocate the 4 small jobs (as we expect) then they should
// all reallocate to the same node, that being the one that doesn't have the big job on.  If job counts
// are used to reallocate the small jobs then this implies the fallback allocation mechanism has been
// used in a situation we don't want it to be used in, and at least one of the small jobs will be on
// the same node as the big job.  (This all relies on xpack.ml.node_concurrent_job_allocations being set
// to at least 4, which we do in the nodeSettings() method.)
// Speed up rechecks to a rate that is quicker than what settings would allow.
// The tests would work eventually without doing this, but the assertBusy() below
// would need to wait 30 seconds, which would make the suite run very slowly.
// The 200ms refresh puts a greater burden on the master node to recheck
// persistent tasks, but it will cope in these tests as it's not doing anything
// else.
// Get datacounts from index instead of via job stats api,
// because then data counts have been persisted to an index (happens each 10s (DataCountsReporter)),
// so when restarting job on another node the data counts
// are what we expect them to be:
/*
// Test regression
// Test classification
// Get top classes
// they should always be in order of Most probable to least
// Test that top classes restrict the number returned
/*
//github.com/elastic/elasticsearch/issues/49908")
// Record which node the job starts off on
// Isolate the node the job is running on from the cluster
// Job should move to a new node in the bigger portion of the cluster
// Job should remain running on the new node, not the one that temporarily detached from the cluster
// The job running on the original node should have been killed, and hence should not have persisted quantiles
// The relocated job was closed rather than killed, and hence should have persisted quantiles
/*
// create and open first job, which succeeds:
// create and try to open second job, which fails:
// Ensure that the second job didn't even attempt to be opened and we still have 1 job open:
// now just double check that the first job is still opened:
// Set our lazy node number
// create and open first job, which succeeds:
// create and try to open second job, which succeeds due to lazy node number:
// Should return while job is opening
// Should get to opening state w/o a node
// Add another Node so we can get allocated
// We should automatically get allocated and opened to new node
// close the first job and check if the latest job gets opened:
// clear all nodes, so that we can set xpack.ml.max_open_jobs setting:
/*
/*
// job document does not exist
// job document does not exist
// job create time should be within the last second
// For the JobConfigProvider expand groups search.
// The search will not return any results
// For the JobConfigProvider expand groups search.
// group-1 will expand to job-1 and job-2
/*
// TODO: in 8.0.0 remove all instances of MAX_OPEN_JOBS_NODE_ATTR from this file
// To simplify the logic in this class all jobs have the same memory requirement
// MachineLearning.MACHINE_MEMORY_NODE_ATTR negative, so this will fall back to allocating by count
// Be careful if changing this - in order for the error message to be exactly as expected
// the value here must divide exactly into both (JOB_MEMORY_REQUIREMENT.getBytes() * 100) and
// MachineLearning.NATIVE_EXECUTABLE_CODE_OVERHEAD.getBytes()
// Be careful if changing this - in order for the error message to be exactly as expected
// the value here must divide exactly into both (JOB_MEMORY_REQUIREMENT.getBytes() * 100) and
// MachineLearning.NATIVE_EXECUTABLE_CODE_OVERHEAD.getBytes()
// This will make the allocation stale for job_id1
// Allocation won't be possible if the stale failed job is treated as opening
// Both anomaly detector jobs and data frame analytics jobs should count towards the limit
/*
// The default categorization analyzer matches what the analyzer in the ML C++ does
// A categorization filter that removes stuff in square brackets
// The Elasticsearch standard analyzer - this is the default for indexing in Elasticsearch, but
// NOT for ML categorization (and you'll see why if you look at the expected results of this test!)
// An example from the ES docs - no idea what it means or whether it's remotely sensible from a categorization point-of-view
/*
// The first part of the Tomcat datestamp can match as an ISO8601
// timestamp if the ordering of candidate patterns is wrong
// If we're not careful then we might detect the first part of these strings as a
// number, e.g. 1.2 in the first example, but this is inappropriate given the
// trailing dot and digit
// It seems sensible that we don't detect these suffices as either base 10 or base 16 numbers
// We don't want the .1. in the middle to get detected as a hex number
//www.elastic.co/ with trailing slash",
//www.elastic.co/guide/en/x-pack/current/ml-configuring-categories.html#ml-configuring-categories is a section",
//www.elastic.co/downloads");
// The embedded newline ensures the regular expressions we're using are compiled with Pattern.DOTALL
// Two timestamps: one local, one UTC
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// We are adding a record but it shouldn't be persisted as part of the bucket
// There should NOT be any nested records
// Refresh policy is set on the bulk request, not the individual index requests
/*
/*
/*
// this looks complicated but Mockito can't mock the final method
// DeleteIndexResponse.isAcknowledged() and the only way to create
// one with a true response is reading from a stream.
/**
/*
/*
/*
/*
/*
// Monday, October 16, 2017 12:00:00 AM UTC
/*
/*
/*
/*
// write some more data
// skip a bucket so there is a non-zero empty bucket count
// check total stats
// send 'flush' signal
// check last data time is equal to now give or take a second
/*
/**
/**
/**
/*
/*
// First in checkAndRun, second due to check between calls to waitForFlushAcknowledgement and third due to close()
/*
/**
// job is created
// This increases the chance of the two threads both getting into
// the middle of the AutodetectProcessManager.close() method
// Close the job in a separate thread
// Also close the job in the current thread, so that we have two simultaneous close requests
// The 10 second timeout here is usually far in excess of what is required.  In the vast
// majority of cases the other thread will exit within a few milliseconds.  However, it
// has been observed that on some VMs the test can fail because the VM stalls at the
// wrong moment.  A 10 second timeout is on a par with the length of time assertBusy()
// would wait under these circumstances.
// Only one of the threads should have called AutodetectCommunicator.close()
// Close the job in a separate thread so that it can simulate taking a long time to close
// Kill the job in the current thread, which will be while the job is "closing"
// Assert close method was awoken by the kill
// let the communicator throw, simulating a problem with the underlying
// autodetect, e.g. a crash
// create a jobtask
// job is created
/*
// run a task that will block while the others are queued up
// now shutdown
// the AbstractRunnables should have had their callbacks called
/*
/*
/*
// read header
/*
/*
// Set up schedule delay time
// First one with soft_limit
// Another with soft_limit
// Now with hard_limit
// And another with hard_limit
// We should have only fired two notifications: one for soft_limit and one for hard_limit
// Wait for flush should return immediately
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
/*
/*
/*
// The final field is the control field
// The "." field is the control field; "..." is the pre-tokenized tokens field
// The final field is the control field
// The final field is the control field
// The final field is the control field
// the csv reader treats this as a line (even though it doesn't end with \n) and skips over it
// The final field is the control field
// Expected line numbers are 2 and 10001, but SuperCSV may print the
// numbers using a different locale's digit characters
/*
/**
// last line is \0
/*
// write the same record this number of times
// write the same record this number of times
/*
/*
/*
/*
// read the ini file - all the settings are in the global section
// Ini4j meddles with escape characters itself, so the assertion below
// fails even though the raw file is fine.  The file is never read by
// Ini4j in the production system.
// Assert.assertEquals("max(\"\\\"quoted\\\" field\") over \"ts\\\\hash\"", value);
/*
// The final field is the control field
// The "." field is the control field; "..." is the pre-tokenized tokens field
// The final field is the control field
// The final field is the control field
// The final field is the control field
// The final field is the control field
// The final field is the control field
// The final field is the control field
// The final field is the control field
/*
/*
/*
/*
/**
// no opening '{'
// nested object 'd' is missing a ','
// reads first object ok
// skips to the end of the 2nd after reading 2 fields
// missing a ':'
// this should throw after PARSE_ERRORS_LIMIT errors
/*
// empty bucket
// empty bucket
// empty bucket
// empty bucket
// sparse bucket
// sparse bucket
/**
// sparse bucket
// sparse bucket (but last one)
/**
// sparse bucket
// sparse bucket (2nd to last one)
// sparse bucket (but last one)
// sparse bucket
// empty bucket
// sparse bucket
// empty bucket
/**
// empty bucket
// empty bucket
// 98 empty buckets
/**
// sparse bucket
// sparse bucket
/*
/*
/*
/*
/*
/*
/*
// Batch 1 - Just verify first and last were updated as Mockito
// is forbiddingly slow when tring to verify all 10000
/*
// Never reduce this below 4, otherwise some of the logic in the test will break
// Blast through many sets of quantiles in quick succession, faster than the normalizer can process them
// Last quantiles state that was actually used must be the last quantiles state we supplied
// Earlier quantiles states that were processed must have been processed in the supplied order
// The quantiles immediately before the intermediate wait for idle must have been processed
/*
/*
/*
// bucket span of > 3000 years should be enough for everyone
/*
/*
/*
/*
/*
/*
/*
/*
// We can't test an abstract class so make a concrete class
// as simple as possible
// cover both code paths
// This is testing AbstractExpiredJobDataRemover.WrappedBatchedJobsIterator
/*
// Init thread pool
/*
/*
/*
/*
// This answer blocks the thread on the executor service.
// In order to unblock it, the test needs to call wait.countDown().
/**
/*
/**
/**
// 10 for header and separators
/*
// First run a refresh using a component that calls the onFailure method of the listener
// Now run another refresh using a component that calls the onResponse method of the listener - this
// proves that the ML memory tracker has not been permanently blocked up by the previous failure
/*
// As soon as the log stream ends startProcess should think the native controller has died
/*
// low disk space
// sufficient disk space
// should resolve to disk2 as disk1 is low on space
// the native component should cleanup itself, but assume it has crashed
// create a new storage provider to test the case of a crashed node
/*
// opening this pipe will throw
// check the pipes successfully opened were then closed
/*
/*
// Try different buffer sizes to smoke out edge case problems in the buffer management
// Since this is all being done in one thread and we know the stream has
// been completely consumed at this point the wait duration can be zero
/*
/*
/**
// write the same record this number of times
// read header
// read records
// same again but using lists
// write the same record this number of times
// read header
// read records
/**
// write the same record this number of times
// read header
// read records
/*
/*
/**
// This usually takes a lot less than 90 seconds, but has been observed to be very slow occasionally
// in CI and a 90 second timeout will avoid the cost of investigating these intermittent failures.
// See https://github.com/elastic/elasticsearch/issues/48511
// Speed up rechecks to a rate that is quicker than what settings would allow.
// The check would work eventually without doing this, but the assertBusy() below
// would need to wait 30 seconds, which would make the test run very slowly.
// The 1 second refresh puts a greater burden on the master node to recheck
// persistent tasks, but it will cope in these tests as it's not doing much
// else.
/*
/**
/**
/*
/**
/*
// Test cases from https://github.com/john-kurkowski/tldextract/tree/master/tldextract/tests
// .info is a valid TLD
/*
/*
/*
/**
/*
// First try same set of assertions as unaliases
// No let's test the aliases
/*
/*
/*
/*
// Summertime
// Non-summertime
/*
/**
// overridable by tests
/*
/*
/**
/**
/*
/**
/**
/** State of the monitoring service, either started or stopped **/
/** Task in charge of collecting and exporting monitoring data **/
/**
/**
// Do not collect more data if the monitoring service is stopping
// otherwise some collectors might just fail.
// Block until the lock can be acquired or 10s. The timed try acquire is necessary as there may be a failure that causes
// the semaphore to not get released and then the node will hang forever on shutdown
/*
/**
/*
/*
// ignore incoming bulk requests when collection is disabled in ES
/**
/**
/**
/*
/**
// the validation is performed by the setting's object itself
/**
// we only care about their value if they are allowed to set it
/**
// notify the user that their setting will be ignored until they get the right license
/**
/**
/**
/**
/**
/**
// fetch the retention, which is depends on a bunch of rules
// Note: listeners are free to override the retention
/**
/**
/**
/**
// Runs at 01:00 AM today or the next day if it's too late
// if it's not after now, then it needs to be the next day!
/*
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
/*
// this can only run when monitoring is allowed and CCR is enabled and allowed, but also only on the elected master node
/*
/**
/**
// This collector can always collect data on the master node
// if they have any other type of license, then they are either okay or already know
// Adds a cluster stats document
/*
/**
// in the future, it may be useful to pass in an object that represents APM (and others), but for now this
// is good enough
/**
// adds the Ephemeral ID (as opposed to the Persistent UUID) to catch node restarts, which is critical for 1 node clusters
/*
/*
/*
/*
/**
/**
/**
/*
/**
/*
/**
/**
// Filters the indices stats to only return the statistics for the indices known by the collector's
// local cluster state. This way indices/index/shards stats all share a common view of indices state.
// The index appears both in the local cluster state and indices stats response
/*
/**
// when an index is completely red, then we don't get stats for it
/*
/**
/*
/**
/**
// This can only run when monitoring is allowed + ML is enabled/allowed, but also only on the elected master node
// fetch details about all jobs
/*
/**
/*
/**
/**
// For testing purpose
// if there's a failure, then we failed to work with the
// _local node (guaranteed a single exception)
/*
/**
/*
/*
/**
// ShardRouting is rendered inside a startObject() / endObject() but without a name,
// so we must use XContentBuilder.field(String, ToXContent, ToXContent.Params) here
/**
/*
/**
// If the shard is assigned to a node, the shard monitoring document refers to this node
/*
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// load the resource as-is
/**
// validate the blacklist only contains recognized IDs
/*
/**
/**
/**
/**
/**
/**
// for every export bulk we flush and pass back the response, which should always be
// null. When we have an exception, we wrap the first and then add suppressed exceptions
// this is tricky to understand but basically we suppress the exception for use
// later on and call the passed in listener so that iteration continues
/*
// if the type is http, then hosts must be set
/**
/**
/**
/**
/** Returns true if only one instance of this exporter should be allowed. */
/**
/** A factory for constructing {@link Exporter} instances.*/
/** Create an exporter with the given configuration. */
/*
// this ensures, that logging is happening by adding an empty consumer per affix setting
/**
// this is a singleton exporter, let's make sure we didn't already create one
// (there can only be one instance of a singleton exporter)
// no exporters are configured, lets create a default local one.
//
// NOTE:    if there are exporters configured and they're all disabled, we don't
//          fallback on the default
//
/**
// wait until we have a usable cluster state
// if no exporters are defined (which is only possible if all are defined explicitly disabled),
// then ignore the request immediately
// get every exporter's ExportBulk and, when they've all responded, respond with a wrapped version
/**
/**
/**
/**
/**
/*
/*
/**
/**
/*
/**
/**
/**
/**
/**
/**
// Watcher does not support master_timeout
/**
// if we should be adding, then we need to check for existence
// if we should be deleting, then just try to delete it (same level of effort as checking)
/**
/**
/**
/**
// no named content used; so EMPTY is fine
// if it's empty, then there's no version in the response thanks to filter_path
// if we don't have it (perhaps more fields were returned), then we need to replace it
// the version in the cluster alert is expected to include the alpha/beta/rc codes as well
/*
/**
/**
/**
/**
/**
// store the payload until we flush
// null out serialized docs to make things easier on the GC
// Builds the bulk action metadata line
// Adds action metadata line bulk separator
// Adds the source of the monitoring document
// Adds final bulk separator
/*
/**
/**
/**
/**
/**
// EMPTY is safe here because we never call namedObject
// avoid parsing the entire payload if we don't need too
// no errors? then we can stop looking
// note: this assumes that "items" is the only array portion of the response (currently true)
/**
/**
// queueable exceptions:
// - RestStatus.TOO_MANY_REQUESTS.getStatus()
// - possibly other, non-ResponseExceptions
/*
/**
/**
// hosts can only be empty if the type is unset
// every host must be configured
// fail if we find them configuring the scheme/protocol in different ways
/**
/**
/**
/**
// no username validation that is independent of other settings
// password must be specified along with username for any auth
/**
// no password validation that is independent of other settings
// username is required for any auth
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// mark resources as dirty after any node failure or license change
/**
// no-op update. We only care about the validator
/**
// allow the user to configure proxies
// allow the user to configure headers that go along with _every_ request
// commercial X-Pack users can have Security enabled (auth and SSL/TLS), and also clusters behind proxies
// timeouts for requests
/**
// the sniffer is allowed to be ENABLED; it's disabled by default until we think it's ready for use
// createHosts(config) ensures that all schemes are the same for all hosts!
// inform the sniffer whenever there's a node failure
/**
// order controls the order that each is checked; more direct checks should always happen first (e.g., version checks)
// block the exporter from working against a monitoring cluster with the wrong version
// load all templates (template bodies are lazily loaded on demand)
// load the pipeline (this will get added to as the monitoring API version increases)
// load the watches for cluster alerts if Watcher is available
/**
/**
// Most users won't define headers
// record and validate each header as best we can
// add each value as a separate header; they literally appear like:
//
//  Warning: abc
//  Warning: xyz
/**
// This configuration does not use secure settings, so it is possible that is has been dynamically updated.
// We need to load a new SSL strategy in case these settings differ from the ones that the SSL service was configured with.
// This configuration uses secure settings. We cannot load a new SSL strategy, as the secure settings have already been closed.
// Due to #registerSettingValidators we know that the settings not been dynamically updated, and the pre-configured strategy
// is still the correct configuration for use in this exporter.
// sending credentials in plaintext!
/**
// if the values could ever be null, then we should only set it if they're not null
/**
/**
// allow the use of ingest pipelines to be completely optional
// widdle down the response to just what we care to check
/**
// add templates not managed by resolvers
// Add dummy templates (e.g. ".monitoring-es-6") to enable the ability to check which version of the actual
// index template (e.g. ".monitoring-es") should be applied.
/**
// don't require pipelines if we're not using them
// add all pipelines
// lazily load the pipeline
/**
// don't create watches if we're not using them
// add a resource per watch
// lazily load the cluster state to fetch the cluster UUID once it's loaded
// wrap the watches in a conditional resource check to ensure the remote cluster has watcher available / enabled
// if this changes between updates, then we need to add OR remove the watches
// we're not ready yet, so keep waiting
/*
/**
// http://localhost:9200
// http://localhost:9200
//localhost:9200").build();   // http://localhost:9200
//localhost:9200").build();  // https://localhost:9200
//localhost:9200").build();  // https://127.0.0.1:9200 (IPv4 localhost)
//10.1.2.3").build();         // http://10.2.3.4:9200
//[::1]").build();           // http://[::1]:9200      (IPv6 localhost)
//[::1]:9200").build();      // http://[::1]:9200      (IPv6 localhost)
//sub.domain").build();      // https://sub.domain:9200
/**
/**
/**
/**
// http://localhost:9200
// http://localhost:9200
// https://localhost:9200
// https://my_host:19200
// https://192.168.0.11:80
/**
// http://localhost:9200
//localhost:9200").build(); // http://localhost:9200
//localhost").build();       // https://localhost:9200
// http://my_host:19200
// http://192.168.0.11:80
/**
// everything is in the default state
/**
//") == false) {
//" + uri;
// "localhost:9200" doesn't have a scheme
// if the host is null, then it means one of two things: we're in a broken state _or_ it had something like underscores
// we want the raw form so that parts of the URI are not decoded
// they explicitly provided the port, which is unparsed when the host is null
// fail for proxies
/**
/**
/**
// setting a port to 0 makes no sense when you're the client; -1 allows us to use the default when we build
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// use client with resources having been verified
/**
// we always check when asked, regardless of clean or dirty, but we do not run parallel checks
/**
/*
/**
/**
/**
/**
/**
// short-circuits on the first failure, thus marking the whole thing dirty
// short-circuit on the first failure
/*
/**
/**
/**
/**
/**
/**
/**
//{}:{}]", host.getSchemeName(), host.getHostName(), host.getPort());
/*
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// it already exists, so we can skip publishing it
/**
/**
/**
// avoid exists and DNE parameters from being an exception by default
// checking the content is the job of whoever called this function by checking the tuple's response
// if we should replace it -- true -- then the resource "does not exist" as far as the caller is concerned
// if we should replace it -- true -- then the resource "does not exist" as far as the caller is concerned
/**
/**
// 200 or 201
/**
// avoid 404 being an exception by default
// 200 or 404 (not found is just as good as deleting it!)
/**
// no named content used; so EMPTY is fine
// if it's empty, then there's no version in the response thanks to filter_path
// the version in the template is expected to include the alpha/beta/rc codes as well
/**
/*
/**
/**
/**
/**
// same as https
/*
/**
/**
/**
/**
/**
/**
/**
// enable SSL / TLS
// enable user authentication
/*
/**
/**
/**
/**
/**
/**
/**
// the internal representation of a template has type nested under mappings.
// this uses xContent to help remove the type before sending to the remote cluster
/*
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
// malformed responses can cause exceptions during validation
/**
// the response should be filtered to just '{"version":{"number":"xyz"}}', so this is cheap and guaranteed
/*
/**
/**
/**
/**
/**
/**
// _xpack does not support master_timeout
/**
/**
// only the master manages watches
// not the elected master
/**
// use DNE to pretend that we're all set; it means that Watcher is unusable
/**
// no named content used; so EMPTY is fine
// if it's empty, then there's no features.watcher response because of filter_path usage
// if Watcher is both available _and_ enabled, then we can use it; either being true is not sufficient
/**
/*
/**
// allow the use of ingest pipelines to be completely optional
/*
// if additional listeners are added here, adjust LocalExporterTests#testLocalExporterRemovesListenersOnClose accordingly
/**
/**
// forces the setup to occur if it hasn't already
// we also remove the listener in resolveBulk after we get to RUNNING, but it's okay to double-remove
// List of templates
// elected master node needs to setup templates; non-master nodes need to wait for it to be setup
// the first pass will be false so that we don't bother users if the master took one-go to setup
// any failure/delay to setup the local exporter stops it until the next pass (10s by default)
// we no longer need to receive cluster state updates
/**
// any required template is not yet installed in the given cluster state, we'll wait.
// if we don't have the ingest pipeline, then it's going to fail anyway
// everything is setup
/**
// we are on the elected master
// Check that there is nothing that could block metadata updates
// build a list of runnables for everything that is missing, but do not start execution
// Check that each required template exists, installing it if needed
// if we don't have the ingest pipeline, then install it
// avoid constantly trying to setup Watcher, which requires a lot of overhead and avoid attempting to setup during a cluster state
// change
// we cannot do anything with watches until the index is allocated, so we wait until it's ready
// let the cluster catch up since requested installations may be ongoing
// everything is setup (or running)
/**
// we ensure that we both have the pipeline and its version represents the current (or later) version
/**
// FIXME this should use the IndexTemplateMetaDataUpgrader
/**
/**
// we aren't sure if no watches exist yet, so add them
/**
// Reference date time will be compared to index.creation_date settings,
// that's why it must be in UTC
// list of index patterns that we clean up; watcher history can be included
// Get the names of the current monitoring indices
// avoid deleting the current alerts index, but feel free to delete older ones
// Never delete any "current" index (e.g., today's index or the most recent version no timestamp, like alerts)
// Probably means that the delete request has timed out,
// the indices will survive until the next clean up.
/**
/*
/**
/*
/*
/*
/**
// everything is just lowercased...
/*
// 7 days
// Note: this verifies the semantics because this is taken for granted that it never returns null!
// hit the minimum
// 1 ms early!
/*
/*
// these settings mimic what ES does when running as a node...
/*
// now the interval should take place
// take down threads
/*
// maximum number of milliseconds before a five digit year comes in, which could change formatting
/**
/**
/**
/**
/**
/**
/*
// starting one by one to allow moving , for example, from a 2 node cluster to a 4 one while updating min_master_nodes
// At least 1 doc must exist per node, but it can be more than 1
// because the first node may have already collected many node stats documents
// whereas the last node just started to collect node stats.
/*
/**
/*
/**
/**
/*
/*
/**
/*
/**
// execute in the same thread
// it validates the request before it tries to execute it
/**
/**
/*
// Will be deleted
// Won't be deleted
// Will be deleted
// In the past, this index would not be deleted, but starting in 6.x the monitoring cluster
// will be required to be a newer template version than the production cluster, so the index
// pushed to it will never be "unknown" in terms of their version
// Won't be deleted
// Will be deleted
// Won't be deleted
// Clean indices that have expired two years ago
// Clean indices that have expired 8 months ago
// Clean indices that have expired 3 months ago
// Clean indices that have expired 15 days ago
// Clean indices that have expired 7 days ago
// Clean indices until now
// Clean indices that have expired for N days, as specified in the global retention setting
/**
/**
/**
/**
/**
/**
/*
// invalid setting
// Note: I used this value to ensure we're not double-validating the setter; the cluster state should be the
// only thing calling this method and it will use the settings object to validate the time value
// once by set, once by get
// Note: I used this value to ensure we're not double-validating the setter; the cluster state should be the
// only thing calling this method and it will use the settings object to validate the time value
// required to be true on the second call for it to see it take effect
// uses allow=false
// uses allow=true
/*
//we set ignore_unavailable to true for this request as the monitoring index gets deleted concurrently with this assertion
//in some cases. When the plugin security is enabled, it expands wildcards to the existing index, which then gets deleted,
//so when es core gets the request with the explicit index name, it throws an index not found exception as that index
//doesn't exist anymore. If we ignore unavailable instead no error will be thrown.
// Will be deleted (if we delete them)
// Won't be deleted
/*
/*
/*
/*
// this controls the blockage
// Collection indices has a default value equals to emptyList(),
// so it won't return a null indices array
/*
/*
// this controls the blockage
// Number of indices that exist in the cluster state and returned in the IndicesStatsResponse
// Number of indices returned in the IndicesStatsResponse only
// Number of indices returned in the local cluster state only
// Total number of indices
/*
// to simplify the test code, we only allow active primaries to relocate, rather than also include active replicas
// must append , if total / primaries stats are included
// This value is used in constructors of various stats objects,
// when the value is not printed out in the final XContent.
// This value is used in constructors of various stats objects,
// when the value is printed out in the XContent. Must be
// incremented for each usage.
// we count initializing as a special type of unassigned!
// we only relocate primaries to simplify this method -- replicas can be relocating
// randomly mark unassigned shards
// mark all as unassigned
// primary should be allocated, but replicas can still be unassigned
// Primary shard is STARTED (active)
// first case means that we MUST assign it because it's this unassigned shard
// Replica shard is STARTED (active)
// sanity checks
/*
// Primaries
// Replica
// Mock paths in a way that pass ShardPath constructor assertions
// Mock paths for ShardPath#getRootDataPath()
/*
/**
// this controls the blockage
// regardless of ML being enabled
// this controls the blockage
// this is controls the blockage
// this is controls the blockage
// since it's the default, we want to ensure we test both with/without it
/*
/**
/*
// this controls the blockage
/*
// This value is used in constructors of various stats objects,
// when the value is not printed out in the final XContent.
// This value is used in constructors of various stats objects,
// when the value is printed out in the XContent. Must be
// incremented for each usage.
// Indices
// Filesystem
// Os
// Process
// Jvm
// Threadpools
/*
/** Used to match no indices when collecting shards information **/
// this controls the blockage
// this controls the blockage
/*
/*
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/*
/**
/*
/**
// validate that it's well formed JSON
/*
// default state.version() will be 0, which is "valid"
// we always need to have the local exporter as it serves as the default one
// the only configured exporter is disabled... yet we intentionally don't fallback on the default
// synchronously checks the cluster state
/**
/**
/**
/*
// ensure that the index is created with the proper ID
// ensure the description contains the API version
/*
/**
/**
/**
/**
/**
/**
/**
/**
// fill out the response enough so that the exception can be constructed
// the version check is what is expected to cause it to be replaced
// the version is there and it's exactly what we specify
// expected = null, which is for malformed/failure
// malformed
// the version check is what is expected to cause it to be replaced
// the version is there and it's exactly what we specify
// expected == null, which is for malformed/failure
// malformed
/*
/*
/**
// it does not exist because it's literally not there
// it does not exist because we need to replace it
// error because of a server error
// error because of a malformed response
// should not matter
// should not matter
/*
/**
// doesn't explode
// {"took": 4, "errors": false, ...
// {, "took", 4, "errors", false
// doesn't explode
// {"took": 4, "errors": true, "items": [ { "index": { "_index": "ignored", "_type": "ignored", "_id": "ignored" },
//                                        { "index": { "_index": "ignored", "_type": "ignored", "_id": "ignored", "error": "blah" }
//                                      ]...
// {, "took", 4, "errors", false                                                      nextToken, currentName
// 1
// 3, 1
// 5, 2
// 7, 3
// no error:
// 8
// 10, 4
// 12, 5
// 14, 6
// 16, 7
// 17
// 18
// 20, 8
// 22, 9
// 24, 10
// 26, 11
// 28, 12 ("error")
// 29
// 30
// there were errors; so go diving for the error
// malformed JSON
// let it log the exception so you can check the output
/*
// ensure that ssl can be used by settings
// ensure that headers can be used by settings
// pretend that one of the templates is missing
// opposite of if it existed before
// second event
// returning an unsupported cluster version
// ensure that the exporter is not able to be used
// wait for it to actually respond
// GET /_xpack
// GET / PUT if we are allowed to use it
// DELETE if we're not allowed to use it
// wait until the cluster is ready (this is done at the "Exporters" level)
// block until the bulk responds
// it DOES exist, but it's an older version
// no version specified
// it's a NEWER resource (template / pipeline)
// we already put it
// if the remote cluster doesn't allow watcher, then we only check for it and we're done
// X-Pack exists and Watcher can be used
// if we have an active license that's not Basic, then we should add watches
// otherwise we need to delete them from the remote cluster
// X-Pack exists but Watcher just cannot be used
// X-Pack is not installed
// it DOES exist, but it's an older version
// no version specified
// it's a NEWER cluster alert
// we already put it
// this can be removed in 7.0
// this can be removed in 7.0
/*
/**
/**
// ensure it didn't magically become clean
// failure in the middle of various templates being checked/published; suggests a node dropped
// -2 from one success + a necessary failure after it!
// last check fails implies that N - 2 publishes succeeded!
// ensure it didn't magically become not-dirty
// failure in the middle of various templates being checked/published; suggests a node dropped
// -2 from one success + a necessary failure after it!
// first one passes for sure, so we need an extra "unsuccessful" GET
// previous publishes must have succeeded
// GETs required for each PUT attempt (first is guaranteed "unsuccessful")
// unsuccessful are PUT attempts + the guaranteed successful PUT (first)
// fail the check so that it has to attempt the PUT
// ensure it didn't magically become not-dirty
// failure in the middle of various templates being checked/published; suggests a node dropped
// last check fails
// ensure it didn't magically become not-dirty
// failure in the middle of various templates being checked/published; suggests a node dropped
// We only have two pipelines for now, so the both GETs need to be "unsuccessful" for until we have a third
// previous publishes must have succeeded
// GETs required for each PUT attempt (first is guaranteed "unsuccessful")
// unsuccessful are PUT attempts
// fail the check so that it has to attempt the PUT
// ensure it didn't magically become not-dirty
// there's only one check
// ensure it didn't magically become not-dirty
// failure in the middle of various watches being checked/published; suggests a node dropped
// getting _and_ putting watches
// -2 from one success + a necessary failure after it!
// last check fails implies that N - 2 publishes succeeded!
// +1 for the "first"
// deleting watches
// - 1 from necessary failure after it!
// there is no form of an unsuccessful delete; only success or error
// ensure it didn't magically become not-dirty
// license needs to be valid, otherwise we'll do DELETEs, which are tested earlier
// failure in the middle of various watches being checked/published; suggests a node dropped
// -2 from one success + a necessary failure after it!
// first one passes for sure, so we need an extra "unsuccessful" GET
// previous publishes must have succeeded
// GETs required for each PUT attempt (first is guaranteed "unsuccessful")
// unsuccessful are PUT attempts + the guaranteed successful PUT (first)
// fail the check so that it has to attempt the PUT
// ensure it didn't magically become not-dirty
// it should be able to proceed!
/**
// it should be able to proceed! (note: we are not using the instance "resources" here)
// empty is possible if they all exist
// empty is possible if they all exist
// empty is possible if they all exist
// empty is possible if they all exist
/*
//" + webServer.getHostName() + ":" + webServer.getPort();
// Force the exporters to be built from closed secure settings (as they would be in a production environment)
// Verify that it was created even though it has a secure setting
// Verify that we cannot modify the SSL settings
//" + webServer.getHostName() + ":" + webServer.getPort())
//" + webServer.getHostName() + ":" + webServer.getPort())
/**
/*
/**
// always let the watcher resources run for these tests; HttpExporterResourceTests tests it flipping on/off
//example.com:443"));
//example.com:443/";
//example.com:443";
//example.com:443";
//localhost:9200")
// a valid ID
//localhost:9200")
//localhost:9200");
//";
//localhost:9200");
// use basic auth
// use headers
// doesn't explode
// it's a simple check: does it start with "https"?
// it's a race whether it triggers this at all
// note: this shouldn't get used with useIngest == false, but it doesn't hurt to try to cause issues
// expected number of resources
// timeouts
// logging owner names
// should have removed everything
// this is configured to throw an error when the resource is checked
// wait for it to actually respond
// always has to check, and never succeeds checks but it does not throw an exception (e.g., version check fails)
// wait for it to actually respond
// sometimes dirty to start with and sometimes not; but always succeeds on checkAndPublish
// wait for it to actually respond
// order matters; sniffer must close first
/**
/*
/**
//" + hostname), scheme, hostname, 9200);
//" + hostname + ":" + port), scheme, hostname, port);
// weird port, but I don't expect it to explode
//" + hostname + ":-1"), scheme, hostname, 9200);
// port without scheme
// fairly ordinary
//localhost"), Scheme.HTTP, "localhost", 9200);
//localhost:9200"), Scheme.HTTP, "localhost", 9200);
//localhost:9200"), Scheme.HTTPS, "localhost", 9200);
//boaz-air.local:9200"), Scheme.HTTPS, "boaz-air.local", 9200);
//server-dash:19200"), Scheme.HTTPS, "server-dash", 19200);
//sub.domain"), Scheme.HTTP, "sub.domain", 9200);
//sub.domain:9200"), Scheme.HTTP, "sub.domain", 9200);
//sub.domain:9200"), Scheme.HTTPS, "sub.domain", 9200);
//sub.domain:19200"), Scheme.HTTPS, "sub.domain", 19200);
// ipv4
//127.0.0.1"), Scheme.HTTP, "127.0.0.1", 9200);
//127.0.0.1:9200"), Scheme.HTTP, "127.0.0.1", 9200);
//127.0.0.1:9200"), Scheme.HTTPS, "127.0.0.1", 9200);
//127.0.0.1:19200"), Scheme.HTTPS, "127.0.0.1", 19200);
// ipv6
//[::1]"), Scheme.HTTP, "[::1]", 9200);
//[::1]:9200"), Scheme.HTTP, "[::1]", 9200);
//[::1]:9200"), Scheme.HTTPS, "[::1]", 9200);
//[::1]:19200"), Scheme.HTTPS, "[::1]", 19200);
//[fdda:5cc1:23:4::1f]"), Scheme.HTTP, "[fdda:5cc1:23:4::1f]", 9200);
//[fdda:5cc1:23:4::1f]:9200"), Scheme.HTTP, "[fdda:5cc1:23:4::1f]", 9200);
//[fdda:5cc1:23:4::1f]:9200"), Scheme.HTTPS, "[fdda:5cc1:23:4::1f]", 9200);
//[fdda:5cc1:23:4::1f]:19200"), Scheme.HTTPS, "[fdda:5cc1:23:4::1f]", 19200);
// underscores
//server_with_underscore"), Scheme.HTTP, "server_with_underscore", 9200);
//server_with_underscore:9200"), Scheme.HTTP, "server_with_underscore", 9200);
//server_with_underscore:19200"), Scheme.HTTP, "server_with_underscore", 19200);
//server_with_underscore"), Scheme.HTTPS, "server_with_underscore", 9200);
//server_with_underscore:9200"), Scheme.HTTPS, "server_with_underscore", 9200);
//server_with_underscore:19200"), Scheme.HTTPS, "server_with_underscore", 19200);
//_prefix.domain"), Scheme.HTTP, "_prefix.domain", 9200);
//_prefix.domain:9200"), Scheme.HTTP, "_prefix.domain", 9200);
//_prefix.domain:19200"), Scheme.HTTP, "_prefix.domain", 19200);
//_prefix.domain"), Scheme.HTTPS, "_prefix.domain", 9200);
//_prefix.domain:9200"), Scheme.HTTPS, "_prefix.domain", 9200);
//_prefix.domain:19200"), Scheme.HTTPS, "_prefix.domain", 19200);
// unset the port (not normal, but ensuring it works)
// port without scheme
//localhost:9200", "htp");
//localhost:9200", "htttp");
//localhost:9200", "httpd");
//localhost:9200", "ws");
//localhost:9200", "wss");
//localhost:9200", "ftp");
//localhost:9200", "gopher");
//9200", "localhost");
//localhost:9200/", "/");
//localhost:9200/sub", "/sub");
//localhost:9200/sub/path", "/sub/path");
/*
/**
// MockHttpResponse always succeeds for checkAndPublish
// if this fails, then the mocked resource needs to be fixed
// the default dirtiness should be irrelevant; it should always be run!
// listener used while checking is blocked, and thus should be ignored
// busy checking, so this should be ignored
// the default dirtiness should be irrelevant; it should always be run!
// wait until the second check has had a chance to run to completion,
// then respond here
// once is the default!
/*
/**
/**
/**
/**
/**
/**
/**
/*
/**
// fail either the check or the publish
// should stop looking at this point
/*
/**
/*
/**
// it does not exist because it's literally not there
// it does not exist because we need to replace it
// error because of a server error
// error because of a malformed response
/*
/**
// it literally does not exist
// it DOES exist, but the version needs to be replaced
// not an error (the third state)
// { "resourceName": { "version": randomLong } }
// expected == null
// invert expected to keep the same value
/*
/**
/*
/**
/**
/*
/**
//the internal representation has the type, the external representation should not
//the internal representation is converted to the external representation for the resource
// it does not exist because it's literally not there
// it does not exist because we need to replace it
// error because of a server error
// error because of a malformed response
/*
/**
// avoid making both null at the same time
/*
/**
// malformed JSON
// malformed response; imagining that we may change it in the future or someone breaks filter_path
// malformed response (should be {version: { number : ... }})
/*
/**
// /_xpack returning a 404 means ES didn't handle the request properly and X-Pack doesn't exist
// /_xpack returning a 400 means X-Pack does not exist
// success only implies that it responded; it also needs to be available and enabled
// returning EXISTS implies that we CANNOT use Watcher to avoid running the publish phase
// success only implies that it responded; it also needs to be available and enabled
// returning DOES_NOT_EXIST implies that we CAN use Watcher and need to run the publish phase
// missing watcher object 'string'
// missing features outer object
// extra {
// success only implies that it responded; it also needs to be available and enabled
// returning an error implies that we CAN use Watcher and need to run the publish phase
/*
/**
/**
// ...
/*
// Now disabling the monitoring service, so that no more collection are started
// indexing some random documents
// start the monitoring service so that /_monitoring/bulk is not ignored
// local exporter is now enabled
// export some documents now, before starting the monitoring service
// This assertion loop waits for in flight exports to terminate. It checks that the latest
// node_stats document collected for each node is at least 10 seconds old, corresponding to
// 2 or 3 elapsed collection intervals.
/**
/**
// actual pipelines
/**
// This is a document indexed by the Monitoring service
// This is a document indexed through the Monitoring Bulk API
/*
// sometimes they need to be added; sometimes they need to be replaced
// these were "newer" or at least the same version, so they shouldn't be replaced
// wait until the cluster is ready (this is done at the "Exporters" level)
// this is not a busy assertion because it's checked earlier
/**
// this would totally break Monitoring UI, but the idea is that it's different from a real template and
// the version controls that; it also won't break indexing (just searching) so this test can use it blindly
// The internal representation still requires a default type of _doc
/**
// something we can quickly check to ensure we have/have not replaced it
// sometimes give it a version that should be overwritten (and sometimes don't give it a version at all)
// randomly supply an older version, or no version at all
// randomly supply a newer version or the expected version
// this just ensures that it's set; not who set it
/*
/*
/**
// Wait for the monitoring index to be created
// Monitoring uses auto_expand_replicas, so it should be green even without replicas
// exactly 3 results are expected
// find distinct _source.timestamp fields
// find distinct _source.source_node fields (which is a map)
/**
//github.com/elastic/elasticsearch/issues/29880")
/**
/**
/**
// Added to debug https://github.com/elastic/elasticsearch/issues/29880
// Remove when fixed
// borrowed from randomized-testing
/**
// delete anything that may happen to already exist
// Monitoring uses auto_expand_replicas, so it should be green even without replicas
/**
// now wait until Monitoring has actually stopped
/**
// remove extraneous fields not actually wanted from the response
/**
/*
// Are you sure that you want to change the name?
// if you change this, it's a very breaking change for Monitoring
/**
// trigger execution
/*
/**
// read fields so the processor succeeds
// mock processor does nothing
/*
//                .put(XPackSettings.SECURITY_ENABLED.getKey(), false)
//                .put(XPackSettings.WATCHER_ENABLED.getKey(), false)
// Disable native ML autodetect_process as the c++ controller won't be available
//                .put(MachineLearningField.AUTODETECT_PROCESS.getKey(), false)
//                .put(XPackSettings.MACHINE_LEARNING_ENABLED.getKey(), false)
// we do this by default in core, but for monitoring this isn't needed and only adds noise.
// security has its own transport service
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
/*
/**
// Aggregations do not need any special fields
/*
/**
/**
/**
/*
/**
// The LinkedHashMaps preserve the order of the fields in the response
/**
/**
/**
/**
/**
/*
/**
/**
// used to look at the _ignored section of the query response for the actual full field name
// if the field was ignored because it was malformed and ignore_malformed was turned on
/*
// The Jackson json parser can generate for numerics - Integers, Longs, BigIntegers (if Long is not enough)
// and BigDecimal (if Double is not enough)
// docvalue_fields is always returning a Double value even if the underlying floating point data type is not Double
// even if we don't extract from docvalue_fields anymore, the behavior should be consistent
// Used to avoid recursive method calls
// Holds the sub-maps in the document hierarchy that are pending to be inspected along with the current index of the `path`.
// Find all possible entries by examining all combinations under the current level ("idx") of the "path"
// e.g.: If the path == "a.b.c.d" and the idx == 0, we need to check the current subMap against the keys:
//       "b", "b.c" and "b.c.d"
// we can only do this optimization until the last element of our pass since geo points are using arrays
// and we don't want to blindly ignore the second element of array if arrayLeniency is enabled
// this is a List with a size of 1 e.g.: {"a" : [{"b" : "value"}]} meaning the JSON is a list with one element
// or a list of values with one element e.g.: {"a": {"b" : ["value"]}}
// in case of being lenient about arrays, just extract the first value in the array
// a List of elements with more than one value. Break early and let unwrapMultiValue deal with the list
// Add the sub-map to the queue along with the current path index
// We exhausted the path and got a map
// If it is an object - it will be handled in the value extractor
// If we reach a concrete value without exhausting the full path, something is wrong with the mapping
// e.g.: map is {"a" : { "b" : "value }} and we are looking for a path: "a.b.c.d"
// A value has already been found so this means that there are more than one
// values in the document for the same path but different hierarchy.
// e.g.: {"a" : {"b" : {"c" : "value"}}}, {"a.b" : {"c" : "value"}}, ...
/*
/**
/*
/**
/*
/**
/**
// Visibility required for tests
/*
/**
/**
/*
/**
/**
/**
/*
/**
/*
/**
/**
/*
/**
// empty - such as a top level attribute in SELECT cause
// present - table name or a table name alias
// can the attr be null - typically used in JOINs
/*
/**
// collection is immutable so use that to our advantage
// array larger than size, mark the ending element as null
// a set from a collection of sets without (too much) copying
/*
// use the same name as in HashSet
// package protected - should be called through Expressions to cheaply create
// a set from a collection of sets without too much copying
/*
/**
// whether the expression can be evaluated statically (folded) or not
// the references/inputs/leaves of the expression tree
/*
// add only primitives
// but filter out multi fields (allow only the top-level value)
// skip nested fields and seen multi-fields
/*
/**
// canonical to actual/original association
// Returns the equivalent expression (if already exists in the set) or null if none is found
/*
/**
// figure out the last nested parent
// return only the qualifier is there's no path
/*
/*
/*
/**
/**
/**
/*
/**
/*
/*
/**
/*
// Whether the expression can become null
// The expression can never become null
// Cannot determine if the expression supports possible null folding
/**
/*
/*
/**
/*
/*
/*
/*
/*
/*
// unfortunately we can't use UnresolvedNamedExpression
/*
/*
// typically used for nested fields or inner/dotted fields
/*
/*
/*
/**
// TODO: Functions supporting distinct should add a dedicated constructor Location, List<Expression>, boolean
/*
/**
/**
/**
/*
// list of functions grouped by type of functions (aggregate, statistics, math etc) and ordered alphabetically inside each group
// a single function will have one entry for itself with its name associated to its instance and, also, one entry for each alias
// it has with the alias name associated to the FunctionDefinition instance
/**
// temporary map to hold [function_name/alias_name : function instance]
// sort the temporary map by key name and add it to the global map of functions
// It is worth double checking if we need this copy. These are immutable anyway.
// It is worth double checking if we need this copy. These are immutable anyway.
/**
/**
/**
/**
// These are ambiguous if you aren't using ctor references but we always do
/**
// These are ambiguous if you aren't using ctor references but we always do
/**
// These are ambiguous if you aren't using ctor references but we always do
/**
// These are ambiguous if you aren't using ctor references but we always do
/**
// These are ambiguous if you aren't using ctor references but we always do
/**
// These are ambiguous if you aren't using ctor references but we always do
/**
// These are ambiguous if you aren't using ctor references but we always do
/**
// These are ambiguous if you aren't using ctor references but we always do
// These are ambiguous if you aren't using ctor references but we always do
/**
// These are ambiguous if you aren't using ctor references but we always do
/*
/*
/*
/**
/*
/**
/**
/**
/**
/**
// try to find alternatives
/**
/**
// TODO: might be removed
// dedicated count optimization
/**
// think about this later.
/**
/**
/**
/**
/**
/*
/**
// unresolved AggNameInput (should always get replaced by the folder)
// NB: the hashcode is currently used for key generation so
// to avoid clashes between aggs with the same arguments, add the class name as variation
/*
/**
/*
/**
/*
// Agg 'enclosed' by another agg. Used for agg that return multiple embedded aggs (like MatrixStats)
/*
// used when the result needs to be extracted from a map (like in MatrixAggs or Percentiles)
/* I can't figure out how rewriting this one's children ever worked because its
/*
/**
// unresolved AggNameInput (should always get replaced by the folder)
/*
/*
/**
// used if the function is monotonic and thus does not have to be computed for ordering purposes
// null means the script needs to be used; expression means the field/expression to be used instead
/*
/*
/**
// start object
// field name
// field value
/*
//
// Utilities
//
// safe missing mapping/value extractor
//
// Operators
//
//
// Logical
//
/*
// Nothing to collect
/*
/*
// used in case the agg itself is not returned in a suitable format (like date aggs)
/**
/*
/**
// Nothing to extract
/*
/**
/*
/**
// Nothing to extract
/*
// Nothing to collect
/*
// No fields to collect
/*
/*
/*
/*
/**
/**
/*
/*
/*
/**
//no-op
/*
/**
/*
/**
/*
/*
/**
/*
/**
/*
/**
/*
/*
// Count needs special handling since in most cases it is not a dedicated aggregation
// for literals get the last count
// when dealing with fields, check whether there's a single-metric (distinct -> cardinality)
// or a bucket (non-distinct - filter agg)
/*
/*
/*
/**
// flatten params
// return vars and aggs in the declared order for binding them to the script
// return only the vars (as parameter for a script)
// agg refs are returned separately to be provided as bucket_paths
// return agg refs in a format suitable for bucket_paths
/*
/*
/*
// FIXME: this needs to be either renamed (drop Sql) or find a pluggable approach (through ScriptWeaver)
/**
/*
// used for sorting based on scripts
/*
/**
/*
//
// Custom type handling
//
// wrap intervals with dedicated methods for serialization
/*
/*
/**
/*
/**
/*
/**
// Cannot use Period.of since it accepts int so use plus which accepts long
// Further more Period and Duration have inconsistent addition methods but plus is there
//
// String parsers
//
// For consistency and validation, each pattern has its own parser
// first check if there's a sign
// take each token and use it to consume a part of the string
// validate each token and that the whole string is consumed
// consumed the string, bail out
// char token
// number char
// go through the group the digits
// negated is not present on TemporalAmount though present in both Period and Duration so handle each class individually
//
// Used the syntax described at the links below
//
// https://docs.microsoft.com/en-us/sql/odbc/reference/appendixes/interval-literal-syntax?view=sql-server-2017
// https://docs.microsoft.com/en-us/sql/odbc/reference/appendixes/interval-literals?view=sql-server-2017
// https://docs.oracle.com/cd/B19306_01/server.102/b14200/sql_elements003.htm#i38598
// patterns
/*
/**
/*
/**
/**
/*
/**
/*
/**
// NB: the id and name are being ignored for binary expressions as most of them
// are operators
/*
/*
/*
/**
// clone the list (to modify it)
// combine (in place) expressions in pairs
// NB: this loop modifies the list (just like an array)
/*
// BETWEEN or range - is a mix of gt(e) AND lt(e)
/**
// upper < lower OR upper == lower and the range doesn't contain any equals
/*
/**
/*
/**
/**
/*
/*
// Check every condition in sequence and if it evaluates to TRUE,
// evaluate and return the result associated with that condition.
// resort to default value
// Check every condition in sequence and if it evaluates to TRUE,
// evaluate and return the result associated with that condition.
// resort to default value
/*
// if the first entry is foldable, so is coalesce
// that's because the nulls are eliminated by the optimizer
// and if the first expression is folded (and not null), the rest do not matter
/*
/**
/*
/*
/*
/*
/*
/**
// Verification takes place is Case function to be
// able to generate more accurate error messages
/*
/**
/*
/*
/*
/**
/*
/*
/*
// common properties
// inferred
/*
/*
/*
// inferred
/*
// inferred
/*
/*
// Cannot fold null due to 3vl, constant folding will do any possible folding.
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
/*
/*
/**
/*
/*
// this should not occur
/*
// arithmetic operation can work on:
// 1. numbers
// 2. intervals (of compatible types)
// 3. dates and intervals
// 4. single unit intervals and numbers
// 1. both are numbers
// 2. 3. 4. intervals
// fall-back to default checks
/*
/**
/*
/**
/*
/**
// 1. both are numbers
/*
/**
/*
/**
/*
/*
// marker class to indicate operations that rely on values
/*
/*
/*
/**
/**
// typical number comparison
// when types are not compatible, cce is thrown
// fall back to null
/*
/*
/*
/*
// Optimization for early return and Query folding to LocalExec
// fold & remove duplicates
/*
/*
/*
/*
/*
/*
/**
/*
/*
/**
// early initialization to force string validation
/**
/**
/**
/*
// right() is not directly foldable in any context but Like can fold it.
/*
/*
/*
/**
// base
// logical
// conditionals
// arithmetic
// comparators
// regex
/*
/*
/**
/**
/*
// value for user types unrecognized
/**
// first get aliases (if specified)
// with security, two exception can be thrown:
// INFE - if no alias matches
// security exception is the user cannot access aliases
// in both cases, that is allowed and we continue with the indices request
// if frozen indices are requested, make sure to update the request accordingly
// these are needed to filter out the different results from the same index response
// since the index name does not support ?, filter the results manually
// filter aliases (if present)
// filter indices (if present)
/**
// merge all indices onto the same one
// build the error message
// and create a MultiTypeField
// skip unmapped
// type is okay, check aggregation
// validate search/agg-able
// everything checks
// lack of parent implies the field is an alias
// as such, create the field manually, marking the field to also be an alias
// TODO: to check whether isSearchable/isAggregateable takes into account the presence of the normalizer
//lenient because we throw our own errors looking at the response e.g. if something was not resolved
//also because this way security doesn't throw authorization exceptions but rather honors ignore_unavailable
/**
/**
// sort fields in reverse order to build the field hierarchy
// ignore size added by the mapper plugin
// apply verification
// filter meta fields and unmapped
// check each type
// Skip internal fields (name starting with underscore and its type reported by field_caps starts
// with underscore as well). A meta field named "_version", for example, has the type named "_version".
// compute the actual indices - if any are specified, take into account the unmapped indices
// add only indices that have a mapping
// put the field in their respective mappings
/*
// lack of parent implies the field is an alias
// return indices in ascending order
/*
/*
/**
// WARNING: if the collection is typed, an incompatible function will be applied to it
// this results in CCE at runtime and additional filtering is required
// preserving the type information is hacky and weird (a lot of context needs to be passed around and the lambda itself
// has no type info so it's difficult to have automatic checking without having base classes).
// use the initial value
/*
/*
/*
/*
// object or nested
// ..]
/*
/**
/*
/*
/*
/**
/**
/*
/*
/**
/*
/**
/*
/*
/*
/**
/*
/*
// run each batch until no change occurs or the limit is reached
/*
/*
/*
/**
// skip children (only properties are interesting)
// parse the list in pre-order and on match, skip the child/branch and move on to the next child/branch
// TODO: maybe add a flatMap (need to double check the Stream bit)
//
// Transform methods
//
//
// transform the node itself and its children
//
// type filtering function
// type filtering function
// stream() could be used but the code is just as complicated without any advantages
// further more, it would include bring in all the associated stream/collector object creation even though in
// most cases the immediate tree would be quite small (0,1,2 elements)
// use the initial value
/**
//
// transform the node properties and use the tree only for navigation
//
/**
/**
/**
/**
// draw children
// if not the last elder, adding padding (since each column has two chars ("|_" or "\_")
// if the child has no parent (elder on the previous level), it means its the last sibling
/**
// eliminate children (they are rendered as part of the tree)
// consider a property if it is not ignored AND
// it's not a child (optional)
//: Objects.toString(prop);
/*
/**
/**
/**
/**
/*
// break the strings into lines
// then compare each line
// find max - we could use streams but autoboxing is not cool
// try to allocate the buffer - 5 represents the column comparison chars
// right side still available
/*
/*
/**
// @formatter:off
//             esType            jdbc type,          size,              defPrecision,dispSize, int,   rat,   docvals
// 53 bits defaultPrecision ~ 15(15.95) decimal digits (53log10(2)),
// 24 bits defaultPrecision - 24*log10(2) =~ 7 (7.22)
// precision is based on long
// since ODBC and JDBC interpret precision for Date as display size
// the precision is 23 (number of chars in ISO8601 with millis) + 6 chars for the timezone (e.g.: +05:00)
// see https://github.com/elastic/elasticsearch/issues/30386#issuecomment-386807288
//
// specialized types
//
//                                                                                 display size = 2 doubles + len("POINT( )")
// IP can be v4 or v6. The latter has 2^128 addresses or 340,282,366,920,938,463,463,374,607,431,768,211,456
// aka 39 chars
//                                                                                 display size = 2 doubles + len("POINT( )")
//
// INTERVALS
// the list is long as there are a lot of variations and that's what clients (ODBC) expect
//           esType:null  jdbc type,                         size,            prec,disp, int,   rat,   docvals
// @formatter:on
// Numeric
// String
// Binary
// Date
// Intervals
// first add ES types
// reuse the ODBC definition (without SQL_)
// note that this will override existing types in particular FLOAT
// special ones
/**
/**
/**
/**
/**
/**
/**
/**
// For now all numeric values that es supports are signed
// data type extract-able from _source or from docvalue_fields
// because of ignore_above. Extracting this from _source wouldn't make sense if it wasn't indexed at all.
// because of date formats
// because of scaling_factor
/**
/**
/*
/**
/**
// if one is int
// promote the highest int
// promote the rational
// try the other side
// promote the highest rational
// interval and dates
// promote
// promote
// Interval * integer is a valid operation
// intervals widening
// null returned for incompatible intervals
// none found
/**
// Special handling for nulls and if conversion is not requires
// only primitives are supported so far
/**
// Special handling for nulls and if conversion is not requires
// We emit an int here which is ok because of Java's casting rules
// We emit an int here which is ok because of Java's casting rules
// We emit an int here which is ok because of Java's casting rules
/**
/**
// TODO floating point conversions are lossy but conversions to integer conversions are not. Are we ok with that?
/*
// return the compatible interval between the two - it is assumed the types are intervals
// YEAR and MONTH -> YEAR_TO_MONTH
// DAY... SECOND -> DAY_TIME
// YEAR_MONTH and DAY_SECOND are NOT compatible
// no need to look at YEAR/YEAR or MONTH/MONTH as these are equal and already handled
// to avoid specifying the combinations, extract the leading and trailing unit from the name
// D > H > S > M which is also the alphabetical order
// look at the trailing unit
//
// Metadata methods, mainly for ODBC.
// As these are fairly obscure and limited in use, there is no point to promote them as a full type methods
// hence why they appear here as utility methods.
//
// https://docs.microsoft.com/en-us/sql/relational-databases/native-client-odbc-date-time/metadata-catalog
// https://github.com/elastic/elasticsearch/issues/30386
// ODBC SQL_DATETME
// this is safe since the vendor SQL types are short despite the return value
// https://github.com/elastic/elasticsearch/issues/30386
// https://docs.microsoft.com/en-us/sql/odbc/reference/syntax/sqlgettypeinfo-function
// ODBC SQL_CODE_TIMESTAMP
// ODBC null
// https://docs.microsoft.com/en-us/sql/odbc/reference/appendixes/decimal-digits
// https://github.com/elastic/elasticsearch/issues/40357
// since the scale is fixed, minimum and maximum should return the same value
// hence why this method exists
// TODO: return info for SCALED_FLOATS (should be based on field not type)
// https://docs.microsoft.com/en-us/sql/odbc/reference/syntax/sqlgettypeinfo-function
// RADIX  - Determines how numbers returned by COLUMN_SIZE and DECIMAL_DIGITS should be interpreted.
// 10 means they represent the number of decimal digits allowed for the column.
// 2 means they represent the number of bits allowed for the column.
// null means radix is not applicable for the given type.
//https://docs.microsoft.com/en-us/sql/odbc/reference/syntax/sqlgettypeinfo-function#comments
//https://docs.microsoft.com/en-us/sql/odbc/reference/appendixes/column-size
/*
/**
/*
//FIXME: Taken from sql-proto.
//Ideally it should be shared but the dependencies across projects and and SQL-client make it tricky.
//Maybe a gradle task would fix that...
// In Java 8 LocalDate.EPOCH is not available, introduced with later Java versions
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
/*
/**
/*
/**
/*
/*
//FIXME: this class comes from sql-proto
// find a way to share it across or potentially just copy it over
// handle intervals
// YEAR/MONTH/YEAR TO MONTH -> YEAR TO MONTH
// +yyy-mm - 7 chars
// DAY/HOUR/MINUTE/SECOND (and variations) -> DAY_TO_SECOND
// +ddd hh:mm:ss.mmmmmmmmm - 23 chars
/*
/**
/*
// object type - only root or nested docs supported
// extract field type
// Check for multifields
/*
/**
/*
/**
/*
// typically AttributeSet which ends up iterating anyway plus creating a redundant array
/*
//FIXME: Taken from sql-proto. 
//Ideally it should be shared but the dependencies across projects and and SQL-client make it tricky.
// Maybe a gradle task would fix that...
// handle intervals
// YEAR/MONTH/YEAR TO MONTH -> YEAR TO MONTH
// +yyy-mm - 7 chars
// DAY/HOUR/MINUTE/SECOND (and variations) -> DAY_TO_SECOND
// +ddd hh:mm:ss.mmmmmmmmm - 23 chars
/*
// use the awesome http://mdaines.github.io/viz.js/ to visualize and play around with the various options
// name
// name
// draw cluster
/* to help align the clusters, add an invisible node (that could
// add edge to the first node in the cluster
// connect cluster only if there are at least two
// connecting the clusters arranges them in a weird position
// so don't
//sb.append(clusterEdges.toString());
// align the cluster by requiring the invisible nodes in each cluster to be of the same rank
// each node has its own id
// first determine node info
// skip null values, children and location
// check any subtrees
// write nested trees
// write node info
//output.append("{ rankdir=LR; rank=same; \n");
// handle children
// the child will always have the next id
// add invisible connection between children for ordering
//output.append("}\n");
// draw node
// then draw all children nodes and connections between them to be on the same level
// now draw connections to the parent
// draw the child
// create a subgraph
/*
/**
/*
// remove packaging from the name - strategy used for naming rules by default
/*
//CamelCase to camel_case
//CAMEL_CASE to camelCase
// % -> .*
// _ -> .
// escape character - can be 0 (in which case every regex gets escaped) or
// should be followed by % or _ (otherwise an exception is thrown)
// escape special regex characters
/**
// escape special regex characters
/**
// the resolver doesn't support escaping...
// corner-case when the escape char is the last char
// parsing fails, go through
/*
/*
/*
// defensive copying
// toObject
/*
/**
/*
/**
// Changing the location doesn't count as mutation because..... it just doesn't, ok?!
// Change the value to another valid value
// If we can change the data type then add that as an option as well
// Replace value
// Replace data type if there are more compatible data types
// invalid conversion then....
/*
/*
/*
/**
/**
// UnresolvedAttribute doesn't have any children
/*
// Distinct isn't supported
// Any children aren't supported
// Distinct isn't supported
// No children aren't supported
// Multiple children aren't supported
// No children aren't supported
// Multiple children aren't supported
// Distinct isn't supported
// No children aren't supported
// Multiple children aren't supported
// Distinct isn't supported
// No children aren't supported
// One child isn't supported
// Many children aren't supported
// Resolve by primary name
// Resolve by alias
// Not resolved
/*
/*
/*
/* Pick an UnresolvedFunction where the name and the
// At this point we only support functions with 0, 1, or 2 arguments.
/**
/*
/*
/*
/**
/*
/*
/*
/*
// validation
/*
/**
// default else
// CASE WHEN 1 = 1 THEN NULL
// ELSE 'default'
// END
// CASE WHEN 1 = 1 THEN 'foo'
// ELSE NULL
// END
// CASE WHEN 1 = 1 THEN NULL
// ELSE NULL
// END
// CASE WHEN 1 = 1 THEN NULL
//      WHEN 2 = 2 THEN 'foo'
// ELSE NULL
// END
/*
/**
/*
/*
/*
/*
// ((3*2+4)/2-2)%2
/*
/*
/*
/*
/*
/**
/**
/**
/**
/**
/**
/*
/**
/*
/*
/**
// start at 1 because we can't change Location.
/*
// Transformation shouldn't apply to children.
/**
// start at 1 because we can't change Location.
/*
// The arg we're looking at *is* the children
// we pass a reasonable type so get reasonable results
// The arg we're looking at is a collection contained within the children
// First make the new children
// we pass a reasonable type so get reasonable results
// Now merge that list of thildren into the original list of children
// Finally! We can assert.....
// The arg we're looking at has nothing to do with the children
// The arg we're looking at is one of the children
// makeArg produced reasonable values
// The arg we're looking at has nothing to do with the children
/*
// -1 because location isn't in the list
/*
/**
// Safe because the ctor has to be a ctor for T
/**
/**
/**
// Wrap to make `randomValueOtherThan` happy.
/**
// AggValueInput just needs a valid java type in a supplier
// But the supplier has to implement equals for randomValueOtherThan
/*
// InnerAggregate's AggregateFunction must be an EnclosedAgg.
// `parent` is nullable.
/*
/*
/*
/*
/*
/*
// safe because this is the lowest possible bounds for Node
// Can't mock enums but luckily we can just pick one
// Can't mock primitives....
// Nor strings
// Location is final and can't be mocked but we have a handy method to generate ones.
// Delegate to the test class for a node if there is one
/**
/**
// The map is built this way
// load classes from jar files
// NIO FileSystem API is not used since it trips the SecurityManager
// https://bugs.openjdk.java.net/browse/JDK-8160798
// so iterate the jar "by hand"
// for folders, just use the FileSystems API
// Chop off the root and file extension
// Go from "path" style to class style
/**
// filter the class that are not interested
// (and IDE folders like eclipse)
/**
/*
/*
/*
/*
/*
/**
// double check back and forth conversion
// double check back and forth conversion
// double check back and forth conversion
// We only handled upper and lower case true and false
// Everything else should fail
// strings
// numeric and intervals
// dates/datetimes and intervals
// Doesn't have a corresponding type in ES
/*
// type checks
/*
/*
// Introduced in ES version 6.3
// Introduced in ES Version 6.4
// Bumped due to ID collision, see #32372
// list of headers that will be stored when a job is created
// overridable by tests
/*
/*
/**
/**
// TODO there is an opportunity to optimize the returned caps to find the minimal set of required caps.
// For example, one leaf may have equally good jobs [A,B], while another leaf finds only job [B] to be best.
// If job A is a subset of job B, we could simply search job B in isolation and get the same results
//
// We can't do that today, because we don't (yet) have way of determining if one job is a sub/super set of another
/**
// Ensure we are working on the same timezone
/*
// If histo used calendar_interval explicitly
// Try to use explicit calendar_interval on config if it exists
// Otherwise fall back to old style where we prefer calendar over fixed (e.g. `1h` == calendar)
// Note that this ignores FIXED_INTERVAL on purpose, it would not be compatible
// If histo used fixed_interval explicitly
// Try to use explicit fixed_interval on config if it exists
// Otherwise fall back to old style
// Note that this ignores CALENDER_INTERVAL on purpose, it would not be compatible
// The histo used a deprecated interval method, so meaning is ambiguous.
// Use legacy method of preferring calendar over fixed
// Try to use explicit calendar_interval on config if it exists
// Both must be calendar intervals
// Otherwise fall back to old style where we prefer calendar over fixed (e.g. `1h` == calendar)
// Need to verify that the config interval is in fact calendar here
// The histo's interval couldn't be parsed as a calendar, so it is assumed fixed.
// Try to use explicit fixed_interval on config if it exists
// Otherwise fall back to old style interval millis
// Need to verify that the config interval is not calendar here
// This _should not_ happen, but if miraculously it does we need to just quit
// If we get here nothing matched, and we can break out
// We are a leaf, save our best caps
// otherwise keep working down the tree
// The request must be gte the config.  The CALENDAR_ORDERING map values are integers representing
// relative orders between the calendar units
// All calendar units are multiples naturally, so we just care about gte
// Both are fixed, good to convert to millis now
// Must be a multiple and gte the config
/**
// query interval must be gte the configured interval, and a whole multiple
// We are a leaf, save our best caps
// otherwise keep working down the tree
/**
// We are a leaf, save our best caps
// otherwise keep working down the tree
/**
// Metrics are always leaves so go ahead and add to best caps
// histogram intervals are averaged and compared, with the idea that
// a larger average == better, because it will generate fewer documents
// Similarly, fewer terms groups will generate fewer documents, so
// we count the number of terms groups
// Iterate over the first Caps and collect the various stats
// Iterate over the second Cap and collect the same stats
// Compare on date interval first
// The "smaller" job is the one with the larger interval
// If dates are the same, the "smaller" job is the one with a larger histo avg histo weight.
// Not bullet proof, but heuristically we prefer:
//  - one job with interval 100 (avg 100) over one job with interval 10 (avg 10)
//  - one job with interval 100 (avg 100) over one job with ten histos @ interval 10 (avg 10)
// because in both cases the larger intervals likely generate fewer documents
//
// The exception is if one of jobs had no histo (avg 0) then we prefer that
// If dates and histo are same, the "smaller" job is the one with fewer terms aggs since
// hopefully will generate fewer docs
// Ignoring metrics for now, since the "best job" resolution doesn't take those into account
// and we rely on the msearch functionality to merge away and duplicates
// Could potentially optimize there in the future to choose jobs with more metric
// coverage
/*
/**
/**
/**
// We have to fall back to deprecated interval because we're not sure if this is fixed or cal
// if interval() was used we know it is fixed and can upgrade
/**
/**
/**
// Translate all subaggs and add to the newly translated agg
// NOTE: using for loop instead of stream because compiler explodes with a bug :/
// Count is derived from a sum, e.g.
// "my_date_histo._count": { "sum": { "field": "foo.date_histogram._count" } } }
/**
// If it's an avg, we have to manually convert it into sum + sum aggs
// Avg metric is translated into a SumAgg, e.g.
// Note: we change the agg name to prevent conflicts with empty buckets
// "the_avg.value" : { "field" : "some_field.avg.value" }}
// Count is derived from a sum, e.g.
// "the_avg._count": { "sum" : { "field" : "some_field.avg._count" }}
// Otherwise, we can cheat and serialize/deserialze into a temp stream as an easy way to clone
// leaf metrics, since they don't have any sub-aggs
/*
/**
/**
/**
// If an index was deleted after execution, give a hint to the user that this is a transient error
// Otherwise just throw
// No error, add to responses
/**
// If an index was deleted after execution, give a hint to the user that this is a transient error
// Otherwise just throw
// No error, add to responses
// If we only have a live index left, just return it directly.  We know it can't be an error already
// We had no rollup aggs, so there is nothing to process
// Return an empty response, but make sure we include all the shard, failure, etc stats
// We were missing some but not all the aggs, unclear how to handle this.  Bail.
// The combination process returns a tree that is identical to the non-rolled
// which means we can use aggregation's reduce method to combine, just as if
// it was a result from another shard
// We expect a filter agg here because the rollup convention is that all translated aggs
// will start with a filter, containing various agg-specific predicates.  If there
// *isn't* a filter agg here, something has gone very wrong!
// Iteratively merge in each new set of unrolled aggs, so that we can identify/fix overlapping doc_counts
// in the next round of unrolling
// Add in the live aggregations if they exist
// Shard failures are ignored atm, so returning an empty array is fine
/**
// During the translation process, some aggregations' doc_counts are stored in accessory
// `sum` metric aggs, so we may need to extract that.  Unfortunately, structure of multibucket vs
// leaf metric is slightly different; multibucket count is stored per-bucket in a sub-agg, while
// metric is "next" to the metric as a sibling agg.
//
// So we only look for a count if this is not a multibucket, as multibuckets will handle
// the doc_count themselves on a per-bucket basis.
//
/**
/**
// The only thing unique between all the multibucket agg is the type of bucket they
// need, so this if/else simply creates specialized closures that return the appropriate
// bucket type.  Otherwise the heavy-lifting is in
// {@link #unrollMultiBucket(InternalMultiBucketAggregation, InternalMultiBucketAggregation, TriFunction)}
//TODO expose getFormatter(), keyed upstream in Core
//TODO expose getFormatter(), keyed upstream in Core
/**
// Iterate over the buckets in the multibucket
// If the original has this key, ignore the rolled version
// Grab the value from the count agg (if it exists), which represents this bucket's doc_count
// Don't generate buckets if the doc count is zero
// current, partially merged tree contains this key.  Defer to the existing doc_count if it is non-zero
// Unlike above where we return null if doc_count is zero, we return a doc_count: 0 bucket
// here because it may have sub-aggs that need merging, whereas above the bucket was just empty/null
// Then iterate over the subAggs in the bucket
/**
// Iterate over the subAggs in each bucket
// Avoid any rollup count metrics, as that's not a true "sub-agg" but rather agg
// added by the rollup for accounting purposes (e.g. doc_count)
// TODO comment from Colin in review:
// RAW won't work here long term since if you do a max on e.g. a date field it will
// render differently for the rolled up and non-rolled up results. At the moment
// the formatter is not exposed on the internal agg objects but I think this is
// something we can discuss exposing
// If count is anything other than -1, this sum is actually an avg
// Note: Avgs have a slightly different name to prevent collision with empty bucket defaults
// Note: Avgs have a slightly different name to prevent collision with empty bucket defaults
// we always set the count fields to Sum aggs, so this is safe
/*
// TODO expose the currently running rollup tasks on this node?  Unclear the best way to do that
/*
// Note: we ignore unknown fields since there may be unrelated metadata
/*
/**
// Ignore unknown fields because there could be unrelated doc types
/**
// Ignore unknown fields because there could be unrelated _meta values
// "job-1"
/*
// If we couldn't find the job in the persistent task CS, it means it was deleted prior to this call,
// no need to go looking for the allocated task
// Delegates DeleteJob to elected master node, so it becomes the coordinating node.
// Non-master nodes may have a stale cluster state that shows jobs which are cancelled
// on the master, which makes testing difficult.
// There should theoretically only be one task running the rollup job
// If there are more, in production it should be ok as long as they are acknowledge shutting down.
// But in testing we'd like to know there were more than one hence the assert
/*
// Does this index have rollup metadata?
// This index has rollup metadata, and since we want _all, just process all of them
// This index has rollup metadata, but is it for the index pattern that we're looking for?
// Do we already have an entry for this index pattern?
// Convert the mutable lists into the RollableIndexCaps
/*
// Does this index have rollup metadata?
// Do we already have an entry for this index?
// Convert the mutable lists into the RollableIndexCaps
/*
// If we couldn't find the job in the persistent task CS, it means it was deleted prior to this GET
// and we can just send an empty response, no need to go looking for the allocated task
// Delegates GetJobs to elected master node, so it becomes the coordinating node.
// Non-master nodes may have a stale cluster state that shows jobs which are cancelled
// on the master, which makes testing difficult.
/**
// If the request was for _all rollup jobs, we need to look through the list of
// persistent tasks and see if at least once has a RollupJob param
// If we're looking for a single job, we can just check directly
// Little extra insurance, make sure we only return jobs that aren't cancelled
/*
// ensure we only filter for the allowed headers
/*
// Both
// Only live
// Only rollup
// Don't support _all on everything right now, for code simplicity
// not best practice, but if the user accidentally only sends "normal" indices we can support that
// Rollup only supports a limited subset of the search API, validate and make sure
// nothing is set that we can't support
// The original request is added as-is (if normal indices exist), minus the rollup indices
// If there are no aggs in the request, our translation won't create any msearch.
// So just add an dummy request to the msearch and return.  This is a bit silly
// but maintains how the regular search API behaves
// Note: we can't apply any query rewriting or filtering on the query because there
// are no validated caps, so we have no idea what job is intended here.  The only thing
// this affects is doc count, since hits and aggs will both be empty it doesn't really matter.
// Find our list of "best" job caps
// TODO this filter agg is now redundant given we filter on job ID
// in the query and the translator doesn't add any clauses anymore
// Translate the agg tree, and collect any potential filtering clauses
// Rewrite the user's query to our internal conventions, checking against the validated job caps
// filter the rewritten query by JobID
// Both versions are acceptable right now since they are compatible at search time
// And add a new msearch per JobID
/**
// Rollup does not support hits at the moment
// no-op
// We only care about job caps that have the query's target field
// For now, we only allow filtering on grouping fields
// make sure it's one of the three groups
// Rewrite the field name to our convention (e.g. "foo" -> "date_histogram.foo.timestamp")
// We already got the task created on the network layer - no need to create it again on the transport layer
/*
// Either the job doesn't exist (the user didn't create it yet) or was deleted after the StartAPI executed.
// In either case, let the user know
/*
// The Task acknowledged that it is stopped/stopping... wait until the status actually
// changes over before returning.  Switch over to Generic threadpool so
// we don't block the network thread
// We have successfully confirmed a stop, send back the response
// Did not acknowledge stop, just return the response
// No request to block, execute async
/**
// Either the job doesn't exist (the user didn't create it yet) or was deleted after the Stop API executed.
// In either case, let the user know
/*
/**
/*
/**
/**
// Put the composite keys into a treemap so that the key iteration order is consistent
// TODO would be nice to avoid allocating this treemap in the future
// Also add a doc count for each key.  This will duplicate data, but makes search easier later
// Go back through and remove all empty counts
/*
/**
// New ID scheme uses a (hopefully) unique placeholder for null
/*
/**
/**
/**
// this is needed to exclude buckets that can still receive new documents
// if the job has a delay we filter all documents that appear before it
// make sure we always compute complete buckets that appears before the configured delay
// do not reset the position as we want to continue from where we stopped
/**
/**
// Add all the metadata in order: date_histo -> histo
// Add all the agg builders to our request in order: date_histo -> histo -> terms
/**
// Avgs are sum + count
// TODO allow non-numeric value_counts.
// Hardcoding this is fine for now since the job validation guarantees that all metric fields are numerics
/*
/**
// Note that while the task is added to the scheduler here, the internal state will prevent
// it from doing any work until the task is "started" via the StartJob api
/**
// If we're aborting, just invoke `next` (which is likely an onFailure handler)
// Otherwise, attempt to persist our state
// If initial position is not null, we are resuming rather than starting fresh.
/*
// It shouldn't be possible to persist ABORTING, but if for some reason it does,
// play it safe and restore the job as STOPPED.  An admin will have to clean it up,
// but it won't be running, and won't delete itself either.  Safest option.
// If we were STOPPING, that means it persisted but was killed before finally stopped... so ok
// to restore as STOPPED
/**
/**
/**
// We're already running so just return acknowledgement
// if we're not already started/indexing, we must be STOPPED to get started
// We were unable to update the persistent status, so we need to shutdown the indexer too.
/**
// update the persistent state to STOPPED.  There are two scenarios and both are safe:
// 1. we persist STOPPED now, indexer continues a bit then sees the flag and checkpoints another
//    STOPPED with the more recent position.
// 2. we persist STOPPED now, indexer continues a bit but then dies.  When/if we resume we'll pick up
//    at last checkpoint, overwrite some docs and eventually checkpoint.
/**
/**
// there is no background job running, we can shutdown safely
/**
// Verify this is actually the event that we care about, then trigger the indexer.
// Note that the status of the indexer is checked in the indexer itself
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// Both jobs functionally identical, so only one is actually needed to be searched
// TODO Is it what we really want to test?
// <-- comes from job1
// <-- comes from job2
// NOTE same name but wrong type
// <-- NOTE right type but wrong name
// interval in job is much higher than agg interval above
// NOTE different field from the one in the query
// <-- NOTE right type but wrong name
// <-- NOTE right type but wrong name
// <--- interval is not a multiple of 3
// Fails because both are actually fixed
// This only tests for calendar/fixed ordering, ignoring the other criteria
// This only tests for fixed ordering, ignoring the other criteria
// This only tests for calendar ordering, ignoring the other criteria
// Job has "obsolete" timezone
// now the reverse, job has "new" timezone
/*
/*
// TODO SearchResponse.Clusters is not public, using null for now.  Should fix upstream.
// Reduce the InternalDateHistogram response so we can fill buckets
// Note: term query for "a"
// Time 100: Two "a" documents, one "b" doc
// Time 200: one "a" document, one "b" doc
// Note: term query for "a"
// Time 100: Two "a" documents, one "b" doc
// Time 200: one "a" document, one "b" doc
// two "a" at 100
// one "a" at 200
// Times 100/200 overlap with currentTree, so doc_count will be zero
// This time (300) was not in the currentTree so it will have a doc_count of one
// In this test we merge real buckets into zero count buckets (e.g. empty list of buckets after unrolling)
// Times 100/200 overlap with currentTree, but doc_count was zero, so returned doc_count should be one
// This time (300) was not in the currentTree so it will have a doc_count of one
// In this test, we merge zero_count buckets into existing buckets to ensure the metrics remain
// All values overlap and were zero counts themselves, so the unrolled response should be empty list of buckets
// NOTE: we manually set the count to 3 here, which is somewhat cheating.  Will have to rely on
// other tests to verify that the avg's count is set correctly
// off target
// The null_value placeholder should be removed from the response and not visible here
// <-- Only one that should show up in rollup
/*
/*
/*
/*
/*
/*
// ResourceAlreadyExists should trigger a GetMapping next
// Bail here with an error, further testing will happen through tests of #updateMapping
// ResourceAlreadyExists should trigger a GetMapping next
// Make sure the version is present, and we have our date template (the most important aspects)
// Bail here with an error, further testing will happen through tests of #updateMapping
// ResourceAlreadyExists should trigger a GetMapping next
// Bail here with an error, further testing will happen through tests of #startPersistentTask
// Bail here with an error, further testing will happen through tests of #startPersistentTask
/*
/*
// so that the jobs aren't exactly equal
// The executed query should match the first job ("foo") because the second job contained a histo and the first didn't,
// so the first job will be "better"
/*
/*
/*
/*
// Have to mock fieldcaps because the ctor's aren't public...
// Have to mock fieldcaps because the ctor's aren't public...
/*
//TODO split this into dedicated unit test classes (one for each config object)
/*
// Setup the composite agg
// Setup the composite agg
//TODO swap this over to DateHistoConfig.Builder once DateInterval is in
// Setup the composite agg
// Setup the composite agg
/*
// Every other doc omit the valueField, so that we get some null buckets
// Setup the composite agg
// 2015-10-01T00:30:00Z
// 2015-10-01T01:30:00Z
// Setup the composite agg
// adds a timezone so that we aren't on default UTC
// 2015-09-30T00:00:00.000-01:00
// 2015-10-01T00:00:00.000-01:00
/*
// Make sure the timestamp is sufficiently in the past that we don't get bitten
// by internal rounding, causing no docs to match
/**
/**
// Force all numbers to longs
// extract query
// extract composite agg
/*
// TODO Should use InternalComposite constructor but it is package protected in core.
// We call stats before a final state check, so this allows us to flip the state
// and make sure the appropriate error is thrown
// <-- Set to aborting right before we return the (empty) search response
// Don't use the indexer's latch because we completely change doNextSearch()
// TODO Should use InternalComposite constructor but it is package protected in core.
// Abort immediately before we are attempting to finish the job because the response
// was empty
// Tests how we handle unknown keys that come back from composite agg, e.g. if we add support for new types but don't
// deal with it everyhwere
// Despite failure in bulk, we should move back to STARTED and wait to try again on next trigger
// There should be one recorded failure
// Note: no docs were indexed
// Tests to make sure that errors in search do not interfere with shutdown procedure
// <- Force a stop so we can see how error + non-INDEXING state is handled
// This will throw an exception
// Despite failure in processing keys, we should continue moving to STOPPED
// There should be one recorded failure
// Note: no docs were indexed
// Despite failure in bulk, we should move back to STARTED and wait to try again on next trigger
// There should be one recorded failure
// Note: no pages processed, no docs were indexed
// Despite failure in bulk, we should move back to STARTED and wait to try again on next trigger
// There should be one recorded failure
// Note: no docs were indexed
/*
/*
// Wait before progressing
// Allow search response to return now
// Wait for the final persistent status to finish
// Wait before progressing
// Allow search response to return now
// Wait for the final persistent status to finish
// Wait before progressing
// Allow search response to return now
// Wait for the final persistent status to finish
// This isn't really realistic, since start/stop/cancelled are all synchronized...
// the task would end before stop could be called.  But to help test out all pathways,
// just in case, we can override markAsCompleted so it's a no-op and test how stop
// handles the situation
/*
/**
/** Caps scores from the passed in Query to the supplied maxScore parameter */
/** Returns the encapsulated query. */
/**
// we must wrap again here, but using the scorer passed in as parameter:
// test scoreMode to avoid NPE - see https://github.com/elastic/elasticsearch/issues/51034
// short-circuit if scores will not need capping
/*
/*
/**
/*
/**
// Organic queries will have their scores capped to this number range,
// We reserve the highest float exponent for scores of pinned queries
/**
/**
/**
/**
// Ensure each pin order using a Boost query with the relevant boost factor
// Ensure the pin order using a Boost query with the relevant boost factor
// Score for any pinned query clause should be used, regardless of any organic clause score, to preserve pin order.
// Use dismax to always take the larger (ie pinned) of the organic vs pinned scores
// Cap the scores of the organic query
/*
/*
// add lower-scoring text
// add higher-scoring text
// Add docs with no relevance
// Test doc pinning
// Occasionally try a query with no matches to check all pins still show
// Check pins are sorted by increasing score, (unlike organic, there are no duplicate scores)
// Check that the pins appear in the requested order (globalHitNumber is cursor independent of from and size window used)
// Test the organic hits are sorted by text relevance
/**
/*
// generate unicode string in 10% of cases
// Have IDs and an organic query - uses DisMax
/**
// BoolQueryBuilder test has this test for a more detailed error message:
// assertEquals("no [query] registered for [unknown_query]", ex.getMessage());
// But ObjectParser used in PinnedQueryBuilder tends to hide the above message and give this below:
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
// generate a private key
/**
// it is a wildcard address
/**
/*
/**
/**
// if the class initializer here runs before the main method, logging will not have been configured; this will lead to status logger
// error messages from the class initializer for ParseField since it creates Logger instances; therefore, we bury the initialization
// of the parser in this class so that we can defer initialization until after logging has been initialized
/**
/**
/**
// EMPTY is safe here because we never use namedObject
/**
// write csr
// write private key
/**
// generate the CA keys and cert
/**
// write out the CA info first if it was generated
// write cert
// write private key
/**
// set permissions to 600
/**
// we can safely nuke the password chars now
/**
/*
/**
/**
/**
// if the class initializer here runs before the main method, logging will not have been configured; this will lead to status logger
// error messages from the class initializer for ParseField since it creates Logger instances; therefore, we bury the initialization
// of the parser in this class so that we can defer initialization until after logging has been initialized
// Common option for multiple commands.
// Not every command uses every option, but where they are common we want to keep them consistent
// For testing
/**
/**
/**
/**
// we can safely nuke the password chars now
/**
/**
// write csr
// write private key
/**
// write out the CA info first if it was generated
// write cert
// write private key
/**
// EMPTY is safe here because we never use namedObject
/**
/**
/**
// set permissions to 600
/**
/*
/**
/**
/**
/**
/**
/* TODO
// set permissions to 600
// TODO : Should we add support for configuring PKI in ES?
// TODO Add info to the READMEs so that the user could regenerate these certs if needed.
// (i.e. show them the certutil cert command that they would need).
// No local CA, generate a CSR instead
// TODO : Should we add support for client certs from Kibana to ES?
/**
/**
/**
// trust the extension for some file-types rather than inspecting the contents
// we don't rely on filename for PEM files because
// (a) users have a tendency to get things mixed up (e.g. naming something "key.crt")
// (b) we need to distinguish between Certs & Keys, so a ".pem" file is ambiguous
// Sniff the file. We could just try loading them, but then we need to catch a variety of exceptions
// and guess what they mean. For example, loading a PKCS#12 needs a password, so we would need to
// distinguish between a "wrong/missing password" exception and a "not a PKCS#12 file" exception.
// No supported file type has less than 2 bytes
// Probably a PEM file, but we need to know what type of object(s) it holds
// Not a PEM
// A Key and something else. Could be a cert + key pair, but we don't support that
// Multiple certificates = chain
/**
// For testing
/*
/**
/*
/**
// TODO baz - fix this to work in intellij+java9, its complaining about java.sql.Date not being on the classpath
// test with a user provided dir
// test without a user provided directory
// test with empty user input
// check the CA cert
// check the CA key
// test generation
// good name
// too long
// too short
// invalid characters only
// invalid for file but DN ok
// invalid with valid chars for filename
// valid but could create hidden file/dir so it is not allowed
/**
/**
/*
/**
// test with a user provided file
// test without a user provided file, with user input (prompted)
// test with empty user input
// check the CA cert
// check the CA key
// Verify we are using AES encryption
// Catch error thrown by the empty mock, we are only interested in the argument passed in
// test generation
// good name
// null
// too long
// too short
// invalid characters only
// invalid for file but DN ok
// invalid with valid chars for filename
// valid but could create hidden file/dir so it is not allowed
/**
// Needed to work within the security manager
// Node 3 uses an auto generated CA, and therefore should not be trusted by the other nodes.
/**
// do nothing, all we care about is the "zip" flag
// Regardless of the commandline options, just work with a single cert
/**
/**
/**
/**
/**
/*
// generate CSR
// cert-per-node
// enter hostnames
// end-of-hosts
// yes, correct
// enter ip names
// end-of-ips
// yes, correct
// don't change advanced settings
// confirm
// Verify the CSR was built correctly
// Verify the key
// Verify the README
// Verify the yml
// Should not be a CA directory in CSR mode
// No CA in CSR mode
// don't generate CSR
// existing CA
// randomise between cert+key, key+cert, PKCS12 : the tool is smart enough to handle any of those.
// validity period
// don't use cert-per-node
// enter hostnames
// end-of-hosts
// yes, correct
// enter ip names
// end-of-ips
// yes, correct
// don't change advanced settings
// confirm
// Verify the Cert was built correctly
// Verify the README
// Verify the yml
// Should not be a CA directory when using an existing CA.
// don't generate CSR
// no existing CA
// randomise whether to change CA defaults.
// Change defaults
// Don't change values
// Don't change defaults
// confirm
// node cert validity period
// cert-per-node
// another cert
// certificate / node name
// enter hostname
// end-of-hosts
// end-of-hosts
// yes, correct
// no ip
// end-of-ip
// yes, correct
// don't change advanced settings
// no more certs
// confirm
// Should have a CA directory with the generated CA.
// Verify the Cert was built correctly
// Verify the README
// Verify the yml
// Test: Re-prompt on bad input.
// Test: Accept default value
// Test: Minimum Days
// I'm sure
// I'm not sure
// I'm not sure
// We want to assert that this password doesn't end up in any output files, so we need to make sure we
// don't randomly generate a real word.
// We rebuild the DN from the encoding because BC uses openSSL style toString, but we use LDAP style.
// We register 1 extension - the subject alternative names
// We don't know exactly when the certificate was generated, but it should have been in the last 10 minutes
/**
//"));
//"));
/**
/*
/*
// If this is the first run (security not yet enabled), then don't clean up afterwards because we want to test restart with data
// Security runs second, and should see the doc from the first (non-security) run
// From file realm, configured in build.gradle
/*
// From file realm, configured in build.gradle
// optional\n" +
// optional\n" +
// optional\n" +
/*
/*
/**
/*
/**
// FIXME this is an antipattern move this out of a bootstrap check!
/*
/* what a PITA that we need an extra indirection to initialize this. Yet, once we got rid of guice we can thing about how
// TODO This is wrong. Settings can change after this. We should use the settings from createComponents
// TODO this is wrong, we should only use the environment that is provided to createComponents
// we load them all here otherwise we can't access secure settings since they are closed once the checks are
// fetched
// overridable by tests
// pkg private for testing - tests want to pass in their set of extensions hence we are not using the extension service directly
// We need to construct the checks here while the secure settings are still available.
// If we wait until #getBoostrapChecks the secure settings will have been cleared/closed.
// audit trail service construction
// realms construction
// to keep things simple, just invalidate all cached entries on license change. this happens so rarely that the impact should be
// minimal
// used by roles actions
// used by roles actions
// for SecurityInfoTransportAction and clear roles cache
// visible for tests
// default to security4
/**
// The following just apply in node mode
// IP Filter settings
// audit settings
// authentication and authorization settings
// hide settings
// hide settings where we don't define them - they are part of a group...
// we pass a null index reader, which is legal and will disable rewrite optimizations
// based on index statistics, which is probably safer...
/*
// in order to prevent scroll ids from being maliciously crafted and/or guessed, a listener is added that
// attaches information to the scroll context so that we can validate the user that created the scroll against
// the user that is executing a scroll operation
/**
// suffix-part, only contains a single '.'
// don't register anything if we are not enabled
// don't register anything if we are not enabled
// security based on Netty 4
// security based on NIO
// don't register anything if we are not enabled
// .security index is not managed by using templates anymore
/*
/**
/*
/**
/*
// If security has been explicitly disabled in the settings, then SSL is also explicitly disabled, and we don't want to report
//  these http/transport settings as they would be misleading (they could report `true` even though they were ignored)
// But, if security has not been explicitly configured, but has defaulted to off due to the current license type,
// then these SSL settings are still respected (that is SSL might be enabled, while the rest of security is disabled).
// the only available output type is "logfile", but the optputs=<list> is to keep compatibility with previous reporting format
/*
/**
/*
/**
/**
/*
/**
/*
/**
/*
// restrict username and realm to current authenticated user.
/*
// restrict username and realm to current authenticated user.
/*
/*
/*
/*
/*
/**
/*
/*
/**
/*
/**
/*
/**
/*
/**
/*
// this is heavy handed since we could also take realm into account but that would add
// complexity since we would need to iterate over the cache under a lock to remove all
// entries that referenced the specific realm
/*
/*
/*
// specific roles were requested but they were built in only, no need to hit the store
/*
/*
/*
/*
/*
/**
/*
/**
// If we don't have a valid name-id to match against, don't do anything
// Invalidate the refresh token first, so the client doesn't trigger a refresh once the access token is invalidated
/*
/**
/*
/**
/*
/**
// it may contain base64 encoded token that needs to be sent to client if mutual auth was requested
// the OAuth2.0 RFC requires the scope to be provided in the
// response if it differs from the user provided scope. If the
// scope was not provided then it does not need to be returned.
// if the scope is not supported, the value of the scope that the
// token is for must be returned
// this is the only non-null value that is currently supported
/*
/**
/*
/*
/*
/*
/*
/**
/*
// We have two sources for the users object, the reservedRealm and the usersStore, we query both at the same time with a
// GroupedActionListener
// we get all users from the realm
// pass an empty list to inform the group listener
// - no real lookups necessary
// nested group listener action here - for each of the users we got and fetch it concurrently - once we are done we notify
// the "global" group listener.
// user store lookups
// no users requested notify
// go and get all users from the users store and pass it directly on to the group listener
/*
/**
/*
/*
/**
// make sure the user is not disabling themselves
/*
/*
/**
/**
/*
/** Returns the audit trail implementations that this service delegates to. */
/*
// Store as a header (not transient) so that it is passed over the network if this request requires execution on other nodes
/*
/*
// changing any of this names requires changing the log4j2.properties file too
// because of the default wildcard value (*) for the field filter, a policy with
// an unspecified filter field will match events that have any value for that
// particular field, as well as events with that particular field missing
// package for testing
// fields that all entries have in common
// `events` is a volatile field! Keep `events` write last so that
// `entryCommonFields` and `includeRequestBody` writes happen-before! `events` is
// always read before `entryCommonFields` and `includeRequestBody`.
// this log filter ensures that audit events are not filtered out because of the log level
// this is the default
// fall through to local_node default
// this is the default
// fall through to local_node default
/**
/**
// "null" values are "unexpected" and should not match any ignore policy
/**
//").test("") == true`
//");
//" : f).collect(Collectors.toList());
/**
/**
// precompute predicate
// precompute predicate
/**
// empty is used for events can be filtered out only by the lack of a field
/**
// Supplier indirection and lazy generation of Streams serves 2 purposes:
// 1. streams might not get generated due to short circuiting logical
// conditions on the `principal` and `realm` fields
// 2. reusability of the AuditEventMetaInfo instance: in this case Streams have
// to be regenerated as they cannot be operated upon twice
// check if local node changed
// no need to synchronize, called only from the cluster state applier thread
// the default origin is local
// the default origin is local
/*
/**
/**
// Save role_descriptors
// Save limited_by_role_descriptors
/**
/**
/**
// move on
// same key, pass the same result
// move on
// move on
// pkg private for testing
/**
// package private class for testing
/**
/**
/**
/**
// pkg scoped for testing
// pkg scoped for testing
/**
/**
/*
/**
/**
/**
/**
// pkg private method for testing
// pkg private method for testing
// pkg private method for testing
/**
/**
// this happens when the license state changes between the call to authenticate and the actual invocation
// to get the realm list
// intentionally ignore the returned exception; we call this primarily
// for the auditing as we already have a purpose built exception
/**
// While we could place this call in the try block, the issue is that we catch all exceptions and could catch exceptions that
// have nothing to do with a tampered request.
/**
// pkg-private accessor testing token extraction with a consumer
/**
// user was authenticated, populate the authenticated by information
// the user was not authenticated, call this so we can audit the correct event
/**
/**
// pkg-private for tests
// we assign the listener call to an action to avoid calling the listener within a try block and auditing the wrong thing when
// an exception bubbles up even after successful authentication
/**
/**
// the user does not exist, but we still create a User object, which will later be rejected by authz
// only cache this as last success if it doesn't exist since this really isn't an auth attempt but
// this might provide a valid hint
/**
// TODO: these should be different log messages if the runas vs auth user is disabled?
/**
// we assign the listener call to an action to avoid calling the listener within a try block and auditing the wrong thing
// when an exception bubbles up even after successful authentication
// There might be an existing audit-id (e.g. generated by the  rest request) but there might not be (e.g. an internal action)
// There should never be an existing audit-id when processing a rest request.
/*
/**
/*
/**
/*
/**
// tokens can still linger on the main index for their maximum lifetime after the tokens index has been created, because
// only after the tokens index has been created all nodes will store tokens there and not on the main security index
/*
/**
/**
/**
/**
/**
/**
// file realm
// native realm
// active directory realm
// LDAP realm
// PKI realm
// SAML realm
// Kerberos realm
// OpenID Connect realm
/*
/**
// a list of realms that are considered standard in that they are provided by x-pack and
// interact with a 3rd party source on a limited basis
// a list of realms that are considered native, that is they only interact with x-pack and no 3rd party auth sources
// pre-computing a list of internal only realms allows us to have much cheaper iteration than a custom iterator
// and is also simpler in terms of logic. These lists are small, so the duplication should not be a real issue here
// don't add the reserved realm here otherwise we end up with only this realm...
/**
// If auth is not allowed, then everything is unlicensed
// If all realms are allowed, then nothing is unlicensed
// Shortcut for the typical case, all the configured realms are allowed
// Otherwise, we return anything in "all realms" that is not in the allowed realm list
// this is an internal realm factory, let's make sure we didn't already registered one
// (there can only be one instance of an internal realm)
// there is no "realms" configuration, add the defaults
// always add built in first!
// iterate over the factories so we can add enabled & available info
// the realms iterator returned this type so it must be enabled
/*
/**
/**
//www.owasp.org/index.php/Password_Storage_Cheat_Sheet">OWASP Password Storage
//pages.nist.gov/800-63-3/sp800-63b.html#sec5">
// UUIDs are 16 bytes encoded base64 without padding, therefore the length is (16 / 3) * 4 + ((16 % 3) * 8 + 5) / 6 chars
/**
/**
// the created token is compatible with the oldest node version in the cluster
// tokens moved to a separate index in newer versions
// the id of the created tokens ought be unguessable
/**
//public for testing
// the created token is compatible with the oldest node version in the cluster
// tokens moved to a separate index in newer versions
/**
// prior versions of the refresh token are not version-prepended, as nodes on those
// versions don't expect it.
// Such nodes might exist in a mixed cluster during a rolling upgrade.
/**
// public for testing
/**
/**
/**
// The chances of a random token string decoding to something that we can read is minimal, so
// we assume that this was a token we have created but is now expired/revoked and deleted
// if the index or the shard is not there / available we assume that
// the token is not valid
/**
// The token was created in a > VERSION_ACCESS_TOKENS_UUIDS cluster
// TODO Remove this conditional after backporting to 7.x
// The token was created in a < VERSION_ACCESS_TOKENS_UUIDS cluster so we need to decrypt it to get the tokenId
// could happen with a token that is not ours
// could happen with a token that is not ours
// could happen with a token that is not ours
/**
/**
/**
/**
/**
// Invalidate the refresh tokens first so that they cannot be used to get new
// access tokens while we invalidate the access tokens we currently know about
/**
// carry-over result of the invalidation for the tokens security index
/**
/**
/**
// first check if token has the old format before the new version-prepended one
// TODO Remove this conditional after backporting to 7.x
/**
/**
// The document has been updated by another thread, get it again.
/**
/*
// newly minted tokens are compatible with the min node version in the cluster
/**
/**
/**
/**
/**
/**
/**
// an existing tokens index always contains tokens (if available and version allows)
// main security index _might_ contain tokens if the tokens index has been created recently
/**
/**
/**
// index doesn't exist so the token is considered invalid as we cannot verify its validity
// if the index or the shard is not there / available we assume that
// the token is not valid
/**
// we know that the minimum length is larger than the default of the ByteArrayOutputStream so set the size to this explicitly
// StreamOutput needs to be closed explicitly because it wraps CipherOutputStream
// public for testing
/**
// Package private for testing
/**
/* As a measure of protected against DOS, we can pass requests requiring a key
/**
/**
/**
//tools.ietf.org/html/rfc6750#section-3.1"></a>
/**
//tools.ietf.org/html/rfc6750#section-3.1"></a>
/**
/**
/**
// this could happen if another realm supports the Bearer token so we should
// see if another realm can use this token!
/**
// collision -- generate a new key
/**
/**
// nothing to do
/**
/**
// maintain the cache we already have
// this won't leak any secrets it's only exposing the current set of hashes
// to prevent too many cluster state update tasks to be queued for doing the same update
/**
/**
/**
// pkg-private for testing
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
// method is used for testing to verify cache expiration since expireAll is final
/*
/**
/**
/**
// We don't invoke the onFailure listener here, instead just pass an empty list
// as the index doesn't exist. Could have been deleted between checks and execution
// optimization for single user lookup
/**
// We don't invoke the onFailure listener here, instead
// we call the response with a null user
/**
/**
/**
/**
// We must have an existing document
// if the index doesn't exist we can never update a user
// if the document doesn't exist, then this update is not valid
/**
// if the index doesn't exist we can never update a user
// if the document doesn't exist, then this update is not valid
/**
// fallback mechanism as a user from 2.x may not have the enabled field
/**
/*
/**
// we want the finally block to clear out the chars before we proceed further so we handle the result here
// this was a reserved username - don't allow this to go to another realm...
/*
/**
// Don't use this for user comparison
/*
/**
/**
/**
// If using SSL, need a custom service because it's likely a self-signed certificate
// Requires permission java.lang.RuntimePermission "setFactory";
// Add basic-auth header
// set true if we are sending a body
// this throws IOException if there is a network problem
// this IOException is if the HTTP response code is 'BAD' (>= 400)
// we cannot do custom name resolution here...
// this sucks but a port can be specified with a value of 0, we'll never be able to connect to it so just default to
// what we know
//" + InetAddresses.toUriString(publishAddress) + ":" + port;
/*
/**
/*
/**
// Visible for testing
/**
// Generate 20 character passwords
/**
// loop for two consecutive good passwords
/**
// TODO: We currently do not support keystore passwords
/**
// keystore password is not valid
// Get x-pack security info.
// Cluster is yellow/green -> all OK
/**
// supplier should own his resources
/**
/*
// change elastic superuser
/**
/*
// pkg private for testing
/*
/**
/**
// comment
// only trim the line because we have a format, our tool generates the formatted text and we shouldn't be lenient
// and allow spaces in the format
/*
/**
/**
//comment
/**
/*
// make modifiable
// make modifiable
// check if just need to return data as no write operation happens
// Nothing to add, just list the data for a username
// pkg private for tests
// at least one role is marked... so printing the legend
// list users without roles
// at least one role is marked... so printing the legend
// pkg private for testing
// pkg private for testing
// pkg private for testing
/*
/**
// authorization scheme check is case-insensitive
/**
/**
/**
/*
/**
// pkg scoped for testing
/**
// if outToken is present then it needs to be communicated with peer, add it to
// response header in thread context.
/*
/*
/**
/**
/**
/**
// process token with gss context
/**
/**
/**
/**
/**
/**
// as acceptor, we can have multiple SPNs, we do not want to use any particular principal so it uses "*"
/*
// we have to return null since the tokenGroups attribute is computed and can only be retrieved using a BASE level search
/*
/**
//" + config.getSetting(ActiveDirectorySessionFactorySettings.AD_DOMAIN_NAME_SETTING) +
// Strictly, we only support unauthenticated sessions if there is a bind_dn or a connection pool, but the
// getUnauthenticatedSession... methods handle the situations correctly, so it's OK to always return true here.
/**
// Exposed for testing
// we did not find the user, cannot authenticate in this realm
// we did not find the user, cannot authenticate in this realm
// pkg-private for testing
/**
/**
// the global catalog does not replicate the necessary information to map a
// netbios dns name to a DN so we need to instead connect to the normal ports.
// This code uses the standard ports to avoid adding even more settings and is
// probably ok as most AD users do not use non-standard ports
/**
/*
//www.apache.org/licenses/LICENSE-2.0
/*
//svn.apache.org/repos/asf/directory/studio/tags/2.0.0.v20170904-M13/plugins/valueeditors/src/main/java/org/apache/directory/studio/valueeditors/msad/InPlaceMsAdObjectSidValueEditor.java
/*
// start with 'S'
// revision
// get count
// check length
// get authority, big-endian
// sub-authorities, little-endian
/*
/**
// pkg private for testing
/**
// we submit to the threadpool because authentication using LDAP will execute blocking I/O for a bind request and we don't want
// network threads stuck waiting for a socket to connect. After the bind, then all interaction with LDAP should be async
// we submit to the threadpool because authentication using LDAP will execute blocking I/O for a bind request and we don't want
// network threads stuck waiting for a socket to connect. After the bind, then all interaction with LDAP should be async
/**
/**
/**
/**
/*
/**
/**
// record failure
// loop break
// loop body
/**
//this value must be escaped to avoid manipulation of the template DN.
/*
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
// Checks the status of the LDAP connection at a specified interval in the background. We do not check on
// create as the LDAP server may require authentication to get an entry and a bind request has not been executed
// yet so we could end up never getting a connection. We do not check on checkout as we always set retry operations
// and the pool will handle a bad connection without the added latency on every operation
/**
/**
/*
/**
/*
/**
/*
/**
/*
/*
/**
/**
/**
// Only if it is an LDAPConnection do we need to close it, otherwise it is a connection pool and we will close all of the
// connections in the pool
/**
/**
/**
/**
/**
/**
/**
/*
// We can't use UnboundID RDN here because it expects attribute=value, not just value
/**
// only fork if binding on the LDAPConnectionReader thread
// avoids repeated forking
/**
/**
/**
/**
/**
/**
/**
/**
/**
// Whenever we get a search result we need to check for a referral.
// A referral is a mechanism for an LDAP server to reference an object stored in a
// different LDAP server/partition. There are cases where we need to follow a referral
// in order to get the actual object we are searching for
// either no referrals to follow or we have explicitly disabled referral following
// on the connection so we just create a new search result that has the values we've
// collected. The search result passed to this method will not have of the entries
// as we are using a result listener and the results are not being collected by the
// LDAP library
// we've gone through too many levels of referrals so we terminate with the values
// collected so far and the proper result code to indicate the search was
// terminated early
// there are referrals to follow, so we start the process to follow the referrals
// synchronize here since we are possibly sending out a lot of requests
// and the result lists are not thread safe and this also provides us
// with a consistent view
// count down and once all referrals have been traversed then we can
// create the results
// for each referral follow it and any other referrals returned until we
// get to a depth that is greater than or equal to the referral hop limit
// or all referrals have been followed. Each time referrals are followed
// from a search result, the depth increases by 1
// Needed in order for the countDown to be correct
/**
// the host must be present in order to follow a referral
// nothing to really do since a null host cannot really be handled, so we treat it as
// an error
// the referral URL often contains information necessary about the LDAP request such as
// the base DN, scope, and filter. If it does not, then we reuse the values from the
// originating search request
// in order to follow the referral we need to open a new connection and we do so using the
// referral connector on the ldap connection
/*
/**
/**
/**
/**
// Parse LDAP urls
// package private to use for testing
/**
//No mixing is allowed because we use the same socketfactory
//.. and ldap://..): ["
/*
/**
// For testing
/**
// Don't wrap in a new ElasticsearchSecurityException
/**
// Add the Id Token string as a synthetic claim
// We only try to update the cached JWK set once if a remote source is used and
// RSA or ECDSA is used for signatures
//")) {
/**
//openid.net/specs/openid-connect-core-1_0.html#ImplicitTokenValidation">specification</a>.
// only "Bearer" is defined in the specification but check just in case
// This should NOT happen and indicates a misconfigured OP. Warn the user but do not fail
/**
// avoid using JWKSet.loadFile() as it does not close FileInputStream internally
/**
/**
/**
/**
//TODO Handle validating possibly signed responses
/**
/**
// All parameters of AuthorizationCodeGrant are singleton lists
/**
/**
/*
//")) {
//")) {
/**
// pkg protected for testing
/**
/*
/**
/*
// For testing
// Add the ID Token as metadata on the authentication, so that it can be used for logout requests
/*
// This should never happen as it's already validated in the settings
// This should never happen as it's already validated in the settings
// This should never happen as it's already validated in the settings
// This should never happen as it's already validated in the settings
// This should never happen as it's already validated in the settings
// This should never happen as it's already validated in the settings
// This should never happen as it's already validated in the settings
/**
/*
/**
/**
/*
/**
/*
// For client based cert validation, the auth type must be specified but UNKNOWN is an acceptable value
// the lock is used in an odd manner; when iterating over the cache we cannot have modifiers other than deletes using
// the iterator but when not iterating we can modify the cache without external locking. When making normal modifications to the cache
// the read lock is obtained so that we can allow concurrent modifications; however when we need to iterate over the keys or values of
// the cache the write lock must obtained to prevent any modifications
// pkg private for testing
// the following block of code maintains BWC:
// When constructing the token object we only return it if the Subject DN of the certificate can be parsed by at least one PKI
// realm. We then consider the parsed Subject DN as the "principal" even though it is potentially incorrect because when several
// realms are installed the one that first parses the principal might not be the one that finally authenticates (does trusted chain
// validation). In this case the principal should be set by the realm that completes the authentication. But in the common case,
// where a single PKI realm is configured, there is no risk of eagerly parsing the principal before authentication and it also
// maintains BWC.
// end BWC code block
// parse the principal again after validating the cert chain, and do not rely on the token.principal one, because that could
// be set by a different realm that failed trusted chain validation. We SHOULD NOT parse the principal BEFORE this step, but
// we do it for BWC purposes. Changing this is a breaking change.
// No extra trust managers specified
// If the token is NOT delegated then it is authenticated, because the certificate chain has been validated by the TLS channel.
// Otherwise, if the token is delegated, then it cannot be authenticated without a trustManager
// do not break since there is no guarantee username is unique in this realm
/*
// noop
/*
/**
/**
/**
/*
/**
/**
/*
/**
/**
// Do not further process unsigned Assertions
// "past now" that is now - the maximum skew we will tolerate. Essentially "if our clock is 2min fast, what time is it now?"
// Allow for IdP initiated SSO where InResponseTo MUST be missing
// If the difference is less than half the length of the string, show it in detail
// In order to compensate for clock skew we construct 2 alternate realities
//  - a "future now" that is now + the maximum skew we will tolerate. Essentially "if our clock is 2min slow, what time is it now?"
//  - a "past now" that is now - the maximum skew we will tolerate. Essentially "if our clock is 2min fast, what time is it now?"
/*
/**
// We handle only EXACT comparison
/*
/**
/**
/*
/**
/*
/**
/*
/**
// 20 bytes (160 bits) of randomness as recommended by the SAML spec
/*
/**
// OpenSAML prints a lot of _stuff_ at info level, that really isn't needed in a command line tool.
// package-protected for testing
// package-protected for testing
/**
// We sort this Set so that it is deterministic for testing
/**
// TODO: We currently do not support keystore passwords
// For testing
/*
/**
/*
/**
// Although we only use this for IDP metadata loading, the SSLServer only loads configurations where "ssl." is a top-level element
// in the realm group configuration, so it has to have this name.
/**
// the metadata resolver needs to be destroyed since it runs a timer task in the background and destroying stops it!
// For testing
// Package-private for testing
/**
// Add the SAML token details as metadata on the authentication
// saml will not support user lookup initially
//")) {
//")) {
// ssl setup
// for some reason the resolver supports its own trust engine and custom socket factories.
// we do not use these as we'd rather rely on the JDK versions for TLS security!
// We don't want to rely on the internal OpenSAML refresh timer, but we can't turn it off, so just set it to run once a day.
// @TODO : Submit a patch to OpenSAML to optionally disable the timer
/**
/**
/*
/*
/**
/**
/**
// Package private for testing
// This will parse and validate the input
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// The draft Interoperable SAML 2 profile requires redirect binding.
// That's annoying, because they require POST binding for the ACS so now SPs need to
// support 2 bindings that have different signature passing rules, etc. *sigh*
/*
/**
/**
/*
/**
// We want to force these classes to be loaded _before_ we fiddle with the context classloader
/**
/**
/**
// NCNames (https://www.w3.org/TR/xmlschema-2/#NCName) can't start with a number, so start them all with "_" to be safe
// Seriously, who thought this was a good idea for an API ???
/**
// Ensure that Schema Validation is enabled for the factory
// Disallow internal and external entity expansion
//apache.org/xml/features/disallow-doctype-decl", true);
//xml.org/sax/features/external-general-entities", false);
//xml.org/sax/features/external-parameter-entities", false);
//apache.org/xml/features/nonvalidating/load-external-dtd", false);
//xml.org/sax/features/validation", true);
//apache.org/xml/features/nonvalidating/load-dtd-grammar", false);
// This is required, otherwise schema validation causes signature invalidation
//apache.org/xml/features/validation/schema/normalized-value", false);
// Make sure that URL schema namespaces are not resolved/downloaded from URLs we do not control
//apache.org/xml/features/honour-all-schemaLocations", true);
// Ensure we do not resolve XIncludes. Defaults to false, but set it explicitly to be future-proof
// Ensure we do not expand entity reference nodes
// Further limit danger from denial of service attacks
//apache.org/xml/features/validation/schema", true);
//apache.org/xml/features/validation/schema-full-checking", true);
//java.sun.com/xml/jaxp/properties/schemaLanguage",
// We ship our own xsd files for schema validation since we do not trust anyone else.
//java.sun.com/xml/jaxp/properties/schemaSource", resolveSchemaFilePaths(schemaFiles));
/**
/*
/**
/*
/**
/**
/*
/**
// each realm should handle exceptions, if we get one here it should be considered fatal
/**
// there is a cached or an inflight authenticate request
// cached credential hash matches the credential hash for this forestalled request
// its credential hash does not match the
// hash of the credential for this forestalled request.
// clear cache and try to reach the authentication source again because password
// might have changed there and the local cached hash got stale
// not authenticated but instead of hammering reuse the result. a new
// request will trigger a retried auth
// attempt authentication against the authentication source
// a new request should trigger a new authentication
// notify any forestalled request listeners; they will not reach to the
// authentication request and instead will use this result if they contain
// the same credentials
// notify any staved off listeners; they will propagate this error
// notify the listener of the inflight authentication request
/**
// each realm should handle exceptions, if we get one here it should be
// considered fatal
// attempt lookup against the user directory
// user not found, invalidate cache so that subsequent requests are forwarded to
// the user directory
// notify forestalled request listeners
// the next request should be forwarded, not halted by a failed lookup attempt
// notify forestalled listeners
/*
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/*
/**
/*
/**
/*
/*
/**
/*
/**
// nothing
// nothing
/**
/**
/**
/**
/**
/**
/*
/**
// prior to doing any authorization lets set the originating action in the context only
// We would like to assert that there is an existing request-id, but if this is a system action, then that might not be
// true because the request-id is generated during authentication
// sometimes a request might be wrapped within another, which is the case for proxied
// requests and concrete shard requests
// this never goes async so no need to wrap the listener
//if we are creating an index we need to authorize potential aliases created at the same time
// if this is performing multiple actions on the index, then check each of those actions.
// pkg-private for testing
// pkg-private for testing
// this user did not really exist
// TODO(jaymode) find a better way to indicate lookup failed for a user and we need to fail authz
/**
// Maps original-index -> expanded-index-name (expands date-math, but not aliases)
// Maps action -> resolved indices set
// Special case for anonymous user
// check for run as
// check for authentication by API key
/*
/**
// the action must be internal OR the thread context must be a system context.
// there is no authentication object AND we are executing in a system context OR an internal action
// AND there
// we have a internal action being executed by a user other than the system user, lets verify that there is a
// originating action that is not a internal action. We verify that there must be a originating action as an
// internal action should never be called by user code from a client
// either there was no originating action or the originating action was an internal action,
// we should not replace under these circumstances
/**
/**
// TODO use a more limited user for tasks
/*
//`*,-*` what we replace indices and aliases with if we need Elasticsearch to return empty responses without throwing exception
/**
// if for some reason we are missing an action... just for safety we'll reject
/*
// Arrays.toString() can handle null values - all good
// check for all and return list of authorized indices
// if we cannot replace wildcards the indices list stays empty. Same if there are no authorized indices.
// we honour allow_no_indices like es core does.
//out of all the explicit names (expanded from wildcards and original ones that were left untouched)
//remove all the ones that the current user is not authorized for and ignore them
//this is how we tell es core to return an empty response, we can let the request through being sure
//that the '-*' wildcard expression will be resolved to no indices. We can't let empty indices through
//as that would be resolved to _all by es core.
//NOTE: shard level requests do support wildcards (as they hold the original indices options) but don't support
// replacing their indices.
//That is fine though because they never contain wildcards, as they get replaced as part of the authorization of their
//corresponding parent request on the coordinating node. Hence wildcards don't need to get replaced nor exploded for
// shard level requests.
//special treatment for AliasesRequest since we need to replace wildcards among the specified aliases too.
//AliasesRequest extends IndicesRequest.Replaceable, hence its indices have already been properly replaced.
//if we replaced the indices with '-*' we shouldn't be adding the aliases to the list otherwise the request will
//not get authorized. Leave only '-*' and ignore the rest, result will anyway be empty.
/*
/**
// validate that the concrete index exists, otherwise there is no remapping that we could do
// user is authorized to put mappings for this index
// the user is not authorized to put mappings for this index, but could have been
// authorized for a write using an alias that triggered a dynamic mapping update
// IndicesAliasesRequest doesn't support empty aliases (validation fails) but
// GetAliasesRequest does (in which case empty means _all)
//TODO Investigate reusing code from vanilla es to resolve index names and wildcards
//the order matters when it comes to exclusions
// we always need to check for date math expressions
// continue
//es core honours allow_no_indices for each wildcard expression, we do the same here by throwing index not found.
// we can use == here to compare strings since the name expression resolver returns the same instance, but add an assert
// to ensure we catch this if it changes
//MetaData#convertFromWildcards checks if the index exists here and throws IndexNotFoundException if not (based on
// ignore_unavailable). We only add/remove the index: if the index is missing or the current user is not authorized
// to access it either an AuthorizationException will be thrown later in AuthorizationService, or the index will be
// removed from the list, based on the ignore_unavailable option.
//it's an alias, ignore expandWildcardsOpen and expandWildcardsClosed.
//complicated to support those options with aliases pointing to multiple indices...
//TODO investigate supporting expandWildcards option for aliases too, like es core does.
/*
// pkg private for testing
// if authenticated by API key then the request must also contain same API key id
// we've already validated that the request is a proxy request so we can skip that but we still
// need to validate that the action is allowed and then move on
// scroll is special
// some APIs are indices requests that are not actually associated with indices. For example,
// search scroll request, is categorized under the indices context, but doesn't hold indices names
// (in this case, the security check on the indices was done on the search request that initialized
// the scroll. Given that scroll is implemented using a context on the node holding the shard, we
// piggyback on it and enhance the context with the original authentication. This serves as our method
// to validate the scroll id only stays with the same user!
// note that clear scroll shard level actions can originate from a clear scroll all, which doesn't require any
// indices permission as it's categorized under cluster. This is why the scroll check is performed
// even before checking if the user has any indices permission.
// if the action is a search scroll action, we first authorize that the user can execute the action for some
// index and if they cannot, we can fail the request early before we allow the execution of the action and in
// turn the shard actions
// we store the request as a transient in the ThreadContext in case of a authorization failure at the shard
// level. If authorization fails we will audit a access_denied message and will use the request to retrieve
// information such as the index and the incoming address of the request
// remote indices are allowed
//all wildcard expressions have been resolved and only the security plugin could have set '-*' here.
//'-*' matches no indices so we allow the request to go through, which will yield an empty response
// check action name
//all wildcard expressions have been resolved and only the security plugin could have set '-*' here.
//'-*' matches no indices so we allow the request to go through, which will yield an empty response
// We use sorted sets for Strings because they will typically be small, and having a predictable order allows for simpler testing
// But we don't have a meaningful ordering for objects like ConfigurableClusterPrivilege, so the tests work with "random" ordering
// TODO: can this be done smarter? I think there are usually more indices/aliases in the cluster then indices defined a roles?
// we need to verify that this user was authenticated by or looked up by a realm type that support password changes
// otherwise we open ourselves up to issues where a user in a different realm could be created with the same username
// and do malicious things
// Ensure that the user is not authenticated with an access token or an API key.
// Also ensure that the user was authenticated by a realm that we can change a password for. The native realm is an internal realm
// and right now only one can exist in the realm configuration - if this changes we should update this check
/*
/**
/**
/**
/**
// this is really a best effort attempt since we cannot guarantee principal uniqueness
// and realm names can change between nodes.
/*
/** 
/**
// NOTE: we expect a rewritten query, so we only need logic for "atomic" queries here:
// extract from all clauses
// extract from all clauses
// we just do SpanTerm, other spans are trickier, they could contain 
// the evil FieldMaskingSpanQuery: so SpanQuery.getField cannot be trusted.
// all terms must have the same field
// all terms must have the same field
// all terms must have the same field
// Both queries are supposed to be equivalent, so if any of them can be extracted, we are good
// TermInSetQuery#field is inaccessible
// there should only be one field
// no field
// no field
// we don't know how to get the fields from it
/*
/**
/**
/*
/**
// support caching for common queries, by inspecting the field
// TODO: If in the future there is a Query#extractFields() then we can do a better job
// we don't know how to safely extract the fields of this query, don't cache.
// we successfully extracted the set of fields: check each one
// don't cache any internal fields (e.g. _field_names), these are complicated.
// we can cache, all fields are ok
/*
/**
/*
/**
/*
// do not audit success again
/*
/**
/**
/*
/*
/**
/*
/**
/*
/**
// we need to special case the internal users in this method, if we apply the anonymous roles to every user including these system
// user accounts then we run into the chance of a deadlock because then we need to get a role that we may be trying to get as the
// internal user. The SystemUser is special cased as it has special privileges to execute internal actions and should never be
// passed into this method. The XPackUser has the Superuser role and we can simply return that
/* this is kinda spooky. We use a read/write lock to ensure we don't modify the cache if we hold
// try to resolve descriptors with role provider
// remove resolved descriptors from the set of roles still needed to be resolved
// Keyed by application + resource
// pkg - private for testing
/**
// if a index privilege is an explicit denial, then we treat it as non-existent since we skipped these in the past when
// merging
/*
/**
// package-private for testing
// this String Set keeps "<date>-<role>" pairs so that we only log a role once a day.
// spawn another worker on the generic thread pool
// just being paranoid :)
// executing the check asynchronously will not conserve the generated deprecation response headers (which is
// what we want, because it's not the request that uses deprecated features, but rather the role definition.
// Furthermore, due to caching, we can't reliably associate response headers to every request).
// sort answer by alias for tests
// collate privileges by index and by alias separately
// compute privileges Automaton for each alias and for each of the indices it points to
// check if the alias grants superiors privileges than the indices it points to
// null iff the index does not have *any* privilege
// compute automaton once per index no matter how many times it is pointed to
// log inferior indices for this role, for this alias
// package-private for testing
/*
// package private for testing
// EMPTY is safe here because we never use namedObject as we are just parsing role names
// we still put the role in the map to avoid unnecessary negative lookups
// we pass true as last parameter because we do not want to reject files if field permissions
// are given in 2.x syntax
// first check if FLS/DLS is enabled on the role...
/*
/**
// if the index or the shard is not there / available we just claim the privilege is not there
// This currently clears _all_ roles, but could be improved to clear only those roles that reference the affected application
// EMPTY is safe here because we never use namedObject
/*
/**
/**
// TODO remove this short circuiting and fix tests that fail without this!
// pkg-private for testing
// for backwardscompat with 2.x
// TODO remove this short circuiting and fix tests that fail without this!
// we pass true as last parameter because we do not want to reject permissions if the field permissions
// are given in 2.x syntax
/**
/*
// write the key
// set permissions to 600
/*
/**
// not using the original exception as its message is confusing
// (e.g. 'No enum constant SetSecurityUserProcessor.Property.INVALID')
/*
/**
/**
/*
// CORS - allow for preflight unauthenticated OPTIONS request
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
/*
/**
/**
/**
/**
/**
/*
/**
/*
/**
/**
/*
/**
// return HTTP status 404 if no API key found for API key id
/*
/**
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
// this doesn't use the RestBuilderListener since we need to override the
// handling of failures in some cases.
// it may contain base64 encoded token that needs to be sent to client if Spnego GSS context negotiation failed
// defined by https://tools.ietf.org/html/rfc6749#section-5.2
// defined by https://tools.ietf.org/html/rfc6749#section-5.2
/**
/**
/**
/**
/**
/**
// Custom error code
/**
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
// if the user asked for specific privileges, but none of them were found
// we'll return an empty result and 404 status code
// either the user asked for all privileges, or at least one of the privileges
// was found
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
// if the user asked for specific roles, but none of them were found
// we'll return an empty result and 404 status code
// either the user asked for all roles, or at least one of the roles
// the user asked for was found
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
// if the request specified mapping names, but nothing was found then return a 404 result
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
/*
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
// Package protected for testing
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
// if the user asked for specific users, but none of them were found
// we'll return an empty result and 404 status code
// either the user asked for all users, or at least one of the users
// was found
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
// TODO: remove deprecated endpoint in 8.0.0
/*
/**
/*
/**
// the paths to check
// captured attributes for each path
/** Create a checker for the given paths, which will warn to the given terminal if changes are made. */
// missing file, so changes later don't matter
// not posix
/** Check if attributes of the paths have changed, warning to the given terminal if they have. */
// we couldn't get attributes in setup, so we can't check them now
/*
/**
/*
/**
// get original permissions
// we are ignoring exceptions here, so we do not need handle whether or not tempFile was initialized nor if the file exists
// Make an attempt to set the username and group to match. If it fails, silently ignore the failure as the user
// will be notified by the FileAttributeChecker that the ownership has changed and needs to be corrected
/*
/**
// protected for testing
/**
/**
// wait until the gateway has recovered from disk, otherwise we think we don't have the
// .security index but they may not have been restored from the cluster state on disk
// Index does not exist
/**
/**
// use a local copy so all checks execute against the same state!
/**
// use a local copy so all checks execute against the same state!
// TODO we should improve this so we don't fire off a bunch of requests to do the same thing (create or update mappings)
// the index already exists - it was probably just created so this
// node hasn't yet received the cluster state update with the index
/**
/**
// {
/**
/*
/**
/**
// old state might be null (undefined) so do Object comparison
/*
/*
/*
// the transport in core normally does this check, BUT since we are serializing to a string header we need to do it
// ourselves otherwise we wind up using a version newer than what we can actually send
// Sometimes a system action gets executed like a internal create index request or update mappings request
// which means that the user is copied over to system actions so we need to change the user
// re-write the authentication since we want the authentication version to match the version of the connection
/**
// If the license state is MISSING, then auth is not allowed.
// However this makes it difficult to installing a valid license, because that might implicitly turn on security.
// When security is enabled on the master node it will then reject any actions that do not have authentication headers
// but there may be in-flight internal actions (that will not have authentication headers) such as "cluster/shard/started"
// which we don't want to reject.
// So, we always send authentication headers for actions that have an implied user (system-user or explicit-origin)
// and then for other (user originated) actions we enforce that there is an authentication header that we can send, iff the
// current license allows authentication.
// we use an assertion here to ensure we catch this in our testing infrastructure, but leave the ISE for cases we do not catch
// in tests and may be hit by a user
// pkg-private method to allow overriding for tests
// apply the default filter to local requests. We never know what the request is or who sent it...
// only fork off if we get called on another thread this means we moved to
// an async execution and in this case we need to go back to the thread pool
// that was actually executing it. it's also possible that the
// thread-pool we are supposed to execute on is `SAME` in that case
// the handler is OK with executing on a network thread and we can just continue even if
// we are on another thread due to async operations
/*
/**
/**
/*
/*
// this happens when client authentication is optional and the client does not provide credentials. If client
// authentication was required then this connection should be closed before ever getting into this class
/*
/**
// exclude default profile -- it's handled differently
// FIXME we need to audit here
// this could happen if a user updates the settings dynamically with a new profile
// if we are always going to allow the bound addresses, then the rule for them should be the first rule in the list
// add all rules to the same list. Allow takes precedence so they must come first!
/*
/**
// this has been adopted from Netty3 there is no replacement in netty4 for this.
/**
/**
// not defined - ie. it's not a local address
/*
/**
/**
/**
// all rule was found. It should be the only rule!
// subnet rule...
// pattern rule - not netmask
// we want the inet addresses to be normalized especially in the IPv6 case where :0:0: is equivalent to ::
// that's why we convert the address here and then format since PatternRule also uses the formatting to normalize
// the value we are matching against
/*
// at this stage no auth has happened, so we do not have any principal anyway
/*
/*
/*
// Do not consume any reads if channel is disallowed
/*
// we create the socket based on the name given. don't reverse DNS
/*
/**
// we create the socket based on the name given. don't reverse DNS
/*
/**
// If there is currently data in the outbound write buffer, flush the buffer.
// If the data is not completely flushed, exit. We cannot produce new write data until the
// existing data has been fully flushed.
// If the driver is ready for application writes, we can attempt to proceed with any queued writes.
// Attempt to encrypt application write data. The encrypted data ends up in the
// outbound write buffer.
// Flush the write buffer to the channel
// It is possible that a read call produced non-application bytes to flush
// The model for closing channels will change at some point, removing the need for this "schedule
// a write" signal. But for now, we need to handle the edge case where the channel is not
// registered.
/*
/**
// This should only be accessed by the network thread associated with this channel, so nothing needs to
// be volatile.
/**
// There is not enough space in the network buffer for an entire SSL packet. Compact the
// current data and expand the buffer if necessary.
// There is not enough space in the application buffer for the decrypted message. Expand
// the application buffer to ensure that it has enough space.
// There is not enough space in the network buffer for an entire SSL packet. We will
// allocate a buffer with the correct packet size the next time through the loop.
// This check prevents us from attempting to send close_notify twice
// There are two potential modes for the driver to be in - REGULAR or CLOSE. REGULAR is the initial mode.
// During this mode the initial data that is read and written will be related to the TLS handshake
// process. Application related data cannot be encrypted until the handshake is complete. Once the
// handshake is complete data read from the channel will be decrypted and placed into the buffer passed
// as an argument to the read call. Additionally, application writes will be accepted and encrypted into
// the outbound write buffer. REGULAR mode will proceed until CLOSE mode begins. CLOSE mode can begin if
// we receive a CLOSE_NOTIFY message from the peer or if initiateClose is called. In CLOSE mode we attempt
// to both send and receive an SSL CLOSE_NOTIFY message. The exception to this is when we enter CLOSE mode
// during a handshake. In this scenario we only need to send the alert to the peer and then close the
// channel. Some SSL/TLS implementations do not properly adhere to the full two-direction close_notify
// process. Additionally, in newer TLS specifications it is not required to wait to receive close_notify.
// However, we will make our best attempt to both send and receive as it is expected by the java SSLEngine
// (it throws an exception if close_notify has not been received when inbound is closed).
// We UNWRAP as much as possible immediately after a read. Do not need to do it here.
// If we attempt to close during a handshake either we are sending an alert and inbound
// should already be closed or we are sending a close_notify. If we send a close_notify
// the peer might send an handshake error alert. If we attempt to receive the handshake alert,
// the engine will throw an IllegalStateException as it is not in a proper state to receive
// handshake message. Closing inbound immediately after close_notify is the cleanest option.
// There is an issue where receiving handshake messages after initiating the close process
// can place the SSLEngine back into handshaking mode. In order to handle this, if we
// initiate close during a handshake we do not wait to receive close. As we do not need to
// receive close, we will not handle reads.
// We do as much as possible to generate the outbound messages in the ctor. At this point, we are
// only interested in interrogating if we need to wait to receive the close message.
/*
// If there is an existing page, close it as it wasn't large enough to accommodate the SSLEngine.
/*
/**
/**
/*
// enable http
// update with a new field
// this part is important. Without this, the document may be read from the translog which would bypass the bug where
// FLS kicks in because the request can't be found and only returns meta fields
// do it in a bulk
//update with new field
// this part is important. Without this, the document may be read from the translog which would bypass the bug where
// FLS kicks in because the request can't be found and only returns meta fields
/*
// enable http
// we authenticate each user on each of the realms to make sure they're all cached
// all users should be cached now on all realms, lets verify
// now, lets run the scenario
// now, user_a should have been evicted, but user_b should still be cached
// selects a random sub-set of the give values
/*
/**
// create roles
// warm up the caches on every node
// enable http
/*
// enable http
// user_a can do all the things
// user_b can do monitoring
// but no admin stuff
// sorry user_c, you are not allowed anything
// user_d can view repos and create and view snapshots on existings repos, everything else is DENIED
// user_e can view repos and snapshots on existing repos, everything else is DENIED
// This snapshot needs to be finished in order to be restored
// user_d can create snapshots, but not concurrently
// The status of the snapshot in the repository can become SUCCESS before it is fully finalized in the cluster state so wait for
// it to disappear from the cluster state as well
/*
// enable http
/*
// multi get doesn't support expressions - this is probably a bug
/*
// Both users have the same role query, but user3 has access to field2 and not field1, which should result in zero hits:
// this is a bit weird the document level permission (all docs with field2:value2) don't match with the field level
// permissions (field1),
// this results in document 2 being returned but no fields are visible:
// user4 has all roles
/*
// can't add a second test method, because each test run creates a new instance of this class and that will will result
// in a new random value:
/*
// suppress test codecs otherwise test using completion suggester fails
// <-- query defined as json in a string
// query that can match nested documents
// Just to make logs less noisy
// test documents users can see
// test documents user cannot see
// index simple data
// Both user1 and user2 can't see field1 and field2, no parent/child query should yield results:
// user 3 can see them but not c3
// With document level security enabled the update is not allowed:
// With no document level security enabled the update is allowed:
// With document level security enabled the update in bulk is not allowed:
// A document that is always included by role query of both roles:
// Term suggester:
// Phrase suggester:
// Completion suggester:
// A document that is always included by role query of both roles:
//        ProfileResult profileResult = queryProfileShardResult.getQueryResults().get(0);
//        assertThat(profileResult.getLuceneDescription(), equalTo("(other_field:value)^0.8"));
/*
/*
// user1 has access to field1, so the query should match with the document:
// user2 has no access to field1, so the query should not match with the document:
// user3 has access to field1 and field2, so the query should match with the document:
// user4 has access to no fields, so the query should not match with the document:
// user5 has no field level security configured, so the query should match with the document:
// user7 has roles with field level security configured and without field level security
// user8 has roles with field level security configured for field1 and field2
// user1 has no access to field1, so the query should not match with the document:
// user2 has access to field1, so the query should match with the document:
// user3 has access to field1 and field2, so the query should match with the document:
// user4 has access to no fields, so the query should not match with the document:
// user5 has no field level security configured, so the query should match with the document:
// user7 has role with field level security and without field level security
// user8 has roles with field level security configured for field1 and field2
// user1 has access to field3, so the query should not match with the document:
// user2 has no access to field3, so the query should not match with the document:
// user3 has access to field1 and field2 but not field3, so the query should not match with the document:
// user4 has access to no fields, so the query should not match with the document:
// user5 has no field level security configured, so the query should match with the document:
// user7 has roles with field level security and without field level security
// user8 has roles with field level security configured for field1 and field2
// user1 has access to field1, so a query on its field alias should match with the document:
// user2 has no access to field1, so a query on its field alias should not match with the document:
// user1 is granted access to field1 only:
// user2 is granted access to field2 only:
// user3 is granted access to field1 and field2:
// user4 is granted access to no fields, so the get response does say the doc exist, but no fields are returned:
// user5 has no field level security configured, so all fields are returned:
// user6 has access to field*
// user7 has roles with field level security and without field level security
// user8 has roles with field level security with access to field1 and field2
// user1 is granted access to field1 only:
// user2 is granted access to field2 only:
// user3 is granted access to field1 and field2:
// user4 is granted access to no fields, so the get response does say the doc exist, but no fields are returned:
// user5 has no field level security configured, so all fields are returned:
// user6 has access to field*
// user7 has roles with field level security and without field level security
// user8 has roles with field level security with access to field1 and field2
// user1 is granted access to field1 only
// user2 is granted access to field2 only
// user3 is granted access to field1 and field2
// user4 is granted access to no fields, so the search response does say the doc exist, but no fields are returned
// user5 has no field level security configured, so all fields are returned
// user6 has access to field*
// user7 has roles with field level security and without field level security
// user8 has roles with field level security with access to field1 and field2
// user1 is granted access to field1 only:
// user2 is granted access to field2 only:
// user3 is granted access to field1 and field2:
// user4 is granted access to no fields:
// user5 has no field level security configured:
// user6 has field level security configured with access to field*:
// user7 has access to all fields due to a mix of roles without field level security and with:
// user8 has field level security configured with access to field1 and field2:
// user1 is granted access to field1 only, and so should be able to load it by alias:
// user2 is not granted access to field1, and so should not be able to load it by alias:
// user1 is granted access to field1 only:
// user2 is granted access to field2 only:
// user3 is granted access to field1 and field2:
// user4 is granted access to no fields:
// user5 has no field level security configured:
// user6 has field level security configured with access to field*:
// user7 has access to all fields
// user8 has field level security configured with access to field1 and field2:
// user1 is granted to use field1, so it is included in the sort_values
// user2 is not granted to use field1, so the default missing sort value is included
// user1 is not granted to use field2, so the default missing sort value is included
// user2 is granted to use field2, so it is included in the sort_values
// user1 is granted to use field1, so it is included in the sort_values when using its alias:
// user2 is not granted to use field1, so the default missing sort value is included when using its alias:
// user1 has access to field1, so the highlight should be visible:
// user2 has no access to field1, so the highlight should not be visible:
// user1 has access to field1, so the highlight on its alias should be visible:
// user2 has no access to field1, so the highlight on its alias should not be visible:
// user1 is authorized to use field1, so buckets are include for a term agg on field1
// user2 is not authorized to use field1, so no buckets are include for a term agg on field1
// user1 is not authorized to use field2, so no buckets are include for a term agg on field2
// user2 is authorized to use field2, so buckets are include for a term agg on field2
// user1 is authorized to use field1, so buckets are include for a term agg on its alias:
// user2 is not authorized to use field1, so no buckets are include for a term agg on its alias:
// index simple data
// Perform the same checks, but using an alias for field1.
// With field level security enabled the update is not allowed:
// With no field level security enabled the update is allowed:
// With field level security enabled the update in bulk is not allowed:
// user6 has access to all fields, so the query should match with the document:
// user1 has access to field1, so the query should match with the document:
// user1 has no access to field2, so the query should not match with the document:
// user2 has no access to field1, so the query should not match with the document:
// user2 has access to field2, so the query should match with the document:
// user3 has access to field1 and field2, so the query should match with the document:
// user3 has access to field1 and field2, so the query should match with the document:
// user4 has access to no fields, so the query should not match with the document:
// user4 has access to no fields, so the query should not match with the document:
// user1 has access to field1, so a query on its alias should match with the document:
// user2 has no access to field1, so the query should not match with the document:
/*
// enable http
// indices: a,b,c,abc
// u1 has all_a_role and read_a_role
// u2 has all_all and read a/b role
// u3 has read b role, but all access to a* and b* via regex
// u4 has read access to a/b and manage access to a*
// u5 may read a and read b
// u6 has all access on a and read access on b
// no access at all
// u8 has admin access and read access on b
// u9 has write access to a and read access to a/b
// u11 has access to create c and delete b
// u12 has data_access to all indices+ crud access to a
// u13 has read access on b and index access on a
// u14 has access to read a and monitor b
// FIXME, indices:admin/get authorization missing here for _settings call
// wait until index ready, but as admin
// indexing a document to have the mapping available, and wait for green state to make sure index is created
// admin refresh before executing
/*
/*
/*
// TODO: When we have an XPackIntegTestCase, this should test that we can send MonitoringBulkActions
/*
// adds a dummy user to the native realm to force .security index creation
// enable http
// no specifying an index, should replace indices with the permitted ones (test & test1)
// _all should expand to all the permitted indices
// wildcards should expand to all the permitted indices
// expected
// test _cat/indices with wildcards that cover unauthorized indices (".security" in this case)
// expected
/*
/**
// first lets try with "admin"... all should work
// now lets try with "user"
/*
// Repeat with unauthorized user!!!!
/*
//clear all scroll ids from the default admin user, just in case any of test fails
// deletion of scroll ids should work
// test with each id, that they do not exist
/*
/**
// relocate all shards to one node such that we can merge it.
// wait for green and then shrink
// verify all docs
/*
// enable http
// for http
// the default of the licensing tests is basic
// generate a new license with a mode that enables auth
// This method first makes sure licensing is enabled everywhere so that we can execute
// monitoring actions to ensure we have a stable cluster and only then do we disable.
// This is done in an assertBusy since there is a chance that the enabling of the license
// is overwritten by some other cluster activity and the node throws an exception while we
// wait for things to stabilize!
// apply the disabling of the license once the cluster is stable
// do this in an await busy since there is a chance that the enabling of the license is
// overwritten by some other cluster activity and the node throws an exception while we
// wait for things to stabilize!
// first update the license so we can execute monitoring actions
// re-apply the update in case any node received an updated cluster state that triggered the license state
// to change
/*
/**
// Clear the realm cache for all realms since we use a SUITE scoped cluster
// enable http
// don't remove the security index template
// we are randomly running a large number of nodes in these tests so we limit the number of worker threads
// since the default of 2 * CPU count might use up too much direct memory for thread-local direct buffers for each node's
// transport threads
/*
/**
/**
//UnicastZen requires the number of nodes in a cluster to generate the unicast configuration.
//The number of nodes is randomized though, but we can predict what the maximum number of nodes will be
//and configure them all in unicast.hosts
/**
//Rules are the only way to have something run before the before (final) method inherited from ESIntegTestCase
/**
// ignore since the thread was never started
//before methods from the superclass are run before this, which means that the current cluster is ready to go
// TODO: disable this assertion for now, due to random runs with mock plugins. perhaps run without mock plugins?
//            assertThat(nodeInfo.getPlugins().getInfos(), hasSize(2));
// Disable native ML autodetect_process as the c++ controller won't be available
//        builder.put(MachineLearningField.AUTODETECT_PROCESS.getKey(), false);
// handle secure settings separately
// security has its own transport service
/**
/**
/**
/**
/**
/**
/**
//one alias per index with prefix "alias-"
// If we get to this point and we haven't added an alias to the request we need to add one
// or the request will fail so use noAliasAdded to force adding the alias in this case
//one alias pointing to all indices
// we need to wrap node clients because we do not specify a user for nodes and all requests will use the system
// user. This is ok for internal n2n stuff but the test framework does other things like wiping indices, repositories, etc
// that the system user cannot do. so we wrap the node client with a user that can do these things since the client() calls
// return a node client
// this is a hack to clean up the .security index since only a superuser can delete it
/*
/**
/**
// Use PEM instead of JKS stores so that we can run these in a FIPS 140 JVM
//TODO: for now isolate security tests from watcher & monitoring (randomize this later)
/**
/**
/**
/*
/**
/**
//Rules are the only way to have something run before the before (final) method inherited from ESSingleNodeTestCase
/**
// ignore since the thread was never started
//before methods from the superclass are run before this, which means that the current cluster is ready to go
// TODO: disable this assertion for now, due to random runs with mock plugins. perhaps run without mock plugins?
// assertThat(nodeInfo.getPlugins().getInfos(), hasSize(2));
// handle secure settings separately
/**
/**
/**
/**
/**
/**
// we need to wrap node clients because we do not specify a user for nodes and all requests will use the system
// user. This is ok for internal n2n stuff but the test framework does other things like wiping indices, repositories, etc
// that the system user cannot do. so we wrap the node client with a user that can do these things since the client() calls
// are all using a node client
/**
/*
// making sure it's not a license expired exception
/*
// ldap realm filtering
//host.domain");
// active directory filtering
//host.domain");
// pki filtering
// client profile
// custom settings, potentially added by a plugin
// custom settings, potentially added by a plugin
/*
// this class sits in org.elasticsearch.transport so that TransportService.requestHandlers is visible
/*
/*\", \"admin:/read/*\" ] },"
/*\" ] },"
/*", "admin:/read/*"),
/*"),
/*\",\"admin:/read/*\"] },"
/*\", \"admin:/read/*\" ] },"
/*\" ] }"
/*\", \"admin:/read/*\" ] },"
/*
// XPackSettings.HTTP_SSL_ENABLED default false
/*
/*
// enable transport tls
// enable ssl for http
// enable client auth for http
// disable http ssl
// set transport auth
// test with transport profile
// enable transport tls
/*
/**
/* The line below simulates the evil cluster. A working cluster would return
/*
// an intentionally corrupt header
/*
// check SSL
// check Token service
// check API Key service
// auditing
// ip filter
// roles
// role-mapping
// anonymous
// FIPS 140
// check SSL : This is permitted even though security has been dynamically disabled by the trial license.
// everything else is missing because security is disabled
/*
// enable http
/*
// no exception thrown
// no exception thrown
// no exception thrown
// no-exception
// no exception thrown
/*
/**
// ensure the cluster listener gets triggered
/*
// XPackSettings.HTTP_SSL_ENABLED default false
/*
//make sure that wherever the _all is among the scroll ids the action name gets translated
/*
/*
/*
//rp.com/cb?code=thisisacode&state=" + state;
//rp.company.com/cb?code=abc");
/*
//op.company.org/");
//op.company.org/"));
//op.company.org/");
/*
// One index request to create the token
// One bulk request (containing one update request) to invalidate the token
/*
// test that we reject a role where field permissions are stored in 2.x format (fields:...)
/*
/*
/*
/*
/*
/*
/*
//sp.example.com/sso/saml2/post");
/*
// 1 to find the tokens for the realm
// 2 more to find the UserTokens from the 2 matching refresh tokens
// 4 updates (refresh-token + access-token)
// Invalidate refresh token 1
// Invalidate access token 1
// Invalidate refresh token 2
// Invalidate access token 2
/*
//sp.example.net/saml";
/*
// setup lifecycle service
/*
/*
// this is the fastest hasher we officially support
// this is the fastest hasher we officially support
/*
/*
/*
// Request will fail before the request hashing algorithm is checked, but we use the same algorithm as in settings for consistency
// Request will fail before the request hashing algorithm is checked, but we use the same algorithm as in settings for consistency
/*
/*
/*
/*
// updates should always return false for create
/*
/**
// mock the setEnabled call on the native users store so that it will invoke the action listener with a response
// we're mocking the setEnabled call on the native users store so that it will invoke the action listener with an exception
/*
/*
/*
/**
/*
// generate random filter policies
// generate random filter policies
// enable auditing
// add only startup filter policies
// reference audit trail containing all filters
// update settings on internal cluster
// filter by username
// filter by realms
// filter by roles
// filter by indices
/*
// filter by username
// filter by realms
// filter by roles
// filter by indices
// user field matches
// null user field does NOT match
// realm field matches
// null realm field does NOT match
// role field matches
// null role among roles field does NOT match
// indices field matches
// null index among indices field does NOT match
// create complete filter policy
// filter by username
// filter by realms
// filter by roles
// filter by indices
// all fields match
// one field does not match or is empty
// create complete filter policy
// filter by username
// filter by missing user name
// filter by realms
// filter by missing realm name
// filter by roles
// filter by missing role name
// filter by indices
// filter by missing index name
// all fields match
// one field does not match or is empty
// first policy: realms and roles filters
// second policy: users and indices filters
// filter by indices
// matches both the first and the second policies
// matches first policy but not the second
// matches the second policy but not the first
// matches neither the first nor the second policies
// a filter for a field consisting of an empty string ("") or an empty list([])
// will match events that lack that field
// possibly renders list empty
// anonymous accessDenied
// authenticationFailed
// accessGranted
// accessDenied
// tamperedRequest
// connection denied
// connection granted
// runAsGranted
// runAsDenied
// authentication Success
// a filter for a field consisting of an empty string ("") or an empty list([])
// will match events that lack that field
// possibly renders list empty
// anonymous accessDenied
// authenticationFailed
// accessGranted
// accessDenied
// tamperedRequest
// connection denied
// connection granted
// runAsGranted
// runAsDenied
// authentication Success
// a filter for a field consisting of an empty string ("") or an empty list([])
// will match events that lack that field
// possibly renders list empty
// filtered roles are a subset of the roles of any policy
// unfiltered role sets either have roles distinct from any other policy or are
// a mix of roles from 2 or more policies
// add roles distinct from any role in any filter policy
// add roles from other filter policies
// anonymous accessDenied
// authenticationFailed
// accessGranted
// accessDenied
// connection denied
// connection granted
// runAsGranted
// runAsDenied
// authentication Success
// a filter for a field consisting of an empty string ("") or an empty list([])
// will match events that lack that field
// possibly renders list empty
// filtered indices are a subset of the indices of any policy
// unfiltered index sets either have indices distinct from any other in any
// policy or are a mix of indices from 2 or more policies
// add indices distinct from any index in any filter policy
// add indices from other filter policies
// anonymous accessDenied
// authenticationFailed
// accessGranted
// accessDenied
// connection denied
// connection granted
// runAsGranted
// runAsDenied
// authentication Success
/** creates address without any lookups. hostname can be null, for missing */
/*
// This is a minimal and brittle parsing of the security log4j2 config
// properties. If any of these fails, then surely the config file changed. In
// this case adjust the assertions! The goal of this assertion chain is to
// validate that the layout pattern we are testing with is indeed the one
// attached to the LoggingAuditTrail.class logger.
// test disabled
// test disabled
// test disabled
// test disabled
// test disabled
// test disabled
// test enabled
// test enabled
// test disabled
// test enabled
// test disabled
// test disabled
// test disabled
// test disabled
// test disabled
// test disabled
// test enabled
// test disabled
// test disabled
// event by default disabled
// event by default disabled
// transport messages without indices
// only check msg existence
// check each field
// null checkField means that the field does not exist
// remove checked field
// null checkField means that the field does not exist
// remove checked field
// check no extra fields
/** creates address without any lookups. hostname can be null, for missing */
/**
/*
// need real http
// get the api key service and wait until api key expiration is not in progress!
//github.com/elastic/elasticsearch/issues/47958")
// create simple api key
// use the first ApiKey for authorized action
// Assert that we can authenticate with the API KEY
// use the first ApiKey for an unauthorized action
// has been invalidated but not yet deleted by ExpiredApiKeysRemover
// active api key
// invalidate API key to trigger remover
// Verify that 1st invalidated API key is deleted whereas the next one may be or may not be as it depends on whether update was
// indexed before ExpiredApiKeysRemover ran
// has been invalidated but not yet deleted by ExpiredApiKeysRemover
// Expire the 1st key such that it cannot be deleted by the remover
// hack doc to modify the expiration time to a day before
// Expire the 2nd key such that it can be deleted by the remover
// hack doc to modify the expiration time to the week before
// Invalidate to trigger the remover
// Verify get API keys does not return api keys deleted by ExpiredApiKeysRemover
// has been expired, not invalidated
// has not been expired as no expiration, is invalidated but not yet deleted by ExpiredApiKeysRemover
// has not been expired as no expiration, not invalidated
// trigger expired keys remover
// for any other API key id, it must deny access
// for any other API key id, it must deny access
/*
// missing space
// missing colon
/**
/*
/**
// TODO implement equals
// Authenticate against the normal chain. 1st Realm will be checked (and not pass) then 2nd realm will successfully authc
// TODO implement equals
// Authenticate against the smart chain.
// "SecondRealm" will be at the top of the list and will successfully authc.
// "FirstRealm" will not be used
// TODO implement equals
// used above one time
// used above one time
// used to create realm ref
// Now assume some change in the backend system so that 2nd realm no longer has the user, but the 1st realm does.
// This will authenticate against the smart chain.
// "SecondRealm" will be at the top of the list but will no longer authenticate the user.
// Then "FirstRealm" will be checked.
// 2 from above + 1 more
// 1 from above + 1 more
// existing to no longer present
// doesn't exist to exists
// green or yellow to red
// red to non red
// green to yellow or yellow to green
// TODO implement equals
// TODO implement equals
// used above one time
// used above one time
// used to create realm ref
// expected
// expected
// this call does not actually go async
// checking authentication from the context
// checking authentication from the user header
//expected
// we do not actually go async
// call service asynchronously but it doesn't actually go async
// we need to use a latch here because the key computation goes async on another thread!
/*
/*
// don't load SSL twice
//dir1.internal:9876", "ldap://dir2.internal:9876", "ldap://dir3.internal:9876")
/*
// set same order for all realms
// As order is same for all realms, it should fall back secondary comparison on name
// Verify that realms are iterated in order based on name
// this is the iterator when licensed
// this is the iterator when licensed
// Default realms are inserted when factories size is 1 and enabled is false
// check that disabled realms are not included in unlicensed realms
// test realms with duplicate values
// first check type_0
// disable ALL using license
// check native or internal realms enabled only
/*
// indicates whether the RUN_AS_USER that is being authenticated is also a superuser
// enable http
// use the http user and try to run as
//the run as user shouldn't have access to the nodes api
// but when running as a different user it should work
/*
/**
//127.0.0.1:389")
//127.0.0.1:389")
//localhost/acs")
//the.issuer.com:8090")
//the.issuer.com:8090/login")
//the.issuer.com:8090/token")
//localhost/cb")
/**
/**
/*
// crank up the deletion interval and set timeout for delete requests
// we start one more node so we need to make sure if we hit max randomization we can still start one
// need real http
// start a new node and see if it can decrypt the token
// hack doc to modify the creation time to the day before
// invalidate a invalid token... doesn't matter that it is bad... we just want this action to trigger the deletion
// Assert that we can authenticate with the access token
// Assert that we can authenticate with the refreshed access token
// We now have two documents, the original(now refreshed) token doc and the new one with the new access doc
// hack doc to modify the refresh time to 50 seconds ago so that we don't hit the lenient refresh case
// safe to use same rest client across threads since it round robins between nodes
// Assert that we only ever got one access_token/refresh_token pair
// invalidate
// Assert that we can authenticate with the access token
// Now attempt to authenticate with an invalid access token string
// Now attempt to authenticate with an invalid access token with valid structure (pre 7.2)
// Now attempt to authenticate with an invalid access token with valid structure (after 7.2)
// get the token service and wait until token expiration is not in progress!
/*
// setup lifecycle service
// License state (enabled by default)
// version 7.2 was an "inflection" point in the Token Service development (access_tokens as UUIDS, multiple concurrent refreshes,
// tokens docs on a separate index), let's test the TokenService works in a mixed cluster with nodes with versions prior to these
// developments
// verify a second separate token service with its own salt can also verify
// This test only makes sense in mixed clusters with pre v7.2.0 nodes where the Key is actually used
// This test only makes sense in mixed clusters with pre v7.2.0 nodes where the Key is actually used
// This test only makes sense in mixed clusters with pre v7.2.0 nodes where the Key is actually used
// This test only makes sense in mixed clusters with pre v7.1.0 nodes where the Key is actually used
// verify a second separate token service with its own passphrase cannot verify
// This test only makes sense in mixed clusters with pre v7.1.0 nodes where the Key is actually used
// Configure Minimum expiration
// Configure Maximum expiration
// Outside range should fail
// the clock is still frozen, so the cookie should be valid
// move the clock forward but don't go to expiry
// move to expiry, stripping nanoseconds, as we don't store them in the security-tokens index
// move one second past expiry
// mock another random token so that we don't find a token in TokenService#getUserTokenFromId
// mock another random token so that we don't find a token in TokenService#getUserTokenFromId
// mock another random token so that we don't find a token in TokenService#getUserTokenFromId
// previous versions serialized the access token encrypted and the cipher text was different each time (due to different IVs)
/*
/*
/**
// first create the index so it exists
// Index a document with the default test user
// Index a document with the default test user
// expected
// Index a document with the default test user
// expected
// joe can snapshot all indices, including '.security'
// the realm cache should clear itself but we don't wish to race it
// authn fails
// users and roles are missing
// restore
// the realm cache should clear itself but we don't wish to race it
// users and roles are retrievable
// joe can create indices
// create some roles
// check that putting a user without a password fails if the user doesn't exist
// create joe with a password and verify the user works
// modify joe without sending the password
// test that role change took effect if anonymous is disabled as anonymous grants monitoring permissions...
// update the user with password and admin role again
// validate that joe cannot auth with the old token
// test with new password and role
// get should work
// authenticate should work
// get role is allowed
/**
/*
// existing to no longer present
// doesn't exist to exists
// green or yellow to red
// red to non red
// green to yellow or yellow to green
/*
// Native users store is initiated with default hashing algorithm
/*
/**
/**
// validate the user works
// disable user
//enable
/*
/**
// Mocked users store is initiated with default hashing algorithm
// test empty password
// the realm assumes it owns the hashed password so it fills it with 0's
// test new password
// Mocked users store is initiated with default hashing algorithm
// Mocked users store is initiated with default hashing algorithm
// now try with the real password
/*
/*
/**
//localhost:" + webServer.getPort() + "/test"), "u1",
/*
// sometimes we fall back to the keystore seed as this is the default when a new node starts
//localhost:9200");
// elastic user is updated last
//localhost:9202" + randomFrom("", "/", "//", "/smth", "//smth/", "//x//x/"));
// fail in strength and match
// passes strength validation, fail by mismatch
// two good passwords
/*
/*
// writing in utf_16 should cause a parsing error as we try to read the file in utf_8
//Test users share the hashing algorithm name for convenience
//Test users share the hashing algorithm name for convenience
// now replacing the content of the users file with something that cannot be read
//L6ToM8G41aOKFIWh7w="));
// writing in utf_16 should cause a parsing error as we try to read the file in utf_8
// writing in utf_16 should cause a parsing error as we try to read the file in utf_8
/*
// writing in utf_16 should cause a parsing error as we try to read the file in utf_8
// now replacing the content of the users file with something that cannot be read
// writing in utf_16 should cause a parsing error as we try to read the file in utf_8
// writing in utf_16 should cause a parsing error as we try to read the file in utf_8
/*
/*
/*
// authenticate
// authenticate with cache
// if cache.ttl <= 0 then the cache is disabled
// authenticate
// authenticate when cache has been disabled
/*
/*
/**
/**
/**
/**
/**
/*
/*
/**
// Get the default schema and overlay with the AD changes
// Add the bind users here since AD is not LDAPv3 compliant
// Must have privileged access because underlying server will accept socket connections
/**
// Thor does not have a UPN of form CN=Thor@ad.test.elasticsearch.com
// verify one and only one session as further attempts should be returned from cache
// verify one and only one session as second attempt should be returned from cache
// verify one and only one session as further attempts should be returned from cache
// Refresh the role mappings
/**
//ad01.testing.elastic.co:20389/")
/*
/*
// the listener shouldn't have ever been called. If it was, then either something called
// onResponse or onFailure was called as part of the timeout
/*
/*
//verify one and only one session -> caching is working
//verify one and only one session -> caching is working
//we need to session again
//verify two and only two binds -> caching is disabled
// maybe disable caching
/**
/**
/*
// Support LDAPS, because it's used in some test
//this last one should work
//none of these should work
/**
//github.com/bcgit/bc-java/commit/5aed687e17a3cd63f34373cafe92699b90076fb6#diff-8e5d8089bc0d504d93194a1e484d3950R179",
/*
// Utility class
// fake realms so ssl will get loaded
/*
/*
// auth
//lookup
// auth
//lookup
//auth
//lookup
//auth
//lookup
// pick a random ldap server and stop it
// don't use this in production
// used here to catch bugs that might get masked by an automatic retry
/*
/**
/**
/**
/**
/*
/*
/*
//example.com:636" };
//primary.example.com:636", "LDAPS://secondary.example.com:10636" };
//primary.example.com:392", "LDAP://secondary.example.com:10392" };
//primary.example.com:636", "ldap://secondary.example.com:392" };
//primary.example.com:392", "ldaps://secondary.example.com:636" };
/*
// Must have privileged access because underlying server will accept socket connections
/*
/**
// create a list of ports
// get a subset to kill
// when running multiple test jvms, there is a chance that something else could
// start listening on this port so we try to avoid this by creating a local socket
// that will be bound to the port the ldap server was running on and connecting to
// a mock server socket.
// NOTE: this is not perfect as there is a small amount of time between the shutdown
// of the ldap server and the opening of the socket
// go one iteration through and attempt a bind
// allow binding even if the previous socket is in timed wait state.
// close immediately as we are not writing anything here.
// first test that there is no round robin stuff going on
// we need at least one good server. Hence the upper bound is number - 2 since we need at least
// one server to use!
// always kill the first one
// when running multiple test jvms, there is a chance that something else could
// start listening on this port so we try to avoid this by creating a local socket
// that will be bound to the port the ldap server was running on and connecting to
// a mock server socket.
// NOTE: this is not perfect as there is a small amount of time between the shutdown
// of the ldap server and the opening of the socket
// now we find the first that isn't stopped
/*
// Can't run in FIPS with verification_mode none, disable this check instead of duplicating the test case
//localhost:389")
/*
//rp.elastic.co/cb?code=" + code + "&state=" + state;
// Expired 55 seconds ago with an allowed clock skew of 60 seconds
// Expired 65 seconds ago with an allowed clock skew of 60 seconds
// Issued 80 seconds in the future with max allowed clock skew of 60
//another.op.org")
// Change the algorithm of the signed JWT to NONE
/**
//test-profiles.com/jane.doe")
//test-profiles.com/jane.doe")
// Claims with different types throw an error
//test-profiles.com/jane.doe")
// Userinfo Claims overwrite ID Token claims
//test-profiles.com/jane.doe2")
//test-profiles.com/jane.doe"));
// Merging Arrays
//test-profiles.com/jane.doe")
// Merging nested objects
//test-profiles.com/jane.doe")
//op.example.com"),
//op.example.org/jwks.json",
//op.example.org/login"),
//op.example.org/token"),
//op.example.org/logout"));
//rp.elastic.co/cb"),
//rp.elastic.co/successfull_logout"));
//rp.elastic.co/cb"),
//rp.elastic.co/successfull_logout"));
//rp.elastic.co/cb"),
//rp.elastic.co/successfull_logout"));
// Change the sub claim to "attacker"
//SecretKey hmacKey = KeyGenerator.getInstance("HmacSha" + hashSize).generateKey();
/*
//op.example.com/login")
//op.example.com/token")
//op.example.com")
//op.example.com/jwks.json")
//rp.my.com")
//op.example.com")
//op.example.com/jwks.json")
//op.example.com/token")
//rp.my.com")
//op.example.com")
//op.example.com/jwks.json")
//op.example.com/token")
//rp.my.com")
//op.example.com/login")
//op.example.com")
//op.example.com/jwks.json")
//rp.my.com")
//op.example.com/login")
//op.example.com")
//op.example.com/jwks.json")
//rp.my.com")
//op.example.com/login")
//op.example.com")
//op.example.com/jwks.json")
//rp.my.com")
//op.example.com/login")
//op.example.com")
//rp.my.com")
//op.example.com/login")
//op.example.com/token")
//op.example.com/jwks.json")
//rp.my.com")
//op.example.com/login")
//op.example.com/token")
//op.example.com")
//op.example.com/jwks.json")
//op.example.com/login")
//op.example.com/token")
//op.example.com")
//op.example.com/jwks.json")
//rp.my.com")
//op.example.com/login")
//op.example.com/token")
//op.example.com")
//op.example.com/jwks.json")
//rp.my.com/cb")
//op.example.com/login")
//op.example.com/token")
//op.example.com")
//op.example.com/jwks.json")
//rp.my.com/cb")
//op.example.com")
//op.example.com/jwks.json")
//op.example.com/token")
//op.example.com/login")
//rp.my.com")
/*
//op.company.org"));
//rp.elastic.co/cb")
//op.company.org")
//rp.elastic.co/cb")
//op.company.org")
//op.example.com/login")
//op.example.com/token")
//op.example.com")
//op.example.com/jwks.json")
//rp.my.com/cb")
//op.example.com/login?scope=scope1+scope2+openid&response_type=code" +
//op.example.com/login")
//op.example.com/token")
//op.example.com")
//op.example.com/jwks.json")
//rp.my.com/cb")
//op.example.com/login?scope=openid+scope1+scope2&response_type=code" +
//op.example.com/login")
//op.example.com/token")
//op.example.com")
//op.example.com/jwks.json")
//rp.my.com/cb")
//op.example.com/login?scope=openid&response_type=code" +
// Random strings, as we will not validate the token here
//op.example.org/logout?id_token_hint="));
//op.example.com/login")
//op.example.com/token")
//op.example.com")
//op.example.com/jwks.json")
//rp.my.com/cb")
//op.example.com/login?scope=openid&response_type=code" +
//op.example.com/login")
//op.example.com/token")
//op.example.com")
//op.example.com/jwks.json")
//rp.my.com/cb")
//op.example.com/login?login_hint=" + thehint +
//rp.elastic.co/cb")
//op.company.org")
/*
//op.example.org/login")
//op.example.org/token")
//op.example.org/logout")
//op.example.com")
//op.example.org/jwks.json")
//rp.elastic.co/cb")
//rp.elastic.co/succ_logout")
/*
// pki1 does not allow delegation
// pki2 allows delegation but has a non-matching username pattern
// pki3 allows delegation and the username pattern (default) matches
// enable http
// trust root is optional
// delegate
// authenticate
// trust root is optional
// delegate
// authenticate
// no roles because no role mappings
// invalidate
// failed authenticate
// trust root is optional
// trust root is optional
// put role mappings for delegated PKI
// delegate
// authenticate
// assert roles
// delete role mappings for delegated PKI
// incomplete cert chain
// swapped order
// bogus certificate
/*
/**
// enable http
// pki1 never authenticates because of the principal pattern
// TODO: consider setting this back to true now that the transport client is gone
//%s/", NetworkAddress.format(inetSocketAddress));
/*
// enable http
/*
// check that the authorizing realm is consulted even for cached principals
/*
//sp.saml.elastic.test/";
//idp.saml.elastic.test/";
// TODO: Refactor the signing to use org.opensaml.xmlsec.signature.support.Signer so that we can run the tests
// Initialise Apache XML security so that the signDoc methods work correctly.
/**
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#'>"
// For the signature to validate, it needs to be made against a fragment assertion, so we:
// - convert the assertion to an xml-string
// - parse it into a new doc
// - sign it there
// - then replace it in the original doc
// This is the most reliable way to get a valid signature
// Encrypting with different cert instead of sp cert will mean that the SP cannot decrypt
// Encrypting with different cert instead of sp cert will mean that the SP cannot decrypt
// Because an assertion can theoretically contains encrypted and unencrypted attributes
// we don't treat a decryption as a hard failure.
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
// check that the content is valid "now"
// and still valid if we advance partway through the session expiry time
// and still valid if we advance past the expiry time, but allow for clock skew
// but fails once we get past the clock skew allowance
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
// check that the content is valid "now"
// and still valid if we advance partway through the expiry time
// and still valid if we advance past the expiry time, but allow for clock skew
// but fails once we get past the clock skew allowance
/* An IdP initiated login has no "in response to"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
// check that the content is valid when signed by the correct key-pair
// check is rejected when signed by a different key-pair
// Ensure we won't read any of the ones we could have picked randomly before
// Restore the keypair to one from the keypair pool of all algorithms and keys
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
// check that the original signed content is valid
// but altered content is rejected
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
// check that the content is valid when signed by the each of the key-pairs
/**
/**
// Expect Assertion > AttributeStatement (x2) > Attribute
// Expect EncryptedAssertion
// Expect Assertion > AttributeStatement (x2) > EncryptedAttribute
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
// check that the content is valid "now"
// and still valid if we advance partway through the expiry time
// and still valid if we advance past the expiry time, but allow for clock skew
// but fails once we get past the clock skew allowance
//some.other.sp/SAML2";
//" + randomAlphaOfLengthBetween(4, 12) + "." + randomAlphaOfLengthBetween(6, 8) + "/";
/*
// First verify that the correct SAML Response can be consumed
/*
//www.w3.org/2000/09/xmldsig#", "Signature").item(0);
//www.w3.org/2000/09/xmldsig#", "Signature").item(0);
// First verify that the correct SAML Response can be consumed
/*
//www.w3.org/2000/09/xmldsig#", "Signature").item(0);
//www.w3.org/2000/09/xmldsig#", "Signature").item(0);
/*
// First verify that the correct SAML Response can be consumed
/*
//www.w3.org/2000/09/xmldsig#", "Signature").item(0);
// First verify that the correct SAML Response can be consumed
/*
//www.w3.org/2000/09/xmldsig#", "Signature").item(0);
// First verify that the correct SAML Response can be consumed
/*
//www.w3.org/2000/09/xmldsig#", "Signature").item(0);
// First verify that the correct SAML Response can be consumed
/*
//www.w3.org/2000/09/xmldsig#", "Signature").item(0);
//www.w3.org/2000/09/xmldsig#", "Signature").item(0);
// First verify that the correct SAML Response can be consumed
/*
//www.w3.org/2000/09/xmldsig#", "Signature").item(0);
// First verify that the correct SAML Response can be consumed
/*
//www.w3.org/2000/09/xmldsig#", "Signature").item(0);
//www.w3.org/2000/09/xmldsig#", "Signature").item(0);
///etc/passwd\" >]>" +
// There is no need to go up to N iterations
//Restore the authenticator with credentials for the rest of the test cases
//Restore the authenticator with credentials for the rest of the test cases
// Some Identity Providers (e.g. Shibboleth) include the AES <EncryptedKey> directly within the <EncryptedData> element
// And some (e.g. Okta) include a <RetrievalMethod> that links to an <EncryptedKey> that is a sibling of the <EncryptedData>
// Because the consumer changes the nodes (and removes them from the document) we need to clone the list
/**
//www.w3.org/2001/04/xmldsig-more#rsa-sha256",
//www.w3.org/2001/04/xmldsig-more#rsa-sha512");
//www.w3.org/2009/xmldsig11#dsa-sha256";
//www.w3.org/2001/04/xmldsig-more#ecdsa-sha256",
//www.w3.org/2001/04/xmldsig-more#ecdsa-sha512");
//We need to explicitly set the Id attribute, "ID" is just our convention
// We don't "have to" set the reference explicitly since we're using enveloped signatures, but it helps with
// creating the XSW test cases
// According to the schema, the signature needs to be placed after the <Issuer> if there is one in the document
// If there are more than one <Issuer> we are dealing with a <Response> so we sign the Response and add the
// Signature after the Response <Issuer>
// Wrap the assertion in an "EncryptedXXX" element and then replace it with the encrypted content
// The node, once encrypted needs to be "standalone", so it needs to have all the namespaces defined locally.
// There might be a more standard way to do this, but this works...
// Save the parent node now, because it will change when we encrypt
// Generate an AES key for the actual encryption
// Encrypt the AES key with the public key of the recipient
// Encryption context for actual content
// Include the key info for passing the AES key
//www.w3.org/2001/04/xmlenc#EncryptedKey");
// Include the content element itself
// - The 3rd argument indicates whether to only encrypt the content (true) or the element itself (false)
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
//www.w3.org/2001/XMLSchema-instance'"
//www.w3.org/2001/XMLSchema'"
//www.w3.org/2000/09/xmldsig#' >"
/*
//sp.example.com/";
//idp.example.net/";
//sp.example.com/saml/acs";
//idp.example.net/saml/sso/redirect";
//an.arbitrary/mfa-profile");
//an.arbitrary/mfa-profile"));
/*
//idp.test/";
//sp.test/saml/logout";
//attacker.bot.net/");
//attacker.bot.net/");
/**
//sp.test/", "https://sp.test/saml/asc", LOGOUT_URL,
/*
//sp.saml/";
//sp.saml/";
//sp.example.com/saml/acs", null, null, null, Collections.emptyList());
//idp.example.com/saml/logout/post");
//idp.example.com/saml/logout/redirect");
//idp.example.com/saml/logout/artifact");
//idp.example.com/saml/logout/redirect"));
/*
//okta.my.corp/")
//kibana.my.corp/")
//kibana.my.corp/saml/login")
//kibana.my.corp/saml/logout")
// What is the friendly name for "principal" attribute "urn:oid:0.9.2342.19200300.100.1.1" [default: principal]
//kibana.my.corp/"));
//kibana.my.corp/saml/login"));
//kibana.my.corp/saml/logout"));
// Verify that OpenSAML things the XML representation is the same as our input
//saml.a/")
//saml.a/")
//saml.b/")
//saml.b/")
//saml.a/")
//saml.a/acs")
//saml.b/")
//saml.b/acs")
//saml.b/"));
//saml.b/acs"));
//saml.example.com/")
//saml.example.com/")
// What is the friendly name for command line attribute "urn:oid:0.9.2342.19200300.100.1.3" [default: none]
// What is the standard (urn) name for attribute "groups" (required)
// What is the standard (urn) name for "name" attribute "displayName" (required)
// What is the friendly name for "principal" "urn:oid:0.9.2342.19200300.100.1.1" [default: principal]
//saml.example.com/"));
//saml.example.com/")
//saml.example.com/")
//saml.example.com/"));
//okta.my.corp/")
//kibana.my.corp/")
//kibana.my.corp/saml/login")
//kibana.my.corp/saml/logout")
// What is the friendly name for "principal" attribute "urn:oid:0.9.2342.19200300.100.1.1" [default: principal]
// Verify generated signature
// Make sure that Signing didn't mangle the XML at all and we can still read metadata
//kibana.my.corp/"));
//kibana.my.corp/saml/login"));
//okta.my.corp/")
//kibana.my.corp/")
//kibana.my.corp/saml/login")
//kibana.my.corp/saml/logout");
// Verify generated signature
//okta.my.corp/")
//kibana.my.corp/")
//kibana.my.corp/saml/login")
//kibana.my.corp/saml/logout");
//Use this keypair for signing the metadata also
//okta.my.corp/")
//kibana.my.corp/")
//kibana.my.corp/saml/login")
//kibana.my.corp/saml/logout");
// Verify generated signature
//Use same keypair for signing the metadata
//okta.my.corp/")
//kibana.my.corp/")
//kibana.my.corp/saml/login")
//kibana.my.corp/saml/logout");
// Verify generated signature
//Use same keypair for signing the metadata
//okta.my.corp/")
//kibana.my.corp/")
//kibana.my.corp/saml/login")
//kibana.my.corp/saml/logout");
// Verify generated signature
//okta.my.corp/")
//kibana.my.corp/")
//kibana.my.corp/saml/login")
//kibana.my.corp/saml/logout")
// What is the friendly name for "principal" attribute
// "urn:oid:0.9.2342.19200300.100.1.1" [default: principal]
//kibana.my.corp/"));
//kibana.my.corp/saml/login"));
//kibana.my.corp/saml/logout"));
// Verify that OpenSAML things the XML representation is the same as our input
// Keys are pre-generated with the same name, so add the serial no to the alias so that keystore entries won't be overwritten
/*
//sp.example.net/";
//idp.example.org/";
//localhost/sso/' />",
/*
/**
//demo_josso_1.josso.dev.docker:8081/IDBUS/JOSSO-TUTORIAL/IDP1/SAML2/MD";
//localhost:" + proxyServer.getPort());
// TODO : "saml_nameid" should be null too, but the logout code requires it for now.
//saml/", null, null, null, Collections.emptyList());
//saml/", null, null, null, Collections.emptyList());
//saml/", null, null, null, Collections.emptyList());
//saml/", null, null, null, Collections.emptyList());
//example.com");
// Should build signing credential and use the key from KS.
// Unknown alias
// Unknown alias, this must throw exception
// Since this is unsupported key type, this must throw exception
// Should throw exception as no RSA keys in the keystore
// Should throw exception when multiple signing keys found and alias not set
// Keys are pre-generated with the same name, so add the serial no to the alias so that keystore entries won't be overwritten
//logout.saml/");
//saml/", null, null, null, Collections.emptyList());
//logout.saml/"));
//idp.test/saml/login";
//url.to/metadata")
//idp.test:443/saml/login").size(), equalTo(0));
//idp.test:443/saml/login").size(), equalTo(0));
//idp.saml/");
/**
/*
//idp.test/";
//idp.test/saml/logout";
// We currently only support signing with SHA256withRSA, this test should be updated if we add support for more
//www.w3.org/2001/04/xmldsig-more#rsa-sha256"));
/*
/*
// 1st is for signing, followed by 2 for encryption
//my.sp.example.net/")
//my.sp.example.net/saml/acs/post")
//my.sp.example.net/\">" +
//my.sp.example.net/saml/acs/post\" index=\"1\" isDefault=\"true\"/>" +
//kibana.apps.hydra/")
//kibana.apps.hydra/saml/acs")
//kibana.apps.hydra/saml/logout")
//hail.hydra/")
//kibana.apps.hydra/\">"
//www.w3.org/2000/09/xmldsig#\">"
//kibana.apps.hydra/saml/logout\"/>"
//kibana.apps.hydra/saml/acs\" index=\"1\" isDefault=\"true\"/>"
//hail.hydra/</md:OrganizationURL>"
//kibana.apps.hydra/")
//kibana.apps.hydra/saml/acs")
//kibana.apps.hydra/saml/logout")
//hail.hydra/")
//kibana.apps.hydra/\">"
//www.w3.org/2000/09/xmldsig#\">"
//www.w3.org/2000/09/xmldsig#\">"
//www.w3.org/2000/09/xmldsig#\">"
//kibana.apps.hydra/saml/logout\"/>"
//kibana.apps.hydra/saml/acs\""
//hail.hydra/</md:OrganizationURL>"
//kibana.apps.hydra/");
//kibana.example.net/");
// Remove spaces between elements, and compress other spaces. These patterns don't use \s because
// that would match newlines.
/*
// See: https://github.com/elastic/x-pack-elasticsearch/issues/2815
/**
/**
/*
// See: http://www.datypic.com/sc/xsd/t-xsd_NCName.html
/*
/*
// lookup first
// now authenticate
// authenticate a different user first
//now lookup b
// authenticate
// authenticate
// authenticate
// After 100 ms (from the original start time), authenticate (read from cache). We don't care about the result
// After 200 ms (from the original start time), authenticate (read from cache). We don't care about the result
// After 300 ms (from the original start time), authenticate again. The cache entry should have expired (despite the previous reads)
// Due to slow VMs etc, the cache might have expired more than once during the test, but we can accept that.
// We have other tests that verify caching works - this test just checks that it expires even when there are repeated reads.
// do something slow
// we use a bunch of different latches here, the first `latch` is used to ensure all threads have been started
// before they start to execute. The `authWaitLatch` is there to ensure we have all threads waiting on the
// listener before we auth otherwise we may run into a race condition where we auth and one of the threads is
// not waiting on auth yet. Finally, the completedLatch is used to signal that each thread received a response!
// do something slow
// do something slow
/*
/*
// Randomly enter the DN in mixed case, lower case or upper case;
/*
//groups can be named by different attributes, depending on the directory,
//we don't care what it is named by
// writing in utf_16 should cause a parsing error as we try to read the file in utf_8
// now replacing the content of the users file with something that cannot be read
// writing in utf_16 should cause a parsing error as we try to read the file in utf_8
// writing in utf_16 should cause a parsing error as we try to read the file in utf_8
//verify
/*
/*
/*
/*
// writing in utf_16 should cause a parsing error as we try to read the file in utf_8
// A DN must have at least 1 '=' symbol
/*
/*
// expected
/*
// metadata.active == false
// dn != ou=sales,dc=example,dc=com
// dn == null
// expression without dn
// This is rejected when validating a request, but is valid when parsing the mapping
/*
// Does match DN
// Does not match - user is not in this group
// Does match - user is one of these groups
// Does not match - mapping is not enabled
// Randomly transform the dn into another valid form that is logically identical,
// but (potentially) textually different
// do nothing
// Upper case just the attribute name for each RDN
// existing to no longer present
// doesn't exist to exists
// green or yellow to red
// red to non red
// green to yellow or yellow to green
/*
//role that has analyze indices privileges only
// this test tries to execute different analyze api variants from a user that has analyze privileges only on a specific index
// namespace
//ok: user has permissions for analyze on test_*
//fails: user doesn't have permissions for analyze on index non_authorized
//fails: user doesn't have permissions for cluster level analyze
//this test tries to execute different analyze api variants from a user that has analyze privileges only at cluster level
//fails: user doesn't have permissions for analyze on index test_1
/*
// A failure would throw an exception
/**
/**
/**
//ignore_unavailable set to false, user is not authorized for this index nor does it exist
//ignore_unavailable and allow_no_indices both set to true, user is not authorized for this index nor does it exist
// We have to use a mock request for other Scroll actions as the actual requests are package private to SearchTransportService
// run as [run as me]
// run as [run as me]
// cannot execute monitor operations
// we should allow waiting for the health of the index or any index if the user has this permission
// multiple indices
//if the user has no permission for composite actions against any index, the request fails straight-away in the main action
//if the user has permission for some index, the request goes through without looking at the indices, they will be checked later
//reindex, msearch, search template, and multi search template delegate to search
// bulk request is allowed
// resolves to same as above
// resolves to same as above
// both deletes should fail
// bulk request is allowed
/*
/**
// put origin in context
// set authentication
/*
/*
//role that has create index only privileges
//role that has create index and manage_aliases on test_*, not enough to manage_aliases aliases outside of test_* namespace
//role that has create index on test_* and manage_aliases on alias_*, can't create aliases pointing to test_* though
//role that has create index on test_* and manage_aliases on both alias_* and test_*
//role that has manage_aliases only on both test_* and alias_*
//randomly create an index with two aliases from user admin, to make sure it doesn't affect any of the test results
//user has create permission only: allows to create indices, manage_aliases is required to add/remove aliases
//user has create permission only: allows to create indices, manage_aliases is required to add aliases although they are part of
// the same create index request
//user has create permission only: allows to create indices, manage_aliases is required to add/remove aliases
//user has create permission only: allows to create indices, manage_aliases is required to retrieve aliases though
//this throws exception no matter what the indices options are because the aliases part cannot be resolved to any alias
//and there is no way to "allow_no_aliases" like we can do with indices.
//user has create permission only: allows to create indices, manage_aliases is required to retrieve aliases though
//this throws exception no matter what the indices options are because the aliases part cannot be resolved to any alias
//and there is no way to "allow_no_aliases" like we can do with indices.
//user has create and manage_aliases permission on test_*. manage_aliases is required to add/remove aliases on both aliases and
// indices
//ok: user has manage_aliases on test_*
//ok: user has manage_aliases on test_*
//fails: user doesn't have manage_aliases on alias_1
//user has create and manage_aliases permission on test_*. manage_aliases is required to add/remove aliases on both aliases and
// indices
//ok: user has manage_aliases on test_*
//fails: user doesn't have manage_aliases on alias_1
//user has create and manage_aliases permission on test_*. manage_aliases is required to add/remove aliases on both aliases and
// indices
//ok: user has manage_aliases on test_*
//ok: user has manage_aliases on test_*
//ok: user has manage_aliases on test_*
//ok: user has manage_aliases on test_*
//fails: all aliases have been deleted, no existing aliases match test_alias_*
//fails: all aliases have been deleted, no existing aliases match _all
// add unauthorized aliases
//fails: user doesn't have manage_aliases on alias_1
//fails: user doesn't have manage_aliases on alias_1
//user has create and manage_aliases permission on test_*. manage_aliases is required to retrieve aliases on both aliases and
// indices
//ok: user has manage_aliases on test_*
//ok: user has manage_aliases on test_*, test_* gets resolved to test_1
//ok: user has manage_aliases on test_*, empty indices gets resolved to _all indices (thus test_1)
//ok: user has manage_aliases on test_*, _all aliases gets resolved to test_alias and empty indices gets resolved to  _all
// indices (thus test_1)
//ok: user has manage_aliases on test_*, empty aliases gets resolved to test_alias and empty indices gets resolved to  _all
// indices (thus test_1)
//ok: user has manage_aliases on test_*, test_* aliases gets resolved to test_alias and empty indices gets resolved to  _all
// indices (thus test_1)
//ok: user has manage_aliases on test_*, _all aliases gets resolved to test_alias and _all indices becomes test_1
//ok: user has manage_aliases on test_*, empty aliases gets resolved to test_alias and empty indices becomes test_1
//fails: user has manage_aliases on test_*, although _all aliases and empty indices can be resolved, the explicit non
// authorized alias (alias_1) causes the request to fail
//fails: user doesn't have manage_aliases on alias_1
//user has create permission on test_* and manage_aliases permission on alias_*. manage_aliases is required to add/remove aliases
// on both aliases and indices
//fails: user doesn't have manage_aliases aliases on test_1
//fails: user doesn't have manage_aliases aliases on test_1
//fails: user doesn't have manage_aliases aliases on test_*, no matching indices to replace wildcards
//user has create permission on test_* and manage_aliases permission on alias_*. manage_aliases is required to add/remove aliases
// on both aliases and indices
//fails: user doesn't have manage_aliases on test_1, create index is rejected as a whole
//user has create permission on test_* and manage_aliases permission on alias_*. manage_aliases is required to add/remove aliases
// on both aliases and indices
//fails: user doesn't have manage_aliases on test_1
//fails: user doesn't have manage_aliases on test_*, wildcards can't get replaced
//user has create permission on test_* and manage_aliases permission on alias_*. manage_aliases is required to retrieve aliases
// on both aliases and indices
//fails: user doesn't have manage_aliases aliases on test_1, nor test_alias
//user doesn't have manage_aliases aliases on test_*, no matching indices to replace wildcards
//no existing indices to replace empty indices (thus _all)
//fails: no existing aliases to replace wildcards
//fails: no existing aliases to replace _all
//fails: no existing aliases to replace empty aliases
//fails: no existing aliases to replace empty aliases
//user has create permission on test_* and manage_aliases permission on test_*,alias_*. All good.
//user has create permission on test_* and manage_aliases permission on test_*,alias_*. All good.
//user has create permission on test_* and manage_aliases permission on test_*,alias_*. All good.
//fails: user doesn't have manage_aliases privilege on non_authorized
//fails: all aliases have been deleted, _all can't be resolved to any existing authorized aliases
//user has create permission on test_* and manage_aliases permission on test_*,alias_*. All good.
//user has manage_aliases only permissions on both alias_* and test_*
//security plugin lets it through, but es core intercepts it due to strict indices options and throws index not found
//fails: no manage_aliases privilege on non_authorized alias
//fails: no manage_aliases privilege on non_authorized index
//user has manage_aliases only permissions on both alias_* and test_*
//ok: manage_aliases on both test_* and alias_*
//no manage_aliases privilege on non_authorized alias
//no manage_aliases privilege on non_authorized index
// test that the remove index wildcard expacnds only to authorized indices
// it should not matter what client we send the request to, but let's pin all requests to a specific node
/*
//indices with names starting with '-' or '+' can be created up to version  2.x and can be around in 5.x
//aliases with names starting with '-' or '+' can be created up to version 5.x and can be around in 6.x
//the union of all indices and aliases gets returned
//the union of all indices and aliases gets returned, foofoobar is an existing alias but that doesn't make any difference
//the union of all indices and aliases gets returned, missing is not an existing index/alias but that doesn't make any difference
//the union of all resolved indices and aliases gets returned, based on indices and aliases that user is authorized for
//wildcards get replaced on each single action
//if a single operation contains wildcards and ends up being resolved to no indices, it makes the whole request fail
//the union of all resolved indices and aliases gets returned
//_all gets replaced with all indices that user is authorized for, on each single action
//current user is not authorized for any index, _all resolves to no indices, the request fails
//current user is not authorized for any index, foo* resolves to no indices, the request fails
//the union of all indices and aliases gets returned
//the union of all indices and aliases gets returned, doesn't matter is some of them don't exist
//union of all resolved indices and aliases gets returned, based on what user is authorized for
//wildcards get replaced within each single action
//union of all resolved indices and aliases gets returned, based on what user is authorized for
//note that the index side will end up containing matching aliases too, which is fine, as es core would do
//the same and resolve those aliases to their corresponding concrete indices (which we let core do)
//alias foofoobar on both sides, that's fine, es core would do the same, same as above
//union of all resolved indices and aliases gets returned, based on what user is authorized for
//note that the index side will end up containing matching aliases too, which is fine, as es core would do
//the same and resolve those aliases to their corresponding concrete indices (which we let core do)
//alias foofoobar on both sides, that's fine, es core would do the same, same as above
//union of all resolved indices and aliases gets returned, based on what user is authorized for
//every single action has its indices replaced with matching (authorized) ones
//the union of all indices and aliases gets returned
//the union of all indices and aliases gets returned, missing is not an existing index/alias but that doesn't make any difference
//the union of all resolved indices and aliases gets returned, based on indices and aliases that user is authorized for
//wildcards get replaced on each single action
//the union of all resolved indices and aliases gets returned, based on indices and aliases that user is authorized for
//wildcards get replaced on each single action
//the union of all resolved indices and aliases gets returned, based on indices and aliases that user is authorized for
//wildcards get replaced on each single action
//even if not set, empty means _all
//the union of all resolved indices and aliases gets returned
//_all gets replaced with all indices that user is authorized for
//set indices options to have wildcards resolved to open indices only (default is open and closed)
//even if not set, empty means _all
//the union of all resolved indices and aliases gets returned
//_all gets replaced with all indices that user is authorized for
//current user is not authorized for any index, foo* resolves to no indices, the request fails
//the union of all resolved indices and aliases gets returned
//_all gets replaced with all indices that user is authorized for
//the union of all resolved indices and aliases gets returned
//_all gets replaced with all indices that user is authorized for
//the union of all resolved indices and aliases gets returned
//_all gets replaced with all indices that user is authorized for
//union of all resolved indices and aliases gets returned, based on what user is authorized for
//note that the index side will end up containing matching aliases too, which is fine, as es core would do
//the same and resolve those aliases to their corresponding concrete indices (which we let core do)
//alias foofoobar on both sides, that's fine, es core would do the same, same as above
//no authorized aliases match bar*, hence aliases are replaced with the no-aliases-expression
//union of all resolved indices and aliases gets returned, based on what user is authorized for
//note that the index side will end up containing matching aliases too, which is fine, as es core would do
//the same and resolve those aliases to their corresponding concrete indices (which we let core do)
//alias foofoobar on both sides, that's fine, es core would do the same, same as above
//current user is not authorized for any index, aliases are replaced with the no-aliases-expression
/**
/**
// make the user authorized
// make the user authorized
//the union of all indices and aliases gets returned
// multiple indices map to an alias so we can only return the concrete index
// write index
// read index
// TODO with the removal of DeleteByQuery is there another way to test resolving a write action?
/*
// this should still fail since the username is still different
/**
/**
// explicit
// implied by write
// implied by write
/**
/**
/*", "action:login", "action:view/dashboard");
/*", "action:login", "action:view/dashboard");
/*");
/*");
/*", "space/builds"))
// Yes, because (ALL,"logstash-*")
// Yes, because (ALL,"logstash-*")
// No, because "log*" includes indices that "logstash-*" does not
// Yes, "foo?", but not "foo*", because "foo*" > "foo?"
// read = Yes, because (READ, "abc*"), write = No
// read = Yes ( READ "abc*"), write = Yes (WRITE, "*xyz"), manage = No
// read = No, write = Yes (WRITE, "*xyz"), manage = No
// read = Yes, write = No
// project-* = Yes, space/* = Not
/*") // project-* = Yes, space/* = Not
/*").addPrivileges(Collections.singletonMap("space:view/dashboard", false)).build()
// explicit false for test
// matches ".sec*" but not ".security*"
// matches both ".sec*" and ".security*"
// explicit true for test
// matches ".sec*" but not ".security*"
// matches both ".sec*" and ".security*"
/*");
/*");
/*"))
/*"))
/*", "ACTION:" + action1);
/*/name"))
/*", "ACTION:" + action1, "ACTION:" + action2, action1, action2)
/*", true)
/*");
/*
//index1 is not authorized and referred to through wildcard
//index1 is not authorized and referred to through wildcard
//wildcard doesn't match any authorized index
//wildcard doesn't match any authorized index
//an unauthorized index is the same as a missing one
//index1 is not authorized and referred to through wildcard, test2 is excluded
//index1 is not authorized and referred to through wildcard, test2 is excluded
//index1 is not authorized and referred to through wildcard, test111 and test112 are excluded
//index1 is not authorized and referred to through wildcard, test111 and test112 are excluded
//index1 is not authorized and referred to through wildcard, test111 and test112 are excluded
//index1 is not authorized, only that specific item fails
//test4 is missing but authorized, only that specific item fails
//default indices options for search request don't ignore unavailable indices, only individual items fail.
//we set ignore_unavailable and allow_no_indices to true, no errors returned, second item doesn't have hits.
//different behaviour compared to get api: we leak information about a non existing index that the current user is not
//authorized for. Should rather be an authorization exception but we only authorize at the shard level in mget. If we
//authorized globally, we would fail the whole mget request which is not desirable.
//different behaviour compared to term_vector api: we leak information about a non existing index that the current user is not
//authorized for. Should rather be an authorization exception but we only authorize at the shard level in mget. If we
//authorized globally, we would fail the whole mget request which is not desirable.
/*
/*
/*
// another user running as the original user
// the user that authenticated for the run as request
// original user being run as
// both user are run as
// different authenticated by type
// wrong user
// run as different user
// run as different looked up by type
/*
// view repositories
// view all indices, including restricted ones
// create snapshot that includes restricted indices
// view snapshots for repo
// try search all
// try create index
// try create another repo
// try delete repo
// try fumble with snapshots
// try destructive/revealing actions on all indices
/*
//the missing index gets automatically created (user has permissions for that), but indexing fails due to missing authorization
//the missing index gets automatically created (user has permissions for that), but indexing fails due to missing authorization
/*
// dv based field data doesn't use index field data cache, so in the end noting should have been added
/*
/** Simple tests for query field extraction */
/*
/**
/*
// basics:
// no document level security:
// no field level security:
// index group associated with an alias:
// match all fields
// match all fields with more than one permission
// tests that field permissions are merged correctly when we authorize with several groups and don't crash when an index has no group
// did not define anything for ba so we allow all
// test with two indices
// allow_restricted_indices: false
// allow_restricted_indices: true
// allow_restricted_indices: false
// allow_restricted_indices: true
/*
/** Simple tests for opt out query cache*/
// whenever the allowed fields match the fields in the query and we do not deny access to any fields we allow caching.
// check we don't cache if a field is not allowed
/*
// swap target and source for success
/*
// swap target and source for success
/*
// try with two indices and mix order a little
// test old syntax for field permissions
/*
// "baz_*foo", "/fool.*bar/"
/*
// adds a listener in ctor
// the superuser role was requested so we get the role descriptors again
// adds a listener in ctor
// adds a listener in ctor
// both role providers can resolve role A, this makes sure that if the first
// role provider in order resolves a role, the second provider does not override it
// make sure custom roles providers populate roles correctly
// make sure negative lookups are cached
/**
/*")
/*")
// these licenses don't allow custom role providers
// no roles should've been populated, as the license doesn't permit custom role providers
// these licenses allow custom role providers
// roleA should've been populated by the custom role provider, because the license allows it
// license expired, don't allow custom role providers
// existing to no longer present
// doesn't exist to exists
// green or yellow to red
// red to non red
// green to yellow or yellow to green
// adds a listener in ctor
// adds a listener in ctor
// adds a listener in ctor
// adds a listener in ctor
// Can't use getDeprecatedReservedMetadata because `Map.of` doesn't accept null values,
// so we clone metadata with a real value and then remove that key
// the non-deprecated metadata is randomly one of:
// {}, {_deprecated:null}, {_deprecated:false},
// {_reserved:true}, {_reserved:true,_deprecated:null}, {_reserved:true,_deprecated:false}
// Use a LHS so that the random-shufle-order of the list is preserved
/*
// we don't do this test if it crosses days
// we don't do this test if it crosses days
/*
/**
// TODO we should add the config dir to the resources so we don't copy this stuff around...
// truncate to remove some
// modify
// the system role will always be checked first
// test that we can read a role where field permissions are stored in 2.x format (fields:...)
/*
/*", "action:login", "data:read/*"), emptyMap()
/*", "action:login", "data:read/*"), emptyMap()),
/*", "data:write/*"), emptyMap())
/*", "action:login", "data:read/*"), emptyMap()),
/*", "action:login", "data:read/*"), emptyMap()),
/*
// test that we can read a role where field permissions are stored in 2.x format (fields:...)
// setup the roles store so the security index exists
/*
/*
// test when user holds no data:
/*
/*
/*
// enable http
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// bound to a maximum of 100 requests
/*
// index doesn't exist and now exists
// reset and call with no change to the index
// index with different health
// swap prev and current
// state not recovered
// state recovered with index
// state not recovered
// state recovered with index
// index doesn't exist and now exists with wrong format
// index doesn't exist and now exists with correct format
// Index initially exists
// Now close it
// In old/mixed cluster versions closed indices have no routing table
// No upgrade actions run
// add the correct mapping no matter what the template
// try with .security index as an alias
// try with .security index as a concrete index
/*
/*
/**
/*
// we are ignoring exceptions here, so we do not need handle whether or not tempFile was initialized nor if the file exists
/**
/*
// Some tests use a client profile. Put the passphrase in the secure settings for the profile (secure settings cannot be set twice)
//github.com/bcgit/bc-java/issues/593#issuecomment-533518845",
// force TLSv1.2 since renegotiation is not supported by 1.3
// Ignore. We expect a timeout.
// A read call will execute the handshake
// Ignore. The other side is not setup to do the ES handshake. So this will fail.
// We except an IOException from the `accept` call because the server socket will be
// closed before the call returns.
// test client authentication is default
// test required client authentication
// test no client authentication
// test optional client authentication
// test profile required client authentication
// test profile no client authentication
// test profile optional client authentication
/*
/*
// force state update to trigger listener
// force state update to trigger listener
// force state update to trigger listener
// force state update to trigger listener
// force state update to trigger listener
// force state update to trigger listener
// force state update to trigger listener
/*
//future.get(); // don't block it's not called really just mocked
/*
// no client nodes as they all get rejected on network connections
// ephemeral port
// enable http
// make sure this is "localhost", no matter if ipv4 or ipv6, but be consistent
/*
// ensure this did not get overwritten by the listener during startup
// allow all by default
// check that all is in cluster state
// now disable ip filtering dynamically and make sure nothing is rejected
// disabling should not have any effect on the cluster state settings
// now also disable for HTTP
// as we permanently switch between persistent and transient settings, just set both here to make sure we overwrite
// issue #762, occurred because in the above test we use HTTP and transport
// HTTP is not applied if disabled
// HTTP is not applied if disabled
/*
// you have to use the shortest possible notation in order to match, so
// 1234:0db8:85a3:0000:0000:8a2e:0370:7334 becomes 1234:db8:85a3:0:0:8a2e:370:7334
// requires network for name resolution
// when "localhost" is used, ES considers all local addresses see PatternRule#isLocalhost()
// when "localhost" is used, ES considers all local addresses see PatternRule#isLocalhost()
// don't use the assert helper because we don't want the audit trail to be invoked here
// for sanity enable license and check that it is denied
/*
/*
/**
// expected
//expected
/*
/*
/*
/*
/*
/*
/*
// Call will put bytes in buffer to flush
// First call will put bytes in buffer to flush
// Second call will will not continue generating non-app bytes because they still need to be flushed
/*
//github.com/bcgit/bc-java/issues/593#issuecomment-533518845",
// Lock the protocol to 1.2 as 1.3 does not support renegotiation
// This tests that the client driver can still receive data based on the prior handshake
// Prior to JDK11 we still need to send a close alert
// Prior to JDK11 we still need to send a close alert
// Sometimes send server messages before closing
// We are immediately fully closed due to SSLEngine inconsistency
// We are immediately fully closed due to SSLEngine inconsistency
// This should not throw exception yet as the SSLEngine will not UNWRAP data while attempting to WRAP
/*
// disable hostname verificate since these certs aren't setup for that
/*
// enable http
//%s/", NetworkAddress.format(inetSocketAddress));
/*
// enable http
/*
// make sure check works with serialization
// test with anonymous disabled
/*
/*
// expected
/*
/*
// enable http
// invert the require auth settings
// Due to the TLSv1.3 bug with session resumption when client authentication is not
// used, we need to set the protocols since we disabled client auth for transport
// to avoid failures on pre 11.0.3 JDKs. See #getProtocols
/**
/*
// Client
// Apache clients implement their own hostname checking, but we don't want that.
// We use a raw socket so we get the builtin JDK checking (which is what we use for transport protocol SSL checks)
// Logging message failures are tricky to debug because you just get a "didn't find match" assertion failure.
// You should be able to check the log output for the text that was logged and compare to the regex above.
/**
/*
/**
/**
/*
/**
// Nodes start trusting testnode.crt and testclient.crt
// Placeholder trusted certificate that will be updated later on
//github.com/elastic/elasticsearch/issues/49094", inFipsJvm());
// Fails as our nodes do not trust testnode_updated.crt
// blocking read for TLSv1.3 to see if the other side closed the connection
// Copy testnode_updated.crt to the placeholder updateable.crt so that the nodes will start trusting it now
/*
/**
// We are trying to test the SSL configuration for which clients/nodes may join a cluster
// We prefer the cluster to only have 1 node, so that the SSL checking doesn't happen until the test methods run
// expected
// expected
/**
// The test simply relies on this (synchronously) connecting (or not), so we don't need a handshake handler
// blocking read for TLSv1.3 to see if the other side closed the connection
// 1 second timeout
/*
/*
/*
/*
/**
/**
// Iterate out along one spoke until we hone in on the point that's nearly exactly radiusMeters from the center:
// Within 10 cm: close enough!
// lon/lat are left de-normalized so that indexing can properly detect dateline crossing.
// too big
// too small
// close poly
/**
// close poly
/*
/**
/*
/*
/**
/**
/**
/**
// noop
/*
// CONTAINS queries are not supported by VECTOR strategy for indices created before version 7.5.0 (Lucene 8.3.0);
// wrap geometry Query as a ConstantScoreQuery
// all shapes must be disjoint / must be contained in relation to the indexed shape.
// at least one shape must intersect / contain the indexed shape.
// Flatten multipoints
// We do not support multi-point queries?
// contains and intersects are equivalent but the implementation of
// intersects is more efficient.
/*
/**
// START_OBJECT
// "shape" field key
// shape value
/*
//www.apache.org/licenses/LICENSE-2.0
/** generates random cartesian geometry; heavy reuse of {@link GeoTestUtil} */
/** returns next pseudorandom polygon */
// this poly is slow to create ... only do it 10% of the time:
// So the poly can cover at most 50% of the earth's surface:
// we tried to cross dateline or pole ... try again
// box
// triangle
// prevent lines instead of boxes
// prevent lines instead of boxes
// repeat until we get a poly that doesn't cross dateline:
//System.out.println("\nPOLY ITER");
//System.out.println("  angle " + angle);
//System.out.println("    len=" + len);
//System.out.println("    lat=" + lats.get(lats.size()-1) + " lon=" + lons.get(lons.size()-1));
// close it
/** Makes an n-gon, centered at the provided x/y, and each vertex approximately
//System.out.println("make gon=" + gons);
//System.out.println("  angle " + angle);
// close poly
/** Keep it simple, we don't need to take arbitrary Random for geo tests */
/*
/*
// check no holes created
// check there are numSides edges
// check that all the points are about a radius away from the center
// check no holes created
// check there are numSides edges
// check that all the points are about a radius away from the center
/*
/** testing for {@link org.elasticsearch.xpack.spatial.index.mapper.ShapeFieldMapper} */
/**
// explicit right orientation test
/**
// explicit false coerce test
/**
// explicit false accept_z_value test
/**
// explicit false ignore_malformed test
// verify nothing changed
// change mapping; orientation
// after 5.x
/*
// multipoint queries not (yet) supported
// XYShape does not support CONTAINS:
// Logic for doToQuery is complex and is hard to test here. Need to rely
// on Integration tests to determine if created query is correct
// TODO improve ShapeQueryBuilder.doToQuery() method to make it
// easier to test here
/*
/*
// radius is same as error distance
// radius is much smaller than error distance
// radius is much larger than error distance
// radius is 5 times longer than error distance
/*
// create test index
// create index that ignores malformed geometry
// index random shapes
// reset query geometry to make sure we pick one from the indexed shapes
// sometimes GeoTestUtil will create invalid geometry; catch and continue:
// reset query geometry as it didn't get indexed
// now test the query variant
/**
/**
// index a null shape
// index the mbr of the collection
// A geometry collection that is fully within the indexed shape
// A geometry collection that is partially within the indexed shape
// A geometry collection that is disjoint with the indexed shape
/*
/** generates random cartesian shapes */
// we use nextPolygon because it guarantees no duplicate points
// don't build too deep
/*
/*
/**
/*
/*
/**
// cache for streams created by ourselves
// reference counter for a given output
// cache of loggers that rely on external/managed printers
/**
// System.out/err can be changed so do some checks
// must be local file
// clear the ref
// clear the streams
// flush the managed ones
/*
// used by subclasses to indicate the parent instance that creates the object
// for example a PreparedStatement has a Connection as parent
// the instance is kept around instead of reproxying to preserve the semantics (instead of creating a new proxy)
// should not occur
/*
// Logging is done through PrintWriter (not PrintStream which maps to System.err/out) to plug into the JDBC API
//m.getReturnType().getSimpleName(),
//array(m.getParameterTypes()),
//m.getReturnType().getSimpleName(),
//array(m.getParameterTypes()),
/*
/**
/*
/*
/**
// invoke Version to perform classpath/jar sanity checks
// enable logging if needed
/*
// invoke Version to perform classpath/jar sanity checks
// the SQLException is bogus as there's no source for it
// but we handle it just in case
// no closing callback
// the SQLException is bogus as there's no source for it
// but we handle it just in case
//
// Jdbc 4.0
//
//
// Jdbc 4.1
//
/**
/*
/*
/**
/*
/**
/*
// 0 - means unknown
/*
/**
//[host|ip]
//[host|ip]:port/(prefix)
//[host|ip]:port/(prefix)(?options=value&amp;)
//TODO: beef this up for Security/SSL
//";
//localhost:9200/");
// can be out/err/url
// follow the JDBC spec and use the JVM default...
// to avoid inconsistency, the default is picked up once at startup and reused across connections
// to cater to the principle of least surprise
// really, the way to move forward is to specify a calendar or the timezone manually
// options that don't change at runtime
// trigger version initialization
// typically this should have already happened but in case the
// EsDriver/EsDataSource are not used and the impl. classes used directly
// this covers that case
// immutable properties
// mutable ones
// override properties set in the URL with the ones specified programmatically
//[http|https]?[host[:port]]*/[prefix]*[?[option=value]&]*";
// parse properties
// further validation happens in the constructor (since extra properties might be specified either way)
// Add the url to unexpected exceptions
// constructor is private to force the use of a factory in order to catch and convert any validation exception
// and also do input processing as oppose to handling this from the constructor (which is tricky or impossible)
/*
/**
/**
// no-op
// we don't support client info - the docs indicate we should return null if properties are not supported
// similar to getClientInfo - return an empty object instead of an exception
// There's no checkOpen on these methods since they are used by
// DatabaseMetadata that can work on a closed connection as well
// in fact, this information is cached by the underlying client
// once retrieved
/*
/**
//www.postgresql.org/docs/9.0/static/information-schema.html">
// missing/null values are sorted (by default) last
//TODO: is the javadoc accurate
// TODO: sync this with the grammar
//https://docs.microsoft.com/en-us/sql/odbc/reference/appendixes/numeric-functions?view=sql-server-2017
//https://docs.microsoft.com/en-us/sql/odbc/reference/appendixes/string-functions?view=sql-server-2017
// https://docs.microsoft.com/en-us/sql/odbc/reference/appendixes/system-functions?view=sql-server-2017
//https://docs.microsoft.com/en-us/sql/odbc/reference/appendixes/time-date-and-interval-functions?view=sql-server-2017
// https://www.postgresql.org/docs/9.0/static/infoschema-routines.html
// return the cluster name as the catalog (database)
// helps with the various UIs
// null means catalog info is irrelevant
// % means return all catalogs
// EMPTY means return those without a catalog
// null means schema info is irrelevant
// % means return all schemas`
// EMPTY means return those without a schema
// TABLE_CAT is the first column
// TABLE_TYPE (4)
// NB: catalog is not a pattern hence why null is send instead
//
// Utility methods
//
// check if the next item it's a type
// it's not, use the default and move on
// this cursor doesn't hold any resource - no need to clean up
/*
/**
// In Java 8 LocalDate.EPOCH is not available, introduced with later Java versions
/*
/*
/*
/**
/**
/**
/**
/*
/*
// the value of scaleOrLength parameter doesn't matter, as it's not used in the called method below
// check also here the unsupported types so that any unsupported interfaces ({@code java.sql.Struct},
// {@code java.sql.Array} etc) will generate the correct exception message. Otherwise, the method call
// {@code TypeConverter.fromJavaToJDBC(x.getClass())} will report the implementing class as not being supported.
// converting to UTC since this is what ES is storing internally
// converting to UTC since this is what ES is storing internally
// converting to UTC since this is what ES is storing internally
// set the null value on the type and exit
// converting to {@code java.util.Date} because this is the type supported by {@code XContentBuilder} for serialization
// anything else other than VARCHAR and TIMESTAMP is not supported in this JDBC driver
/*
// temporary calendar instance (per connection) used for normalizing the date and time
// even though the cfg is already in UTC format, JDBC 3.0 requires java.sql.Time to have its date
// removed (set to Jan 01 1970) and java.sql.Date to have its HH:mm:ss component removed
// instead of dealing with longs, a Calendar object is used instead
// statement can be null so we have to extract the timeZone from the non-nullable cfg
// TODO: should we consider the locale as well?
// TODO: the error message in case the value in the column cannot be converted to a Date refers to a column index
// (for example - "unable to convert column 4 to a long") and not to the column name, which is a bit confusing.
// Should we reconsider this? Maybe by catching the exception here and rethrowing it with the columnLabel instead.
// TODO: the B6 appendix of the jdbc spec does mention CHAR, VARCHAR, LONGVARCHAR, DATE, TIMESTAMP as supported
// jdbc types that should be handled by getDate and getTime methods. From all of those we support VARCHAR and
// TIMESTAMP. Should we consider the VARCHAR conversion as a later enhancement?
// the cursor can return an Integer if the date-since-epoch is small enough, XContentParser (Jackson) will
// return the "smallest" data type for numbers when parsing
// TODO: this should probably be handled server side
// ignore fetch size since scrolls cannot be changed in flight
/*
/*
/*
/*
// ignore
// no-op - always escape
// no-op (doc is confusing - says no-op but also to throw an exception)
// execute the query and handle the rs closing and initialization
// close previous result set
// the spec is somewhat unclear. It looks like there are 3 states:
// unset (in this case -1 which the user cannot set) - in this case, the default fetch size is returned
// 0 meaning the hint is disabled (the user has called setFetch)
// >0 means actual hint
// tl;dr - unless the user set it, returning the default is fine
// no-op
/*
/*
/*
/**
/*
/*
/**
/**
// Creates a PreparedQuery
/*
/*
/*
/*
/**
/**
/**
// TODO: JDBC escape syntax
// https://db.apache.org/derby/docs/10.5/ref/rrefjdbc1020262.html
/**
/**
/* comment");
/**
// double quotes mean escaping
/*
// handles Statement, PreparedStatement and CallableStatement
/*
/**
/**
/**
/**
/**
// if the value type is the same as the target, no conversion is needed
// make sure though to check the internal type against the desired one
// since otherwise the internal object format can leak out
// (for example dates when longs are requested or intervals for strings)
//
// JDK 8 types
//
/**
// These types are already represented correctly in JSON
// Parser might return it as integer or long - need to update to the correct type
// Parser might return it as integer or long - need to update to the correct type
// Double might be represented as string for infinity and NaN values
// Float might be represented as string for infinity and NaN values
//TODO: should we support conversion to TIMESTAMP?
//The spec says that getLong() should support the following types conversions:
//TINYINT, SMALLINT, INTEGER, BIGINT, REAL, FLOAT, DOUBLE, DECIMAL, NUMERIC, BIT, BOOLEAN, CHAR, VARCHAR, LONGVARCHAR
//case TIMESTAMP:
//    return ((Number) val).longValue();
/*
// apart from the mappings in {@code DataType} three more Java classes can be mapped to a {@code JDBCType.TIMESTAMP}
// according to B-4 table from the jdbc4.2 spec
// fallback to DataType
// fall-back to iteration for checking class hierarchies (in case of custom objects)
/*
/*
/* This test will only work properly in gradle because in gradle we run the tests
// can happen (if the driver jar was not loaded)
// mimic DriverManager and unregister the driver
/*
//" + webServerAddress() + "/?" + sslUrlProps;
/*
//] url, received [jdbc:es:]", e.getMessage());
//localhost").baseUri().toString(), is("http://localhost:9200/"));
//localhost:1234").baseUri().toString(), is("http://localhost:1234/"));
//localhost:1234/").baseUri().toString(), is("http://localhost:1234/"));
//a:1/foo/bar/tar").baseUri().toString(), is("http://a:1/foo/bar/tar"));
//[::1]:54161/foo/bar").baseUri().toString(), is("http://[::1]:54161/foo/bar"));
//a:1/?debug=true");
//a:1/"));
//a:1/?debug=true&debug.output=jdbc.out");
//a:1/"));
//a:1/foo/bar/tar?debug=true&debug.out=jdbc.out"));
//a:1/foo/bar/tar?debug=true&debug.output=jdbc.out");
//a:1/foo/bar/tar"));
//test?ssl=true");
//test:9200/"));
//http://test?ssl=true");
//test:9200/"));
//https://test:9200");
//test:9200/"));
//https://test:9200?ssl=true");
//test:9200/"));
//test?ssl=false");
//test:9200/"));
//http://test?ssl=false");
//test:9200/"));
//https://test?ssl=false"));
//test:9200?pagee.size=12"));
//test:9200?foo=bar"));
//test:9200?pagee.size=12&validate.properties=true"));
//test:9200?&validate.properties=true&something=some_value"));
//test:9200?something=some_value", properties, 0));
//test:9200?pagee.size=12&validate.properties=false");
// URL properties test
//test:9200?validate.properties=false&something=some_value&query.timeout=" + queryTimeout + "&connect.timeout="
// Properties test
// also putting validate.properties in URL to be overriden by the properties value
//test:9200?validate.properties=true&something=some_value", properties, 0);
// Should be overridden
//test?connect.timeout=1&page.timeout=2";
// No properties
// Properties override
// Driver default override for connection timeout
//test?" + sslUrlProps.toString()).sslConfig());
//test?" + sslUrlProps.toString(), props, 0).sslConfig());
//test?" + sslUrlProps.toString(), props, 0).sslConfig());
//github.com/elastic/elasticsearch/issues/41557")
//test?" + sslUrlProps);
// always using "false" so that the SSLContext doesn't actually start verifying the keystore and trustore
// locations, as we don't have file permissions to access them.
// because SslConfig doesn't expose its internal properties (and it shouldn't),
// we compare a newly created SslConfig with the one from the JdbcConfiguration with the equals() method
//test:9200/"));
//test?" + wrongSetting + "=foo";
//test";
/*
//localhost:9200/", new Properties(), 10), false));
/*
/*
//test")));
//test")));
// February 29th, 2016. 01:17:55 GMT = 1456708675000 millis since epoch
//l:1", null, 0), "?");
/*
/*
/*
/**
/*
/* multiline  \n" +
/* lines */ ? here \n" +
/* * * * "));
/*
// Simulate sending over XContent
/*
/**
/*
/* This test will only work properly in gradle because in gradle we run the tests
/*
/**
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
/*
/**
/**
/**
/*
/*
/*
/*
/*
/**
/*
/*
// Building the cli will attempt the connection and run the assertion
// forceClose will close it
/*
/*
/*
/*
/*
/*
/*
/*
/*
// tag::admin_properties
// end::admin_properties
//" + scheme + "://" + elasticsearchAddress(), props);
// h2 doesn't have the same sort of DESCRIBE that we have so we emulate it
/*
// Without monitor/main the JDBC driver - ES server version comparison doesn't take place, which fails everything else
// by moving to field caps these calls do not require the monitor permission
//            expectUnauthorized("cluster:monitor/main", user,
//                    () -> es(userProperties(user)).createStatement().executeQuery("SELECT * FROM test"));
//            expectUnauthorized("cluster:monitor/main", user,
//                    () -> es(userProperties(user)).createStatement().executeQuery("SHOW TABLES LIKE 'test'"));
//            expectUnauthorized("cluster:monitor/main", user,
//                    () -> es(userProperties(user)).createStatement().executeQuery("DESCRIBE test"));
// Metadata methods only available to JDBC
/* Since there is no easy way to get a result from the admin side with
/*
/*
/*
/*
/**
/*
/*
// Without monitor/main everything should work just fine
/**
// TODO return a better error message for bad scrolls
// one scroll access denied per shard
/*
/**
/**
/**
/**
/**
/**
/**
/**
/* We can't wipe the cluster between tests because that nukes the audit
/* Since we don't wipe the cluster between tests we only need to
// The log file can roll over without being caught by assertLogs() method: in those tests where exceptions are being handled
// and no audit logs being read (and, thus, assertLogs() is not called) - for example testNoMonitorMain() method: there are no
// calls to auditLogs(), and the method could run while the audit file is rolled over.
// If this happens, next call to auditLogs() will make the tests read from the rolled over file using the main audit file
// offset, which will most likely not going to work since the offset will happen somewhere in the middle of a json line.
// once the audit file rolled over, it will stay like this
// Clear the static state so other subclasses can reuse it later
/* Scrolling doesn't have to access the index again, at least not through sql.
//This user has permission to run sql queries so they are given preliminary authorization
//the following get index is granted too but against the no indices placeholder, as ignore_unavailable=true
/* Scrolling doesn't have to access the index again, at least not through sql.
/* The user has permission to query the index but one of the
/* Scrolling doesn't have to access the index again, at least not through sql.
/* The user has permission to query the index but one of the
//This user has permission to run sql queries so they are given preliminary authorization
//the following get index is granted too but against the no indices placeholder, as ignore_unavailable=true
/**
// use a second variable since the `assertBusy()` block can be executed multiple times and the
// static auditFileRolledOver value can change and mess up subsequent calls of this code block
// the audit log file rolled over during the test
// and we need to consume the rest of the rolled over file plus the new audit log file
// once the audit file rolled over, it will stay like this
// the order in the array matters, as the readers will be used in that order
// The "index" is used as a way of reading from both rolled over file and current audit file in order: rolled over file
// first, then the audit log file. Very rarely we will read from the rolled over file: when the test happened to run
// at midnight and the audit file rolled over during the test.
// start with the rolled over file first
// the current audit log file reader should always be non-null
// start with the current audit logging file
// when the end of the file is reached, either stop or move to the next reader
// TODO we may want to extend this and the assertions to SearchAction.NAME as well
/*
// Use a sorted list for indices for consistent error reporting
/*
// role defined in roles.yml
// run 30 queries and pick randomly each time one of the 5-15 users created previously
// expect the user that ran the query to be the same as the one returned by the `USER()` function
/*
/*
// TODO in this case we should probably remove the source filtering entirely. Right? It costs but we don't need it.
/*
/*
/*
/*
/*
/*
/*
/*
/*
// using a smaller fetchSize for nested documents' tests to uncover bugs
// similar to https://github.com/elastic/elasticsearch/issues/35176 quicker
/*
/*
/**
//
// uncomment this to printout the result set and create new CSV tests
//
//JdbcTestUtils.logLikeCLI(elastic, log);
// pass the testName as table for debugging purposes (in case the underlying reader is missing)
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
/*
/*
/*
/*
// we just need to get a response that's not a date parsing error
/*
/**
/*
/*
/**
/*
// default (no _source setting) or explicit setting
// enable _source at index level
/*
// _source for `keyword` fields doesn't matter, as they should be taken from docvalue_fields
// default (no _source setting) or explicit setting
// enable _source at index level
/*
// because "coerce" is true, a "123.456" floating point number STRING should be converted to 123, no matter the numeric field type
/*
// scaling_factor is required for "scaled_float"
// important here is to pass floatingPointNumber as a string: "float_field": "123.456"
// because "coerce" is true, a "123.456" floating point number STRING should be converted to 123.456 as number
// and converted to 123.5 for "scaled_float" type
/*
/*
/*
// Use Integer as the json parser that is used to read the values from the response will create
// Integers for short and byte values
/*
// Use Integer as the json parser that is used to read the values from the response will create
// Integers for short and byte values
// default (no _source setting) or explicit setting
// enable _source at index level
// ignore_malformed is true, thus test a non-number value
// on purpose use a string instead of a number and check for null when querying the field's value
/*
// default (no _source setting) or explicit setting
// enable _source at index level
// pass true or false as string "true" or "false
// adding the boolean as a String here because parsing the response will yield a "true"/"false" String
/*
// default (no _source setting) or explicit setting
// enable _source at index level
/*
/*
/*
/*
// _source for `keyword` fields doesn't matter, as they should be taken from docvalue_fields
// default (no _source setting) or explicit setting
// enable _source at index level
// even if the _source is disabled, selecting only the keyword sub-field should work as expected
/*
// default (no _source setting) or explicit setting
// enable _source at index level
// ignore_malformed is true, thus test a non-number value
// on purpose use a string instead of a number and check for null when querying the field's value
// if the _source is disabled, selecting only the integer sub-field shouldn't work as well
/*
// default (no _source setting) or explicit setting
// enable _source at index level
// ignore_malformed is true, thus test a non-number value
// text or keyword subfield
// on purpose use a string instead of a number and check for null when querying the field's value
// selecting only the keyword subfield when the _source is disabled should work
// if the _source is disabled, selecting only the integer field shouldn't work
/*
// default (no _source setting) or explicit setting
// enable _source at index level
// root field ignore_malformed
// sub-field ignore_malformed
/*
// default (no _source setting) or explicit setting
// enable _source at index level
// root field ignore_malformed
// sub-field ignore_malformed
// create two aliases - one within a hierarchy, the other just a simple field w/o hierarchy
/*
/**
// from xcontent we can get float or double, depending on the conversion
// method of the specific xcontent format implementation
// add a client_id to the request
// since we default to JSON if a format is not specified, randomize setting it or not, explicitly;
// for any other format, just set the format explicitly
// randomly use the Accept header for the response format
/*", "application/" + format));
// randomly set the "columnar" parameter, either "true" (non-default) or explicit "false" (the default anyway)
// randomize binary response enforcement for drivers (ODBC/JDBC) and CLI
// set it explicitly or leave the default (null) as is
// send the query either as body or as request parameter
/*
/**
// CLI only supports a single node at a time so we just give it one.
/**
// failed to connect to the cli so there is nothing to do here
/**
/**
/*
/**
/**
//" + address;
//" + address;
/*
// Feed it passwords if needed
// Read the newline echoed after the password prompt
/*
/*
// Read the newline echoed after the password prompt
// Read until the first "good" line (skip the logo or read until an exception)
// it's almost the bottom of the logo, so read the next line (the version) and break out of the loop
// if it's an exception, just break out of the loop and don't read the next line
// as it will swallow the exception and IT tests won't catch it
/**
// After the connection test passess we emit an empty line and then the prompt
/**
// Try and shutdown the client normally
/*
/**
/**
/**
// Every line gets an extra new line because, I dunno, but it looks right in the CLI
// Every line gets an extra new line because, I dunno, but it looks right in the CLI
/*
/*
/*
/**
/**
/**
// Create an index without any types
//assertEquals("line 1:15: [test] doesn't have any types so it is incompatible with sql" + END, readLine());
/*
/**
// Test for issue: https://github.com/elastic/elasticsearch/issues/42851
// Even though fetch size and limit are smaller than the noRows, all buckets
// should be processed to achieve the global ordering of the aggregate function.
/*
/**
/*
/*
/*
/**
// pass the testName as table for debugging purposes (in case the underlying reader is missing)
// make sure ES uses UTC (otherwise JDBC driver picks up the JVM timezone per spec/convention)
/*
// Common
// Type specific
// lakes
// road_segments
// road_segments, divided_routes
// road_segments, streams
// divided_routes
// forests, named_places
// bridges, buildings
// buildings
// buildings
// ponds
// ponds
// map_neatlines
//") == false) {
/*
/**
// Load GIS extensions
// TODO: use UTC for now until deciding on a strategy for handling date extraction
/*
/**
/**
/*
/**
/*.csv-spec");
// Run the time tests always in UTC
// TODO: https://github.com/elastic/elasticsearch/issues/40779
// pass the testName as table for debugging purposes (in case the underlying reader is missing)
/*
/**
/**
// trigger data loading for type inference
/**
// No type information in headers, no need to parse columns - trigger auto-detection
/* Read the next line. It might be a separator designed to look like the cli.
// Copy the rest of test
/**
// read the query
// pick up the query
// keep reading the query
// read the results
// read data
// clean-up and emit
/*
/**
/**
/**
/*
// frozen index
// frozen index
// dep_id
// dep_name (from departments)
// from
// to
// an empty value in the csv file is treated as 'null', thus skipping it in the bulk request
// append department
// remove last ,
/*
/*
/**
// Create an index without any types
// see https://github.com/elastic/elasticsearch/issues/34719
//assertEquals("Found 1 problem(s)\nline 1:15: [test] doesn't have any types so it is incompatible with sql", e.getMessage());
/*
/**
// each document will have a nested field with 1 - 5 values
/**
/**
/**
/**
// don't check the very last row in the result set
/**
// set size smaller than an agg page
// the page was set to a pivot row (since the initial 3 is lower as a pivot page takes number of pivot entries + 1)
// now try with a larger fetch size (8 * 2 + something) - should be 2
//
// run a query with a limit that is not a multiple of the fetch size
// set size smaller than an agg page
// last entry
/*
/**
/**
/**
// metadata doesn't consume a ResultSet thus it shouldn't close it
// to help debugging, indicate the previous column (which also happened to match and thus was correct)
// use the type not the name (timestamp with timezone returns spaces for example)
// since H2 cannot use a fixed timezone, the data is stored in UTC (and thus with timezone)
// H2 treats GEOMETRY as OTHER
// since csv doesn't support real, we use float instead.....
// handle intervals
// csv doesn't support NULL type so skip type checking
// when lenient is used, an int is equivalent to a short, etc...
// The ResultSet is consumed and thus it should be closed
// fix for CSV which returns the shortName not fully-qualified name
// handle nulls first
// hack for JDBC CSV nulls
// then timestamp
// then date
// and floats/doubles
// We need to convert the expected object to libs/geo Geometry for comparision
// geo points are loaded form doc values where they are stored as long-encoded values leading
// to lose in precision
// intervals
// finally the actual comparison
/**
// integer upcast to long
// Used to convert the DATE read from CSV file to a java.sql.Date at the System's timezone (-Dtests.timezone=XXXX)
/*
// Some context might linger due to fire and forget nature of scroll cleanup
/**
// JDBC only supports a single node at a time so we just give it one.
//" because we want the example in
/* This doesn't include "jdbc:es://" because we want the example in
//" + elasticsearchAddress();
//" + elasticsearchAddress;
//
// methods below are used inside the documentation only
//
//" + elasticsearchAddress();
// tag::connect-dm
//" + elasticsearchAddress;     // <1>
// <2>
// end::connect-dm
//" + elasticsearchAddress();
// tag::connect-ds
//" + elasticsearchAddress;     // <1>
// <2>
// end::connect-ds
/**
// in the tests, don't be lenient towards multi values
// We use system default timezone for the connection that is selected randomly by TestRuleSetupAndRestoreClassEnv
// from all available JDK timezones. While Joda and JDK are generally in sync, some timezones might not be known
// to the current version of Joda and in this case the test might fail. To avoid that, we specify a timezone
// known for both Joda and JDK
/*
/*
// header
/**
/*.txt").
// the root folder searched inside the classpath - default is the root classpath
// default file match
// check whether we're dealing with a jar
// Java 7 java.nio.fileFileSystem can be used on top of ZIPs/JARs but consumes more memory
// hence the use of the JAR API
// normal file access
// do not to cache files (to avoid keeping file handles around)
/*
/*
// http://www.h2database.com/html/features.html#in_memory_databases
// Initialize h2 so we can use it for testing
/**
// H2 in-memory will keep the db alive as long as this connection is opened
// close
/*
/**
/*
// Makes sure that column types survived the round trip
// This is the current limitation of JDBC parser that it cannot detect improper use of '?'
/* foo */");
/* foo */", results.getString(2));
/* ?, */ ? -- ?")) {
/*
/*
// default has multi value disabled
// default has multi value disabled
// Byte values testing
// Short values testing
// Integer values testing
// Long values testing
// Double values testing
// Float values testing
// true values
// false values
// other (non 0 = true) values
// other false values
// bulk validation for all fields which are not of type date
// bulk validation for all fields which are not of type date
// 2018-03-12 17:00:00 UTC
// UTC +10 hours
// +1 day
// 2018-03-12 05:00:00 UTC
// UTC -10 hours
// -1 day
// 2018-03-12 17:00:00 UTC
// UTC +10 hours
// 1984-05-02 14:59:12 UTC
// 2018-03-12 17:20:30.123 UTC
// UTC +10 hours
/*
// add the known value as the first one in list. Parsing from _source the value will pick up the first value in the array.
// add the known value as the first one in list. Parsing from _source the value will pick up the first value in the array.
/**
// random Byte
// random Integer
// random Short
// random Long
// random Double
// random Float
/*
/*
// tag::simple_example
// end::simple_example
/*
/**
/*"));
// 404 here just means we had no indexes
/**
// TODO: use UTC for now until deciding on a strategy for handling date extraction
//
// spec reader
//
// returns source file, groupName, testName, its line location, and the custom object (based on each test parser)
// ignore comments
//")) {
// parse test name
// only if the parser is ready, add the object - otherwise keep on serving it lines
// do not to cache files (to avoid keeping file handles around)
/*
/**
/*.sql-spec");
// not initialized
// using a smaller fetchSize for nested documents' tests to uncover bugs
// similar to https://github.com/elastic/elasticsearch/issues/42581
// we skip the tests in case of these locales because ES-SQL is Locale-insensitive for now
// while H2 does take the Locale into consideration
/*
/**
// for drivers and the CLI return the number as is, while for REST cast it implicitly to Double (the JSON standard).
// by default, drivers and the CLI respond in binary format
/*
/**
/*
/**
/**
//github.com/elastic/x-pack-elasticsearch/issues/2074")
// TODO: what exactly is this test suppossed to do. We need to check the 2074 issue above.
// Default TimeZone is UTC
// Normal join not supported
// Neither is a self join
// Nor fancy stuff like CTEs
// Create an index without any types
// see https://github.com/elastic/elasticsearch/issues/34719
//containsString("1:15: [test] doesn't have any types so it is incompatible with sql"));
// put an explicit "columnar": false parameter or omit it altogether, it should make no difference
// Helps with debugging in case something crazy happens on the server.
// Improves error reporting readability
// We default to JSON but we force it randomly for extra coverage
// JSON is the default but randomly set it sometime for extra coverage
/*", "application/json"));
// We default to JSON but we force it randomly for extra coverage
// JSON is the default but randomly set it sometime for extra coverage
/*", "application/json"));
// CSV/TSV tests
/**
/**
// We are not interested in internal indices
/*
/**
// used for "client_id" request parameter value, but also for getting the stats from ES
// "client_id" parameter will not be sent in the requests
// and "clientType" will only be used for getting the stats back from ES
// initialize the "base" metric values with whatever values are already recorded on ES
//
// random WHERE and ORDER BY queries
//
//
// random HAVING and GROUP BY queries
//
//
// random LIMIT queries
//
//
// random LOCALly executed queries
//
//
// random COMMANDs
//
//
// random TRANSLATE requests
//
//
// random failed queries
//
// not interested in the exception type, but in the fact that the metrics are incremented
// when an exception is thrown
// We default to JSON but we force it randomly for extra coverage
// JSON is the default but randomly set it sometime for extra coverage
/*", "application/json"));
// Helps with debugging in case something crazy happens on the server.
// Improves error reporting readability
// We default to JSON but we force it randomly for extra coverage
// JSON is the default but randomly set it sometime for extra coverage
/*", "application/json"));
/*
/**
// Using an ObjectParser here (vs. ConstructingObjectParser) because the latter needs to instantiate a concrete class
// and we would duplicate the code from this class to its subclasses
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
/*
/**
/**
/**
// Figure out the column widths:
// 1. Start with the widths of the column names
// TODO read the width from the data type?
// 2. Expand columns to fit the largest value
/**
// The header lines
// left padding
// right padding
// emdash creates issues
/* Now format the results. Sadly, this means that column
/**
// Pad
// Trim
/**
/* Each column has either a '|' or a '\n' after it
/*
/*
/**
// here the position in "objects" is the same as the fields parser declarations below 
// "cursor" is required constructor parameter
// This is needed just to test round-trip compatibility with proto.SqlClearCursorRequest
/*
/*
/**
/**
/*
/*
/**
/*
/**
/**
/**
// This is needed just to test round-trip compatibility with proto.SqlQueryRequest
/*
/**
/*
/**
// TODO: Simplify cursor handling
// TODO investigate reusing Page here - it probably is much more efficient
// We might have rows without columns and we might have columns without rows
// So we send the column size twice, just to keep the protocol simple
/**
// columns can be specified (for the first REST request for example), or not (on a paginated/cursor based request)
// if the columns are missing, we take the first rows' size as the number of columns
/**
// use the ISO format
// use the SQL format for intervals when sending back the response for CLI
// all other clients will receive ISO 8601 formatted intervals
/*
/**
/*
/**
// This is needed just to test parsing of SqlTranslateRequest, so we can reuse SqlQuerySerialization
/*
/**
/*
/**
/*
/*
/*
/*
/*
/*
/**
/**
/*
/*
/*
/**
// remove jansi since it has issues on Windows in closing terminals
// the CLI uses JNA anyway
/* Initialize the logger from the a properties file we bundle. This makes sure
/**
// Most likely Elasticsearch is not running
// Most likely we connected to something other than Elasticsearch
/*
// append the line without trailing ;
// Skip empty commands
// special case to handle exit
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
//TODO: need tree structure
/*
/**
//localhost:9200/";
/**
/*
/*
/**
/*
/**
/**
/**
/*
/*
/**
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
// TODO: We can relax compatibility requirement later when we have a better idea about protocol compatibility guarantees
/*
/**
/*
/**
/*
/**
// print the version centered on the last line
/*
/*
// Successfully finished the entire query!
/*
/**
/*
/*
//localhost:9200/", con.connectionString());
//localhost:9200/"), con.baseUri());
//foobar:9242/", null);
//foobar:9242/", con.connectionString());
//foobar:9242/"), con.baseUri());
//user:pass@foobar:9242/", null);
//user:pass@foobar:9242/", con.connectionString());
//foobar:9242/"), con.baseUri());
//user@foobar:9242/", null);
//user@foobar:9242/", con.connectionString());
//foobar:9242/"), con.baseUri());
// Stubbed so we don't need permission to read the file
// Stub building the actual configuration because we don't have permission to read the keystore.
//user@foobar:9242/", "keystore_location"));
//user@foobar:9242/", null));
// Stubbed so we don't need permission to read the file
//user@foobar:9242/", "keystore_location"));
/*
/*
/*
/**
/*
/*
/* This test will only work properly in gradle because in gradle we run the tests
/*
/*
/*
/*
// Set a separator
/*
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
// Validation
// Binary communication
// Timeouts
// 30s
// 1m
// 90s
// 45s
// Auth
// NB: this is password instead of pass since that's what JDBC DriverManager/tools use
// Base URI for all request
// Proxy
// page
// auth
// page
// auth
// validate specified properties to pick up typos and such
// auth
/**
/*
/**
// TODO allow customizing the time zone - this is what session set/reset/get should be about
// method called only from CLI
// method called only from CLI
// "application/cbor" or "application/json"
/*
/**
/**
// update path if needed
// update query if needed
// due to the way the URL API is designed, the proxy needs to be passed in first
// the rest of the connection setup
// setup basic stuff first
// timeouts
// disable content caching
// HTTP params
// HttpURL adds this header by default, HttpS does not
// adding it here to be consistent
//con.setRequestProperty("Accept-Encoding", GZIP);
// check if x-pack or sql are not available (x-pack not installed or sql not enabled)
// by checking the error message the server is sending back 
// consume streams
// http://docs.oracle.com/javase/7/docs/technotes/guides/net/http-keepalive.html
// ignore
// keep on ignoring
/**
/*
/*
// try http first
// nope, check socks
// returns hostname (string), port (int)
/*
/**
/**
// Set up the factory similarly to how XContent does
// Do not automatically close unclosed objects/arrays in com.fasterxml.jackson.core.json.UTF8JsonGenerator#close() method
// Don't close the stream because we might need to reset and reply it if there is an error. The caller closes the stream.
/**
// Mark so we can rewind to get the entire response in case we have to render an error.
/**
/**
/**
/**
/* It'd be lovely to use the high level constructs that we have in core like ObjectParser
// Intentionally ignored
// Intentionally ignored
// Parse the array and add each item to the corresponding list of metadata.
// Arrays of objects are not supported yet and just ignored and skipped.
// Any additional metadata object added by the metadataToXContent method is ignored
// and skipped, so that the parser does not fail on unknown fields. The parser only
// support metadata key-pairs and metadata arrays of values.
/**
// So far as I know, this is always caused by the response being too large
/*
// SSL alternative
// PCKS12
// ssl
/*
// current folder, ignore it
// top folder, skip previous element
// should it be skipped?
// Based on "Algorithms on Strings, Trees and Sequences by Dan Gusfield".
// returns -1 if the two strings are within the given threshold of each other, -1 otherwise
// if one string is empty, the edit distance is necessarily the length of the other
// swap the two strings to consume less memory
// 'previous' cost array, horizontally
// cost array, horizontally
// placeholder to assist in swapping p and d
// fill in starting table values
// these fills ensure that the value above the rightmost entry of our
// stripe will be ignored in following loop iterations
// compute stripe indices, constrain to array size
// the stripe may lead off of the table if s and t are of different sizes
// ignore entry left of leftmost
// iterates through [min, max] in s
// diagonally left and up
// 1 + minimum of cell to the left, to the top, diagonally left and up
// copy current distance counts to 'previous row' distance counts
// if p[n] is greater than the threshold, there's no guarantee on it being the correct
// distance
// 1 switches or 1 extra char
/*
/**
/*
/**
// check if URI can be parsed correctly without adding scheme
// if the connection string is in format host:port or just host, the host is going to be null
// if the connection string contains IPv6 localhost [::1] the parsing will fail
// We couldn't parse URI without adding scheme, let's try again with scheme this time
//" + connectionString);
// We managed to parse URI and all necessary pieces are present, let's make sure the scheme is correct
/**
/*
// Allow for optional snapshot and qualifier
// check classpath
// This is similar to how Elasticsearch's Build class digs up its build information.
// Since version info is not critical, the parsing is lenient
/*
//" + webServer.getHostName() + ":" + webServer.getPort();
// we don't care what the cursor is, because the ES node that will actually handle the request (as in running an ES search)
// will not see/have access to the "binary_format" response, which is the concern of the first node getting the request
//" + webServer.getHostName() + ":" + webServer.getPort();
/*
// The UTF-8 BOM
// An invalid UTF-8 character
/*
/*
//localhost:9200/");
//server:9200/"), parseURI("server:9200", DEFAULT_URI));
//server:9200/"), parseURI("server", DEFAULT_URI));
//server:9201/"), parseURI("http://server:9201", DEFAULT_URI));
//server:9201/"), parseURI("https://server:9201", DEFAULT_URI));
//server:9200/"), parseURI("https://server", DEFAULT_URI));
//[::1]:51082/"), parseURI("[::1]:51082", DEFAULT_URI));
//user@server:9200/"), parseURI("https://user@server", DEFAULT_URI));
//user:password@server:9200/"), parseURI("user:password@server", DEFAULT_URI));
//server:9201/some_path"), parseURI("https://server:9201/some_path", DEFAULT_URI));
//server:9201/?query"), parseURI("https://server:9201/?query", DEFAULT_URI));
//server:9201/]: Only http and https protocols are supported",
//server:9201/", DEFAULT_URI)).getMessage()
//",
//server:9100"),
//server:9100?query"), "http://server:9100?query", DEFAULT_URI));
//server:9100/"),
//server:9100/?query"), "http://server:9100/?query", DEFAULT_URI));
//server:9100"),
//server:9100"), "http://server:9100", DEFAULT_URI));
//server:9100"),
//server:9100"), ""));
//server:9100"),
//server:9100"), null));
//server:9100"),
//server:9100"), "/"));
//server:9100/_sql"),
//server:9100"), "/_sql"));
//server:9100/_sql"),
//server:9100"), "_sql"));
//server:9100/es_rest/_sql"),
//server:9100/es_rest"), "/_sql"));
//server:9100/es_rest/_sql"),
//server:9100/es_rest"), "_sql"));
//server:9100/es_rest/_sql"),
//server:9100/es_rest/"), "/_sql"));
/*
/*
/**
/*
/**
/**
/**
/**
/**
/*
/**
/*
/**
/*
/**
/**
/*
/**
/*
/**
//binary values will be parsed back and returned as base64 strings when reading from json and yaml
/**
/*
/*
/**
/*
/**
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
// TODO: Simplify cursor handling
// TODO investigate reusing Page here - it probably is much more efficient
/**
/*
/**
/*
// handle intervals
// YEAR/MONTH/YEAR TO MONTH -> YEAR TO MONTH
// +yyy-mm - 7 chars
// DAY/HOUR/MINUTE/SECOND (and variations) -> DAY_TO_SECOND
// +ddd hh:mm:ss.mmmmmmmmm - 23 chars
/*
/*
/**
//ieeexplore.ieee.org/document/344061">[1]</a>
//dl.acm.org/citation.cfm?id=627558">[2]</a>
//scholar.google.com/citations?user=pdDeRScAAAAJ">[3]</a>
//en.wikipedia.org/wiki/Visitor_pattern">Visitor</a>
//cr.openjdk.java.net/~briangoetz/amber/pattern-match.html">the future</a>
//docs.scala-lang.org/tour/pattern-matching.html">core feature</a>.
/*
/*
/*
/*
/*
/*
/*
/**
/**
/**
/**
//new ImplicitCasting()
//
// Shared methods around the analyzer rules
//
// first take into account the qualified version
// if the field is unqualified
// first check the names directly
// but also if the qualifier might not be quoted and if there's any ambiguity with nested fields
// none found
// if it's a object/compound type, keep it unresolved with a nice error message
// incompatible mappings
// unsupported types
// compound fields
// inlined queries (SELECT 1 + 2) are already resolved
// if the children are not resolved, there's no way the node can be resolved
// okay, there's a chance so let's get started
// if the grouping is unresolved but the aggs are, use the former to resolve the latter
// solves the case of queries declaring an alias in SELECT and referring to it in GROUP BY
// e.g. SELECT x AS a ... GROUP BY a
// use the matched expression (not its attribute)
// try resolving the order expression (the children are resolved as this point)
// if resolved, return it; otherwise keep it in place to be resolved later
//TODO: likely have to expand * inside functions as well
// the field exists, but cannot be expanded (no sub-fields)
// a qualifier is specified - since this is a star, it should be a CompoundDataType
// resolve the so-called qualifier first
// since this is an unresolved start we don't know whether it's a path or an actual qualifier
// the wildcard couldn't be expanded because the field doesn't exist at all
// so, add to the list of expanded attributes its qualifier (the field without the wildcard)
// the qualifier will be unresolved and later used in the error message presented to the user
// qualifier is unknown (e.g. unsupported type), bail out early
// now use the resolved 'qualifier' to match
// filter the attributes that match based on their path
// use the path only to match non-compound types
// generate a new (right) logical plan with different IDs for all conflicting attributes
// Allow ordinal positioning in order/sort by (quite useful when dealing with aggs)
// Note that ordering starts at 1
// report error
// It is valid to filter (including HAVING) or sort by attributes not present in the SELECT clause.
// This rule pushed down the attributes for them to be resolved then projects them away.
// As such this rule is an extended version of ResolveRefs
// if there are any references in the output
// try and resolve them to the source in order to compare the source expressions
// e.g. ORDER BY a + 1
//      \ SELECT a + 1
// a + 1 in SELECT is actually Alias("a + 1", a + 1) and translates to ReferenceAttribute
// in the output. However it won't match the unnamed a + 1 despite being the same expression
// so explicitly compare the source
// if there's a match, remove the item from the reference stream
// collect aliases
// found a match, no need to resolve it further
// so filter it out
// Add missing attributes but project them away afterwards
// resolution failed and the failed expressions might contain resolution information so copy it over
// transform the orders with the failed information
// everything worked
// Again, add missing attributes and project them away
// resolution failed and the failed expressions might contain resolution information so copy it over
// transform the orders with the failed information
// look at unary trees but ignore subqueries
// no more attributes, bail out
// missing attributes can only be grouping expressions
// however take into account aliased groups
// SELECT x AS i ... GROUP BY i
// but we can't add an agg if the group is missing
// pass failure information to help the verifier
// propagation failed, return original plan
// LeafPlans are tables and BinaryPlans are joins so pushing can only happen on unary
//
// Resolve aliases defined in SELECT that are referred inside the WHERE clause:
// SELECT int AS i FROM t WHERE i > 10
//
// As such, identify all project and aggregates that have a Filter child
// and look at any resolved aliases that match and replace them.
// TODO: look into Generator for significant terms, etc..
//
// Replace a project with aggregation into an aggregation
//
//
// Detect implicit grouping with filtering and convert them into aggregates.
// SELECT 1 FROM x HAVING COUNT(*) > 0
// is a filter followed by projection and fails as the engine does not
// understand it is an implicit grouping.
//
// no literal or aggregates - it's a 'regular' projection
// folding might not work (it might wait for the optimizer)
// so check whether any column is referenced
//
// Handle aggs in HAVING. To help folding any aggs not found in Aggregation
// will be pushed down to the Aggregate and then projected. This also simplifies the Verifier's job.
//
// HAVING = Filter followed by an Agg
// the condition might contain an agg (AVG(salary)) that could have been resolved
// (salary cannot be pushed down to Aggregate since there's no grouping and thus the function wasn't resolved either)
// so try resolving the condition in one go through a 'dummy' aggregate
// that's why try to resolve the condition
// if it got resolved
// replace the condition with the resolved one
// else bail out
// preserve old output
//
// Handle aggs in ORDER BY. To help folding any aggs not found in Aggregation
// will be pushed down to the Aggregate and then projected. This also simplifies the Verifier's job.
// Similar to Having however using a different matching pattern since HAVING is always Filter with Agg,
// while an OrderBy can have multiple intermediate nodes (Filter,Project, etc...)
//
// 1. collect aggs inside an order by
// 2. find first Aggregate child and update it
// agg already contains all aggs
// save aggregates
// if the plan was updated, project the initial aggregates
// BinaryOperations are ignored as they are pushed down to ES
// and casting (and thus Aliasing when folding) gets in the way
// aliases inside GROUP BY are irrelevant so remove all of them
// however aggregations are important (ultimately a projection)
// transformUp (post-order) - that is first children and then the node
// but with a twist; only if the tree is not resolved or analyzed
/*
// Since the pre-analyzer only inspect (and does NOT transform) the tree
// it is not built as a rule executor.
// Further more it applies 'the rules' only once and needs to return some
// state back.
// mark plan as preAnalyzed (if it were marked, there would be no analysis)
/*
/*
/*
/**
// start bottom-up
// if the children are unresolved, so will this node; counting it will only add noise
//
// First handle usual suspects
//
// then take a look at the expressions
// everything is fine, skip expression
// we're only interested in the children
// again the usual suspects
// handle Attributes different to provide more context
// only work out the synonyms for raw unresolved attributes
// add only primitives (object types would only result in another error)
// type resolution
// Concrete verifications
// if there are no (major) unresolved failures, do more in-depth analysis
// collect Attribute sources
// only Aliases are interesting since these are the only ones that hide expressions
// FieldAttribute for example are self replicating.
// for filtering out duplicated errors
// if the children are unresolved, so will this node; counting it will only add noise
// everything checks out
// mark the plan as analyzed
// gather metrics
/**
// check whether an orderBy failed or if it occurs on a non-key
// aggregates are allowed
// take aliases declared inside the aggregates which point to the grouping (but are not included in there)
// to correlate them to the order
// Make sure you can apply functions on top of the grouped by expressions in the ORDER BY:
// e.g.: if "GROUP BY f2(f1(field))" you can "ORDER BY f4(f3(f2(f1(field))))"
//
// Also, make sure to compare attributes directly
// nothing matched, cannot group by it
// get the location of the first missing expression as the order by might be on a different line
// variation of checkGroupMatch customized for HAVING, which requires just aggregations
// resolve FunctionAttribute to backing functions
// scalar functions can be a binary tree
// first test the function against the grouping
// and if that fails, start unpacking hoping to find matches
// unwrap function to find the base
// Score can't be used in having
// First and Last cannot be used in having
// Min & Max on a Keyword field will be translated to First & Last respectively
// skip literals / foldable
// skip aggs (allowed to refer to non-group columns)
// left without leaves which have to match; that's a failure since everything should be based on an agg
// The grouping can not be an aggregate function or an inexact field (e.g. text without a keyword)
// TIME data type is not allowed for grouping key
// https://github.com/elastic/elasticsearch/issues/40639
// check whether plain columns specified in an agg are mentioned in the group-by
// The grouping can not be an aggregate function
// The agg can be:
// 1. plain column - in which case, there should be an equivalent in groupings
// 2. aggregate over non-grouped column
// 3. scalar function on top of 1 and/or 2. the function needs unfolding to make sure
//    the 'source' is valid.
// Note that grouping can be done by a function (GROUP BY YEAR(date)) which means date
// cannot be used as a plain column, only YEAR(date) or aggs(?) on top of it
// 1:1 match
// resolve FunctionAttribute to backing functions
// scalar functions can be a binary tree
// first test the function against the grouping
// and if that fails, start unpacking hoping to find matches
// found group for the expression
// unwrap function to find the base
// Score can't be an aggregate function
// skip literals / foldable
// skip aggs (allowed to refer to non-group columns)
// TODO: need to check whether it's possible to agg on a field used inside a scalar for grouping
// left without leaves which have to match; if not there's a failure
// make sure to match directly on the expression and not on the tree
// (since otherwise exp might match the function argument which would be incorrect)
// check if the query has a grouping function (Histogram) but no GROUP BY
// if it does have a GROUP BY, check if the groupings contain the grouping functions (Histograms)
// Make sure that SCORE is only used in "top level" functions
// nested fields shouldn't be used in aggregates or having (yet)
// check in having
/**
// geo shape fields shouldn't be used in aggregates or having (yet)
// geo shape fields shouldn't be used in order by clauses
// check only exact fields are used inside PIVOTing
// if that is not the case, no need to do further validation since the declaration is fundamentally wrong
// check values
// check all values are foldable
// and that their type is compatible with that of the column
// check aggregate function, in particular formulas that might hide literals or scalars
// skip aggregate functions
// check mixture of Agg and column (wrapped in scalar)
/*
/**
// version check first
// configuration settings
/*
/**
// Base64 uses this encoding instead of UTF-8
/*
// try to provide a better resolution of what failed
/*
/**
// there are some results
// retry
// no results
// if there are no buckets but a next page, go fetch it instead of sending an empty response to the client
// update after-key with the new value
/**
/**
/*
/**
// page size
// if the computed limit is zero, or the size is zero it means either there's nothing left or the limit has been reached
// note that a composite agg might be valid but return zero groups (since these can be filtered with HAVING/bucket selector)
// however the Querier takes care of that and keeps making requests until either the query is invalid or at least one response
// is returned.
/*
/*
// the last page contains no data, handle that to avoid NPEs and such
// consume buckets until all pivot columns are initialized or the next grouping starts
// to determine a group, find all group-by extractors (CompositeKeyExtractor)
// extract their values and keep iterating through the buckets as long as the result is the same
// does the bucket below to the same group?
// done computing row
// be sure to remember the last consumed group before changing to the new one
// save the data
// create a new row
// rerun the bucket through all the extractors but update only the non-null components
// since the pivot extractors will react only when encountering the matching group
// check the last group using the following:
// a. limit has been reached, the rest of the data is ignored.
// b. the last key has been sent before (it's the last page)
// c. all the values are initialized (there might be another page but no need to ask for the group again)
// d. or no data was added (typically because there's a null value such as the group)
// otherwise we can't tell whether it's complete or not
// so discard the last group and ask for it on the next page
// lastly initialize the size and remainingData
// compare the equality of two composite key WITHOUT the last group
// this method relies on the internal map implementation which preserves the key position
// hence why the comparison happens against the current key (not the previous one which might
// have a different order due to serialization)
// there's no other key, it's the same group
/*
// TODO: add retry/back-off
// prepare the request
// set query timeout
// always track total hits accurately
/**
// keep the top N entries.
// schema is set on the first page (as the rest don't hold the schema anymore)
// 1. consume all pages received
// 1a. trigger a next call if there's still data
// trigger a next call
// make sure to bail out afterwards as we'll get called by a different thread
// no more data available, the last thread sends the response
// 2. send the in-memory view to the client
// if the queue overflows and no limit was specified, throw an error
/**
// can happen when only a count is requested which is derived from the response
/**
// create response extractors for the first time
// wrap only agg inputs
/**
// create response extractors for the first time
// collect hitNames
/**
// TODO: need to handle rejections plus check failures (shard size, etc...)
// clean-up the scroll in case of exception
// in case of failure, report the initial exception instead of the one resulting from cleaning the scroll
// compare row based on the received attribute sort
// if a sort item is not in the list, it is assumed the sorting happened in ES
// and the results are left as is (by using the row ordering), otherwise it is sorted based on the given criteria.
//
// Take for example ORDER BY a, x, b, y
// a, b - are sorted in ES
// x, y - need to be sorted client-side
// sorting on x kicks in, only if the values for a are equal.
// thanks to @jpountz for the row ordering idea as a way to preserve ordering
// if things are equals, move to the next comparator
// no comparator means the existing order needs to be preserved
// check the values - if they are equal move to the next comparator
// otherwise return the row order
// everything is equal, fall-back to the row order
/*
// find the nth set bit
/*
/**
/*
/*
/**
/*
// clean-up
// no more data, let's clean the scroll before continuing
// no-hits
/*
/**
// Since the results might contain nested docs, the iteration is similar to that of Aggregation
// namely it discovers the nested docs and then, for iteration, increments the deepest level first
// and eventually carries that over to the top level
// page size
/* SearchResponse can contain a null scroll when you start a
// compute remaining limit (only if the limit is specified - that is, positive).
// if the computed limit is zero, or the size is zero it means either there's nothing left or the limit has been reached
// or the scroll has ended
// TODO: add support for multi-nested doc
// multiple inner_hits results sections can match the same nested documents, thus we eliminate the duplicates by
// using the offset as the "deduplicator" in a HashMap
// Then sort the resulting List based on the offset of the same inner hit. Each inner_hit match will have an offset value,
// relative to its location in the _source
// increment last row
// then check size
// reset the current branch
// bump the parent - if it's too big it, the loop will restart again from that position
// restart the loop
// TODO: improve this for multi-nested responses
/*
// add the source
// Iterate through all the columns requested, collecting the fields that
// need to be retrieved from the result documents
// NB: the sortBuilder takes care of eliminating duplicates
// add the aggs (if present)
// set page size
// now take into account the the minimum page (if set)
// that is, return the multiple of the minimum page size closer to the set size
// limit the composite aggs only for non-local sorting
// Aggs can't be sorted using search sorting. That sorting is handled elsewhere.
// if no sorting is specified, use the _doc one
// sorting only works on not-analyzed fields - look for a multi-field replacement
// if only aggs are needed, don't retrieve any docs and remove scoring
// disable source fetching (only doc values are used)
/*
/**
/**
// get the composite value
/*
/**
/**
// let's make sure first that we are not dealing with an geo_point represented as an array
// we expect the point in [lon lat] or [lon lat alt] formats
/*
//TODO: need to investigate when this can be not-null
//if (innerKey == null) {
//    throw new SqlIllegalArgumentException("Invalid innerKey {} specified for aggregation {}", innerKey, name);
//}
// COUNT(expr) and COUNT(ALL expr) uses this type of aggregation to account for non-null values only
/**
// Stats & ExtendedStats
/*
/*
/**
/**
// Nothing to write
/*
/**
/*
/**
/*
/*
/*
/*
/*
/**
/*
// Aggregate functions
// Statistics
// histogram
// Scalar functions
// Conditional
// Date
// Math
// SQL and ODBC require MOD as a _function_
// String
// DataType conversion
// Scalar "meta" functions
// Geo Functions
// Special
/*
/*
/**
/*
// marker type for compound aggregates, that is aggregate that provide multiple values (like Stats or Matrix)
// and thus cannot be used directly in SQL and are mainly for internal use
/*
/*
/*
/**
/*
/*
/**
/*
/*
/*
/**
/*
/*
/**
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
/*
/*
/**
/*
/*
// interval must be Literal interval
/*
/*
/*
/*
/*
/**
// base
// datetime
// math
// string
// geo
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
/*
// Truncate first to minutes (ignore any seconds and sub-seconds fields)
/*
/*
/**
/*
/*
/*
/**
/*
/*
// used for applying ranges
/*
/**
/**
/*
/*
/*
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/*
/*
// for the moment we'll use no specific Locale, but we might consider introducing a Locale parameter, just like the timeZone one
/*
/*
/*
// by ISO 8601 standard, Monday is the first day of the week and has the value 1
// non-ISO 8601 standard considers Sunday as the first day of the week and value 1
/*
/*
/*
/**
/*
/*
/*
/*
/*
/*
/**
/*
/**
/*
/*
/**
/*
/**
/*
/*
/*
/*
/**
/*
/**
/*
/*
/**
/*
/**
/*
/**
/*
/**
//TODO change this to use _source instead of the exact form (aka field.keyword for geo shape fields)
// basically, transform the script to InternalSqlScriptUtils.[function_name](other_function_or_field_name)
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/*
/*
/**
/*
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
//en.wikipedia.org/wiki/Degree_(angle)">degrees</a>.
/*
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/*
// fallback to integer
//fallback to generic double
/**
/*
/*
/*
/**
//en.wikipedia.org/wiki/Radian">radians</a>.
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
//TODO investigate if a data type Long (BIGINT) wouldn't be more appropriate here
/*
/**
/*
/**
/*
/**
/*
/*
/**
/*
/**
// basically, transform the script to InternalSqlScriptUtils.[function_name](function_or_field1, function_or_field2,...)
/*
/*
/*
/**
/*
/**
/*
/**
/*
/**
// basically, transform the script to InternalSqlScriptUtils.[function_name](function_or_field1, function_or_field2,...)
/*
/*
/*
/**
/*
/**
/*
/**
/*
/**
/*
/**
// basically, transform the script to InternalSqlScriptUtils.[function_name](function_or_field1, function_or_field2,...)
/*
/*
/*
/**
/*
/**
/*
/**
/*
/**
/**
/**
/*
/**
/*
/**
// basically, transform the script to InternalSqlScriptUtils.[function_name](function_or_field1, function_or_field2,...)
/*
/*
// SQL is 1-based when it comes to string manipulation
/*
/**
/*
//TODO change this to use _source instead of the exact form (aka field.keyword for text fields)
/*
/**
/*
/*
/**
//
// Conditional
//
//
// Regex
//
// TODO: this needs to be improved to avoid creating the pattern on every call
//
// Math
//
//
// Date/Time functions
//
//
// String functions
//
// processes doc value as a geometry
// TODO: Add support for geo_shapes when it is there
//
// Casting
//
// we call asDateTime here to make sure we handle JodaCompatibleZonedDateTime properly,
// since casting works for ZonedDateTime objects only
/*
/*
/**
/*
// combining
// folding
// boolean
// needs to occur before BinaryComparison combinations (see class)
// prune/elimination
// order by alignment of the aggs
// 1. add the IN filter
// everything should have resolved to an alias
// fallback - should not happen
// 2. preserve the PIVOT
//
// Replace any reference attribute with its source, if it does not affect the result.
// This avoid ulterior look-ups between attributes and its source across nodes, which is
// problematic when doing script translation.
//
// collect aliases
// non attribute defining plans get their references removed
// everything was eliminated, the grouping
// check whether OrderBy relies on nested fields which are not used higher up
// resolve function references (that maybe hiding the target)
// collect Attribute sources
// only Aliases are interesting since these are the only ones that hide expressions
// FieldAttribute for example are self replicating.
// track the direct parents
// traverse the tree since the field might be wrapped in a function
// no nested fields in sort
// count the nested parents (if any) inside the parents
// traverse the tree since the field might be wrapped in a function
// projection has no nested field references, remove any nested orders
// remove orders that are not ancestors of the nested projections
// no orders left, eliminate it all-together
// everything was eliminated, the order isn't needed anymore
// if the first found aggregate has no grouping, there's no need to do ordering
// take into account
/**
// put the items in reverse order so the iteration happens back to front
// if the first found aggregate has no grouping, there's no need to do ordering
// take into account
// Check if the groupings (a, y) match the orderings (b, x) through the aggregates' aliases (x, y)
// e.g. SELECT a AS x, b AS y ... GROUP BY a, y ORDER BY b, x
// move grouping in front
// NB: it is important to start replacing casts from the bottom to properly replace aliases
// eliminate redundant casts
// eliminate lower project but first replace the aliases in the upper one
// if the pivot custom columns are not used, convert the project + pivot into a GROUP BY/Aggregate
// TODO: add rule for combining Agg/Pivot with underlying project
// normally only the upper projections should survive but since the lower list might have aliases definitions
// that might be reused by the upper one, these need to be replaced.
// for example an alias defined in the lower list might be referred in the upper - without replacing it the alias becomes invalid
//TODO: this need rewriting when moving functions of NamedExpression
// collect aliases in the lower list
// replace any matching attribute with a lower alias (if there's a match)
// but clean-up non-top aliases at the end
// replace attributes of foldable expressions with the foldable trees
// SELECT 5 a, 3 + 2 b ... WHERE a < 10 ORDER BY b
// find aliases of all projections
// propagate folding up to unary nodes
// anything higher and the propagate stops
// might need to implement an Attribute map
// finally clean-up aliases
// exclude any nulls found
// For Coalesce find the first non-null foldable child (if any) and break early
// Remove or foldable conditions that fold to FALSE
// Stop at the 1st foldable condition that folds to TRUE
//
// common factor extraction -> (a || b) && (a || c) => a || (b && c)
//
// (a || b || c || ... ) && (a || b) => (a || b)
// (a || b || c || ... ) && (a || b || d || ... ) => ((c || ...) && (d || ...)) || a || b
//
// common factor extraction -> (a && b) || (a && c) => a && (b || c)
//
// (a || b || c || ... ) && (a || b) => (a || b)
// (a || b || c || ... ) && (a || b || d || ... ) => ((c || ...) && (d || ...)) || a || b
// TODO: eliminate conjunction/disjunction
// true for equality
// false for equality
/**
// combine conjunction
// Only equalities, not-equalities and inequalities with a foldable .right are extracted separately;
// the others go into the general 'exps'.
// equals on different values evaluate to FALSE
// var cannot be equal to two different values at the same time
// check
// if equals is outside the interval, evaluate the whole expression to FALSE
// eq outside the lower boundary
// eq matches the boundary but should not be included
// eq outside the upper boundary
// eq matches the boundary but should not be included
// it's in the range and thus, remove it
// evaluate all NotEquals against the Equal
// clashing and conflicting: a = 1 AND a != 1
// clashing and redundant: a = 1 AND a != 2
// evaluate all inequalities against the Equal
// a = 2 AND a </<= ?
// a = 2 AND a < 2
// a = 2 AND a </<= 1
// a = 2 AND a >/>= ?
// a = 2 AND a > 2
// a = 2 AND a >/>= 3
// combine disjunction:
// a = 2 OR a > 3 -> nop; a = 2 OR a > 1 -> a > 1
// a = 2 OR a < 3 -> a < 3; a = 2 OR a < 1 -> nop
// a = 2 OR 3 < a < 5 -> nop; a = 2 OR 1 < a < 3 -> 1 < a < 3; a = 2 OR 0 < a < 1 -> nop
// a = 2 OR a != 2 -> TRUE; a = 2 OR a = 5 -> nop; a = 2 OR a != 5 -> a != 5
// foldable right term Equals
// foldable right term NotEquals
// foldable right term (=limit) BinaryComparision
// split expressions by type
// has the expression been modified?
// evaluate the impact of each Equal over the different types of Expressions
// Equals OR NotEquals
// a = 2 OR a != ? -> ...
// a = 2 OR a != 2 -> TRUE
// a = 2 OR a != 5 -> a != 5
// Equals OR Range
// might modify list, so use index loop
// a = 2 OR 2 < a < ? -> 2 <= a < ?
// else : a = 2 OR 2 <= a < ? -> 2 <= a < ?
// update range with lower equality instead or simply superfluous
// a = 2 OR ? < a < 2 -> ? < a <= 2
// else : a = 2 OR ? < a <= 2 -> ? < a <= 2
// update range with upper equality instead
// a = 2 OR 1 < a < 3
// equality is superfluous
// Equals OR Inequality
// a = 1 OR a > 2 -> nop
// a = 2 OR a > 2 -> a >= 2
// else (0 < comp || bc instanceof GreaterThanOrEqual) :
// a = 3 OR a > 2 -> a > 2; a = 2 OR a => 2 -> a => 2
// update range with equality instead or simply superfluous
// a = 2 OR a < 1 -> nop
// a = 2 OR a < 2 -> a <= 2
// else (comp < 0 || bc instanceof LessThanOrEqual) : a = 2 OR a < 3 -> a < 3; a = 2 OR a <= 2 -> a <= 2
// update range with equality instead or simply superfluous
// combine conjunction
// Ranges need to show up before BinaryComparisons in list, to allow the latter be optimized away into a Range, if possible
// keep ranges' order
// keep non-ranges' order
// finally try combining any left BinaryComparisons into possible Ranges
// this could be a different rule but it's clearer here wrt the order of comparisons
// >/>= AND </<=
// </<= AND >/>=
// combine disjunction
// NB: the loop modifies the list (hence why the int is used)
// make sure the comparison was done
// boundary equality (useful to differentiate whether a range is included or not)
// and thus whether it should be preserved or ignored
// evaluate lower
// values are comparable
// boundary equality
// AND
// (2 < a < 3) AND (1 < a < 3) -> (2 < a < 3)
// (2 < a < 3) AND (2 <= a < 3) -> (2 < a < 3)
// OR
// (1 < a < 3) OR (2 < a < 3) -> (1 < a < 3)
// (2 <= a < 3) OR (2 < a < 3) -> (2 <= a < 3)
// evaluate upper
// values are comparable
// boundary equality
// AND
// (1 < a < 2) AND (1 < a < 3) -> (1 < a < 2)
// (1 < a < 2) AND (1 < a <= 2) -> (1 < a < 2)
// OR
// (1 < a < 3) OR (1 < a < 2) -> (1 < a < 3)
// (1 < a <= 3) OR (1 < a < 3) -> (2 < a < 3)
// AND - at least one of lower or upper
// can tighten range
// range was comparable
// OR - needs both upper and lower to loosen range
// can loosen range
// if the range in included, no need to add it
// NB: the loop modifies the list (hence why the int is used)
// 2 < a AND (2 <= a < 3) -> 2 < a < 3
// 2 < a AND (1 < a < 3) -> 2 < a < 3
// found a match
// a < 2 AND (1 < a <= 2) -> 1 < a < 2
// a < 2 AND (1 < a < 3) -> 1 < a < 2
// found a match
/**
// NB: the loop modifies the list (hence why the int is used)
// skip if cannot evaluate
// if bc is a higher/lower value or gte vs gt, use it instead
// AND
// a > 3 AND a > 2 -> a > 3
// a > 2 AND a >= 2 -> a > 2
// OR
// a > 2 OR a > 3 -> a > 2
// a >= 2 OR a > 2 -> a >= 2
// found a match
// if bc is a lower/higher value or lte vs lt, use it instead
// AND
// a < 2 AND a < 3 -> a < 2
// a < 2 AND a <= 2 -> a < 2
// OR
// a < 2 OR a < 3 -> a < 3
// a <= 2 OR a < 2 -> a <= 2
// found a match
// minimal reuse of the same matrix stat object
// minimal reuse of the same matrix stat object
// if the stat has at least two different functions for it, promote it as stat
// also keep the promoted function around for reuse
// 1. first check whether there are at least 2 aggs for the same fields so that there can be a promotion
// no promotions found - skip
// start promotion
// 2. promote aggs to InnerAggs
// count the extended stats
// then if there's a match, replace the stat inside the InnerAgg
// percentile per field/expression
// count gather the percents for each field
// create a Percentile agg for each field (and its associated percents)
// percentile per field/expression
// count gather the percents for each field
// create a PercentileRanks agg for each field (and its associated values)
// not everything is foldable, bail out early
// not everything is foldable, bail-out early
/**
/*
/**
/**
/**
// remove leading and trailing ' for strings and also eliminate escaped single quotes
/*
/**
/*
// extension of ANTLR that does the upper-casing once for the whole stream
// the ugly part is that it has to duplicate LA method
// This approach is the official solution from the ANTLR authors
// in that it's both faster and easier than having a dedicated lexer
// see https://github.com/antlr/antlr4/issues/1002
// this part is copied from ANTLRInputStream
// undefined
/*
// check special ODBC wildcard case
// treat % as null
// https://docs.microsoft.com/en-us/sql/odbc/reference/develop-app/value-list-arguments
// special case for legacy apps (like msquery) that always asks for 'TABLE'
// which we manually map to all concrete tables supported
// if the ODBC enumeration is specified, skip validation
/*
// no predicate, quick exit
// shortcut to avoid double negation later on (since there's no IsNull (missing in ES is a negated exists))
// shouldn't happen but adding validation in case the string parsing gets wonky
// these chars already have a meaning
// lastly validate that escape chars (if present) are followed by special chars
//
// Arithmetic
//
// Minus already processed together with literal number
//
// Full-text search predicates
//
//
// Functions template
//
// maps CURRENT_XXX to its respective function e.g: CURRENT_TIMESTAMP()
// since the functions need access to the Configuration, the parser only registers the definition and not the actual function
//
// Logical constructs
//
//
// Literal
//
// only YEAR TO MONTH or DAY TO HOUR/MINUTE/SECOND are valid declaration
// negation outside the interval - use xor
// negation inside the interval
// expect numbers for now
// as the number parsing handles the -, there's no need to look at that
// try to downsize to int if possible (since that's the most common type)
// no conversion is required for null values
// no conversion is required if the value is already have correct type
// otherwise we need to make sure that xcontent-serialized value is converted to the correct type
/**
// parse yyyy-MM-dd
// parse HH:mm:ss
// parse yyyy-mm-dd hh:mm:ss(.f...)
// basic validation
// needs to be format nnnnnnnn-nnnn-nnnn-nnnn-nnnnnnnnnnnn
// since the length is fixed, the validation happens on absolute values
// not pretty but it's fast and doesn't create any extra objects
// skip separators
/**
/**
// Skip parentheses, e.g.: - (( (2.15) ) )
/*
/*
// unwrap query (and validate while at it)
// return WITH
// add WHERE
// GROUP BY
// HAVING
// if there are multiple FROM clauses, convert each pair in a inner join
// PIVOT
// check if there are multiple join clauses. ANTLR produces a right nested tree with the left join clause
// at the top. However the fields previously references might be used in the following clauses.
// As such, swap/reverse the tree.
// We would return this if we actually supported JOINs, but we don't yet.
// new Join(source(ctx), left, plan(ctx.right), type, condition);
/*
// ANTLR GENERATED CODE: DO NOT EDIT
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// ANTLR GENERATED CODE: DO NOT EDIT
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// ANTLR GENERATED CODE: DO NOT EDIT
/**
//\3\2\62;\3"+
// ANTLR GENERATED CODE: DO NOT EDIT
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
// ANTLR GENERATED CODE: DO NOT EDIT
/**
// ANTLR GENERATED CODE: DO NOT EDIT
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/**
/**
/**
/**
// when debugging, use the exact prediction mode (needed for diagnostics as well)
// Remove quotes
// tree cannot be modified during rule enter/exit _unless_ it's a terminal node
// replace nonReserved words with IDENT tokens
/**
/*
/*
// OUTER
// OUTER
// OUTER
// right side can be null
// left side can be null
// both sides can be null
// INNER
// resolve the join if
// - the children are resolved
// - there are no conflicts in output
// - the condition (if present) is resolved to a boolean
/*
/*
// derived properties
// resolve the grouping set ASAP so it doesn't get re-resolved after analysis (since the aliasing information has been removed)
// grouping can happen only on "primitive" fields, thus exclude multi-fields or nested docs
// the verifier enforces this rule so it does not catch folks by surprise
// make sure to have the column as the last entry (helps with translation) so substract it
// for multiple args, concat the function and the value
//FIXME: the value attributes are reused and thus will clash - new ids need to be created
// for each attribute, associate its value
// take into account while iterating that attributes are a multiplication of actual values
// everything should have resolved to an alias
// fallback - verifier should prevent this
// Since pivot creates its own columns (and thus aliases)
// remember the backing expressions inside a dedicated aliases map
// make sure to initialize all expressions
/*
/*
/*
/**
/**
/*
// for each batch
// for each batch
/*
// to avoid duplicating code, the type/verification filtering happens inside the listeners instead of outside using a CASE
// verification is on, exceptions can be thrown
// Type.All
// check errors manually to see how far the plans work out
// no analysis failure, can move on
// mapped failed
// cannot continue
/*
// show only fields that exist in ES
/*
/*
/*
// to avoid redundancy, indicate whether frozen fields are required by specifying the type
/*
/**
// https://github.com/elastic/elasticsearch/issues/35376
// ODBC expects some fields as SHORT while JDBC as Integer
// which causes conversion issues and CCE
// JDBC specific
// bail-out early if the catalog is present but differs
// save original index name (as the pattern can contain special chars)
// special case for '%' (translated to *)
// otherwise use a merged mapping
// populate the data only when a target is found
// JDBC is 1-based so we start with 1 here
// skip the nested, object and unsupported types
// schema is not supported
// TODO: is the buffer_length correct?
// no DECIMAL support
// everything is nullable
// no remarks
// no column def
// SQL_DATA_TYPE apparently needs to be same as DATA_TYPE except for datetime and interval data types
// SQL_DATETIME_SUB ?
// char octet length
// position
// skip nested fields
/*
// flag indicating whether tables are reported as `TABLE` or `BASE TABLE`
// first check if where dealing with ODBC enumeration
// namely one param specified with '%', everything else empty string
// https://docs.microsoft.com/en-us/sql/odbc/reference/syntax/sqltables-function?view=ssdt-18vs2017#comments
// catalog enumeration
// enumerate only if pattern is "" and no types are specified (types is null)
// send only the cluster, everything else null
// enumerate types
// if no types are specified (the parser takes care of the % case)
// empty string for catalog
// empty string for table like and no index specified
// send only the types, everything else is made of empty strings
// NB: since the types are sent in SQL, frozen doesn't have to be taken into account since
// it's just another BASE TABLE
// no enumeration pattern found, list actual tables
// if the catalog doesn't match, don't return any results
// initialize types for name resolution
// sort by type (which might be legacy), then by name
/*
/**
//docs.microsoft.com/en-us/sql/odbc/reference/syntax/sqlgettypeinfo-function">SQLGetTypeInfo</a>
// ODBC
// sort by SQL int type (that's what the JDBC/ODBC specs want) followed by name
// don't be specific on nullable
// all strings are case-sensitive
// everything is searchable,
// only numerics are signed
//no fixed precision scale SQL_FALSE
// not auto-incremented
// SQL_DATA_TYPE - ODBC wants this to be not null
// Radix
/*
/*
/*
/*
/*
// indicates whether the filter is regular or agg-based (HAVING xxx)
// gets setup automatically and then copied over during cloning
/*
/*
/*
/*
/*
/**
/*
/*
/*
/*
// this is mainly a marker interface to validate a plan before being executed
/*
/*
/*
// analysis and optimizations have converted the grouping into actual attributes
// TODO: Translate With in a subplan
//TODO: pick up on nested/parent-child docs
// 2. Hash?
// 3. Cartesian
// 3. Fallback to nested loop
/*
// first, map the logical plan
// second, pack it up
// verify the mapped plan
/*
/*
/**
// track all aliases (to determine their reference later on)
// track scalar pipelines
// TODO: remove exceptions from the Folder
// if there's at least one agg in the tree
// group found - finding the dedicated agg
// TODO: when dealing with expressions inside Aggregation, make sure to extract the field
// return matching group or the tail (last group)
/**
// change analyzed to non non-analyzed attributes
// handle functions
// dates are handled differently because of date histograms
// use scripting for functions
// all other scalar functions become a script
// histogram
// date histogram
// interval of exactly 1 year
// When the histogram is `INTERVAL '1' YEAR`, the interval used in the ES date_histogram will be
// a calendar_interval with value "1y". All other intervals will be fixed_intervals expressed in ms.
// typical interval
// When the histogram in SQL is applied on DATE type instead of DATETIME, the interval
// specified is truncated to the multiple of a day. If the interval specified is less
// than 1 day, then the interval used will be `INTERVAL '1' DAY`.
// numeric histogram
// bumped into into an invalid function (which should be caught by the verifier)
// catch corner-case
// track aliases defined in the SELECT and used inside GROUP BY
// SELECT x AS a ... GROUP BY a
// build the group aggregation
// NB: any reference in grouping is already "optimized" by its source so there's no need to look for aliases
// tracker for compound aggs seen in a group
// followed by actual aggregates
// unwrap alias (since we support aliases declared inside SELECTs to be used by the GROUP BY)
// An alias can point to :
// - field
//   SELECT emp_no AS e ... GROUP BY e
// - a function
//   SELECT YEAR(hire_date) ... GROUP BY YEAR(hire_date)
// - an agg function over the grouped field
//   SELECT COUNT(*), AVG(salary) ... GROUP BY salary;
// - a scalar function, which can be applied on a column or aggregate and can require one or multiple inputs
//   SELECT SIN(emp_no) ... GROUP BY emp_no
//   SELECT CAST(YEAR(hire_date)) ... GROUP BY YEAR(hire_date)
//   SELECT CAST(AVG(salary)) ... GROUP BY salary
//   SELECT AVG(salary) + SIN(MIN(salary)) ... GROUP BY salary
// unwrap aliases since it's the children we are interested in
// literal
// look at functions
//
// look first for scalar functions which might wrap the actual grouped target
// (e.g.
// CAST(field) GROUP BY field or
// ABS(YEAR(field)) GROUP BY YEAR(field) or
// ABS(AVG(salary)) ... GROUP BY salary
// )
// traverse the pipe to find the mandatory grouping expression
// bail out if the def is resolved
// get the backing expression and check if it belongs to a agg group or whether it's
// an expression in the first place
// is there a group (aggregation) for this expression ?
// a scalar function can be used only if has already been mentioned for grouping
// (otherwise it is the opposite of grouping)
// normally this case should be caught by the Verifier
// found match for expression; if it's an attribute or scalar, end the processing chain with
// the reference to the backing agg
/*
// or found an aggregate expression (which has to work on an attribute used for grouping)
// (can happen when dealing with a root group)
// not an aggregate and no matching - go to a higher node (likely a function YEAR(birth_date))
// add the computed column
// apply the same logic above (for function inputs) to non-scalar functions with small variations:
//  instead of adding things as input, add them as full blown column
// is there a group (aggregation) for this expression ?
// attributes can only refer to declared groups
// handle histogram
// handle literal
// fallback to regular agg functions
// the only thing left is agg function
// make sure to add the inner id (to handle compound aggs)
// not a Function or literal, means its has to be a field or field expression
// fallback
// handle count as a special case agg
// COUNT(*) or COUNT(<literal>)
// if the count points to the total track hits, enable accurate count retrieval
// COUNT(<field_name>)
// the only variant left - COUNT(DISTINCT) - will be covered by the else branch below as it maps to an aggregation
// the compound agg hasn't been seen before so initialize it
// add the agg (without any reference)
// FIXME: concern leak - hack around MatrixAgg which is not
// generalized (afaik)
// check whether sorting is on an group (and thus nested agg) or field
// if it's a reference, get the target expression
// TODO: might need to validate whether the target field or group actually exist
// check whether the lookup matches a group
// else it's a leafAgg
// scalar functions typically require script ordering
// is there an expression to order by?
// ignore constant
// nope, use scripted sorting
// score
// field
// agg function
// unknown
// references (aka aggs) are in place
// after all attributes have been resolved
// replace the aggregate extractors with pivot specific extractors
// these require a reference to the pivoting column in order to compare the value
// due to the Pivot structure - the column is the last entry in the grouping set
// pivot grouping
//
// local
//
// local exec currently means empty or one entry so limit can't really be applied
// rule for folding physical plans together
/*
// Agg filter / Function or Agg association
// COUNT(DISTINCT) uses cardinality aggregation which works on exact values (not changed by analyzers or normalizers)
// use the `keyword` version of the field, if there is one
// TODO: see whether escaping is needed
// assume the Optimizer properly orders the predicates to ease the translation
//
// Agg context means HAVING -> PipelineAggs
//
// for a date constant comparison, we need to use a format for the date, to make sure that the format is the same
// no matter the timezone provided by the user
// RangeQueryBuilder accepts an Object as its parameter, but it will call .toString() on the ZonedDateTime instance
// which can have a slightly different format depending on the ZoneId used to create the ZonedDateTime
// Since RangeQueryBuilder can handle date as String as well, we'll format it as String and provide the format as well.
// Possible geo optimization
// Special case for ST_Distance translatable into geo_distance query
// equality should always be against an exact match
// (which is important for strings)
// dates equality uses a range query because it's the one that has a "format" parameter
// assume the Optimizer properly orders the predicates to ease the translation
//
// Agg context means HAVING -> PipelineAggs
//
// equality should always be against an exact match (which is important for strings)
//
// Agg context means HAVING -> PipelineAggs
//
// for a date constant comparison, we need to use a format for the date, to make sure that the format is the same
// no matter the timezone provided by the user
// RangeQueryBuilder accepts an Object as its parameter, but it will call .toString() on the ZonedDateTime
// instance which can have a slightly different format depending on the ZoneId used to create the ZonedDateTime
// Since RangeQueryBuilder can handle date as String as well, we'll format it as String and provide the format.
//
// Agg translators
//
/*
// verify Pivot
/*
/*
/*
// enforce CBOR response for drivers and CLI (unless instructed differently through the config param)
/*".equals(accept)) {
// */* means "I don't care" which we should treat like not specifying the header
/* means "I don't care" which we should treat like not specifying the header
/*
// XContent branch
// TextFormat
/*
/*
/**
/*
/**
/**
/*
/*
// overridable by tests
/**
/*
/*
/**
/*
/*
/**
/**
// check if the cursor is already wrapped first
// if there are headers available, it means it's the first request
// so initialize the underlying formatter and wrap it in the cursor
// if there's a cursor, wrap the formatter in it
// format with header
// should be initialized (wrapped by the cursor)
// format without header
// if this code is reached, it means it's a next page without cursor wrapping
/**
//tools.ietf.org/html/rfc4180
//www.iana.org/assignments/media-types/text/csv
//www.w3.org/TR/sparql11-results-csv-tsv/
//LFCR
// header is a parameter specified by ; so try breaking it down
// only CR
// if the header is requested (and the column info is present - namely it's the first page) return the info
/**
/**
/**
// utility method for consuming a row.
/**
/**
/**
/*
/**
// keep wrapping the text formatter
/*
/*
/*
/**
// The configuration is always created however when dealing with the next page, only the timeouts are relevant
// the rest having default values (since the query is already created)
/*
/**
// the plan executor holds the metrics
/*
/**
/*
/**
/*
// make script null safe
/*
/**
// if there's a group, move everything under the composite agg
// first iterate to compute the sources
// maybe it's the default group agg ?
/*
/*
/*
/*
/*
/**
/*
/**
// For testing
/*
/**
// ASC is the default order of CompositeValueSource
// field based
/*
/**
/*
/**
/*
/*
/*
/*
/*
/*
/*
/*
/*
// TODO: look at keyed
/*
/*
/*
/*
// Sort missing values (NULLs) as last to get the first/last non-null value
/*
/*
/*
/*
/**
/*
/**
/*
/**
/*
/**
/*
// due to the way Elasticsearch aggs work
// promote the object to expect types so that the comparison works
/*
/**
// fields extracted from the response - not necessarily what the client sees
// for example in case of grouping or custom sorting, the response has extra columns
// that is filtered before getting to the client
// the list contains both the field extraction and its id (for custom sorting)
// aliases found in the tree
// pseudo functions (like count) - that are 'extracted' from other aggs
// scalar function processors - recorded as functions get folded;
// at scrolling, their inputs (leaves) get updated
// used when pivoting for retrieving at least one pivot row
// computed
// associate Attributes with aliased FieldAttributes (since they map directly to ES fields)
/**
// find the relevant column of each aggregate function
// assemble a comparator for it
/**
// find the column index
// if the index is already set there is a collision,
// so continue searching for the other tuple with the same id
//
// copy methods
//
//
// reference methods
//
// Only if the field is not an alias (in which case it will be taken out from docvalue_fields if it's isAggregatable()),
// go up the tree of parents until a non-object (and non-nested) type of field is found and use that specific parent
// as the field to extract data from, from _source. We do it like this because sub-fields are not in the _source, only
// the root field to which those sub-fields belong to, are. Instead of "text_field.keyword_subfield" for _source extraction,
// we use "text_field", because there is no source for "keyword_subfield".
/*
/* There is no query so we must add the nested query
// The query already has the nested field. Nothing to do.
/* The query doesn't have the nested field so we have to ask
/* It successfully added it so we can use the rewritten
/* There is no nested query with a matching path so we must
// replace function/operators's input with references
// find the processor inputs (Attributes) and convert them into references
// no need to promote them to the top since the container doesn't have to be aware
// update proc (if needed)
// resolve it Expression
//
// agg methods
//
//
// boiler plate
//
/*
/*
/*
/*
// path included. If field full path is a.b.c, full field name is "a.b.c" and name is "c"
// these field types can only be extracted from docvalue_fields (ie, values already computed by Elasticsearch)
// because, for us to be able to extract them from _source, we would need the mapping of those fields (which we don't have)
// nested fields are handled by inner hits
/*
/*
/**
// only for readability via toString()
/*
/**
/**
/*
/*
/*
// No leaf queries are nested
// No leaf queries are nested
// No leaf queries are nested
/*
/*
// TODO: it'd be great if these could be constants instead of Strings, needs a core change to make the fields public first
// TODO: add zero terms query support, I'm not sure the best way to parse it yet...
// appliers.put("zero_terms_query", (qb, s) -> qb.zeroTermsQuery(s));
/*
// TODO: it'd be great if these could be constants instead of Strings, needs a core change to make the fields public first
// TODO: add zero terms query support, I'm not sure the best way to parse it yet...
// appliers.put("zero_terms_query", (qb, s) -> qb.zeroTermsQuery(s));
/*
/**
// TODO: make this configurable
// field -> (useDocValues, format)
// I'm not at the right path so let my child query have a crack at it
// I already have the field, no rewriting needed
//TODO: Add all filters in nested sorting when https://github.com/elastic/elasticsearch/issues/33079 is implemented
// Adding multiple filters to sort sections makes sense for nested queries where multiple conditions belong to the same
// nested query. The current functionality creates one nested query for each condition involving a nested field.
// throw new SqlIllegalArgumentException("nested query should have been grouped in one place");
// disable score
/*
/*
/**
/**
/**
/**
/**
/**
/**
/*
// TODO: it'd be great if these could be constants instead of Strings, needs a core change to make the fields public first
// dedicated constructor for QueryTranslator
/*
/*
/*
// make script null safe
/*
/*
/*
/*
// the value might contain multiple lines (plan execution for example)
// TODO: this needs to be improved to properly scale each row across multiple lines
/*
// Typed object holding properties for a given query
/*
/**
/**
/**
/*
/**
/**
// cursors
// plus all their dependencies
// and custom types
/**
// return the string only after closing the resource
/**
/**
/*
// Only one instance allowed
// Nothing to write
// There is nothing to clean
/*
/*
// no-op
/*
/*
// NB: private since the columnCount is for public cases inferred by the columnCount
// only on the next-page the schema becomes null however that's an internal detail hence
// why this method is not exposed
/*
/*
/*
/**
// number or rows in this set; while not really necessary (the return of advanceRow works)
/*
/**
/**
/*
/**
/**
/*
/*
/*
//TODO is it worth keeping this when we have ListRowSet?
// no-op
/*
/*
// TODO we plan to support joins in the future when possible, but for now we'll just fail early if we see one
// Note: JOINs are not supported but we detect them when
// occurs when dealing with local relations (SELECT 5+2)
/*
/*
/**
// map that holds total/paging/failed counters for each client type (rest, cli, jdbc, odbc...)
// map that holds one counter per sql query "feature" (having, limit, order by, group by...) 
// counter for "translate" requests
/**
/**
/**
/**
// queries metrics
// compute the ODBC total metric
// features metrics
// translate operation metric
/*
// default to "odbc_32" if the client_id is not provided or it has a wrong value
/*
/**
/*
// In Java 8 LocalDate.EPOCH is not available, introduced with later Java versions
/**
/**
/**
/**
/**
/**
/**
// remove the remainder
/*
/**
/**
/*
/*
/*
/*
/**
/**
/**
/*
/*
/*
/*
// disabled on client
// disabled on proxy client
// disabled on server
/*
//github.com/elastic/elasticsearch/issues/37320")
// Add Netty so we can test JDBC licensing because only exists on the REST layer.
// enable http
// Enable http so we can test JDBC licensing because only exists on the REST layer.
// TODO test SqlGetIndicesAction. Skipping for now because of lack of serialization support.
/*
/*
// test multiple version of the attribute name
// to make sure all match the same thing
// NB: the equality is done on the same since each plan bumps the expression counter
// unqualified
// unquoted qualifier
// quoted qualifier
/*
/*
// check the nested alias is seen
// or its hierarhcy
// check typos
// non-existing parents for aliases are not seen by the user
// We get only one message back because the messages are grouped by the node that caused the issue
// GROUP BY
// related https://github.com/elastic/elasticsearch/issues/36853
// We get only one message back because the messages are grouped by the node that caused the issue
//
// Pivot verifications
//
/*
// define a sub-field
// covers the scenario described in https://github.com/elastic/elasticsearch/issues/43876
// each index will have one field with different name than all others
// first pass: create the field caps
// second pass: update indices
//TODO: what about nonAgg/SearchIndices?
/*
/*
// Initialize comparators for fields (columns)
// Insert random no of documents (rows) with random 0/1 values for each field
/*
/* Randomly choose between internal protocol round trip and String based
/*
/*
/*
/*
/*
/* We use values that are parsed from json as "equal" to make the
/* We use values that are parsed from json as "equal" to make the
/*
// the path to the randomly generated fields path
// the actual value we will be looking for in the test at the end
// build the rest of the path and the expected path to check against in the error message
// if the number of generated values is 1, just check we return the correct value
// if we have an array with more than one value in it, check that we throw the correct exception and exception message
// "a" : [{"b" : "value"}]
// "a" : [{"b" : "value1"}, {"b" : "value2"}]
// "a" : [{"b" : [{"c" : "value"}]}]
// "a" : [{"b" : [{"c" : ["value1", "value2"]}]}]
// "a" : [{"b" : {"c" : ["value"]}]}]
// parsing will, by default, build a Double even if the initial value is BigDecimal
// Elasticsearch does this the same when returning the results
/*
/*
/*
/*
/*
/*
/*
// discover available processors
// FIXME: the project split causes different behaviour between Gradle vs IDEs
// Eclipse considers classes from both projects, Gradle does not
// hence why this is disabled for now
/*
/*
/*
/*
/*
// null precision means default precision
/*
// null precision means default precision
/*
// test transforming only the properties (source, expression),
// skipping the children (the three parameters of the function) which are tested separately
/*
/*
// test transforming only the properties (source, expression),
// skipping the children (the three parameters of the function) which are tested separately
/*
/*
// test transforming only the properties (source, expression),
// skipping the children (the two parameters of the binary function) which are tested separately
/*
/*
// Tested against MS-SQL Server and H2
// Tested against MS-SQL Server and H2
/*
/*
// test transforming only the properties (source, expression),
// skipping the children (the two parameters of the binary function) which are tested separately
/*
/*
/*
// GMT: Tuesday, September 1, -0064 2:53:07.662 AM
// GMT: Monday, April 14, 4003 2:13:32.338 PM
/*
//github.com/elastic/elasticsearch/issues/33796
// at least Java 9
// and COMPAT setting needs to be first on the list
/*
// 1 Jan 1988 is Friday - under Sunday,1 rule it is the first week of the year (under ISO rule it would be 53 of the previous year
// hence the 5th Jan 1988 Tuesday is the second week of a year
//1988-01-05T09:22:10Z[UTC]
//2001-02-04T09:22:10Z[UTC]
//1977-02-08T09:22:10Z[UTC]
//1974-03-17T09:22:10Z[UTC]
//1977-04-20T09:22:10Z[UTC]
//1994-04-20T09:22:10Z[UTC]
//1972-07-12T09:22:10Z[UTC]
//1998-08-12T09:22:10Z[UTC]
// Tested against MS-SQL Server and H2
// Tested against MS-SQL Server and H2
//1988-01-05T09:22:10Z[UTC]
//2001-02-04T09:22:10Z[UTC]
//1977-02-08T09:22:10Z[UTC]
//1974-03-17T09:22:10Z[UTC]
//1977-04-20T09:22:10Z[UTC]
//1994-04-20T09:22:10Z[UTC]
//1980-07-26T09:22:10Z[UTC]
//1997-09-19T09:22:10Z[UTC]
/*
/*
/*
//           minX                                                               minX, maxX, maxY, minY
//           minY                                                               minX, maxX, maxY, minY
/*
/*
/*
/*
/*
/*
// test transforming only the properties (source, expression, operation),
// skipping the children (the two parameters of the binary function) which are tested separately
/*
/*
// if we decide to add DIFFERENCE(string,string) in the future, here we'd add it as well
// test transforming only the properties (source, expression),
// skipping the children (the two parameters of the binary function) which are tested separately
/*
/*
// test transforming only the properties (source, expression),
// skipping the children (the two parameters of the binary function) which are tested separately
/*
/*
// test transforming only the properties (source, expression),
// skipping the children (the two parameters of the binary function) which are tested separately
// generate all the combinations of possible children modifications and test all of them
/*
/*
// test transforming only the properties (source, expression),
// skipping the children (the two parameters of the binary function) which are tested separately
// generate all the combinations of possible children modifications and test all of them
/*
// the "start" parameter is optional and is treated as null in the constructor
// when it is not used. Need to take this into account when generating random
// values for it.
/*
// test transforming only the properties (source, expression),
// skipping the children (the two parameters of the binary function) which are tested separately
// generate all the combinations of possible children modifications and test all of them
/*
/*
// accepts chars as well
// validate input
// validate input
// ES-SQL is not locale sensitive (so far). The obvious test for this is the Turkish language, uppercase letter I conversion
// in non-Turkish locale the lowercasing would create i and an additional dot, while in Turkish Locale it would only create "i"
// unicode 0069 = i
// unicode 0049 = I (regular capital letter i)
// in Turkish locale this would be lowercased to a "i" without dot (unicode 0131)
// restore the original Locale
// special uppercasing for small letter sharp "s" resulting "SS"
// ES-SQL is not Locale sensitive (so far).
// in Turkish locale, small letter "i" is uppercased to "I" with a dot above (unicode 130), otherwise in "i" (unicode 49)
// restore the original Locale
// euro symbol
// euro symbol
// euro (3), lamda (2), theta (2), 'white sun with rays' (3), math 'A' (4) symbols
/*
// test transforming only the properties (source, expression),
// skipping the children (the two parameters of the binary function) which are tested separately
// generate all the combinations of possible children modifications and test all of them
/*
/*
/*
/*
/*
// a
// b
// x -> a
// SELECT 5 a, 10 b FROM foo WHERE a < 10 ORDER BY b
// a
// b
// WHERE a < 10
// SELECT
// ORDER BY
// ORDER BY b -> ORDER BY 10
// WHERE a < 10
//
// Constant folding
//
// check now with an alias
// Null folding
// date-time
// math function
// string function
// arithmetic
// comparison
// regex
// CASE WHEN a = 1 THEN 'foo1'
//      WHEN 1 = 2 THEN 'bar1'
//      WHEN 2 = 1 THEN 'bar2'
//      WHEN a > 1 THEN 'foo2'
// ELSE 'default'
// END
//
// ==>
//
// CASE WHEN a = 1 THEN 'foo1'
//      WHEN a > 1 THEN 'foo2'
// ELSE 'default'
// END
// CASE WHEN 1 = 2 THEN 'foo1'
//      WHEN 1 = 1 THEN 'foo2'
// ELSE 'default'
// END
//
// ==>
//
// 'foo2'
// CASE WHEN 1 = 2 THEN 'foo1'
// ELSE myField
// END
//
// ==>
//
// myField (non-foldable)
//
// Logical simplifications
//
//
// Range optimization
//
// 6 < a <= 5  -> FALSE
// 6 < a <= 5.5 -> FALSE
// Conjunction
// a <= 6 AND a < 5  -> a < 5
// 6 <= a AND 5 < a  -> 6 <= a
// 5 <= a AND 5 < a  -> 5 < a
// 2 < a AND (2 <= a < 3) -> 2 < a < 3
// a < 4 AND (1 < a < 3) -> 1 < a < 3
// a <= 2 AND (1 < a < 3) -> 1 < a <= 2
// 3 <= a AND 4 < a AND a <= 7 AND a < 6 -> 4 < a < 6
// 3 <= a AND TRUE AND 4 < a AND a != 5 AND a <= 7 -> 4 < a <= 7 AND a != 5 AND TRUE
// TRUE AND a != 5 AND 4 < a <= 7
// 1 <= a AND a < 5  -> 1 <= a < 5
// a != NULL AND a > 1 AND a < 5 AND a == 10  -> (a != NULL AND a == 10) AND 1 <= a < 5
// (2 < a < 3) AND (1 < a < 4) -> (2 < a < 3)
// (2 < a < 3) AND a < 2 -> 2 < a < 2
// (2 < a < 3) AND (2 < a <= 3) -> 2 < a < 3
// (2 < a < 3) AND (1 < a < 3) -> 2 < a < 3
// (2 < a <= 3) AND (1 < a < 3) -> 2 < a < 3
// (0 < a <= 1) AND (0 <= a < 2) -> 0 < a <= 1
// Disjunction
// 2 < a OR 1 < a OR 3 < a -> 1 < a
// 2 < a OR 1 < a OR 3 <= a -> 1 < a
// a < 1 OR a < 2 OR a < 3 ->  a < 3
// a < 2 OR a <= 2 OR a < 1 ->  a <= 2
// a < 2 OR 3 < a OR a < 1 OR 4 < a ->  a < 2 OR 3 < a
// (2 < a < 3) OR (1 < a < 4) -> (1 < a < 4)
// (2 < a < 3) OR (1 < a < 4) -> (1 < a < 4)
// (2 < a < 3) OR (1 < a < 2) -> same
// (2 < a < 3) OR (2 < a <= 3) -> 2 < a <= 3
// (2 < a < 3) OR (1 < a < 3) -> 1 < a < 3
// (2 < a <= 3) OR (1 < a < 3) -> same (the <= prevents the ranges from being combined)
// (a = 1 AND b = 3 AND c = 4) OR (a = 2 AND b = 3 AND c = 4) -> (b = 3 AND c = 4) AND (a = 1 OR a = 2)
// (0 < a <= 1) OR (0 < a < 2) -> 0 < a < 2
// Equals & NullEquals
// 1 <= a < 10 AND a == 1 -> a == 1
// 1 <= a < 10 AND a <=> 1 -> a <=> 1
// The following tests should work only to simplify filters and
// not if the expressions are part of a projection
// See: https://github.com/elastic/elasticsearch/issues/35859
// a == 1 AND a == 2 -> FALSE
// a <=> 1 AND a <=> 2 -> FALSE
// 1 < a < 10 AND a == 10 -> FALSE
// 1 < a < 10 AND a <=> 10 -> FALSE
// a != 3 AND a = 3 -> FALSE
// a != 4 AND a = 3 -> a = 3
// a = 2 AND a < 2 -> FALSE
// a = 2 AND a <= 2 -> a = 2
// a = 2 AND a <= 1 -> FALSE
// a = 2 AND a > 2 -> FALSE
// a = 2 AND a >= 2 -> a = 2
// a = 2 AND a > 3 -> FALSE
// a = 2 AND a < 3 AND a > 1 AND a != 4 -> a = 2
// a = 2 AND 1 < a < 3 AND a > 0 AND a != 4 -> a = 2
// a = 2 OR a > 1 -> a > 1
// a = 2 OR a > 2 -> a >= 2
// a = 2 OR a < 3 -> a < 3
// a = 3 OR a < 3 -> a <= 3
// a = 2 OR 1 < a < 3 -> 1 < a < 3
// a = 2 OR 2 < a < 3 -> 2 <= a < 3
// a = 3 OR 2 < a < 3 -> 2 < a <= 3
// a = 2 OR a != 2 -> TRUE
// a = 2 OR a != 5 -> a != 5
// a = 2 OR 3 < a < 4 OR a > 2 OR a!= 2 -> TRUE
/**
/*
/*
/*
/*
// Create expression in the form of a = b OR a = b OR ... a = b
// 1000 elements is ok
// 5000 elements cause stack overflow
// Create expression in the form of abs(abs(abs ... (i) ...)
// 200 elements is ok
// 5000 elements cause stack overflow
// Create expression in the form of a + a + a + ... + a
// 1000 elements is ok
// 5000 elements cause stack overflow
// Test with queries in the form of `SELECT * FROM (SELECT * FROM (... t) ...)
// 200 elements is ok
// 500 elements cause stack overflow
/**
/**
/*
/*
/*
// nested fields are ignored
// no index specified
// no index specified
/*
//
// catalog enumeration
//
// everything else should be null
// everything else should be null
// when types are null, consider them equivalent to '' for compatibility reasons
// everything else should be null
//
// table types enumeration
//
// missing type means pattern
// everything else should be null
// when a type is specified, apply filtering
/*
// test numeric as signed
// make sure precision is returned as boolean (not int)
// no auto-increment
// boolean = 16
/*
/*
/*
// use both date-only interval (1 DAY) and time-only interval (1 second) to cover CURRENT_TIMESTAMP and TODAY scenarios
// the range queries optimization should create a single "range" query with "from" and "to" populated with the values
// in the two branches of the AND condition
/*
/*
// usage action
/*
// headers
// values
// headers
// values
/*
/*
/*
// Leaf queries don't contain nested fields.
// Leaf queries don't contain nested fields.
// Leaf queries don't contain nested fields.
/*
// TODO add the predicate
// TODO mutate the predicate
/*
// Use a TreeMap so we get the fields in a predictable order.
/*
// add does nothing if the field is already there
// add does nothing if the path doesn't match
// if the field isn't in the list then add rewrites to a query with all the old fields and the new one
// enrich adds the filter if the path matches
// but doesn't if it doesn't match
// enriching with the same query twice is fine
// But enriching using another query will keep only the first query
/*
/*
/*
/* Randomly choose between internal protocol round trip and String based
/*
/*
/**
/*
/*
/** Runs rest tests against external cluster */
/**
// ensure watcher is started, so that a test can stop watcher and everything still works fine
// all good here, we are done
/**
/**
/**
// This waits for pending tasks to complete, so must go last (otherwise
// it could be waiting for pending tasks while monitoring is still running).
// Don't check rollup jobs because we clear them in the superclass.
/**
/**
// The actual method call that sends the API requests returns a Future, but we immediately
// call .get() on it so there's no need for this method to do any other awaiting.
/**
/*
// create mapping
// create index
/*
// index some more docs
// Assert that we wrote the new docs
// index some more docs
// Since updates are loaded on checkpoint start, we should see the updated config on this next run
// assert that we have the new field and its value is 42 in at least some docs
// waitForCheckpoint: true should make the transform continue until we hit the first checkpoint, then it will stop
// Wait until the first checkpoint
// Even though we are continuous, we should be stopped now as we needed to stop at the first checkpoint
/*
// preserve indices in order to reuse source indices in several test cases
// it's not possible to run it as @BeforeClass as clients aren't initialized then, so we need this little hack
// Make sure we wrote to the audit
// Since calls to write the AbstractAuditor are sent and forgot (async) we could have returned from the start,
// finished the job (as this is a very short DF job), all without the audit being fully written.
/*
/**
// refresh the index
// delete again, should fail
/*
// preserve indices in order to reuse source indices in several test cases
// it's not possible to run it as @BeforeClass as clients aren't initialized then, so we need this little hack
// at random test the old deprecated roles, to be removed in 9.0.0
// Alternate testing between admin and lowly user, as both should be able to get the configs and stats
// check all the different ways to retrieve all stats
// Verify that both transforms have valid stats
/* TODO progress is now checkpoint progress and it may be that no checkpoint is in progress here
// only pivot_1
// only continuous
// check all the different ways to retrieve all transforms
// only pivot_1
// Get rid of the first transform task, but keep the configuration
// Verify that the task is gone
// Verify that both transforms, the one with the task and the one without have statistics
// Alternate testing between admin and lowly user, as both should be able to get the configs and stats
// Verify that the transform has stats and the total docs process matches the expected
/* TODO progress is now checkpoint progress and it may be that no checkpoint is in progress here
// No continuous checkpoints have been seen and thus all exponential averages should be 0.0
// Doing only new users so that there is a deterministic number of docs for progress
// We should now have exp avgs since we have processed a continuous checkpoint
/*
// The mapping does not need to actually be the "OLD" mapping, we are testing that the old doc gets deleted, and the new one
// created.
// Old should now be gone
// New should be here
/*
// preserve indices in order to reuse source indices in several test cases
// it's not possible to run it as @BeforeClass as clients aren't initialized then, so we need this little hack
/*
// preserve indices in order to reuse source indices in several test cases
// it's not possible to run it as @BeforeClass as clients aren't initialized then, so we need this little hack
// at random test the old deprecated roles, to be removed in 9.0.0
// we expect 27 documents as there shall be 27 user_id's
// get and check some users
// we expect only 1 document due to the query
// we expect 27 documents as there shall be 27 user_id's
// get and check some users
// get and check some users
// Should be less than the total number of users since we filtered every user who had an average review less than or equal to 3.8
// get and check some users
// assert that other users are unchanged
// we expect 3 documents as there shall be 5 unique star values and we are bucketing every 2 starting at 0
// we expect 27 documents as there shall be 27 user_id's
// get and check some users
// preview is limited to 100
// preview is limited to 100
// we expect 21 documents as there shall be 21 days worth of docs
// Do `containsString` as actual ending timestamp is indeterminate due to how data is generated
// we expect 27 documents as there shall be 27 user_id's
// get and check some users
// we expect 27 documents as there shall be 27 user_id's
// get and check some users
// we expect 27 documents as there shall be 27 user_id's
// get and check some users
// we expect 27 documents as there shall be 27 user_id's
// get and check some users
// We get a conflict sometimes depending on WHEN we try to write the state, should eventually pass though
// get and check some users
/*
// create mapping
// create index
/*
// randomly return the old or the new endpoints, old endpoints to be removed for 8.0.0
// create mapping
// create index
// clear the builder
/**
//Set frequency high for testing
// start the transform
// start the transform
// wait until the dataframe has been created and all data is available
// start the transform
// wait until the dataframe has been created and all data is available
// Ignore 404s because they imply someone was racing us to delete this
// we might have disabled wiping indices, but now its time to get rid of them
// note: can not use super.cleanUpCluster() as this method must be static
// transforms should be all gone
// the configuration index should be empty
// 404 here just means we had no data frame transforms, true for some tests
/*
// Make sure we never retry on failure to speed up the test
// Set logging level to trace
// see: https://github.com/elastic/elasticsearch/issues/45562
// reduces bulk failure spam
// If the tests failed in the middle, we should force stop it. This prevents other transform tests from failing due
// to this left over transform
// Verify we have failed for the expected reason
// verify that we cannot stop a failed transform
// Verify that we can force stop a failed transform
// Verify we have failed for the expected reason
// Verify that we cannot start the transform when the task is in a failed state
// It should not take this long, but if the scheduler gets deferred, it could
// create mapping
/*
// no transforms, no stats
// create transforms
// Verify that we have one stat document
// Simply because we wait for continuous to reach checkpoint 1, does not mean that the statistics are written yet.
// Since we search against the indices for the statistics, we need to ensure they are written, so we will wait for that
// to be the case.
// we should see some stats
// Refresh the index so that statistics are searchable
/*
// How many times the transform task can retry on an non-critical failure
// deprecated endpoints, to be removed for 8.0.0
// deprecated actions, to be removed for 8.0.0
// the transform services should have been created
/*
// Wait until the gateway has recovered from disk.
// The atomic flag prevents multiple simultaneous attempts to run alias creation
// if there is a flurry of cluster state updates in quick succession
// check if old audit index exists, no need to create the alias if it does not
/*
// numPages
// numInputDocuments
// numOutputDocuments
// numInvocations
// indexTime
// searchTime
// indexTotal
// searchTotal
// indexFailures
// searchFailures
/*
/**
/*
/*
/**
/*
/*
/*
// Little extra insurance, make sure we only return transforms that aren't cancelled
// Mutates underlying state object with the assigned node attributes
// If the index to search, or the individual config is not there, just return empty
// We gathered all there is, no need to continue
// Small assurance that we are at least below the max. Terms search has a hard limit of 10k, we should at least be below that.
// If the persistent task does NOT exist, it is STOPPED
// There is a potential race condition where the saved document does not actually have a STOPPED state
// as the task is cancelled before we persist state.
// Transforms that have not been started and have no state or stats.
// Any transform in collection could NOT have a task, so, even though the list is initially sorted
// it can easily become arbitrarily ordered based on which transforms don't have a task or stats docs
// No work to do, but we must respond to the listener
/*
// remove all internal fields
/*
// If the destination index does not exist, we can assume that we may have to create it on start.
// We should check that the creating user has the privileges to create the index.
// We need to read the source indices mapping to deduce the destination mapping
// set headers to run transform as calling user
// quick check whether a transform has already been created under that name
// Early check to verify that the user can create the destination index and can read from the source
// No security enabled, just create the transform
// <3> Return to the listener
// <2> Put our transform
/*
// <5> Wait for the allocated task's state to STARTED
// <4> Create the task in cluster state so that it will start executing on the node
// Create the allocated task and wait for it to be started
// If the task already exists that means that it is either running or failed
// Since it is not failed, that means it is running, we return a conflict.
// <2> If the destination index exists, start the task, otherwise deduce our mappings for the destination index and create it
// <2> run transform validations
// <1> Get the config to verify it exists and is valid
// We succeeded in canceling the persistent task, but the
// problem that caused us to cancel it is the overall result
// We want to return to the caller without leaving an unassigned persistent task
/**
// For some reason, the task is not assigned to a node, but is no longer in the `INITIAL_ASSIGNMENT` state
// Consider this a failure.
// We just want it assigned so we can tell it to start working
// checking for `isNotStopped` as the state COULD be marked as failed for any number of reasons
// But if it is in a failed state, _stats will show as much and give good reason to the user.
// If it is not able to be assigned to a node all together, we should just close the task completely
/*
// Delegates stop transform to elected master node so it becomes the coordinating node.
// if tasks is empty allMatch is 'vacuously satisfied'
// If there were failures attempting to stop the tasks, we don't know if they will actually stop.
// It is better to respond to the user now than allow for the persistent task waiting to timeout
// Wait until the persistent task is stopped
// Switch over to Generic threadpool so we don't block the network thread
// As it stands right now, this will ALWAYS be INTERNAL_SERVER_ERROR.
// FailedNodeException does not overwrite the `status()` method and the logic in ElasticsearchException
// Just returns an INTERNAL_SERVER_ERROR
// If all the previous exceptions don't have a valid status, we have an unknown error.
// This map is accessed in the predicate and the listener callbacks
// Either the task has successfully stopped or we have seen that it has failed
// If force is true, then it should eventually go away, don't add it to the collection of failures.
// If all the tasks are now flagged as failed, do not wait for another ClusterState update.
// Return to the caller as soon as possible
// No exceptions AND the tasks have gone away
// We are only stopping one task, so if there is a failure, it is the only one
// waitForPersistentTasksCondition throws a IllegalStateException on timeout
// collect which tasks are still running
// should not happen
/*
// set headers to run transform as calling user
// GET transform and attempt to update
// We don't want the update to complete if the config changed between GET and INDEX
// If it is a noop don't bother even writing the doc, save the cycles, just return here.
// Early check to verify that the user can create the destination index and can read from the source
// No security enabled, just create the transform
// <3> Return to the listener
// If we failed to INDEX AND we created the destination index, the destination index will still be around
// This is a similar behavior to _start
// <2> Update our transform
// <1> Create destination index if necessary
// If we are running, we should verify that the destination index exists and create it if it does not
// Verify we have source indices. The user could defer_validations and if the task is already running
// we allow source indices to disappear. If the source and destination indices do not exist, don't do anything
// the transform will just have to dynamically create the destination index without special mapping.
// <0> Validate the pivot if necessary
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/**
/**
/**
/**
/**
/*
// threshold when to audit concrete index names, above this threshold we only report the number of changes
// 1st get index to see the indexes the user has access to
// 2nd get stats request
// SeqNoStats could be `null`, assume the global checkpoint to be -1 in this case
// we have already seen this index, just check/add shards
// 1st time we see this shard for this index, add the entry for the shard
// or there is already a checkpoint entry for this index/shard combination
// but with a higher global checkpoint. This is by design(not a problem) and
// we take the higher value
// 1st time we see this index, create an entry for the index and add the shard checkpoint
// checkpoint extraction is done in 2 steps:
// 1. GetIndexRequest to retrieve the indices the user has access to
// 2. IndicesStatsRequest to retrieve stats about indices
// between 1 and 2 indices could get deleted or created
// create the final structure
// <3> got the source checkpoint, notify the user
// <2> got the next checkpoint, get the source checkpoint
// <1> got last checkpoint, get the next checkpoint
/**
// spam protection: only warn the first time
/*
/**
/*
// we only want to know if there is at least 1 new document
// for time based synchronization
// for the purpose of testing
/*
/**
/**
// we need to retrieve the config first before we can defer the rest to the corresponding provider
/*
/**
/*
/**
// not expected to happen but for the sake of completeness
// update the config in the same, current index using optimistic concurrency control
// create the config in the current version of the index assuming there is no existing one
// this leaves a dup behind in the old index, see dup handling on the top
// the transform already exists
// not expected to happen but for the sake of completeness
// use sort to get the last
// do not fail if checkpoint does not exist but return an empty checkpoint
// use sort to get the last
// use sort to get the last
// We only care about the `id` field, small optimization
// important: preserve order
// some required Ids were not found
// If the index is NOT the latest or we are null, that means we have not created this doc before
// so, it should be a create option without the seqNo and primaryTerm set
// not expected to happen but for the sake of completeness
// use sort to get the last
// the limit for getting stats and transforms is 1000, as long as we do not have 10 indices this works
// skip old versions
// Getting the max RestStatus is sort of arbitrary, would the user care about 5xx over 4xx?
// Unsure of a better way to return an appropriate and possibly actionable cause to the user.
/**
// disable scoring by using index order
/*
/**
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
// TODO: revisit number of shards, number of replicas
// <1>
// _doc type
// PROPERTIES
// META_FIELDNAME
// META
/*
/* Changelog of internal index versions
// constants for mappings
// data types
// the configurations are expected to be small
// the audits are expected to be small
// do not allow anything outside of the defined schema
// the schema definitions
// overall doc type
// add the schema for transform configurations
// add the schema for transform stats
// add the schema for checkpoints
// end type
// end properties
// end mapping
/**
/**
// The check for existence of the template is against local cluster state, so very cheap
// Installing the template involves communication with the master node, so it's more expensive but much rarer
// The check for existence of the template is against local cluster state, so very cheap
// Installing the template involves communication with the master node, so it's more expensive but much rarer
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
/*
// TODO: move into context constructor
// Node attributes
// We only want to update this internal value if it is persisted as such
// TODO gather information on irrecoverable failures and update isIrrecoverableFailure
// This calls AsyncTwoPhaseIndexer#finishWithIndexingFailure
// It increments the indexing failure, and then calls the `onFailure` logic
// If we are failed, we should call next to allow failure handling to occur if necessary.
// If we're aborting, just invoke `next` (which is likely an onFailure handler)
// If we should stop at the next checkpoint, are STARTED, and with `initialRun()` we are in one of two states
// 1. We have just called `onFinish` completing our request, but `shouldStopAtCheckpoint` was set to `true` before our check
// there and now
// 2. We are on the very first run of a NEW checkpoint and got here either through a failure, or the very first save state call.
//
// In either case, we should stop so that we guarantee a consistent state and that there are no partially completed checkpoints
// This means that the indexer was triggered to discover changes, found none, and exited early.
// If the state is `STOPPED` this means that TransformTask#stop was called while we were checking for changes.
// Allow the stop call path to continue
// set both to stopped so they are persisted as such
// If we are `STOPPED` on a `doSaveState` call, that indicates we transitioned to `STOPPED` from `STOPPING`
// OR we called `doSaveState` manually as the indexer was not actively running.
// Since we save the state to an index, we should make sure that our task state is in parity with the indexer state
// If we are going to stop after the state is saved, we should NOT persist `shouldStopAtCheckpoint: true` as this may
// cause problems if the task starts up again.
// Additionally, we don't have to worry about inconsistency with the ClusterState (if it is persisted there) as the
// when we stop, we mark the task as complete and that state goes away.
// We don't want adjust the stored taskState because as soon as it is `STOPPED` a user could call
// .start again.
// This could be `null` but the putOrUpdateTransformStoredDoc handles that case just fine
// Persist the current state and stats in the internal index. The interval of this method being
// called is controlled by AsyncTwoPhaseIndexer#onBulkResponse which calls doSaveState every so
// often when doing bulk indexing calls or at the end of one indexing run.
// for auto stop shutdown the task
// Only do this clean up once, if it succeeded, no reason to do the query again.
// If we have failed, we should attempt the clean up again later
// for auto stop shutdown the task
// This should never happen. We ONLY ever update this value if at initialization or we just finished updating the document
// famous last words...
// Considered a recoverable indexing failure
/*
/*
/**
/**
/**
// keep the 1st byte of every object
/**
/*
// the checkpoint of this transform, storing the checkpoint until data indexing from source to dest is _complete_
// Note: Each indexer run creates a new future checkpoint which becomes the current checkpoint only after the indexer run finished
// Successfully marked as failed, reset counter so that task can be restarted
/*
/**
// do a complete query/index, this is used for batch transforms and for bootstrapping (1st run)
// Partial run modes in 2 stages:
// identify buckets that have changed
// recalculate buckets based on the update list
// constant for checkpoint retention, static for now
// 10 days
// every 100 checkpoints
// Indicates that the source has changed for the current run
// Keeps track of the last exception that was written to our audit, keeps us from spamming the audit index
// hold information for continuous mode (partial updates)
// give runState a default
/**
// if we haven't set the page size yet, if it is set we might have reduced it after running into an out of memory
// On each run, we need to get the total number of docs and reset the count of processed docs
// Since multiple checkpoints can be executed in the task while it is running on the same node, we need to gather
// the progress here, and not in the executor.
// If nextCheckpoint > 1, this means that we are now on the checkpoint AFTER the batch checkpoint
// Consequently, the idea of percent complete no longer makes sense.
// If we are continuous, we will want to verify we have the latest stored configuration
// If the transform config index or the transform config is gone, something serious occurred
// We are in an unknown state and should fail out
// If we are not on the initial batch checkpoint and its the first pass of whatever continuous checkpoint we are on,
// we should verify if there are local changes based on the sync config. If not, do not proceed further and exit.
// No changes, stop executing
// If we failed determining if the source changed, it's safer to assume there were changes.
// We should allow the failure path to complete as normal
// This indicates an early exit since no changes were found.
// So, don't treat this like a checkpoint being completed, as no work was done.
// reset the page size, so we do not memorize a low page size forever
// reset the changed bucket to free memory
// Reset our failure count as we have finished and may start again with a new checkpoint
// With bucket_selector we could have read all the buckets and completed the transform
// but not "see" all the buckets since they were filtered out. Consequently, progress would
// show less than 100% even though we are done.
// NOTE: this method is called in the same thread as the processing thread.
// Theoretically, there should not be a race condition with updating progress here.
// NOTE 2: getPercentComplete should only NOT be null on the first (batch) checkpoint
// If the last checkpoint is now greater than 1, that means that we have just processed the first
// continuous checkpoint and should start recording the exponential averages
// This should not happen as we simply create a new one when we reach continuous checkpoints
// but this is a paranoid `null` check
// delete old checkpoints, on a failure we keep going
// Treat this as a "we reached the end".
// This should only happen when all underlying indices have gone away. Consequently, there is no more data to read.
// Any other state is a bug, should not happen
// ignore trigger if indexer is running, prevents log spam in A2P indexer
// the failure handler must not throw an exception due to internal problems
// irrecoverable error without special handling
// Since our schedule fires again very quickly after failures it is possible to run into the same failure numerous
// times in a row, very quickly. We do not want to spam the audit log with repeated failures, so only record the first one
/**
// we reached the end
// NOTE: progress is also mutated in onFinish
// we reached the end
// cleanup changed Buckets
// reset the runState to fetch changed buckets
// advance the cursor for changed bucket detection
// initialize the map of changed buckets, the map might be empty if source do not require/implement
// changed bucket detection
// reached the end?
// reset everything and return the end marker
// else
// collect all buckets that require the update
// remember the after key but do not store it in the state yet (in the failure we need to retrieve it again)
// reset the runState to fetch the partial updates next
/*
// skip all internal fields
// Any other state is a bug, should not happen
/**
/**
/*
/**
// either 1st run or not a continuous transform
// if incremental update is not supported, do a full run
// continuous mode: we need to get the changed buckets first
/*
// The amount of time we wait for the cluster state to respond when being marked as failed
// NOTE: TransformPersistentTasksExecutor#createTask pulls in the stored task state from the ClusterState when the object
// is created. TransformTask#ctor takes into account setting the task as failed if that is passed in with the
// persisted state.
// TransformPersistentTasksExecutor#startTask will fail as TransformTask#start, when force == false, will return
// a failure indicating that a failed task cannot be started.
//
// We want the rest of the state to be populated in the task when it is loaded on the node so that users can force start it again
// later if they want.
// <7> load next checkpoint
// extra safety: reset position and progress if next checkpoint is empty
// prevents a failure if for some reason the next checkpoint has been deleted
// TODO: do not use the same error message as for loading the last checkpoint
// <6> load last checkpoint
// <5> Set the previous stats (if they exist), initialize the indexer, start the task (If it is STOPPED)
// Since we don't create the task until `_start` is called, if we see that the task state is stopped, attempt to start
// Schedule execution regardless
// Since we have not set the value for this yet, it SHOULD be null
// <4> set fieldmappings for the indexer, get the previous stats (if they exist)
// <3> Validate the transform, assigning it to the indexer, and get the field mappings
// <2> Get the transform config
// <1> Check the index templates are installed
// If it is STARTED or INDEXING we want to make sure we revert to started
// Otherwise, the internal indexer will never get scheduled and execute
// If we are STOPPED, STOPPING, or ABORTING and just started executing on this node,
// then it is safe to say we should be STOPPED
// TransformTask#start will fail if the task state is FAILED
/*
/**
/**
// TODO change once we allow missing_buckets
/*
// Default interval the scheduler sends an event if the config does not specify a frequency
// reset to started as no indexer is running
// reset to stopped as something bad happened
/**
/**
// If our state is failed AND the indexer is null, the user needs to _stop?force=true so that the indexer gets
// fully initialized.
// If we are NOT failed, then we can assume that `start` was just called early in the process.
// Even though the indexer information is persisted to an index, we still need TransformTaskState in the clusterstate
// This keeps track of STARTED, FAILED, STOPPED
// This is because a FAILED state can occur because we cannot read the config from the internal index, which would imply that
// we could not read the previous state information from said index.
// kick off the indexer
/**
// cleanup potentially failed state.
// If there is no indexer the task has not been triggered
// but it still needs to be stopped and removed
// If state was in a failed state, we should stop immediately
// shouldStopAtCheckpoint only comes into play when onFinish is called (or doSaveState right after).
// if it is false, stop immediately
// If the indexerState is STARTED and it is on an initialRun, that means that the indexer has previously finished a checkpoint,
// or has yet to even start one.
// Either way, this means that we won't get to have onFinish called down stream (or at least won't for some time).
// Ignore if event is not for this job
// ignore trigger if indexer is running or completely stopped
// if it runs for the 1st time we just do it, if not we check for changes
/**
// If we are already flagged as failed, this probably means that a second trigger started firing while we were attempting to
// flag the previously triggered indexer as failed. Exit early as we are already flagged as failed.
// If the indexer is `STOPPING` this means that `TransformTask#stop` was called previously, but something caused
// the indexer to fail. Since `ClientTransformIndexer#doSaveState` will persist the state to the index once the indexer stops,
// it is probably best to NOT change the internal state of the task and allow the normal stopping logic to continue.
// If we are stopped, this means that between the failure occurring and being handled, somebody called stop
// We should just allow that stop to continue
// We should not keep retrying. Either the task will be stopped, or started
// If it is started again, it is registered again.
// The idea of stopping at the next checkpoint is no longer valid. Since a failed task could potentially START again,
// we should set this flag to false.
// The end user should see that the task is in a failed state, and attempt to stop it again but with force=true
// Even though the indexer information is persisted to an index, we still need TransformTaskState in the clusterstate
// This keeps track of STARTED, FAILED, STOPPED
// This is because a FAILED state could occur because we failed to read the config from the internal index, which would imply that
// we could not read the previous state information from said index.
/**
// there is no background transform running, we can shutdown safely
/*
/**
// generator to create unique but deterministic document ids, so we
// - do not create duplicates if we re-run after failure
// - update documents
// This indicates not that the value contained in the `aggResult` is null, but that the `aggResult` is not
// present at all in the `bucket.getAggregations`. This could occur in the case of a `bucket_selector` agg, which
// does not calculate a value, but instead manipulates other results.
// Execution should never reach this point!
// Creating transforms with unsupported aggregations shall not be possible
// If the double is invalid, this indicates sparse data
// If the type is numeric or if the formatted string is the same as simply making the value a string,
//    gather the `value` type, otherwise utilize `getValueAsString` so we don't lose formatted outputs.
// if the account is `0` iff there is no contained centroid
// If the two geo_points are equal, it is a point
// If only the lat or the lon of the two geo_points are equal, than we know it should be a line
// neither points are equal, we have a polygon that is a square
/*
// the field mapping should not explicitly be set and allow ES to dynamically determine mapping via the data.
// the field mapping should be determined explicitly from the source field mapping if possible.
/**
/*
// objects for re-using
/**
// it should not be possible to get into this code path
// should never happen
// else: more than 1 group by, need to nest it
// the source might not define an filter optimization
// should never happen
/*
// Full collection of numeric field type strings
// have to add manually since scaled_float is in a module
/**
// collects the fieldnames used as source for aggregations
// collects the aggregation types by source name
// collects the fieldnames and target fieldnames used for grouping
// execution should not reach this point
// For pipeline aggs, since they are referencing other aggregations in the payload, they do not have any
// sourcefieldnames to put into the payload. Though, certain ones, i.e. avg_bucket, do have determinant value types
/**
/*
// TODO: overwrites types, requires resolve if
// types are mixed
/*
/**
/**
// circuit breaking exceptions are at the bottom
/**
/*
/**
/*
/*
// numPages
// numInputDocuments
// numOutputDocuments
// numInvocations
// indexTime
// searchTime
// indexTotal
// searchTotal
// indexFailures
// searchFailures
// not enabled -> no transforms, no stats
/*
/*
/*
// Stop at next checkpoint is false.
/*
// Test with the task state being null
// test again with a non failed task but this time it has internal state
/*
// always start fresh
/*
// re-use the mock client for the whole test suite as the underlying thread pool and the
// corresponding context if recreated cause unreliable test execution
// see https://github.com/elastic/elasticsearch/issues/45238 and https://github.com/elastic/elasticsearch/issues/42577
// for this test we only need the indices
// IndicesStatsResponse is package private, therefore using a mock
// it's not possible to run it as @BeforeClass as clients aren't initialized
// use a mock for the checkpoint service
// create transform
// by design no exception is thrown but an empty checkpoint is returned
// add a 2nd checkpoint
// both checkpoints should be there
// delete transform
// checkpoints should be empty again
// create transform
// same as current
/*
// low-level compare
// low-level compare
// low-level compare
// global checkpoints should be max() of all global checkpoints
/**
// never create an empty set
/**
// always create the full list
// we need at least one replica for testing
// SeqNoStats asserts that checkpoints are logical
// add broken seqNoStats if requested
// overwrite
// shuffle the shard stats
/*
/*
// we can dynamically change the auditor, like attaching and removing the log appender
/*
/**
// for now we ignore seqNoPrimaryTermAndIndex
// for now we ignore seqNoPrimaryTermAndIndex
/*
/*
// the index does not exist yet
// create one transform and test with an existing index
// same test, but different code path
// the index does not exist yet
// create one transform and test with an existing index
// same test, but different code path
// create transform
// read transform
// try to create again
// delete transform
// delete again
// try to get deleted transform
// create
// read
// delete
// delete again
// getting a non-existing checkpoint returns null
// create transform
// expand 1 id
// expand 2 ids explicitly
// expand 3 ids wildcard and explicit
// expand 3 ids _all
// expand 1 id _all with pagination
// expand 2 later ids _all with pagination
// expand 1 id explicitly that does not exist
// expand 1 id implicitly that does not exist
// remove one of the put docs so we don't retrieve all
// returned docs will be ordered by id
// Put when referencing the old index should create the doc in the new index, even if we have seqNo|primaryTerm info
// create some other docs to check they are not getting accidentally deleted
// create 100 checkpoints
// read a random checkpoint
// test delete based on checkpoint number (time would allow more)
// test delete based on time (checkpoint number would allow more)
// zero delete
// delete the rest
// test that the other docs are still there
/*
/*
/*
/*
// Audit every checkpoint for the first 10
// Then audit every 10 while < 100
// Then audit every 100 < 1000
// Then audit every 1000 for the rest of time
/*
/*
// used for synchronizing with the test
/* TransformProgress */ null,
// run indexer a 2nd time
// assert that page size has been reduced again
// Simulate completely null aggs
/*
/*
// see https://github.com/elastic/elasticsearch/issues/48957
// verify that shutdown has not been called
// verify shutdown has been called
// verify that shutdown has not been called
// verify shutdown has been called
/*
// aggregations potentially useful for writing tests, to be expanded as necessary
// If the second aggregation was some non-numeric mapped field
// remove the document ids and test uniqueness
/*
// avg
// cardinality
// value_count
// max
// min
// sum
// geo_centroid
// geo_bounds
// scripted_metric
// bucket_script
// bucket_selector
// weighted_avg
/*
// register aggregations as NamedWriteable
// test a failure during the search operation, transform creation fails if
// search has failures although they might just be temporary
// parseAggregators expects to be already inside the xcontent object
/*
/*
/*
/*
/**
//maximum allowed number of dimensions
//number of vector dimensions
// encode array of floats as array of integers and store into buf
// this code is here and not int the VectorEncoderDecoder so not to create extra arrays
// encode vector magnitude at the end
/*
/**
/*
/**
/*
// package private access only for {@link ScoreScriptUtils}
/*
/*
/**
// Validate the encoded vector's length.
// Calculate l1 norm (Manhattan distance) between a query's dense vector and documents' dense vectors
// Calculate l2 norm (Euclidean distance) between a query's dense vector and documents' dense vectors
// Calculate a dot product between a query's dense vector and documents' dense vectors
// Calculate cosine similarity between a query's dense vector and documents' dense vectors
/*
// not exposed by Lucene
// no-op
/*
/*
/*
// this allows to set indexVersion as it is a private setting
// assert that after decoding the indexed value is equal to expected
// assert that after decoding the indexed value is equal to expected
// test that error is thrown when a document has number of dims more than defined in the mapping
// test that error is thrown when a document has number of dims less than defined in the mapping
/*
/*
// this allows to set indexVersion as it is a private setting
// Check that new vectors cannot be indexed.
/*
/*
// encode vector magnitude at the end
/*
/*
// if local node is voting only, have additional checks on election quorum definition
// if all votes are from voting only nodes, do not elect as master (no need to transfer state)
// if there's a vote from a full master node with same state (i.e. last accepted term and version match), then that node
// should become master instead, so we should stand down. There are two exceptional cases, however:
// 1) if we are in term 0. In that case, we allow electing the voting-only node to avoid poisonous situations where only
//    voting-only nodes are bootstrapped.
// 2) if there is another full master node with an older state. In that case, we ensure that
//    satisfiesAdditionalQuorumConstraints cannot go from true to false when adding new joinVotes in the same election.
//    As voting-only nodes only broadcast the state to the full master nodes, eventually all of them will have caught up
//    and there should not be any remaining full master nodes with older state, effectively disabling election of
//    voting-only nodes.
/*
/*
/*
// generated deterministically for repeatable tests
/*
// start a fresh full master node, which will be brought into the cluster as master by the voting-only nodes
/*
/**
/*
/*
// This setting is only here for backward compatibility reasons as 6.x indices made use of it. It can be removed in 8.x.
// overridable by tests
// only initialize these classes if Watcher is enabled, and only after the plugin security policy for Watcher is in place
// http client
// notification
// conditions
// actions
// inputs
// schedulers
// note: clock is needed here until actions can be constructed directly instead of by guice
// bulk processor configuration
// notification services
// http settings
// encryption settings
/**
// Attach a listener to every index so that we can react to alias changes.
// This listener will be a no-op except on the index pointed to by .watches
// These are all old templates from pre 6.0 era, that need to be deleted
/**
/*
/**
// package private for testing
// package private for testing
/**
/**
/**
/**
/**
// if there is no master node configured in the current state, this node should not try to trigger anything, but consider itself
// inactive. the same applies, if there is a cluster block that does not allow writes
// no local shards, exit early
/**
// changed alias means to always read a new configuration
/**
// exit early, when there are shards, but the current configuration is inactive
// check for different shard ids
// sort the collection, so we have a stable order
// check for different allocation ids
/**
// find all allocation ids for this shard id in the cluster state
// sort the list so it is stable
/**
/**
/*
/*
// indicates that the node has been shutdown and we should never start watcher after this.
// Close if the indices service is being stopped, so we don't run into search failures (locally) that will
// happen because we're shutting down and an watch is scheduled.
/**
// wait until the gateway has recovered from disk, otherwise we think may not have .watches and
// a .triggered_watches index, but they may not have been restored from the cluster state on disk
// if this is not a data node, we need to start it ourselves possibly
//waiting to set state to stopped until after all currently running watches are finished
//only transition from stopping -> stopped (which may not be the case if restarted quickly)
// no local shards, empty out watcher and dont waste resources!
// also check if non local shards have changed, as loosing a shard on a
// remote node or adding a replica on a remote node needs to trigger a reload too
// shardrouting is not comparable, so we need some order mechanism
/**
/**
// for testing purposes only
/*
/**
// template check makes only sense for non existing indices, we could refine this
/**
/**
/**
// this method contains the only async code block, being called by the cluster state listener
// the reason for this is, that loading he watches is done in a sync manner and thus cannot be done on the cluster state listener
// thread
//
// this method itself is called by the cluster state listener, so will never be called in parallel
// setting the cluster state version allows us to know if the async method has been overtaken by another async method
// this is unlikely, but can happen, if the thread pool schedules two of those runnables at the same time
// by checking the cluster state version before and after loading the watches we can potentially just exit without applying the
// changes
/**
/**
// exit early if another thread has come in between
// if we had another state coming in the meantime, we will not start the trigger engines with these watches, but wait
// until the others are loaded
// also this is the place where we pause the trigger service execution and clear the current execution service, so that we make sure
// that existing executions finish, but no new ones are executed
/**
/**
// no index exists, all good, we can start
// find out local shards
// yes, this can happen, if the state is not recovered
// find out all allocation ids
// find out if this hit should be processed locally
// based on the shard allocation ids, get the bucket of the shard, this hit was in
/**
/**
/*
/*
/*
// common fields
// action fields
// result fields
/*
/*
/*
// different error states, depending on how successful the bulk operation was
/**
/**
/*
// Parser for human specified timeouts and 2.x compatibility
/*
/*
// the account associated with this action was deleted
// Apply action fields
// Apply default fields
/**
// Apply the transformation to a simple string
// Apply the transformation to a map
// Apply the transformation to an array of strings
// Apply the transformation to a list of strings
// Apply the transformation to the key
// Copy the value directly in the map if it does not exist yet.
// We don't try to merge maps or list.
/*
/*
// for validation -- throws exception if account not present
/*
// for tests
/*
/*
/*
/*
// the account associated with this action was deleted
/*
/*
/*
// the account associated with this action was deleted
/*
/*
// for validation -- throws exception if account not present
/*
/*
/*
/*
/*
// if the password is null, do not render it out, so we have the possibility to call toXContent when we want to update a watch
// if the password is not null, ensure we never return the original password value, unless it is encrypted with the CryptoService
/*
// picking a reasonable high value here to allow for setups with lots of watch executions or many http inputs/actions
// this is also used as the value per route, if you are connecting to the same endpoint a lot, which is likely, when
// you are querying a remote Elasticsearch cluster
// ssl setup
// headers
// BWC - hack for input requests made to elasticsearch that do not provide the right content-type header!
// auth
// preemptive auth, no need to wait for a 401 first
// timeouts
// headers
// not every response has a content, i.e. 204
/**
// if a proxy scheme is configured use this, but fall back to the same than the request in case there was no special
// configuration given
/**
// for testing
// this could be really simple, as the apache http client has a UriBuilder class, however this class is always doing
// url path escaping, and we have done this already, so this would result in double escaping
// if the passed URL ends with a slash, adding an empty string to the
// unescaped paths will ensure the slash will be added back
/**
// visible for testing
// the default is to accept everything, this should change in the next major version, being 8.0
// we could emit depreciation warning here, if the whitelist is empty
/*
/*
/*
/*
// Users and 2.x specify the timeout this way
// Users and 2.x specify the timeout this way
/**
/*
// putting the content type first, so it can be overridden by custom headers
// Users and 2.x specify the timeout this way
// Users and 2.x specify the timeout this way
/*
/**
// in order to prevent dots in field names, that might occur in headers, we simply de_dot those header names
// when writing toXContent
/*
/**
// default
// min
// max
/*
/*
/**
/**
/*
/**
/*
//Doesn't even start with __ so can't have a content type
// There must be a __<content_type__:: prefix so the minimum length before detecting '__::' is 3
// Assume that the content type name is less than 10 characters long otherwise we may falsely detect strings that start with '__
// and have '__::' somewhere in the content
//There must be a __<content_type__:: prefix so the minimum length before detecting '__::' is 3
/*
// checking if the given value is a date math expression
// checking if the given value is a path expression
/*
/*
/*
/*
// this method performs lenient comparison, potentially between different types. The second argument
// type (v2) determines the type of comparison (this is because the second argument is configured by the
// user while the first argument is the dynamic path that is evaluated at runtime. That is, if the user configures
// a number, it expects a number, therefore the comparison will be based on numeric comparison). If the
// comparison is numeric, other types (e.g. strings) will converted to numbers if possible, if not, the comparison
// will fail and `false` will be returned.
//
// may return `null` indicating v1 simply doesn't equal v2 (without any order association)
// special case for numbers. If v1 is not a number, we'll try to convert it to a number
// could not convert to number
// special case for strings. If v1 is not a string, we'll convert it to a string
// special case for date/times. If v1 is not a dateTime, we'll try to convert it to a datetime
// cannot convert to date...
/*
// All instances has to produce the same hashCode because they are all equal
/*
/**
/*
/**
// TODO: ctx should have its members extracted into execute parameters, but it needs to be a member for bwc access in params
/*
/*
// the condition of the lock is used to wait and signal the finishing of all executions on shutdown
// a marker to not accept new executions, used when the watch service is powered down
/**
// We shouldn't get here, because, ExecutionService#started should have been set to false
/**
// We may have current executions still going on.
// We should try to wait for the current executions to have completed.
// Otherwise we can run into a situation where we didn't delete the watch from the .triggered_watches index,
// but did insert into the history index. Upon start this can lead to DocumentAlreadyExistsException,
// because we already stored the history record during shutdown...
// (we always first store the watch record and then remove the triggered watch)
//fully stop Watcher after all executions are finished
/*
/**
/**
// for testing only
// Lets show the longest running watch first:
// Lets show the execution that pending the longest first:
/**
/**
//pull this to a local reference since the class reference can be swapped, and need to ensure same object is used for put/remove
// TODO log watch record in logger, when saving in history store failed, otherwise the info is gone!
/**
// at the moment we store the status together with the watch,
// so we just need to update the watch itself
// we do not want to update the status.state field, as it might have been deactivated in-between
// do not rethrow this exception, otherwise the watch history will contain an exception
// even though the execution might have been fine
// TODO should we really just drop this exception on the floor?
// it is possible that the watch store update failed, the execution phase is finished
// failed watches stack traces are only logged in debug, otherwise they should be checked out in the history
/*
//Using the generic pool here since this can happen from a write thread and we don't want to block a write
//thread to kick off these additional write/delete requests.
//Intentionally not using the HistoryStore or TriggerWatchStore to avoid re-using the same synchronous
//BulkProcessor which can cause a deadlock see #41390
/**
// input
// condition
// actions
/**
/**
// clear old executions in background, no need to wait
// the watch execution task takes another runnable as parameter
// the best solution would be to move the whole execute() method, which is handed over as ctor parameter
// over into this class, this is the quicker way though
/*
/*
// set the watch early to ensure calls to watch() below succeed.
// we always want to execute a manually triggered watch as the user has triggered this via an
// external API call
/*
/*
/*
// EMPTY is safe here because we never use namedObject
/*
/**
/**
/**
// non existing index, return immediately
/*
/*
/**
/**
/**
/*
/*
/**
/**
/**
/**
/*
/**
/*
// expecting closing of two json object to start the next element in the array
/*
/*
/*
//Attempt to auto detect content type, if not set in response
// EMPTY is safe here because we never use namedObject
// special handling if a list is returned, i.e. JSON like [ {},{} ]
/*
/*
/*
/*
/*
/**
// EMPTY is safe here because we never use namedObject
/*
// Parser for human specified timeouts and 2.x compatibility
/*
/*
/**
/*
/*
/*
/*
/**
/*
/**
/*
/**
// all are guarded by this
// cached cluster setting, required when recreating the notification clients
// using the new "reloaded" secure settings
// cached secure settings, required when recreating the notification clients
// using the new updated cluster settings
// register a grand updater for the whole group, as settings are usable together
// Used for testing only
// update cached cluster settings
// use these new dynamic cluster settings together with the previously cached
// secure settings
// `SecureSettings` are available here! cache them as they will be needed
// whenever dynamic cluster settings change and we have to rebuild the accounts
// use these new secure settings together with the previously cached dynamic
// cluster settings
// build complete settings combining cluster and secure settings
// obtain account names and create accounts
// note this is not final since we mock it in tests and that causes
// trouble since final methods can't be mocked...
// must read under sync block otherwise it might be inconsistent
/**
// get the secure settings out
// filter and cache them...
/*
// required as java doesn't always find the correct mailcap to properly handle mime types
/*;; x-java-content-handler=com.sun.mail.handlers.multipart_mixed");
// exists only to allow ensuring class is initialized
// applying the defaults on missing emails fields
// null check needed, because if the local host does not resolve, this may be null
// this can happen in wrongly setup linux distributions
// saveChanges may rewrite/remove the message id, so
// we need to add it back
// unprivileged code such as scripts do not have SpecialPermission
// if we cannot get the context class loader, changing does not make sense, as we run into the danger of not being able to
// change it back
// unprivileged code such as scripts do not have SpecialPermission
//password = passStr != null ? passStr.toCharArray() : null;
/**
/**
// Secure strings can not be retreived out of a settings object and should be handled differently
/**
/*
/**
/*
/*
/*
/**
/*
/**
// settings that can be configured as smtp properties
// ensure logging of setting changes
// do an initial load
/*
/**
// no mustache, do validation
/*
// embedded
/**
// reject external image source (only allow embedded ones)
/*
/**
/*
/*
/*
/**
/**
/**
/**
/**
/**
/**
/*
/*
// one further to skip the end_object from the attachment
/*
// check for status 200, only then append attachment
/*
/*
/*
// total polling of 10 minutes happens this way by default
// IMPORTANT NOTE: This is only a temporary solution until we made the execution of watcher more async
// This still blocks other executions on the thread and we have to get away from that
// requires us to interval another run, no action to take, except logging
/**
/**
/**
// EMPTY is safe here becaus we never call namedObject
/**
/**
/**
// package protected, so it can be used by the object parser in ReportingAttachmentParser
/*
// exists only to allow ensuring class is initialized
/*
/**
/*
/**
//docs.atlassian.com/jira/REST/cloud/ for the format of the error response body.
// EMPTY is safe here because we never call namedObject
/*
/**
//www.atlassian.com/software/jira
// ensure logging of setting changes
// do an initial load
/*
/**
//v2.developer.pagerduty.com/docs/send-an-event-events-api-v2
// TODO externalize this into something user editable
/**
// contexts can be either links or images, and the v2 api needs them separate
// this field exists because in versions prior 6.0 we accidentally used context instead of contexts and thus the correct data
// was never picked up on the pagerduty side
// we need to keep this for BWC
/*
// "link" context fields
// "image" context fields
/*
/**
/*
/*
/**
// ensure logging of setting changes
// do an initial load
/*
// this excludes the whole body, even though we should only exclude a small field inside of the body
// as this makes debugging pagerduty services much harder, this should be changed to only filter for
// body.service_key - however the body is currently just a string, making filtering much harder
// if for some reason we failed to parse the body, lets fall back on the http status code.
// ok... we have an error
// lets first try to parse the error response in the body
// based on https://developer.pagerduty.com/documentation/rest/errors
// EMPTY is safe here because we never call namedObject
// we don't use this code.. so just consume the token
// too bad... we couldn't parse the body... note that we don't log this error as there's no real
// need for it. This whole error parsing is a nice to have, nothing more. On error, the http
// response object is anyway added to the action result in the watch record (though not searchable)
/*
// this writes out the request to the byte array output stream with the correct excludes
// for slack
/*
/*
/**
// ensure logging of setting changes
// do an initial load
/*
/*
/**
//api.slack.com/docs/attachments#attachment_structure
/*
/*
/*
/*
/*
/*
/**
/*
/**
/*
/*
//This tightly binds the REST API to the java API. pkg private for testing
/*
/*
/*
// this parameter is only needed when current watches are supposed to be returned
// it's used in the WatchExecutionContext.toXContent() method
/*
/*
/*
/*
/** Creates a ctx map and puts it into the returned map as "ctx". */
/** Creates a ctx map. */
/*
/*
/*
/**
// Here we convert a watch search request body into an inline search template,
// this way if any Watcher related context variables are used, they will get resolved.
/**
/**
// TODO this is to retain BWC compatibility in 7.0 and can be removed for 8.0
/*
/**
// Due the inconsistency with templates in ES 1.x, we maintain our own template format.
// This template format we use now, will become the template structure in ES 2.0
// Here we convert watcher template into a ES core templates. Due to the different format we use, we
// convert to the template format used in ES core
// Templates are always of lang mustache:
/*
/*
// try to compile so we catch syntax errors early
// TODO: deprecate one of these styles (returning a map or returning an opaque value below)
/*
/*
/*
/**
// TODO: ctx should have its members extracted into execute parameters, but it needs to be a member bwc access in params
/*
// We need to make a copy, so that we don't modify the original instance that we keep around in a watch:
/*
// Parser for human specified timeouts and 2.x compatibility
/*
/*
/*
// exit early in case nothing changes
// this may reject this action, but prevents concurrent updates from a watch execution
/*
/**
// a watch execution updates the status in between, we still want this want to override the active state
// two has been chosen arbitrary, maybe one would make more sense, as a watch would not execute more often than
// once per second?
/*
/**
/*
/**
/*
// use execute so that the runnable is not wrapped in a RunnableFuture<?>
/*
// When we return the watch via the Get Watch REST API, we want to return the watch as was specified in
// the put api, we don't include the status in the watch source itself, but as a separate top level field,
// so that it indicates the status is managed by watcher itself.
// special case. This API should not care if the index is missing or not,
// it should respond with the watch not being found
/*
/**
// ensure we only filter for the allowed headers
/*
// adhere to the contract of returning the original state if nothing has changed
/*
/**
/*
/*
/**
/**
/**
/*
/**
/**
/**
// for bwc reasons, active/total contain the same values
/**
/**
/*
/*
/*
/**
/*
/*
// filter out expired dates before sorting
// no date in the future found, return -1 to the caller
/*
/*
/*
/*
/**
// computed once
/*
/*
/**
/*
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
/*
//Failed to parse as a date try datemath parsing
// should never be, it's fully controlled internally (not coming from the user)
/*
/*
/*
// why are we calling putAll() here instead of assigning a brand
// new concurrent hash map you may ask yourself over here
// This requires some explanation how TriggerEngine.start() is
// invoked, when a reload due to the cluster state listener is done
// If the watches index does not exist, and new document is stored,
// then the creation of that index will trigger a reload which calls
// this method. The index operation however will run at the same time
// as the reload, so if we clean out the old data structure here,
// that can lead to that one watch not being triggered
// only update the schedules data structure if the scheduled trigger really has changed, otherwise the time would be reset again
// resulting in later executions, as the time would only count after a watch has been stored, as this code is triggered by the
// watcher indexing listener
// this also means that updating an existing watch would not retrigger the schedule time, if it remains the same schedule
// visible for testing
/**
/*
/*
/*
/*
//32 represents the last day of the month
// order doesn't matter
//days are already sorted
/*
/*
// we don't care about order
/*
//32 represents the last day of the month
// order doesn't matter
/*
/*
/**
// EMPTY is safe here because we never use namedObject
// Parser for human specified and 2.x backwards compatible throttle period
// verify the status is valid (that every action indeed has a status)
// we need to create the initial statuses for the actions
/*
/**
/*
/*
/*
// this test emulates an index with 10 shards, and ensures that triggering only happens on a
// single shard
// no matter how many shards we had, this should have been only called once
//
// tests for cluster state updates
//
// a shard is marked as relocating, no change in the routing yet (replica might be added,
// shard might be offloaded)
// setup 5 shards, one replica, 10 shards total, all started
// no matter how many copies of a shard exist, a watch should always be triggered exactly once
// random number of shards
// ensure that non data nodes, deal properly with this cluster state listener
// 4 nodes, each node has one shard, now node 3 fails, which means only one node should
// reload, where as two should not
// this test emulates on of those two nodes
// now create a cluster state where node 4 is missing
// ensure no configuration replacement has happened
// if the creates a .watches alias that points to two indices, set watcher to be inactive
// index foo pointing to .watches
// regular cluster state with correct single alias pointing to watches index
// index bar pointing to .watches
// cluster state with two indices pointing to the .watches index
// ensure no configuration replacement has happened
//
// helper methods
//
/**
// now point the alias, if the watch index is not .watches
/*
/*
// Trying to start a second time, but that should have no effect.
// the internal index format, required
// mark watcher manually as stopped
// Starting via cluster state update, as the watcher metadata block is removed/set to true
// no change, keep going
// shard moved over to node 2
// set current allocation ids
// no more local shards, lets pause execution
// no further invocations should happen if the cluster state does not change in regard to local shards
// add a replica in the local node
// randomize between addition or removal of a replica
// make sure that cluster state changes can be processed on nodes that do not hold data
// first add the shard allocation ids, by going from empty cs to CS with watcher index
// now remove watches index, and ensure that pausing is only called once, no matter how often called (i.e. each CS update)
// ensure not all templates are added, otherwise life cycle service would start
// this emulates a node outage somewhere in the cluster that carried a watcher shard
// the number of shards remains the same, but we need to ensure that watcher properly reloads
// previously we only checked the local shard allocations, but we also need to check if shards in the cluster have changed
// initialize the previous state, so all the allocation ids are loaded
// the internal index format, required
// reset the mock, the user has to mock everything themselves again
/*
// makes sure WatcherMetaData is registered in Custom MetaData
// random order of insertion
// serialize metadata
// deserialize metadata again
// check that custom metadata still present
// consume null
// consume "watcher"
// consume endObject
/*
// ensure index module is not called, even if watches index is tried
// this will trip an assertion if the watcher indexing operation listener is null (which it is) but we try to add it
// also no component creation if not enabled
// old calculation was 5 * number of processors
/*
// cluster state setup, with one node, one shard
// response setup, successful refresh response
// empty scroll response, no further scrolling needed
// one search response containing active and inactive watches
// if we have to reload the watcher service, the execution service should not be paused, as this might
// result in missing executions
/*
/**
// adding an action that throws an error and is associated with a 60 minute throttle period
// with such a period, on successful execution we other executions of the watch will be
// throttled within the hour... but on failed execution there should be no throttling
// there should be a single history record with a failure status for the action:
// now we'll trigger the watch again and make sure that it's not throttled and instead
// writes another record to the history
// within the 60 minute throttling period
// there should be a single history record with a failure status for the action:
// now lets confirm that the ack status of the action is awaits_successful_execution
/*
// check that action toXContent contains all the results
// check that action toXContent contains all the results
// check that action toXContent contains all the results
/*
/*
// setup mock response
// http attachment
// setup mock response, second one is an error
// setup email attachment parsers
//localhost/first").endObject()
//localhost/second").endObject()
/*
// Have a sample document in the index, the watch is going to evaluate
/*
// check for unique message ids, should be two as well
/*
// Keystore and private key will share the same password
/*
// wrong type for field
// unknown field
// unknown refresh policy
// using doc_id with bulk fails regardless of using ID
// using doc_id with _id
// intentionally immutable because the other side needs to cut out _id
// should the result resemble a failure or a partial failure
/*
//" + host + ":" + port;
// overridden
// will overrides the defaults
// should not be overridden byt the defaults
// should not be overridden byt the defaults
//internal-jira.elastic.co:443");
/**
// test the fields is not overwritten
/*
/*
// cover the special case that randomIssueDefaults() left an empty map here as
// well as in the action1, so that those would be equal - make sure they are not
// another low probability case, that a random proxy is exactly the same including
// port number
//internal-jira.elastic.co:443");
/**
/*
/*
/*
/*
//example.org");
/*
/*
// error response
// success
/*
/*
//Since the default throttle period is 5 seconds but we have overridden the period in the watch this should trigger
// create a mapping with a wrong @timestamp field, so that the index action of the watch below will fail
/*
/*
//This should fail since we are not supplying a url
/*
/**
/*
// enable http
// for http
/*
// Certain headers keys like via and host are illegal and the jdk http client ignores those, so lets
// prepend all keys with `_`, so we don't run into a failure because randomly a restricted header was used:
// We can't use the client created above for the server since it is only a truststore
//Can't use TrustAllConfig in FIPS mode
// We can't use the client created above for the server since it only defines a truststore
// Known server with a valid cert from a commercial CA
// this test fakes a proxy server that sends a response instead of forwarding it to the mock web server
// ensure we hit the proxyServer and not the webserver
// no proxy configured at all
//elastic.co").build(), HttpProxy.NO_PROXY);
// no system wide proxy configured, proxy in request
//elastic.co").proxy(new HttpProxy("localhost", 23456)).build(),
//localhost:23456"));
// system wide proxy configured, no proxy in request
//elastic.co").build(),
//localhost:1234"));
// proxy in request, no system wide proxy configured. request
//elastic.co").proxy(new HttpProxy("localhost", 23456, Scheme.HTTP)).build(),
//localhost:23456"));
// proxy in request, system wide proxy configured. request wins
//elastic.co").proxy(new HttpProxy("localhost", 23456, Scheme.HTTPS)).build(),
//localhost:23456"));
// this test fakes a proxy server that sends a response instead of forwarding it to the mock web server
// on top of that the proxy request is HTTPS but the real request is HTTP only
// We can't use the client created above for the server since it is only a truststore
// ensure we hit the proxyServer and not the webserver
// this test fakes a proxy server that sends a response instead of forwarding it to the mock web server
// ensure we hit the proxyServer and not the webserver
// %2F is a slash that needs to be encoded to not be misinterpreted as a path
// ensure that fromUrl acts the same way than the above builder
//localhost:%s%s", webServer.getPort(), path)).build();
// under no circumstances have a double encode of %2F => %25 (percent sign)
// finally fixing https://github.com/elastic/x-plugins/issues/1141 - yay! Fixed due to switching to apache http client internally!
//" + webServer.getHostName() + ":" + webServer.getPort() + "/foo";
// not allowed by RFC, only allowed for GET or HEAD
//" + webServer.getHostName() + ":" + webServer.getPort() + "/foo";
//blocked.domain.org:" + webServer.getPort() +
//blocked.domain.org:" + webServer.getPort() + "/foo";
//" + webServer.getHostName() + ":" + webServer.getPort() + "/redirect" + i;
// blacklisted
//example*", "https://bar.com/foo",
//www.test.org"));
//example.org"), is(true));
//example.com"), is(true));
//examples.com"), is(true));
//example-website.com"), is(true));
//noexample.com"), is(false));
//bar.com/foo"), is(true));
//bar.com/foo2"), is(false));
//bar.com"), is(false));
//www.test.org"), is(true));
//www.test.org"), is(true));
//example.org/foo/", "/foo/");
//example.org/foo", "/foo");
//example.org/", "");
//example.org", "");
//%s:%s", webServer.getHostName(), webServer.getPort());
/**
/*
// setting an unroutable IP to simulate a connection timeout
// it's supposed to be 10, but we'll give it an error margin of 2 seconds
// expected
// it's supposed to be 7, but we'll give it an error margin of 2 seconds
// expected
// it's supposed to be 7, but we'll give it an error margin of 2 seconds
// expected
/*
/*
// it's supposed to be 10, but we'll give it an error margin of 2 seconds
// it's supposed to be 3, but we'll give it an error margin of 2 seconds
// it's supposed to be 3, but we'll give it an error margin of 2 seconds
/*
//www.example.org:1234/foo/bar/org?param=test", builder);
// ssl support, getting the default port right
//www.example.org/test", builder);
// test without specifying port
//www.example.org", builder);
// encoded values
//www.example.org?foo=%20white%20space", builder);
//"));
//]"));
//www.test.de/%7B%7Bfoo%7D%7D").build();
/*
//www.example.org:1234/foo/bar/org?param=test", builder);
// test without specifying port
//www.example.org", builder);
// encoded values
//www.example.org?foo=%20white%20space", builder);
//");
//]"));
//localhost:9200/generic/createevent");
// micros and nanos don't round trip will full precision so exclude them from the test
// micros and nanos don't round trip will full precision so exclude them from the test
/*
// 2
// body is null
//3
/*
/*
/*
/*
/*
/*
/*
// comparing as strings
// comparing as strings
// comparing as strings
// comparing as numbers
// comparing as strings
// comparing as strings
// comparing as strings
// comparing as numbers
// comparing as strings
// comparing as strings
// comparing as strings
// comparing as numbers
// comparing as strings
// comparing as strings
// comparing as strings
// comparing as numbers
// comparing as strings
// comparing as strings
// comparing as strings
// comparing as numbers
// comparing as strings
// comparing as strings
// comparing as strings
// comparing as numbers
/*
/*
/*
// we use a small thread pool to force executions to be queued
/*
/*
// introduce a very short sleep time which we can use to check if the duration in milliseconds is correctly created
// watch level transform
// action throttler
// action level conditional
// action level transform
// the action
// test execution duration, make sure it is set at all
// no exact duration check here, as different platforms handle sleep differently, so this might not be exact
// test stats
// watch level transform
// action throttler
// action level condition (unused)
// action level transform (unused)
// the action
// watch level transform
// action throttler
// action level condition (unused)
// action level transform (unused)
// the action
// watch level transform
// action throttler
// action level condition (unused)
// action level transform (unused)
// the action
// watch level transform
// action throttler
// action level condition
// action level transform
// the action
// the action level transform is executed before the action itself
// watch level transform
// action throttler
// action level conditional
// action level transform
// the action
// action throttler
// unused with throttle
// action throttler
// action condition (always fails)
// note: sometimes it can be met _with_ success
// unused with failed condition
// action throttler
// action condition (always fails)
// unused with failed condition
// watch level transform
// action throttler
// execute needs to fail
// execute needs to fail
// action throttler, no throttling
// the action
// Should be null
// Should still be null, header is not yet set
// Should no longer be null now that the proper header is set
/*
// First return a scroll response with a single hit and then with no hits
// the elasticsearch migration helper is doing reindex using aliases, so we have to
// make sure that the watch store supports a single alias pointing to the watch index
// the elasticsearch migration helper is doing reindex using aliases, so we have to
// make sure that the watch store supports only a single index in an alias
// this is a special condition that could lead to an NPE in earlier versions
/*
/**
/**
// only one action should have failed via condition
// only one action should have failed via condition
//actionConditions.add(conditionPasses);
/*
// all actions should be successful
/**
/**
/**
/*
//" + randomFrom("localhost", "internal-jira.elastic.co") + ":" + randomFrom(80, 8080, 449, 9443);
/*
/**
// email
// the action should fail as no email server is available
/*
/**
// one for the input, one for the webhook
// the action should fail as no email server is available
// delete all history indices to ensure that we only need to check a single index
// switch between delaying the input or the action http request
// ensure watcher history index has been written with this id
// ensure that enabled is set to false
/*
/**
// the action should fail as no email server is available
/*
/**
// the action should fail as no email server is available
/*
/**
/*
// execute another watch which with a transform that should conflict with the previous watch. Since the
// mapping for the transform construct is disabled, there should be no problems.
// time might have rolled over to a new day, thus we need to check that this field exists only in one of the history indices
/*
/*
/* note, first line does not need to be parsed
// hackedy hack...
// first pass JSON and check for correct inputs
// now execute
// final payload check
// parsing it back as well!
// no exception means all good
//github.com/elastic/x-plugins/issues/3736
/* https://github.com/elastic/x-plugins/issues/3736
//github.com/elastic/x-plugins/issues/3736
/* https://github.com/elastic/x-plugins/issues/3736
/*
// enable http
// for http
/*
/*
// http is the default
// get is the default
//127.0.0.1:12345").build();
/*
/*
// { "script" : { "lang" : "mockscript", "source" : "1" } }
/*
// single account, this will also be the default
// counter is still 1 because the account is cached
// update secure setting only
// update both
// update secure first
// update cluster second
// update cluster first
// update secure second
/*
// falling back on the default
// falling back on the default
// falling back on the default
/*
// we need to remove the `_`... we only added support for `_` for readability
// the actual properties (java mail properties) don't contain underscores
// default properties
// bcc should not be there... (it's bcc after all)
/*
// the yaml factory in es always emits unix line breaks
// this seems to be a bug in jackson yaml factory that doesn't default to the platform line break
/*
// verifying the email password is stored encrypted in the index
// verifying the password is not returned by the GET watch API
// now we restart, to make sure the watches and their secrets are reloaded from the index properly
// now lets execute the watch manually
/*
/*
// only the whole string is tested if this is a mustache template, not parts of it
//4 attachments, zero warning, one warning, two warnings, and one with html that should be stripped
//text
//html - pull the arguments as it is run through the sanitizer
/*
/*
//www.google.com/search?q={{ctx.metadata.name}}'>Testlink</a> meta" +
//www.google.com/search?q&#61;{{ctx.metadata.name}}\" rel=\"nofollow\">Testlink</a> " +
//www.google.com/search?q&#61;{{ctx.metadata.name}}\" rel=\"nofollow\">Testlink</a> " +
//test.com/nastyimage.jpg\"/>This is a bad image";
//test.com/nastyimage.jpg\" />This is a bad image";
//test.com/nastyimage.jpg\" />This is a bad image";
/*
/*
/*
/*
/*
//www.example.org/ovb/api/reporting/generate/dashboard/My-Dashboard";
// returns interval HTTP code for five times, then return expected data
// first invocation to the original URL
// all other invocations to the redirected urls from the JSON payload
// test that the header "kbn-xsrf" has been set to "reporting" in all requests
//www.example.org/", randomBoolean(), null, null, null, null);
// closing json bracket is missing
// path must be a field, but is an object here
//www.example.org/", randomBoolean(), null, null, null, null);
//www.example.org/", randomBoolean(), TimeValue.timeValueMillis(1), 10, null, null);
//www.example.org/", randomBoolean(), TimeValue.timeValueMillis(1), 1, null, null);
//www.example.org/", randomBoolean(), TimeValue.timeValueMillis(1), null, null, null);
//www.example.org/", randomBoolean(), TimeValue.timeValueMillis(1), null, null, null);
//www.example.org/REPLACEME", randomBoolean(),
//www.example.org/REPLACEME", randomBoolean(), null, -10, null, null));
//www.example.org/", randomBoolean(), TimeValue.timeValueMillis(1), null, null, proxy);
//parameterize the messages
//add a parameter
//parameterize the messages
//ensure the reportId is parameterized in
/*
/**
/**
// Must have privileged access because underlying server will accept socket connections
/*
//internal-jira.elastic.co:443";
//Setting test as invalid scheme url
//localhost");
//localhost/foo", "/foo");
//localhost/foo/", "/foo/");
// this ensures we retain backwards compatibility
//localhost/", JiraAccount.DEFAULT_PATH);
//localhost", JiraAccount.DEFAULT_PATH);
//internal-jira.elastic.co:443");
/*
/*
/*
// since its a snippet
// hardcoded if the SOURCE is null
// assert the actuals were the same as expected
/*
// in earlier versions of the PD action the wrong JSON was sent, because the contexts field was named context
// the pagerduty API accepts any JSON, thus this was never caught
//www.elastic.co/products/x-pack/alerting", "Go to the Elastic.co Alerting website"),
//www.elastic.co/assets/blte5d899fd0b0e6808/icon-alerting-bb.svg",
//www.elastic.co/products/x-pack/alerting", "X-Pack-Alerting website link with log")
/*
/*
/*
//elastic.co"));
//elastic.co");
//elastic.co"));
// ensure at least one attachment in the event the text is null
// relies on the fact that all the templates we use are inline templates without param place holders
// the url path contains sensitive information, which should not be exposed
/*
// make sure we test true/false/no params
/*
// issue #852
// both buckets have to include the following keys
// issue #4614
/*
// NOTE: we use toString() here because two ZonedDateTime are *not* equal, we need to check with isEqual
// for date/time equality, but no hamcrest matcher exists for that
/*
// 0 is special - no unit required
// start object
// field name
// value
// start object
// field name
// value
// start object
// field name
// value
// start object
// field name
// value
// start object
// field name
// value
// This code is lifted strait from 2.x's TimeValueTests.java
// Extra fractional tests just because that is the point
// And some crazy fractions just for fun
/*
// now delete one template from the cluster state and lets retry
// now delete one template from the cluster state and lets retry
// if a node is newer than the master node, the template needs to be applied as well
// otherwise a rolling upgrade would not work as expected, when the node has a .watches shard on it
/*
//This will be escaped as it is constructed
//This will be escaped at the end
/*
/*
/*
/*
// we do this by default in core, but for watcher this isn't needed and only adds noise.
// watcher settings that should work despite randomization
// SLM can cause timing issues during testsuite teardown: https://github.com/elastic/elasticsearch/issues/50302
// SLM is not required for tests extending from this base class and only add noise.
// security has its own transport service
// security has its own transport
// we have to explicitly add it otherwise we will fail to set the check_index_on_close setting
// ILM is required for watcher template index settings
/**
// now that the license is enabled and valid we can freeze all nodes clocks
// Clear all internal watcher state for the next test method:
/**
//github.com/elastic/elasticsearch-migration/
// alias for .watches, setting the index template to the same as well
// Create an index to get the template
// Now replace it with a randomly named index
// alias for .triggered-watches, ensuring the index template is set appropriately
// Now replace it with a randomly-named index
// TODO remove this shitty method... the `assertConditionMet` is bogus
// The watch_history index gets created in the background when the first watch is triggered
// so we to check first is this index is created and shards are started
// Verify that the index templates exist:
/**
/*
/*
/*
// use a single clock across all nodes using this plugin, this lets keep it static
/*
/**
/*
/*
/*
/*
/*
/**
//                .put("recycler.page.limit.heap", "60%")
/*
// First clean everything and index the watcher (but not via put alert api!)
// Now for each scheduler impl run the benchmark
// Finally print out the results in an asciidoc table:
/*
// Have a sample document in the index, the watch is going to evaluate
// The watch's condition won't meet because there is no data that matches with the query
// Index sample doc after we register the watch and the watch's condition should meet
// Deleting the same watch for the second time
// Have a sample document in the index, the watch is going to evaluate
// In watch store we fail parsing if an watch contains undefined fields.
// The watch index template the mapping is defined as strict
// Have a sample document in the index, the watch is going to evaluate
// in this watcher the condition will fail, because max_score isn't extracted, only total:
// Check that the input result payload has been filtered
// reset, so we don't miss event docs when we filter over the _timestamp field.
/*
// valid watch record:
// unknown condition:
// unknown trigger:
// We need to wait until all the records are processed from the internal execution queue, only then we can assert
// that numRecords watch records have been processed as part of starting up.
// the watch history should contain entries for each triggered watch, which a few have been marked as not executed
// Watcher could prevent to start if a watch record tried to executed twice or more and the watch didn't exist
// for that watch record or the execution threadpool rejected the watch record.
// A watch record without a watch is the easiest to simulate, so that is what this test does.
// we rarely create an .watches alias in the base class
// We need to wait until all the records are processed from the internal execution queue, only then we can assert
// that numRecords watch records have been processed as part of starting up.
// but even then since the execution of the watch record is async it may take a little bit before
// the actual documents are in the output index
/*
// Transforms the value of a1, equivalent to:
//      ctx.vars.a1_transform_value = ctx.vars.watch_transform_value + 10;
//      ctx.payload.a1_transformed_value = ctx.vars.a1_transform_value;
//      return ctx.payload;
// Transforms the value of a2, equivalent to:
//      ctx.vars.a2_transform_value = ctx.vars.watch_transform_value + 20;
//      ctx.payload.a2_transformed_value = ctx.vars.a2_transform_value;
//      return ctx.payload;
// defaults to match all;
// we don't store the computed vars in history
/*
// issue: https://github.com/elastic/x-plugins/issues/2338
// See https://github.com/elastic/x-plugins/issues/2913
// The result of the search input will be a failure, because a missing index does not exist when
// the query is executed
// wrapping this randomly into a chained input to test this as well
// as fields with dots are allowed in 5.0 again, the mapping must be checked in addition
// lets make sure the body fields are disabled
// See https://github.com/elastic/x-plugins/issues/2913
// wrapping this randomly into a chained input to test this as well
// as fields with dots are allowed in 5.0 again, the mapping must be checked in addition
// lets make sure the body fields are disabled
// also ensure that the status field is disabled in the watch history
/*
// verifying the basic auth password is stored encrypted in the index when security
// is enabled, and when it's not enabled, it's stored in plain text
// verifying the password is not returned by the GET watch API
// making sure we have the basic auth
// now we restart, to make sure the watches and their secrets are reloaded from the index properly
// now lets execute the watch manually
// now trigger the by the scheduler and make sure that the password is also correctly transmitted
// verifying the basic auth password is stored encrypted in the index when security
// is enabled, when it's not enabled, the password should be stored in plain text
// verifying the password is not returned by the GET watch API
// making sure we have the basic auth
// now we restart, to make sure the watches and their secrets are reloaded from the index properly
// now lets execute the watch manually
// the auth username exists
/*
//need to use the real scheduler
/*
// source: https://discuss.elastic.co/t/need-help-for-energy-monitoring-system-alerts/89415/3
// advance past the first starting object
// no body in the search request
/*
/*
//github.com/elastic/elasticsearch/issues/36782")
// this is the standard setup when starting watcher in a regular cluster
// the index does not exist, a watch gets added
// the watch should be executed properly, despite the index being created and the cluster state listener being reloaded
// now we start with an empty set up, store a watch and expected it to be executed
/*
// There shouldn't be more a1 actions in the index after we ack the watch, even though the watch was triggered
// There should be more a2 actions in the index after we ack the watch
// Now delete the event and the ack states should change to AWAITS_EXECUTION
// There shouldn't be more a1 actions in the index after we ack the watch, even though the watch was triggered
// There shouldn't be more a2 actions in the index after we ack the watch, even though the watch was triggered
// Now delete the event and the ack states should change to AWAITS_EXECUTION
// There shouldn't be more actions in the index after we ack the watch, even though the watch was triggered
/*
/*
// When using the MockScriptPlugin we can map File scripts to inline scripts:
// the name of the file script is used in test method while the source of the file script
// must match a predefined script from CustomScriptPlugin.pluginScripts() method
// put a watch that has watch level transform:
// put a watch that has a action level transform:
// put a watch that has watch level transform:
// put a watch that has a action level transform:
/*
// the "name" field
/*
/*
// need this to prevent some compilation errors, i.e. in 1.8.0_91
/*
// we now know the watch is executing... lets deactivate it
// wait until no watch is executing
// lets activate it again
// some time in 2050
// now that we filtered out the watch status state, lets put it back in
// now, let's restart
/*
// This is a special case, since locking is removed
// Deleting a watch while it is being executed is possible now
// This test ensures that there are no leftovers, like a watch status without a watch in the watch store
// Also the watch history is checked, that the error has been marked as deleted
// The mock webserver does not support count down latches, so we have to use sleep - sorry!
// without this sleep the delete operation might overtake the watch execution
// the watch is gone, no leftovers
// the watch history shows a successful execution, even though the watch was deleted
// during execution
// watch has been executed successfully
// no exception occurred
/*
/*
// run every second so we can ack it
// since we're forcing, lets ack the action, such that it'd supposed to be throttled
// but forcing will ignore the throttling
// lets wait for the watch to be ackable
// the action should be manually skipped/throttled
/*
// does not matter if the watch does not exist or the index does not exist, we expect the same response
// if the watches index is an alias, remove the alias randomly, otherwise the index
/*
// https://github.com/elastic/x-plugins/issues/2490
/*
/*
// mock an index response that calls the listener
// set up threadcontext with some arbitrary info
/*
/*
/**
/*
// simple watch, input and simple action
/*
/*
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
/*
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
/*
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
/*
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
/*
/*
/*
/*
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
/*
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
// advancing to the start object
/*
// having a low value here speeds up the tests tremendously, we still want to run with the defaults every now and then
// fun part here (aka WTF): DayOfWeek with Joda is MON-SUN, starting at 1
//                          DayOfWeek with Watcher is SUN-SAT, starting at 1
// ensure job was only called twice independent from its name
// add watch with schedule
// add watch with same id but different watch
/*
// randomized testing sets arbitrary locales and timezones, and we do not care
// we always have to output in standard locale and independent from timezone
// check for header line
/*
// not started yet, so both nulls
//if date has millisecond precision its nanosecond field will be rounded to millis (equal millis * 10^6)
/*
// by default headers are hidden
// but they are required when storing a watch
// test equals
/*
// a fake trigger service that advances past the trigger end object, which cannot be done with mocking
// parse in default mode:
// chain
/*
/*
// as default timeout seems not enough on the jenkins VMs
/*
/*
// the error should be rethrown on another thread
// we never logged anything
// restore the uncaught exception handler
/*
/**
/*
// Create SPNs and UPNs
/**
/**
/**
/**
/*
// Client login and init token preparation
// Client login and init token preparation
// Client login and init token preparation
/*
/**
// KDC properties
// LDAP properties
/**
// start ldap server
// create ldap backend conf
// Kdc Server
// transport
/**
/**
/**
/*
// Client login and init token preparation
// Service Login
// Handle Authz header which contains base64 token
// Authenticate service on client side.
/*
/**
/**
/**
/**
/**
/**
/**
/**
// refresh Krb5 config during tests as the port keeps changing for kdc server
/**
/*
/*
// we increase the timeout here to 90 seconds to handle long waits for a green
// cluster health. the waits for green need to be longer than a minute to
// account for delayed shards
/**
// create additional user and role
// Wait for watcher to actually start....
/* Shut down watcher after every test because watcher can be a bit finicky about shutting down when the node shuts
/**
// index documents for the rollup job
// create the rollup job
// start the rollup job
// Fetch a basic watch
// Fetch a watch with "fun" throttle periods
/*
// password doesn't come back because it is hidden
// check that the rollup job is started using the RollUp API
// check that the rollup job is started using the Tasks API
/*");
// check that the rollup job is started using the Cluster State API
/*
// trigger .ml-config index creation
// .ml-config has mappings for analytics as the feature was introduced in 7.3.0
// .ml-config does not yet have correct mappings, it will need an update after cluster is upgraded
// trigger .ml-config index mappings update
// assert that the mappings are updated
/*
//github.com/elastic/elasticsearch/issues/36816")
// create jobs and datafeeds
// open job and started datafeed
// The persistent task params for the job & datafeed left open
// during upgrade should be updated with new fields
// open the migrated job and datafeed
/*
/*
/**
/**
// top-level
// "all"
// "rules"
/*
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/**
/*
/*
/*
// create the test-index index
// wait for the replica to recover
// index some documents
// we have to wait for the post-operation global checkpoint sync to propagate to the replica
// int looks funny here since global checkpoints are longs but the response parser does not know enough to treat them as long
/*
// create the test-index index
// index documents for the rollup job
// create the rollup job
// fast cron so test runs quickly
// start the rollup job
// Wait for the job to finish, by watching how many rollup docs we've indexed
// Refresh the rollup index to make sure all newly indexed docs are searchable
// Do the live agg results match the rollup agg results?
// Does searching the live index via rollup_search work match the live search?
// check that the rollup job is started using the RollUp API
// check that the rollup job is started using the Tasks API
/*");
// check that the rollup job is started using the Cluster State API
/*
//127.0.0.1:" + getEphemeralPortFromProperty("8080") + "/c2id/clients";
//127.0.0.1:" + getEphemeralPortFromProperty("8080") + "/c2id-login/api/";
/**
//my.elasticsearch.org/rp\"," +
//my.fantastic.rp/cb\"]" +
//my.fantastic.rp/cb\"]" +
// C2ID doesn't have a non JS login page :/, so use their API directly
// see https://connect2id.com/products/server/docs/guides/login-page
// Initiate the authentication process
// Actually authenticate the user with ldapAuth
// Get the consent screen
//loa.c2id.com/basic\"," +
// If needed, submit the consent
// Use existing realm that can't authenticate the response, or a non-existent realm
//localhost:8080"));
//localhost:8080"));
/**
/**
/*
//localhost:" + getFromProperty("636");
//127.0.0.1:" + getFromProperty("636");
/*
// fake realms so ssl will get loaded
//openldap does not use cn as naming attributes by default
//base search on a groups means that the user can be in just one group
//openldap does not use cn as naming attributes by default
// The certificate used in the vagrant box is valid for "localhost", but not for "127.0.0.1"
//openldap does not use cn as naming attributes by default
// The certificate used in the vagrant box is valid for "localhost" (but not for "127.0.0.1")
/*
/*
//auth
//lookup
/*
/*
/**
/*
/**
/**
//one alias per index with prefix "alias-"
// If we get to this point and we haven't added an alias to the request we need to add one
// or the request will fail so use noAliasAdded to force adding the alias in this case
//one alias pointing to all indices
/*
// While it's possible to use a Hamcrest matcher for this, the failure is much less legible.
/*
/**
// Ask for recovery to be quick
/*
/**
// We don't know whether the job is on an old or upgraded node, so cannot assert that the mappings have been upgraded
// Use a custom index because other rolling upgrade tests meddle with the shared index
// The name of the concrete index underlying the results index alias may or may not have been changed
// by the upgrade process (depending on what other tests are being run and the order they're run in),
// so navigating to the next level of the tree must account for both cases
// TODO: as the years go by, the field we assert on here should be changed
// to the most recent field we've added that is NOT of type "keyword"
/*
// wait for long enough that we give delayed unassigned shards to stop being delayed
// create the rollup job with an old interval style
// start the rollup job
// check that the rollup job is started using the RollUp API
// check that the rollup job is started using the Tasks API
/*");
// check that the rollup job is started using the Cluster State API
/*
// usual case, clients have different versions
// tests assumes exactly two clients to simplify some logic
// Creates two access and refresh tokens and stores them in the token_backwards_compatibility_it index to be used for tests in the
// mixed/upgraded clusters
// Creates access and refresh tokens and uses the refresh token. The new resulting tokens are used in different phases
// refresh the token just created. The old token is invalid (tested further) and the new refresh token is tested in the upgraded
// cluster
// assert previous access token still works
// Creates access and refresh tokens and tries to use the access tokens several times
// invalidate access token
// invalidate refresh token
// Verify that an old token continues to work during all stages of the rolling upgrade
// 2 is invalidated in another mixed-cluster test, 5 is invalidated in the old cluster
// Verify that an old, invalidated token remains invalidated during all stages of the rolling upgrade
// Creates two access and refresh tokens and stores them in the token_backwards_compatibility_it index to be used for tests in the
// mixed/upgraded clusters
// verify new nodes can refresh tokens created by old nodes and vice versa 
// Verify that we can invalidate an access and refresh token in a mixed cluster
// The token might be already invalidated by running testInvalidatingTokenInMixedCluster in a previous stage
// we don't try to assert it works before invalidating. This case is handled by testTokenWorksInMixedCluster
// invalidate refresh token
// invalidate access token
// invalidate refresh token
/*
// match templates, independent of the version, at least 2 should exist
/**
// Even if we get back to started, we may periodically get set back to `indexing` when triggered.
// Though short lived due to no changes on the source indices, it could result in flaky test behavior
// We want to make sure our latest state is written before we turn the node off, this makes the testing more reliable
// A continuous transform should automatically become started when it gets assigned to a node
// if it was assigned to the node that was removed from the cluster
// Add a new user and write data to it
// This is so we can have more reliable data counts, as writing to existing entities requires
// rescanning the past data
// Index the data
// The frequency and delay should see the data once its indexed
// Want to make sure we get the latest docs
// create mapping
// create index
/*
// to account for slow as hell VMs
/**
// we increase the timeout here to 90 seconds to handle long waits for a green
// cluster health. the waits for green need to be longer than a minute to
// account for delayed shards
/*
/*
/*
/*
/*
// Auto follow stats are kept in-memory on master elected node
// and if this node get updated then auto follow stats are reset
// Auto follow stats are kept in-memory on master elected node
// and if this node get updated then auto follow stats are reset
// can't run this test when executing rolling upgrade against current version.
// At this point the leader cluster has not been upgraded, but follower cluster has been upgrade.
// Create a leader index in the follow cluster and try to follow it in the leader cluster.
// This should fail, because the leader cluster at this point in time can't do file based recovery from follower.
// At this point all nodes in both clusters have been updated and
// the leader cluster can now follow not_supported index in the follower cluster:
/*
/**
/**
/**
/**
// "all"
// "rules"
/**
/**
//github.com/elastic/elasticsearch/issues/44410")
// this ACS comes from the config in build.gradle
//localhost:54321" + SP_ACS_PATH_1);
//github.com/elastic/elasticsearch/issues/44410")
// this ACS comes from the config in build.gradle
//localhost:54321" + SP_ACS_PATH_2);
//github.com/elastic/elasticsearch/issues/44410")
//localhost:54321" + SP_ACS_PATH_WRONG_REALM);
/**
/**
/**
/**
/**
/**
// The ACS url provided from the SP is going to be wrong because the gradle
// build doesn't know what the web server's port is, so it uses a fake one.
/**
// Yes this is seriously bad - but would you prefer I run a headless browser for this?
/**
/**
/**
/**
/*
/**
// check that the extension's policy works.
/*
/**
/*
// set a custom header
// set a custom header
// set a custom header
// set a custom header
/*
/*
/**
/*
/**
/*
/**
/*
/*
/**
// Authenticate as the custom realm superuser
// But "run-as" the role mapped user
/*
/*
/**
// roleB has all permissions on index "foo", so creating "foo" should succeed
// the first custom roles provider has set ROLE_A to only have read permission on the index,
// the second custom roles provider has set ROLE_A to have all permissions, but since
// the first custom role provider appears first in order, it should take precedence and deny
// permission to create the index
/*
// answer yes to continue prompt
/*
// the mock filesystem we use so permissions/users/groups can be modified
// the config dir for each test to use
// settings used to create an Environment for tools
/** checks the user exists with the given password */
// CommandTestCase#execute runs passwd with default settings, so bcrypt with cost of 10
/** Checks the user does not exist in the users or users_roles files*/
/** checks the role has the given users, or that the role does not exist if not users are passed. */
// roles unchanged
// roles unchanged
// output should not contain '*' which indicates unknown role
// output should not contain '*' which indicates unknown role
// output should not contain '*' which indicates unknown role
/*
// TODO: maybe we should actually check the key is...i dunno...valid?
/*
/*
// no posix file permissions, nothing to test, done here
// assert the temporary file was created while trying to write the file
// now assert the temporary file was cleaned up after the write failed
// finally, assert the original file contents remain
/*
/**
/*
// Noop
// This is required because if tests are not enable no
// tests will be run in the entire project and all tests will fail.
/*
/*
/**
/**
// ignore since the thread was never started
//" + randomNodeHttpAddress())
// Ensures that the exporter is actually on
// Checks that the monitoring index templates have been installed
// Waits for monitoring indices to be created
// Waits for indices to be ready
// Checks that the HTTP exporter has successfully exported some data
/*
/*
/*
// delete the watcher history to not clutter with entries from other test
// all good here, we are done
// all good here, we are done
//github.com/elastic/elasticsearch/issues/32299")
// get master publish address
// put watch
// trigger
// input
// condition
// actions
// check watch count
// check watch history
/*
/** Runs rest tests against external cluster */
// all good here, we are done
// all good here, we are done
/*
// delete the watcher history to not clutter with entries from other test
// create one document in this index, so we can test in the YAML tests, that the index cannot be accessed
// all good here, we are done
// all good here, we are done
/*
// delete the watcher history to not clutter with entries from other test
// create one document in this index, so we can test in the YAML tests, that the index cannot be accessed
// all good here, we are done
// all good here, we are done
// check history, after watch has fired
// check history, after watch has fired
// check history, after watch has fired
/*
//TODO we still have passwords in Strings in headers. Maybe we can look into using a CharSequence?
/*
// Watcher constants:
// ML constants:
// Transform constants:
/*
/**
// While it's possible to use a Hamcrest matcher for this, the failure is much less legible.
// ".write" rather than simply "write" to avoid the danger of clashing
// with the read alias of a job whose name begins with "write-"
/*
// follow referrals defaults to false here which differs from the default value of the setting
// this is needed to prevent test logs being filled by errors as the default configuration of
// the tests run against a vagrant samba4 instance configured as a domain controller with the
// ports mapped into the ephemeral port range and there is the possibility of incorrect results
// as we cannot control the URL of the referral which may contain a non-resolvable DNS name as
// this name would be served by the samba4 instance
//localhost:" + getFromProperty("636"));
//localhost:" + getFromProperty("3269"));
// We use certificates in PEM format and `ssl.certificate_authorities` instead of ssl.trustore
// so that these tests can also run in a FIPS JVM where JKS keystores can't be used.
/*
// fake realms so ssl will get loaded
/*
/**
// don't remove the security index template
// expected
/**
/**
// if mapGroupsAsRoles is turned on we use empty role mapping
/*
//test a user with no assigned groups, other than the default groups
//Default Users group
//test a user of one groups
//Default Users group
/*
/**
/*
//Login with the UserPrincipalName
//login with sAMAccountName
//Login with the UserPrincipalName
//github.com/elastic/elasticsearch/issues/29840")
// don't use this in production
// used here to catch bugs that might get masked by an automatic retry
/*
/*
//auth
//lookup
/*
/**
/*
/**
/*
/**
// Pick a secondary realm that has the inverse value for 'loginWithCommonName' compare with the primary realm
// It's easier to test 2 realms when using file based role mapping, and for the purposes of
// this test, there's no need to test native mappings.
/**
/*
//falling back on the 'memberOf' attribute
//seeAlso only has Avengers
/*
/** Runs rest tests against external cluster */
/*
/** Runs rest tests against external cluster */
/*
/** Runs rest tests against external cluster */
